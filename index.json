[{"categories":null,"contents":"This document lists the commands and steps, without a lot of detail, that should be taken to migrate a DG collection. Use this document as a template for recording actual collection migration, where additional details may be necessary.\nCheck our Migration Google Sheet to verify that there is NO worksheet for the target collection. If one exists you should rename it for safe-keeping to get it out of the way.\nMap the smb://storage/mediadb/DGingest/Migration-to-Alma/outputs and smb://storage/mediadb/DGingest/Migration-to-Alma/exports to the workstation as /Volumes/outputs and /Volumes/exports, respectively.\nOn your workstation, cd into the migrate-MODS-to-dcterms project directory and verify that the main branch and its venv are active. Your prompt should look something like this: (.venv) ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/migrate-MODS-to-dcterms ‹main›. Use the source .venv/bin/activate command if needed.\ncd ~/GitHub/migrate-MODS-to-dcterms source .venv/bin/activate Run main.py. Assuming a target collection ID of __target-collection__, it should look like this: time python3 main.py --collection_name __target-collection__ Run to-google-sheet.py. Assuming a target collection ID of __target-collection__, it should look like this: time python3 to-google-sheet.py --collection_name __target-collection__ The collection\u0026rsquo;s stakeholder(s) should be engaged to clean-up the metadata found in the new __target-collection__ worksheet in our Migration Google Sheet.\nRun manage-collections.py. Assuming a target collection ID of __target-collection__, it should look like this:\ntime python3 manage-collections.py --collection_name __target-collection__ Note that the manage-collections.py script does LOTS of things for you. It will take care of rearranging most \u0026ldquo;compound\u0026rdquo; object data to achieve our intended Alma/Primo structures. Don\u0026rsquo;t skip this step!\nThe scripts may fail to change compound parent and child objects\u0026rsquo; collection_id as they should, so you should intervene and copy the pending-review ID (81313013130004641) into every row in the spreadsheet replacing ALL collection_id values. This will import the objects into the suppressed pending-review collection so they can be reviewed before being moved to the proper sub-collection.\nNote: The above was inserted as step 8 on 2024-Aug-22.\nNavigate to ~/GitHub/worksheet-file-finder, set your VENV as needed, and run the streamlit_app.py script/application to check the file_name_1 column (typically column AW) values against network storage, typically /Volumes/exports/__target-collection__/OBJ. This step is now a Streamlit Python app so it should be largely self-explanatory. See the app\u0026rsquo;s README.md file for additional details if needed.\nThe streamlit_app.py will also check that the column headings in your worksheet are correct!\nNote: The above was inserted as step 9 on 2024-Aug-22 and dramatically modified on 2024-Oct-15.\nNote that there are options in worksheet-file-finder to automatically generate thumbnail images (.clientThumb files in the case of Alma migration) AND copy both the found OBJ and clientThumb to a new /Volumes/outputs/OBJs subdirectory. Note: The above was inserted as step 10 on 2024-Dec-18.\nRun expand-csv.py. Assuming a target collection ID of __target-collection__, it should look like this: time python3 expand-csv.py --collection_name __target-collection__ **Attention! The expand-csv.py script now accepts optional parameters --first_row (or -f) and --last_row (or -l) that can be used to limit the rows of the __target-collection__ Google Sheet that the values.csv file contains.\ntime python3 expand-csv.py --collection_name __target-collection__ --first_row 50 --last_row 500 If the --first_row parameter is omitted it defaults to 2, and if the last is omitted it defaults to 5000. The --last_row limit is automatically trimmed to the last row of data in the sheet so specifying a number larger than the row count effectively includes all rows including and after --first_row.\nExamine the /Volumes/outputs/__target-collection__ directory to confirm that a new values.csv file has been created.\nIn Alma, invoke the Digital Uploader via the Resources | Digital Uploader menu selection.\nIn the Digital Uploader choose Add New Ingest from the menu tab in the upper center of the window.\nGive the ingest a descriptive and unique name Ingest Details | Name and note the all-important ID value displayed below it.\nSelect Add Files then navigate to the /Volumes/outputs/__target-collection__ network directory and select the values.csv file there.\nClick on Upload All to send the values.csv file off for later processing and click OK.\nThe Digital Uploader page should show the aforementioned ID with a status of Upload Complete.\nReturn to the terminal prompt where we will now enter some aws S3... commands following guidance provided in AWS-S3-Storage-Info.md.\nList the contents of our upload directory like so:\naws s3 ls s3://na-st01.ext.exlibrisgroup.com/01GCL_INST/upload/ --recursive --human-readable --summarize Verify that there\u0026rsquo;s a short list of files in ../upload/ including our values.csv file. Copy the ID portion of the values.csv (two subdirectories after upload) path for use in the next step.\nPaste the copied path into the following aws S3 command AND be sure to change the __target-collection__ to our intended target.\naws s3 cp /Volumes/exports/OBJs/ s3://na-st01.ext.exlibrisgroup.com/01GCL_INST/upload/__PASTE__/ --recursive This should copy the /Volumes/outputs/OBJs subdirectory contents of our collection into AWS for ingest.\nList the contents of our upload directory to verify, like so: aws s3 ls s3://na-st01.ext.exlibrisgroup.com/01GCL_INST/upload/ --recursive --human-readable --summarize Return to the Digital Uploader and select the ingest (should be at the top of the list) and click Submit Selected.\nWait for the Run MD Import button to be enabled, then click it.\nAfter a short time you should see a pop-up that says: Import job xxxxxxxxxxxx04641 submitted successfully.\nNavigate in the menus to Resources | Monitor and View Imports to check on progress.\nBe patient while the ingest takes place and you\u0026rsquo;re almost done!\nOnce the import/ingest is complete, report all of the new MMS_ID values and copy/paste them back into the corresponding mms_id cells of the __target-collection__ worksheet.\nMove the completed __target-collection__ worksheet and its ..._READY-FOR-EXPANSION companion to the Migration-Arcive Google Sheet. See the Migration-to-Alma-D Google Sheet README tab for instruction.\nNote: The two sections above were appended as steps 28 and 29 (now 29 and 30) on 2024-Aug-22.\nUse Resources and Manage Collections to pull up the Pending Review collection and select up to 50 records at a time, then click the Move Selected option, search for and select the __target-collection__. Moving all of the Pending Review content in this manner may take many iterations. Note: Section 30 (now 31) above was appended on 2024-Oct-15.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/151-alma-collection-migration-template/","tags":["Alma","migration"],"title":"Collection Migration Template"},{"categories":null,"contents":"Follow the guidance provided in https://knowledge.exlibrisgroup.com/Primo/Community_Knowledge/How_to_%E2%80%93_Force_records_from_external_data_sources_to_be_updated_or_deleted_in_Primo_VE for building an XML file that looks like this:\n\u0026lt;ListRecords\u0026gt; \u0026lt;record\u0026gt; \u0026lt;header status=\u0026#34;deleted\u0026#34;\u0026gt; \u0026lt;identifier\u0026gt;oai:repositoryx.grinnell.edu:grinnell_16184\u0026lt;/identifier\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;/record\u0026gt; \u0026lt;record\u0026gt; \u0026lt;header status=\u0026#34;deleted\u0026#34;\u0026gt; \u0026lt;identifier\u0026gt;oai:repositoryx.grinnell.edu:grinnell_16185\u0026lt;/identifier\u0026gt; \u0026lt;/header\u0026gt; \u0026lt;/record\u0026gt; \u0026lt;/ListRecords\u0026gt; The above .xml file will remove TWO Primo VE records created from import of OAI exported from Digital.Grinnell. It will do so for grinnell:16184 and grinnell:16185, a pair of objects with similar titles that got de-duped into https://grinnell.primo.exlibrisgroup.com/permalink/01GCL_INST/1prvshj/alma991011546867904641.\nFinding the DIP/OAI Identifiers The OAI identifiers for these objects were determined by appending \u0026amp;showPnx=true to the end of the object\u0026rsquo;s Primo permalink, so https://grinnell.primo.exlibrisgroup.com/permalink/01GCL_INST/1prvshj/alma991011546867904641\u0026amp;showPnx=true which yields a wealth of data including these two lines:\n\u0026#34;ilsApiId\u0026#34; : \u0026#34;oai:repositoryx.grinnell.edu:grinnell_16185\u0026#34;, \u0026#34;ilsApiId\u0026#34; : \u0026#34;oai:repositoryx.grinnell.edu:grinnell_16184\u0026#34;, Note that the aforementioned XML syntax is much easier to create in-bulk, and it\u0026rsquo;s perfectly acceptable:\n\u0026lt;ListRecords\u0026gt; \u0026lt;record\u0026gt;\u0026lt;header status=\u0026#39;deleted\u0026#39;\u0026gt;\u0026lt;identifier\u0026gt;oai:repositoryx.grinnell.edu:grinnell_16184\u0026lt;/identifier\u0026gt;\u0026lt;/header\u0026gt;\u0026lt;/record\u0026gt; \u0026lt;record\u0026gt;\u0026lt;header status=\u0026#39;deleted\u0026#39;\u0026gt;\u0026lt;identifier\u0026gt;oai:repositoryx.grinnell.edu:grinnell_16185\u0026lt;/identifier\u0026gt;\u0026lt;/header\u0026gt;\u0026lt;/record\u0026gt; \u0026lt;/ListRecords\u0026gt; Final Outcome After running the file shown above, I believe the old OAI records did disappear from Primo after a few minutes time. During the \u0026ldquo;purge\u0026rdquo; Primo did display a placeholder \u0026ldquo;inkingParameter1\u0026rdquo; link to https://digital.grinnell.edu/islandora/object/ for the recently removed OAI record. This too is expected to disappear after about 30 to 60 minutes time.\nBefore and After Permalink here is https://grinnell.primo.exlibrisgroup.com/permalink/01GCL_INST/1prvshj/alma991011547306404641. This is an object in the Ancient Coins collection and the corresponding DIPs were removed on 8-Nov-2024. The XML file used in this operation was subsequently renamed to remove-DIP-records-Nov-8.xml to make way for the next DIP removal job on November 11.\nFigure 1 \u0026middot; Before\u0026hellip; Figure 2 \u0026middot; After processing and about an hour wait\u0026hellip; Yay! The MMS Id and permalink remain unchanged too!\nA similar DIP removal job was run on November 11 using the XML file named remove-DIP-records-Nov-11.xml. That file was intended to remove ALL remaining DIP records from collections that migrated to Alma before November 11. That file contained 4301 records to be removed.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/150-discovery-import-profile-dip-record-removal/","tags":["DIP","OAI","Alma","Primo VE"],"title":"Discovery Import Profile (DIP) Record Removal"},{"categories":null,"contents":"Dealing with Alma/Primo Dedup and FRBR Together, Alma and Primo (mostly Primo) apply \u0026ldquo;dedup\u0026rdquo; and \u0026ldquo;FRBR\u0026rdquo; (Functional Requirements for Bibliographic Records) rules that unexpectedly \u0026ldquo;group\u0026rdquo; similar titles together. Since the objects migrating from Islandora to Alma are all intended to be single and un-grouped (unless they are parts of a compound object where we expect and an intentional \u0026ldquo;grouping\u0026rdquo; of records), we need to take action to deal with these rules. Unfortunately, we cannot just turn the rules off or universally modify them since those rules also apply to ALL bib records, not just digital items.\nUnderstanding the Rules I have yet to see these rules in-the-flesh, but the observed effects suggest that\u0026hellip;\nAny two or more bib records that have matching titles and/or alternative titles will be grouped together.\nAny two or more bib records that have a matching dc:identifier value will be grouped together. Keep in mind that dc:identifier, and similar fields, are multi-valued so this means that if any one value is identical to an identifier value from another bib, the two will be grouped together.\nI deem this to be a very \u0026ldquo;good\u0026rdquo; rule. It should help to catch true duplicate bibs since dc:identifier, and other ID fields, should be universally unique.\nLike dc:identifier, alternative title is a multi-valued field, so ANY alt title value from a record that has an identical title or alt title in another will be will be grouped together. This caused major headaches for migration since many collections used the alt title field to carry subject-like information.\nAlma and Primo appear to tokenize the title and alternative title fields (and other fields too) such that embedded punctuation can cause two similar records to be grouped together. For example\u0026hellip; The record https://grinnell.primo.exlibrisgroup.com/permalink/01GCL_INST/1g018f9/alma991011506419704641 was originally \u0026ldquo;grouped\u0026rdquo; with https://grinnell.primo.exlibrisgroup.com/permalink/01GCL_INST/1g018f9/alma991011506420404641 even though the two records have very different content and no identical title, alt title, or identifier fields values. The alt titles of these bibs are \u0026ldquo;First year tutorial, Fall 2001\u0026rdquo; and \u0026ldquo;Tutorial, Fall 2001\u0026rdquo;, respectively. The presence of the embedded comma in each apparently causes the \u0026ldquo;Fall 2001\u0026rdquo; portion of each alt title to engage dedup and group these very different bib together.\nResolving Dedup and FRBR Problems The following email and Slack excerpts chronicle steps taken to deal specifically with the situation documented in bullet 4 above.\nFrom Slack\u0026hellip; Following up on our conversation yesterday (https://gcl-org.slack.com/archives/C04EMUD2M7Z/p1723563107993219)…\nThere’s no change in the Syllabi and Curricular Materials collection this morning, we still have 52 items but I was hoping to see 53 (the correct total) after making a change that would de-dup on pair of items that got incorrectly tagged as duplicates because of an errant (and identical) pair of alt title elements.\nThe MMS_IDs are 991 01150 64197 04641 and 991 01150 64204 04641, objects grinnell:3452 and grinnell:3459. They both happen to have HAD alt title values that included Tutorial, Fall 2001. Yesterday I changed one of the object’s alt title values in the hope that it would un-dedup, but clearly it did not.\nSo, now the question is what to do about this? I hate to blow these away and re-import them. Not a terrible solution this time, but I fear this is going to keep happening so I’m holding out hope there is a better way.\nJulia identified a potentially better way of dealing with this at https://knowledge.exlibrisgroup.com/Primo/Product_Documentation/020Primo_VE/Primo_VE_(English)/090Dedup_and_FRBR_for_Primo_VE/010Understanding_the_Dedup_and_FRBR_Processes_(Primo_VE).\nEx Libris support suggested this\u0026hellip; Yes, you can put all the MMS IDs for your DG content in the set, including ones that have not been deduped. The job will mark all records in the set ineligible for dedup/frbr. You can run the job at any time, including after your migration is done. You could do it now and again after the migration too. There is no harm in running it now on the existing records, and again after the full migration.\nFor the job \u0026ldquo;\u0026ldquo;Prevent FRBR and/or Dedup in Discovery\u0026rdquo;, the documentation notes that it will prevent dedup/frbr on records that have not been grouped. However, in my experience, it also breaks up the existing groups.\nMy response to the related help ticket\u0026hellip; Since I was reminded of this, I took steps to give the process a try. Not sure about the outcome yet so I wonder if you could help us determine if it worked or not? Moments ago, I ran job 6980681020004641 and gave it a set of 151 MMS Ids to process. Among those IDs was 991011506419704641, one of two bib records that previously grouped together as matching. This was presumably because an alt title in each record had embedded punctuation, a comma, AND a small portion of their alt titles were an identical match. It seems the presence of punctuation in the title or alt title has an unexpected impact on the dedup results? The other matched record is 991011506420404641 and you can see them both grouped together at https://grinnell.primo.exlibrisgroup.com/discovery/search?query=any,contains,991011506420404641\u0026amp;tab=Everything\u0026amp;search_scope=MyInst_and_CI\u0026amp;sortby=rank\u0026amp;vid=01GCL_INST:GCL\u0026amp;lang=en\u0026amp;offset=0.\nIt\u0026rsquo;s been 20 minutes since I ran the job and the two records in Primo still show up as versions of the same thing, which they are NOT. Can you tell me if the process worked, and if it did, how long do I have to wait to see the result in Primo? Or do I need to list BOTH MMS IDs in my set/job for the group to be break apart? Thanks. -Mark M.\nOutcome Eventually, after about an hour the targeted group of bibs was \u0026ldquo;disolved\u0026rdquo; yielding two distinct records as was intended, rather than a group with two \u0026ldquo;versions\u0026rdquo;.\nExperience with PHPP Community Contribution objects having identical titles of \u0026ldquo;411 First Avenue\u0026rdquo; suggests that DIP records \u0026ndash;see post Discovery Import Profile (DIP) Record Removal\u0026ndash; MUST be removed BEFORE running the dedup suppression job, otherwise the dedup may not be successfully suppressed.\nSuppressing Dedup and FRBR Rules The workflow needed to suppress grouping of specific records is documented at https://knowledge.exlibrisgroup.com/Primo/Product_Documentation/020Primo_VE/Primo_VE_(English)/090Dedup_and_FRBR_for_Primo_VE/Suppressing_Groups_of_Records_from_Dedup%2F%2FFRBR_for_Primo_VE. Note that while this is a \u0026ldquo;post-processing\u0026rdquo; step, it\u0026rsquo;s useful only AFTER records have been imported, it does appear to \u0026ldquo;dissolve\u0026rdquo; existing unintended grouping as was suggested in the email excerpt which appears in Ex Libris support suggested this... above.\nIn a nutshell, the workflow involves the following steps:\nCreate a single column .csv file with a column heading of MMS Id.\nExport all of the existing digital item MMS IDs from our Digital Grinnell collection and all subordinate collections in Alma.\nPopulate the .csv file with this list of exported MMS IDs, one ID per line.\nNote that steps 1-3 can easily be compressed into a single operation using Analytics and the List-ALL-Active-DCAP01-Bibs- MMS-Only query stored in /Shared Folders/Grinnell College Libraries 01GCL_INST/DG Migration.\nCreate a new itemized record \u0026ldquo;set\u0026rdquo; using the .csv file as input. Note that the set named All DG Imported Active Bibs (ID = 6993436050004641) may be updated and used as necessary.\nSelect and run the Prevent FRBR and/or Dedup in Discovery applying the itemized set mentioned above.\nSet both Prevent FRBR in Discovery? and Prevent Dedup in Discovery? parameters to YES.\nSubmit the job and wait for results. Note that once the job completes it may take hours before the effects are fully visible in Primo!\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/149-dealing-with-alma-dedup-and-frbr/","tags":["dedup","FRBR","Alma","Primo"],"title":"Dealing with Alma Dedup and FRBR"},{"categories":null,"contents":"When a \u0026ldquo;new\u0026rdquo; Digital.Grinnell or CollectionBuilder object is cataloged for the first time, we need to give it a universally unique identifier, probably as dc:identifier or in CollectionBuilder terms, an objectid. One way to do that is to introduce a Google Sheets App Script in our metadata worksheet. The following is based on a modified form of the technique that\u0026rsquo;s documented in How To Create Static Timestamps in Google Sheets.\nThe aforementioned YouTube video shows us how to add a Google Sheets App Script to a Google Sheet. In our case that App Script, named dg_timestamp, looks like this:\nfunction onEdit(e){ var range = e.range; if (range.getColumn() == 2) { var timestampCell = range.offset(0, -1); var existingID = timestampCell.getValue(); if (!existingID | existingID.length === 0) { var t = new Date(); timestampCell.setValue(\u0026#34;dg_\u0026#34; + (t.getTime() - t.getMilliseconds()) / 1000); } } } This script assumes that our \u0026ldquo;timestampCell\u0026rdquo; will be in column A, the FIRST column, and performing an edit in column B, the SECOND column, will trigger the \u0026ldquo;timestampCell\u0026rdquo; to be populated with a static/permanent identifier as described above.\nAdding and Managing the dg_timestamp Script As the video says, you can add or manage an App Script by selecting the Extensions menu in your Google Sheet, and then select App Script from the drop-down menu.\nThat\u0026rsquo;s all folks\u0026hellip; until next time (or should I say \u0026ldquo;timestamp\u0026rdquo;). \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/148-the-dg_timestamp-google-sheet-apps-script/","tags":["Digital.Grinnell","UNIX epoch","identifier","timestamp"],"title":"`dg_timestamp` Google Sheet Apps Script"},{"categories":null,"contents":"This is an exact copy of https://static.grinnell.edu/dlad-blog/posts/146-updating-rootstalk-august-2023/ with the addition of an important note pertaining to the section titled `Pushing to Production`. The production branch update of Rootstalk in February 2024 resulted in a Git error stating Updates rejected because tip of current branch behind remote counterpart. Since my local production branch was correct after updates were applied, I used a git push --force-with-lease command from https://stackoverflow.com/posts/70300076/timeline to get things back in-sync.\nIn this document I will attempt to capture the ordered steps required to publish a new \u0026ldquo;issue\u0026rdquo; of Rootstalk in it\u0026rsquo;s newest environment.\nThe development of this blog post, and modifications to the https://github.com/Digital-Grinnell/rootstalk-issue-workflow repo, took place on my personal Mac Mini while simultaneous work on Rootstalk was completed in the ~/GitHub/npm-rootstalk AND ~/GitHub/rootstalk-issue-workflow* local repositories on my Grinnell College MacBook Pro. The two machines shared a single keyboard and mouse, plus copy/paste capabilities, via Universal Control.\n*Note that work in ~/GitHub/roostalk-issue-workflow must be completed on my GC-issued MacBook Pro as access to InDesign is only available on that college-owned machine.\nFollowing (and Modifying) the Documented Workflow Following the Rootstalk New Issue Workflow is the KEY to making this happen as smoothly as possible. Since Rootstalk has a new NPM wrapper it will probably be necessary to modify that document to reflect changes required by NPM.\nNote that the effort documented here was interrupted by many things, including a week of COVID-19, and subsequently pushed from August into September 2023.\nWorking Locally With the npm-rootstalk Repo Rather than repeating a lot of documenation here, I\u0026rsquo;ll simply mention that the README.md file in the npm-rootstalk repo has been updated to reflect the latest/greatest development and production workflows. A great deal of that document has been deemed obsolete but it remains visible to authorized project developers nonetheless.\nUpdate - Automated Proofreading Rootstalk really needs an updated run of htmlproofer so I started re-visiting that tool by reading https://static.grinnell.edu/dlad-blog/posts/133-automated-proofreading-with-htmlproofer/#first-an-updated-install. I subsequently found a ~/GitHub/html-proofer-3.19.3 diretory on my Grinnell College MacBook, but I found that I could not run the htmlproofer command there as I witnessed the following\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub ╰─$ htmlproofer -v rbenv: htmlproofer: command not found The `htmlproofer\u0026#39; command exists in these Ruby versions: 3.0.2 So, following the workflow documented in First, An Updated Install went like this\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub ╰─$ gem install html-proofer Fetching zeitwerk-2.6.11.gem Fetching yell-2.2.2.gem Fetching ethon-0.16.0.gem Fetching typhoeus-1.4.0.gem Fetching rainbow-3.1.1.gem Fetching ttfunk-1.7.0.gem Fetching ruby-rc4-0.1.5.gem Fetching hashery-2.1.2.gem Fetching Ascii85-1.1.0.gem Fetching afm-0.2.2.gem Fetching pdf-reader-2.11.0.gem Fetching nokogiri-1.15.4-arm64-darwin.gem Fetching timers-4.3.5.gem Fetching io-event-1.3.2.gem Fetching fiber-annotation-0.2.0.gem Fetching fiber-local-1.0.0.gem Fetching html-proofer-5.0.8.gem Fetching console-1.23.2.gem Fetching async-2.6.4.gem Successfully installed zeitwerk-2.6.11 Successfully installed yell-2.2.2 Successfully installed ethon-0.16.0 Successfully installed typhoeus-1.4.0 Successfully installed rainbow-3.1.1 Successfully installed ttfunk-1.7.0 Successfully installed ruby-rc4-0.1.5 Successfully installed hashery-2.1.2 Successfully installed Ascii85-1.1.0 Successfully installed afm-0.2.2 Successfully installed pdf-reader-2.11.0 Successfully installed nokogiri-1.15.4-arm64-darwin Successfully installed timers-4.3.5 Building native extensions. This could take a while... Successfully installed io-event-1.3.2 Successfully installed fiber-annotation-0.2.0 Successfully installed fiber-local-1.0.0 Successfully installed console-1.23.2 Successfully installed async-2.6.4 Successfully installed html-proofer-5.0.8 Parsing documentation for zeitwerk-2.6.11 Installing ri documentation for zeitwerk-2.6.11 Parsing documentation for yell-2.2.2 Installing ri documentation for yell-2.2.2 Parsing documentation for ethon-0.16.0 Installing ri documentation for ethon-0.16.0 Parsing documentation for typhoeus-1.4.0 Installing ri documentation for typhoeus-1.4.0 Parsing documentation for rainbow-3.1.1 Installing ri documentation for rainbow-3.1.1 Parsing documentation for ttfunk-1.7.0 Installing ri documentation for ttfunk-1.7.0 Parsing documentation for ruby-rc4-0.1.5 Installing ri documentation for ruby-rc4-0.1.5 Parsing documentation for hashery-2.1.2 Installing ri documentation for hashery-2.1.2 Parsing documentation for Ascii85-1.1.0 Installing ri documentation for Ascii85-1.1.0 Parsing documentation for afm-0.2.2 Installing ri documentation for afm-0.2.2 Parsing documentation for pdf-reader-2.11.0 Installing ri documentation for pdf-reader-2.11.0 Parsing documentation for nokogiri-1.15.4-arm64-darwin Installing ri documentation for nokogiri-1.15.4-arm64-darwin Parsing documentation for timers-4.3.5 Installing ri documentation for timers-4.3.5 Parsing documentation for io-event-1.3.2 Installing ri documentation for io-event-1.3.2 Parsing documentation for fiber-annotation-0.2.0 Installing ri documentation for fiber-annotation-0.2.0 Parsing documentation for fiber-local-1.0.0 Installing ri documentation for fiber-local-1.0.0 Parsing documentation for console-1.23.2 Installing ri documentation for console-1.23.2 Parsing documentation for async-2.6.4 Installing ri documentation for async-2.6.4 Parsing documentation for html-proofer-5.0.8 Installing ri documentation for html-proofer-5.0.8 Done installing documentation for zeitwerk, yell, ethon, typhoeus, rainbow, ttfunk, ruby-rc4, hashery, Ascii85, afm, pdf-reader, nokogiri, timers, io-event, fiber-annotation, fiber-local, console, async, html-proofer after 3 seconds 19 gems installed ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub And following that\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub ╰─$ htmlproofer -v 5.0.8 \u0026hellip;which looks good! So from there the process was\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub ╰─$ cd npm-rootstalk ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main› ╰─$ ll total 184 -rw-r--r-- 1 mcfatem 1278142703 1.1K Aug 18 09:19 LICENSE -rw-r--r-- 1 mcfatem 1278142703 16K Aug 30 11:33 README.md drwxr-xr-x 3 mcfatem 1278142703 96B Aug 18 09:19 archetypes -rw-r--r-- 1 mcfatem 1278142703 3.0K Aug 18 09:19 config.yml drwxr-xr-x 12 mcfatem 1278142703 384B Aug 18 09:19 content drwxr-xr-x 7 mcfatem 1278142703 224B Aug 18 09:19 layouts -rw-r--r-- 1 mcfatem 1278142703 64K Aug 18 09:19 package-lock.json -rw-r--r-- 1 mcfatem 1278142703 937B Aug 18 09:19 package.json drwxr-xr-x 29 mcfatem 1278142703 928B Aug 23 12:00 public drwxr-xr-x 3 mcfatem 1278142703 96B Aug 18 09:20 resources drwxr-xr-x 9 mcfatem 1278142703 288B Aug 18 09:20 static drwxr-xr-x 3 mcfatem 1278142703 96B Aug 18 09:19 themes ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main› ╰─$ hugo Start building sites … hugo v0.112.7+extended darwin/arm64 BuildDate=unknown | EN -------------------+------ Pages | 431 Paginator pages | 0 Non-page files | 6 Static files | 632 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 489 ms ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main› ╰─$ ll total 184 -rw-r--r-- 1 mcfatem 1278142703 1.1K Aug 18 09:19 LICENSE -rw-r--r-- 1 mcfatem 1278142703 16K Aug 30 11:33 README.md drwxr-xr-x 3 mcfatem 1278142703 96B Aug 18 09:19 archetypes -rw-r--r-- 1 mcfatem 1278142703 3.0K Aug 18 09:19 config.yml drwxr-xr-x 12 mcfatem 1278142703 384B Aug 18 09:19 content drwxr-xr-x 7 mcfatem 1278142703 224B Aug 18 09:19 layouts -rw-r--r-- 1 mcfatem 1278142703 64K Aug 18 09:19 package-lock.json -rw-r--r-- 1 mcfatem 1278142703 937B Aug 18 09:19 package.json drwxr-xr-x 29 mcfatem 1278142703 928B Sep 13 11:28 public drwxr-xr-x 3 mcfatem 1278142703 96B Aug 18 09:20 resources drwxr-xr-x 9 mcfatem 1278142703 288B Aug 18 09:20 static drwxr-xr-x 3 mcfatem 1278142703 96B Aug 18 09:19 themes ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main› ╰─$ htmlproofer ./public Running 3 checks (Images, Links, Scripts) in [\u0026#34;./public\u0026#34;] on *.html files ... Checking 3177 external links ... * At ./public/volume-ix-issue-1/tech-on-the-prarie/index.html:586: internally linking to #ref14; the file exists, but the hash \u0026#39;ref14\u0026#39; does not * At ./public/volume-ix-issue-1/tech-on-the-prarie/index.html:591: internally linking to #ref15; the file exists, but the hash \u0026#39;ref15\u0026#39; does not HTML-Proofer found 3741 failures! The output from the htmlproofer ./public command was too big to use effectively so I turned back to Using htmlproofer with Docker where I found that version 3.19.2 is still the latest. So I moved to the Capture More Info: ./html-proofer.sh section of the post 133 and found that the html-proofer.sh script was NOT part of the new npm-rootstalk repo. Time to get get that script back from a previous version of the repo\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main› ╰─$ cp -f ~/GitHub/.out-of-the-way/rootstalk/html-proofer.sh . And now we have our html-proofer.sh script where it belongs again. It looks like this:\n#!/bin/bash ## ## Add options to the end of the COMMAND string to change html-proofer behavior. For a list of available options run: ## docker run --rm -it -v $(pwd):/src klakegg/html-proofer:3.19.2 --help ## Common options might include: ## --allow-hash-href ## --check-html ## --empty-alt-ignore ## hugo # generate a new site cd public # move into the new site\u0026#39;s files COMMAND=\u0026#34;docker run --rm -it -v $(pwd):/src klakegg/html-proofer:3.19.2 --check-html\u0026#34; date \u0026gt; /tmp/rootstalk-html-proofer.tmp echo $COMMAND \u0026gt;\u0026gt; /tmp/rootstalk-html-proofer.tmp time $COMMAND | sed -e \u0026#39;s/\\x1b\\[[0-9;]*m//g\u0026#39; \u0026gt;\u0026gt; /tmp/rootstalk-html-proofer.tmp tail -1 /tmp/rootstalk-html-proofer.tmp ## ## I\u0026#39;m unable to effectively control many of the bogus issues reported by html-proofer, things like: ## * internally linking to .., which does not exist (line 682) ## \u0026lt;a href=\u0026#34;..\u0026#34;\u0026gt;Back\u0026lt;/a\u0026gt; ## So, let\u0026#39;s try to implement some `grep` and `sed` commands that will automatically count and remove ## them from the output. ## BOGUS=`grep -c \u0026#39;internally linking to ..,\u0026#39; /tmp/rootstalk-html-proofer.tmp` echo \u0026#34;Removing ${BOGUS} false-negative errors from the output...\u0026#34; sed \u0026#39;/internally linking to \\.\\.,/,+1d\u0026#39; /tmp/rootstalk-html-proofer.tmp \u0026gt; ${HOME}/Downloads/rootstalk-html-proofer.out echo \u0026#34;${BOGUS} false-negative errors were removed from this output.\u0026#34; \u0026gt;\u0026gt; ${HOME}/Downloads/rootstalk-html-proofer.out ## ## Successfully running scripted Azure CLI in a Docker container proved to be virtually ## impossible, perhaps because the CLI login and commands are so \u0026#34;interative\u0026#34;? ## So, the `az` commands that follow will require the Azure CLI be installed on the ## host. See https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-macos?source=recommendations#install-with-homebrew ## for doing that with Homebrew. Also, you should run `az login` before attempting the ## `az storage blob upload...` command shown here. ## az storage blob upload \\ --account-name rootstalk \\ --container-name documentation \\ --name rootstalk-html-proofer.out \\ --file ${HOME}/Downloads/rootstalk-html-proofer.out \\ --overwrite \\ --auth-mode key ## ## The output should be available now for download at https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out ## echo \u0026#34;The output should be available now for download at https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out.\u0026#34; ## It looks like it should work as-is, so let\u0026rsquo;s give it a go\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main●› ╰─$ ./html-proofer.sh Start building sites … hugo v0.112.7+extended darwin/arm64 BuildDate=unknown | EN -------------------+------ Pages | 431 Paginator pages | 0 Non-page files | 6 Static files | 632 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 456 ms docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?. So, Docker isn\u0026rsquo;t running on my Mac. Going to launch Docker Desktop and try again\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main●› ╰─$ ./html-proofer.sh Start building sites … hugo v0.112.7+extended darwin/arm64 BuildDate=unknown | EN -------------------+------ Pages | 431 Paginator pages | 0 Non-page files | 6 Static files | 632 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 492 ms WARNING: The requested image\u0026#39;s platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested real\t2m29.158s user\t0m0.115s sys\t0m0.122s HTML-Proofer found 848 failures! Removing 2 false-negative errors from the output... Argument \u0026#39;--overwrite\u0026#39; is in preview and under development. Reference and support levels: https://aka.ms/CLI_refstatus There are no credentials provided in your command and environment, we will query for account key for your storage account. It is recommended to provide --connection-string, --account-key or --sas-token in your command as credentials. You also can add `--auth-mode login` in your command to use Azure Active Directory (Azure AD) for authorization if your login account is assigned required RBAC roles. For more information about RBAC roles in storage, visit https://docs.microsoft.com/azure/storage/common/storage-auth-aad-rbac-cli. In addition, setting the corresponding environment variables can avoid inputting credentials in your command. Please use --help to get more information about environment variable usage. Skip querying account key due to failure: AADSTS700082: The refresh token has expired due to inactivity. The token was issued on 2022-12-02T17:43:57.4286890Z and was inactive for 90.00:00:00. Trace ID: 0c32ac0c-67e4-41cc-8b81-326161df2c00 Correlation ID: d7fadcb4-51c1-421c-adf8-75f378294d9f Timestamp: 2023-09-13 16:51:51Z Server failed to authenticate the request. Please refer to the information in the www-authenticate header. RequestId:d1addbca-001e-002e-5b62-e67766000000 Time:2023-09-13T16:51:52.1924115Z ErrorCode:NoAuthenticationInformation The output should be available now for download at https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out. So, it looks like html-proofer worked and found 846 (848 minus 2) probable failures. However, the Azure credentials I was using for remote storage have expired and/or changed so the script wasn\u0026rsquo;t able to save the output in Azure. Fortunately, there is a local copy at /tmp/rootstalk-html-proofer.tmp so I\u0026rsquo;m just going to copy that to BLOB storage using Microsoft Azure Storage Explorer.\nDone. The new file is publicly available at https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out.\nPushing to Production I found a git command at https://stackoverflow.com/questions/13897717/push-commits-to-another-branch to help me push the new main back to production to trigger a production update. However\u0026hellip;\n╭─mark@Marks-Mac-Mini ~/GitHub/npm-rootstalk ‹main› ╰─$ git push origin main:production To https://github.com/Digital-Grinnell/npm-rootstalk.git ! [rejected] main -\u0026gt; production (non-fast-forward) error: failed to push some refs to \u0026#39;https://github.com/Digital-Grinnell/npm-rootstalk.git\u0026#39; hint: Updates were rejected because a pushed branch tip is behind its remote hint: counterpart. Check out this branch and integrate the remote changes hint: (e.g. \u0026#39;git pull ...\u0026#39;) before pushing again. hint: See the \u0026#39;Note about fast-forwards\u0026#39; in \u0026#39;git push --help\u0026#39; for details. So, trying to remedy that\u0026hellip;\n╭─mark@Marks-Mac-Mini ~/GitHub/npm-rootstalk ‹main› ╰─$ git checkout production Switched to branch \u0026#39;production\u0026#39; Your branch is up to date with \u0026#39;origin/production\u0026#39;. ╭─mark@Marks-Mac-Mini ~/GitHub/npm-rootstalk ‹production› ╰─$ git pull origin main From https://github.com/Digital-Grinnell/npm-rootstalk * branch main -\u0026gt; FETCH_HEAD Successfully rebased and updated refs/heads/production. Now my SOURCE CONTROL tab in VSCode shows a status of Sync Changes 1 \u0026lt;down\u0026gt; 46 \u0026lt;up\u0026gt;. Since my local production branch now looks just like main, that sounds like the right thing to do at this point so here goes\u0026hellip; I clicked the Sync... button and got an email back indicating that Azure Static Web Apps CI/CD: All jobs that ran were successful.\nOk, so what about the production instance on DigitalOcean?\nWell, rather than checking in at DigitalOcean, I decided to just have a look at https://rootstalk.grinnell.edu and it looks GREAT!\nA quick look at the footer on https://rootstalk.grinnell.edu/about shows Compiled: Sep 13, 2023 at 1:50pm CDT • GitHub Hash: f1dddf64 and that feels right too!\nThat\u0026rsquo;s all folks\u0026hellip; until next time.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/147-updating-rootstalk-february-2024/","tags":["Rootstalk","NPM","proofreading","submissions"],"title":"Updating Rootstalk - February 2024"},{"categories":null,"contents":"In this document I will attempt to capture the ordered steps required to publish a new \u0026ldquo;issue\u0026rdquo; of Rootstalk in it\u0026rsquo;s newest environment.\nThe development of this blog post, and modifications to the https://github.com/Digital-Grinnell/rootstalk-issue-workflow repo, took place on my personal Mac Mini while simultaneous work on Rootstalk was completed in the ~/GitHub/npm-rootstalk AND ~/GitHub/rootstalk-issue-workflow* local repositories on my Grinnell College MacBook Pro. The two machines shared a single keyboard and mouse, plus copy/paste capabilities, via Universal Control.\n*Note that work in ~/GitHub/roostalk-issue-workflow must be completed on my GC-issued MacBook Pro as access to InDesign is only available on that college-owned machine.\nFollowing (and Modifying) the Documented Workflow Following the Rootstalk New Issue Workflow is the KEY to making this happen as smoothly as possible. Since Rootstalk has a new NPM wrapper it will probably be necessary to modify that document to reflect changes required by NPM.\nNote that the effort documented here was interrupted by many things, including a week of COVID-19, and subsequently pushed from August into September 2023.\nWorking Locally With the npm-rootstalk Repo Rather than repeating a lot of documenation here, I\u0026rsquo;ll simply mention that the README.md file in the npm-rootstalk repo has been updated to reflect the latest/greatest development and production workflows. A great deal of that document has been deemed obsolete but it remains visible to authorized project developers nonetheless.\nUpdate - Automated Proofreading Rootstalk really needs an updated run of htmlproofer so I started re-visiting that tool by reading https://static.grinnell.edu/dlad-blog/posts/133-automated-proofreading-with-htmlproofer/#first-an-updated-install. I subsequently found a ~/GitHub/html-proofer-3.19.3 diretory on my Grinnell College MacBook, but I found that I could not run the htmlproofer command there as I witnessed the following\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub ╰─$ htmlproofer -v rbenv: htmlproofer: command not found The `htmlproofer\u0026#39; command exists in these Ruby versions: 3.0.2 So, following the workflow documented in First, An Updated Install went like this\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub ╰─$ gem install html-proofer Fetching zeitwerk-2.6.11.gem Fetching yell-2.2.2.gem Fetching ethon-0.16.0.gem Fetching typhoeus-1.4.0.gem Fetching rainbow-3.1.1.gem Fetching ttfunk-1.7.0.gem Fetching ruby-rc4-0.1.5.gem Fetching hashery-2.1.2.gem Fetching Ascii85-1.1.0.gem Fetching afm-0.2.2.gem Fetching pdf-reader-2.11.0.gem Fetching nokogiri-1.15.4-arm64-darwin.gem Fetching timers-4.3.5.gem Fetching io-event-1.3.2.gem Fetching fiber-annotation-0.2.0.gem Fetching fiber-local-1.0.0.gem Fetching html-proofer-5.0.8.gem Fetching console-1.23.2.gem Fetching async-2.6.4.gem Successfully installed zeitwerk-2.6.11 Successfully installed yell-2.2.2 Successfully installed ethon-0.16.0 Successfully installed typhoeus-1.4.0 Successfully installed rainbow-3.1.1 Successfully installed ttfunk-1.7.0 Successfully installed ruby-rc4-0.1.5 Successfully installed hashery-2.1.2 Successfully installed Ascii85-1.1.0 Successfully installed afm-0.2.2 Successfully installed pdf-reader-2.11.0 Successfully installed nokogiri-1.15.4-arm64-darwin Successfully installed timers-4.3.5 Building native extensions. This could take a while... Successfully installed io-event-1.3.2 Successfully installed fiber-annotation-0.2.0 Successfully installed fiber-local-1.0.0 Successfully installed console-1.23.2 Successfully installed async-2.6.4 Successfully installed html-proofer-5.0.8 Parsing documentation for zeitwerk-2.6.11 Installing ri documentation for zeitwerk-2.6.11 Parsing documentation for yell-2.2.2 Installing ri documentation for yell-2.2.2 Parsing documentation for ethon-0.16.0 Installing ri documentation for ethon-0.16.0 Parsing documentation for typhoeus-1.4.0 Installing ri documentation for typhoeus-1.4.0 Parsing documentation for rainbow-3.1.1 Installing ri documentation for rainbow-3.1.1 Parsing documentation for ttfunk-1.7.0 Installing ri documentation for ttfunk-1.7.0 Parsing documentation for ruby-rc4-0.1.5 Installing ri documentation for ruby-rc4-0.1.5 Parsing documentation for hashery-2.1.2 Installing ri documentation for hashery-2.1.2 Parsing documentation for Ascii85-1.1.0 Installing ri documentation for Ascii85-1.1.0 Parsing documentation for afm-0.2.2 Installing ri documentation for afm-0.2.2 Parsing documentation for pdf-reader-2.11.0 Installing ri documentation for pdf-reader-2.11.0 Parsing documentation for nokogiri-1.15.4-arm64-darwin Installing ri documentation for nokogiri-1.15.4-arm64-darwin Parsing documentation for timers-4.3.5 Installing ri documentation for timers-4.3.5 Parsing documentation for io-event-1.3.2 Installing ri documentation for io-event-1.3.2 Parsing documentation for fiber-annotation-0.2.0 Installing ri documentation for fiber-annotation-0.2.0 Parsing documentation for fiber-local-1.0.0 Installing ri documentation for fiber-local-1.0.0 Parsing documentation for console-1.23.2 Installing ri documentation for console-1.23.2 Parsing documentation for async-2.6.4 Installing ri documentation for async-2.6.4 Parsing documentation for html-proofer-5.0.8 Installing ri documentation for html-proofer-5.0.8 Done installing documentation for zeitwerk, yell, ethon, typhoeus, rainbow, ttfunk, ruby-rc4, hashery, Ascii85, afm, pdf-reader, nokogiri, timers, io-event, fiber-annotation, fiber-local, console, async, html-proofer after 3 seconds 19 gems installed ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub And following that\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub ╰─$ htmlproofer -v 5.0.8 \u0026hellip;which looks good! So from there the process was\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub ╰─$ cd npm-rootstalk ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main› ╰─$ ll total 184 -rw-r--r-- 1 mcfatem 1278142703 1.1K Aug 18 09:19 LICENSE -rw-r--r-- 1 mcfatem 1278142703 16K Aug 30 11:33 README.md drwxr-xr-x 3 mcfatem 1278142703 96B Aug 18 09:19 archetypes -rw-r--r-- 1 mcfatem 1278142703 3.0K Aug 18 09:19 config.yml drwxr-xr-x 12 mcfatem 1278142703 384B Aug 18 09:19 content drwxr-xr-x 7 mcfatem 1278142703 224B Aug 18 09:19 layouts -rw-r--r-- 1 mcfatem 1278142703 64K Aug 18 09:19 package-lock.json -rw-r--r-- 1 mcfatem 1278142703 937B Aug 18 09:19 package.json drwxr-xr-x 29 mcfatem 1278142703 928B Aug 23 12:00 public drwxr-xr-x 3 mcfatem 1278142703 96B Aug 18 09:20 resources drwxr-xr-x 9 mcfatem 1278142703 288B Aug 18 09:20 static drwxr-xr-x 3 mcfatem 1278142703 96B Aug 18 09:19 themes ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main› ╰─$ hugo Start building sites … hugo v0.112.7+extended darwin/arm64 BuildDate=unknown | EN -------------------+------ Pages | 431 Paginator pages | 0 Non-page files | 6 Static files | 632 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 489 ms ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main› ╰─$ ll total 184 -rw-r--r-- 1 mcfatem 1278142703 1.1K Aug 18 09:19 LICENSE -rw-r--r-- 1 mcfatem 1278142703 16K Aug 30 11:33 README.md drwxr-xr-x 3 mcfatem 1278142703 96B Aug 18 09:19 archetypes -rw-r--r-- 1 mcfatem 1278142703 3.0K Aug 18 09:19 config.yml drwxr-xr-x 12 mcfatem 1278142703 384B Aug 18 09:19 content drwxr-xr-x 7 mcfatem 1278142703 224B Aug 18 09:19 layouts -rw-r--r-- 1 mcfatem 1278142703 64K Aug 18 09:19 package-lock.json -rw-r--r-- 1 mcfatem 1278142703 937B Aug 18 09:19 package.json drwxr-xr-x 29 mcfatem 1278142703 928B Sep 13 11:28 public drwxr-xr-x 3 mcfatem 1278142703 96B Aug 18 09:20 resources drwxr-xr-x 9 mcfatem 1278142703 288B Aug 18 09:20 static drwxr-xr-x 3 mcfatem 1278142703 96B Aug 18 09:19 themes ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main› ╰─$ htmlproofer ./public Running 3 checks (Images, Links, Scripts) in [\u0026#34;./public\u0026#34;] on *.html files ... Checking 3177 external links ... * At ./public/volume-ix-issue-1/tech-on-the-prarie/index.html:586: internally linking to #ref14; the file exists, but the hash \u0026#39;ref14\u0026#39; does not * At ./public/volume-ix-issue-1/tech-on-the-prarie/index.html:591: internally linking to #ref15; the file exists, but the hash \u0026#39;ref15\u0026#39; does not HTML-Proofer found 3741 failures! The output from the htmlproofer ./public command was too big to use effectively so I turned back to Using htmlproofer with Docker where I found that version 3.19.2 is still the latest. So I moved to the Capture More Info: ./html-proofer.sh section of the post 133 and found that the html-proofer.sh script was NOT part of the new npm-rootstalk repo. Time to get get that script back from a previous version of the repo\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main› ╰─$ cp -f ~/GitHub/.out-of-the-way/rootstalk/html-proofer.sh . And now we have our html-proofer.sh script where it belongs again. It looks like this:\n#!/bin/bash ## ## Add options to the end of the COMMAND string to change html-proofer behavior. For a list of available options run: ## docker run --rm -it -v $(pwd):/src klakegg/html-proofer:3.19.2 --help ## Common options might include: ## --allow-hash-href ## --check-html ## --empty-alt-ignore ## hugo # generate a new site cd public # move into the new site\u0026#39;s files COMMAND=\u0026#34;docker run --rm -it -v $(pwd):/src klakegg/html-proofer:3.19.2 --check-html\u0026#34; date \u0026gt; /tmp/rootstalk-html-proofer.tmp echo $COMMAND \u0026gt;\u0026gt; /tmp/rootstalk-html-proofer.tmp time $COMMAND | sed -e \u0026#39;s/\\x1b\\[[0-9;]*m//g\u0026#39; \u0026gt;\u0026gt; /tmp/rootstalk-html-proofer.tmp tail -1 /tmp/rootstalk-html-proofer.tmp ## ## I\u0026#39;m unable to effectively control many of the bogus issues reported by html-proofer, things like: ## * internally linking to .., which does not exist (line 682) ## \u0026lt;a href=\u0026#34;..\u0026#34;\u0026gt;Back\u0026lt;/a\u0026gt; ## So, let\u0026#39;s try to implement some `grep` and `sed` commands that will automatically count and remove ## them from the output. ## BOGUS=`grep -c \u0026#39;internally linking to ..,\u0026#39; /tmp/rootstalk-html-proofer.tmp` echo \u0026#34;Removing ${BOGUS} false-negative errors from the output...\u0026#34; sed \u0026#39;/internally linking to \\.\\.,/,+1d\u0026#39; /tmp/rootstalk-html-proofer.tmp \u0026gt; ${HOME}/Downloads/rootstalk-html-proofer.out echo \u0026#34;${BOGUS} false-negative errors were removed from this output.\u0026#34; \u0026gt;\u0026gt; ${HOME}/Downloads/rootstalk-html-proofer.out ## ## Successfully running scripted Azure CLI in a Docker container proved to be virtually ## impossible, perhaps because the CLI login and commands are so \u0026#34;interative\u0026#34;? ## So, the `az` commands that follow will require the Azure CLI be installed on the ## host. See https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-macos?source=recommendations#install-with-homebrew ## for doing that with Homebrew. Also, you should run `az login` before attempting the ## `az storage blob upload...` command shown here. ## az storage blob upload \\ --account-name rootstalk \\ --container-name documentation \\ --name rootstalk-html-proofer.out \\ --file ${HOME}/Downloads/rootstalk-html-proofer.out \\ --overwrite \\ --auth-mode key ## ## The output should be available now for download at https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out ## echo \u0026#34;The output should be available now for download at https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out.\u0026#34; ## It looks like it should work as-is, so let\u0026rsquo;s give it a go\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main●› ╰─$ ./html-proofer.sh Start building sites … hugo v0.112.7+extended darwin/arm64 BuildDate=unknown | EN -------------------+------ Pages | 431 Paginator pages | 0 Non-page files | 6 Static files | 632 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 456 ms docker: Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?. So, Docker isn\u0026rsquo;t running on my Mac. Going to launch Docker Desktop and try again\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/npm-rootstalk ‹main●› ╰─$ ./html-proofer.sh Start building sites … hugo v0.112.7+extended darwin/arm64 BuildDate=unknown | EN -------------------+------ Pages | 431 Paginator pages | 0 Non-page files | 6 Static files | 632 Processed images | 0 Aliases | 0 Sitemaps | 1 Cleaned | 0 Total in 492 ms WARNING: The requested image\u0026#39;s platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested real\t2m29.158s user\t0m0.115s sys\t0m0.122s HTML-Proofer found 848 failures! Removing 2 false-negative errors from the output... Argument \u0026#39;--overwrite\u0026#39; is in preview and under development. Reference and support levels: https://aka.ms/CLI_refstatus There are no credentials provided in your command and environment, we will query for account key for your storage account. It is recommended to provide --connection-string, --account-key or --sas-token in your command as credentials. You also can add `--auth-mode login` in your command to use Azure Active Directory (Azure AD) for authorization if your login account is assigned required RBAC roles. For more information about RBAC roles in storage, visit https://docs.microsoft.com/azure/storage/common/storage-auth-aad-rbac-cli. In addition, setting the corresponding environment variables can avoid inputting credentials in your command. Please use --help to get more information about environment variable usage. Skip querying account key due to failure: AADSTS700082: The refresh token has expired due to inactivity. The token was issued on 2022-12-02T17:43:57.4286890Z and was inactive for 90.00:00:00. Trace ID: 0c32ac0c-67e4-41cc-8b81-326161df2c00 Correlation ID: d7fadcb4-51c1-421c-adf8-75f378294d9f Timestamp: 2023-09-13 16:51:51Z Server failed to authenticate the request. Please refer to the information in the www-authenticate header. RequestId:d1addbca-001e-002e-5b62-e67766000000 Time:2023-09-13T16:51:52.1924115Z ErrorCode:NoAuthenticationInformation The output should be available now for download at https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out. So, it looks like html-proofer worked and found 846 (848 minus 2) probable failures. However, the Azure credentials I was using for remote storage have expired and/or changed so the script wasn\u0026rsquo;t able to save the output in Azure. Fortunately, there is a local copy at /tmp/rootstalk-html-proofer.tmp so I\u0026rsquo;m just going to copy that to BLOB storage using Microsoft Azure Storage Explorer.\nDone. The new file is publicly available at https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out.\nPushing to Production I found a git command at https://stackoverflow.com/questions/13897717/push-commits-to-another-branch to help me push the new main back to production to trigger a production update. However\u0026hellip;\n╭─mark@Marks-Mac-Mini ~/GitHub/npm-rootstalk ‹main› ╰─$ git push origin main:production To https://github.com/Digital-Grinnell/npm-rootstalk.git ! [rejected] main -\u0026gt; production (non-fast-forward) error: failed to push some refs to \u0026#39;https://github.com/Digital-Grinnell/npm-rootstalk.git\u0026#39; hint: Updates were rejected because a pushed branch tip is behind its remote hint: counterpart. Check out this branch and integrate the remote changes hint: (e.g. \u0026#39;git pull ...\u0026#39;) before pushing again. hint: See the \u0026#39;Note about fast-forwards\u0026#39; in \u0026#39;git push --help\u0026#39; for details. So, trying to remedy that\u0026hellip;\n╭─mark@Marks-Mac-Mini ~/GitHub/npm-rootstalk ‹main› ╰─$ git checkout production Switched to branch \u0026#39;production\u0026#39; Your branch is up to date with \u0026#39;origin/production\u0026#39;. ╭─mark@Marks-Mac-Mini ~/GitHub/npm-rootstalk ‹production› ╰─$ git pull origin main From https://github.com/Digital-Grinnell/npm-rootstalk * branch main -\u0026gt; FETCH_HEAD Successfully rebased and updated refs/heads/production. Now my SOURCE CONTROL tab in VSCode shows a status of Sync Changes 1 \u0026lt;down\u0026gt; 46 \u0026lt;up\u0026gt;. Since my local production branch now looks just like main, that sounds like the right thing to do at this point so here goes\u0026hellip; I clicked the Sync... button and got an email back indicating that Azure Static Web Apps CI/CD: All jobs that ran were successful.\nOk, so what about the production instance on DigitalOcean?\nWell, rather than checking in at DigitalOcean, I decided to just have a look at https://rootstalk.grinnell.edu and it looks GREAT!\nA quick look at the footer on https://rootstalk.grinnell.edu/about shows Compiled: Sep 13, 2023 at 1:50pm CDT • GitHub Hash: f1dddf64 and that feels right too!\nThat\u0026rsquo;s all folks\u0026hellip; until next time.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/146-updating-rootstalk-august-2023/","tags":["Rootstalk","NPM","proofreading"],"title":"Updating Rootstalk - August 2023"},{"categories":null,"contents":"NPM for the Win, EVERYWHERE My first experiences with the EVERYWHERE component of this subject involved my personal blog and then Rootstalk. So, I\u0026rsquo;m not going to elaborate on this here, I\u0026rsquo;ll just provide a link to specifics documented in my personal blog:\nhttps://blog.summittdweller.com/wrap-everything-in-npm/ That\u0026rsquo;s all folks\u0026hellip; use the Force links, Luke!\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/145-npm-to-rule-them-all/","tags":["NPM","Hugo","Eleventy","Pagefind"],"title":"NPM to Rule Them All"},{"categories":null,"contents":"Not Just a Pagefind Issue Take note of the question mark at the end of the title, otherwise it could be somewhat misleading. This is not really a problem with Pagefind or Hugo, but one with cloud deployment of static apps, particularly as an Azure Static Web App or DigitalOcean static site.\nThe Nutshell As you may know from post 143, I have successfully installed and configured Pagefind in Rootstalk, but thus far it only works locally. When I try to deploy Pagefind to the cloud, specifically as an Azure Static Web App, I can\u0026rsquo;t make it work because there\u0026rsquo;s no apparent way to invoke the necessary npx pagefind... command AFTER Hugo compiles the site, but BEFORE the site gets deployed. Azure leverages GitHub Actions to build Hugo sites, but that process also involves some custom/proprietary Azure scripts. Therein lies the problem.\nThe Azure script auto-detects the presence of a Hugo project, or any one of many other platforms. That makes the process super-easy to use, but nearly impossible to \u0026ldquo;customize\u0026rdquo;. Azure does provide some configuration variables to influence the script behavior, most notably there\u0026rsquo;s an app_build_command variable that looks promising, but it only works for node builds, and not for Hugo.\nThree Possible Solutions I can reasonably see three possible solutions to this dilema. In order of simplicity they are:\n1. Skip Azure and Deploy Only to DigitalOcean Ultimately, Rootstalk\u0026rsquo;s production instance is a DigitalOcean (DO) static web app, Azure is only used for \u0026ldquo;staging\u0026rdquo; of a locally viable Rootstalk instance. DO doesn\u0026rsquo;t use GitHub Actions, a drawback in my book, so its build script, called an \u0026ldquo;App Spec\u0026rdquo; in DO, is a little more \u0026ldquo;open\u0026rdquo;. The critical portion of the App Spec I use to deploy Rootstalk in DO reads like this:\nstatic_sites: - build_command: hugo -d public That\u0026rsquo;s it, just a simple hugo -d public command. So, in theory I should be able to just change that to read:\nstatic_sites: - build_command: hugo -d public \u0026amp;\u0026amp; npm_config_yes=true npx pagefind --source \u0026#34;public\u0026#34; --bundle-dir ../static/_pagefind It\u0026rsquo;s basically just a \u0026ldquo;compound\u0026rdquo; command to run npx immediately after hugo is done compiling.\nThe problem with this very simple solution, assuming it even works, is that it\u0026rsquo;s somewhat unique to Rootstalk which is already deployed to DigitalOcean. Other Hugo projects, and I have many like this blog, that need Azure or another cloud provider other than DO, would not benefit from this fix.\n2. Wrap Hugo in npm So, node and npm seem to be all the rage these days, and perhaps for good reason. I recently fell in love with Eleventy/11ty because it\u0026rsquo;s Javascript, not Go, and it\u0026rsquo;s elegantly simple with tons of flexibility. If my Azure Static Web App was framed in node.js, as both Eleventy and Pagefind are, there would be no problem. The Azure scripts used to deploy those frameworks are far more customizable than Hugo, and there\u0026rsquo;s documentation to prove it.\nSo, how might I approach something like this? Well, A Powerful Blog Setup with Hugo and NPM by Tom Hombergs looks like a promising place to start. The process that Tom advocates leverages a neat little package called hugo-bin.\nIf strategy #1 fails I\u0026rsquo;ll certainly give this #2 a try. Even if #1 works, I might give this strategy a spin if/when I re-tool this blog.\n3. Transition from Hugo to Eleventy Reasonable? Yes. A lot of work? That\u0026rsquo;s relative. The right thing to do? Probably.\nSo, it\u0026rsquo;s already been done, see This Blog in Eleventy + Ghost and Searching for a Search Solution. That\u0026rsquo;s all I\u0026rsquo;m going to say about this option, a definite maybe.\nThat\u0026rsquo;s all folks\u0026hellip; for now.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/144-a-pagefind-problem/","tags":["Pagefind","Azure","Hugo"],"title":"A Pagefind Problem?"},{"categories":null,"contents":"Critical Info The changes outlined in this post introduce a new theme, Lightbi, for Rootstalk, as well as a new project repository and dev deployment of the site as an Azure Static Web App. Those critical details are:\nProject Repository - https://github.com/Digital-Grinnell/rootstalk-with-lightbi. This new repo effectively replaces https://github.com/Digital-Grinnell/rootstalk which is being archived. Development Deployment - https://victorious-ground-0e1427110.3.azurestaticapps.net/ is the new deployment from the main branch of https://github.com/Digital-Grinnell/rootstalk-with-lightbi. Also\u0026hellip; To reduce costs I\u0026rsquo;ve already eliminated the old deployment of the old main branch to https://icy-tree-020380010.azurestaticapps.net/. With these changes the test and evaluation (staging) deployment of the project\u0026rsquo;s develop branch as an Azure App Service at https://thankful-flower-0a2308810.1.azurestaticapps.net/ is also going away.\nAll of these Azure development changes are reflected in the new project\u0026rsquo;s README.md file.\nRootstalkZen is No Longer Responsive The theme that was originally created for Rootstalk, namely RootstalkZen, was intended to be responsive, and it probably was at one time. Unfortuantely, RootstalkZen was created and implemented just as the COVID 19 pandemic was advancing around the world, and that forced the student who created it to leave the Grinnell College campus (along with everyone else) much earlier than planned. Subsequently, the theme became part of Rootstalk without a lot of documentation or collaboration. Before long the theme got modified to the point where it was no longer properly responsive. Steps were taken to try and mitigate that \u0026ldquo;bad\u0026rdquo; behavior, but the issue was never adequately addressed, until now (May, 2023).\nMatomo Analytics Matomo analytics collected for https://rootstalk.grinnell.edu, the production instance of Rootstalk, during the month of April 2023 (see https://analytics.summittservices.com/index.php?module=CoreHome\u0026amp;action=index\u0026amp;date=last7\u0026amp;period=range\u0026amp;idSite=15#?period=range\u0026amp;date=2023-04-01,2023-04-30\u0026amp;category=Dashboard_Dashboard\u0026amp;subcategory=1) show that about 37% (77) of the 206 site visits were made from a smartphone or similar, small device. That percentage was deemed high enough to warrant taking steps to correct the errant responsive behavior.\nThe Lightbi Theme I used the Command-Option-M hotkey in Firefox to toggle Responsive Design Mode controls allowing me to evaluate differnt web pages in various media formats. I browsed through demos of the complete list of Hugo themes looking for a responsive theme that included the 3-column layout that is feature of Rootstalk we wish to keep. I tested several themes including Mediumish, Blist, and Hugo W3 Simple before settling on Lightbi and its live demo at https://lightbi-hugo-theme.netlify.app/.\nLightbi is relatively simple utilizing .css rather than .scss and it provides a single point of customization in its ./static/css/main.css file. I started by overriding a number of the theme\u0026rsquo;s example site (./themes/lightbi/exampleSite) template files. The overrides are in subdirectories of ./layouts and I tried to be generous with comments included in those files.\nSearch is Also Broken In addtion to poor responsive behavior Rootstalk using the RootstalkZen search feature also appeared to be broken. It was also noted that having \u0026ldquo;Search\u0026rdquo; hidden away in the menu was not conducive to making it easily accessible.\nImplementing Pagefind Not long ago I took steps to add Pagefind search to my personal blog and it works nicely! So, I\u0026rsquo;m going to try and implement the same for Rootstalk beginning the process with the guidance shared by Bryce Wray in his Pagefind is quite a find for site search post.\nThat\u0026rsquo;s all folks\u0026hellip; for now.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/143-significant-rootstalk-retooling/","tags":["Rootstalk","Azure","Lightbi"],"title":"Significant Rootstalk Retooling"},{"categories":null,"contents":"Concerning DigitalOcean I took steps yesterday to push an update of Rootstalk to our production \u0026ldquo;starter\u0026rdquo; project on DigitalOcean (DO), and my intent was to make this the last such deployment on DO. I had been thinking for sometime about moving Rootstalk production to Azure App Service where Rootstalk is currently developed. However, after moving my \u0026ldquo;personal\u0026rdquo; digital@Grinnell.edu Azure account to \u0026ldquo;pay-as-you-go\u0026rdquo; status, the projected monthly cost of services shot up from less than $1/month (mostly for object storage) to something closer to $10 or $15/month.\nConsequently, since the \u0026ldquo;starter\u0026rdquo; edition of Rootstalk on DO costs NOTHING (other elements of the libraries\u0026rsquo; account with DO make the \u0026ldquo;starter\u0026rdquo; free to use), and seems to perform just fine, I\u0026rsquo;ve elected to leave Rootstalk production as it is on DO for the forseeable future.\nConcerning Azure Meanwhile, I will be taking steps to move billing for my Azure account from my personal pocketbook to a formal Grinnell College Libraries credit card (as approved by the Library more than a month ago). In order to minimize the impact of that move, I will eliminate the deployment of the Rootstalk GitHub project\u0026rsquo;s main branch in an effort to reduce future Azure fees. Going forward this will remove the https://icy-tree-020380010.azurestaticapps.net/ deployment of Rootstalk. Rest assured that test and evaluation (staging) deployment of the project\u0026rsquo;s develop branch will still be available as an Azure App Service at https://thankful-flower-0a2308810.1.azurestaticapps.net/.\nThe Azure development changes described above are now reflected in the project\u0026rsquo;s README.md file.\nMatomo On the Scene In addition to the aforementioned project deployment and account changes, analytics are now being provided for Rootstalk, both production and develop instances, via a new Matomo analytics server with a dashboard at https://analytics.summittservices.com.\nThat\u0026rsquo;s all folks\u0026hellip; for now.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/142-rootstalk-updates/","tags":["Rootstalk","Azure","DigitalOcean","Matomo"],"title":"Rootstalk Updates"},{"categories":null,"contents":" See Gating My Content \u0026amp; More - Parts 1 and 2 for prerequsite and background info.\nIntroducing Wieting Content Part 3 of this process, the introduction of Wieting Theatre Guild content, is described in the project repo\u0026rsquo;s README.md file. Since the project is in a private repo I\u0026rsquo;ll include the pertinent parts here from this gist.\nPart 4 - Protecting Pages with StatiCrypt CLI + More Let\u0026rsquo;s jump in with StatiCrypt CLI. The following experience is from the README.md file in my https://github.com/SummittDweller/wieting-guild-pages project.\nDisplaying Embedded PDFs Some of the pages I added to https://wieting-guild.tamatoledo.com are intended to display .pdf content. The old shortcode responsible for that feature will need to be ported from wieting-one-click-hugo-cms and made to function in Eleventy.\nTime for some research into how PDF\u0026rsquo;s can be best embedded into Eleventy content\u0026hellip; I found this:\nhttps://www.npmjs.com/package/eleventy-plugin-pdfembed The template content used to embed a PDF then looks like this:\n{% pdfembed 'https://documentcloud.adobe.com/view-sdk-demo/PDFs/Bodea-Brochure.pdf' %}\nProperly Adding a Plugin to .eleventy.js So, the initial contents of my .eleventy.js file was:\nmodule.exports = function (eleventyConfig) { eleventyConfig.addPassthroughCopy(\u0026#34;./src/css/\u0026#34;); eleventyConfig.addWatchTarget(\u0026#34;./src/css/\u0026#34;); eleventyConfig.addShortcode(\u0026#34;year\u0026#34;, () =\u0026gt; `${new Date().getFullYear()}`); return { dir: { input: \u0026#34;src\u0026#34;, output: \u0026#34;public\u0026#34;, }, }; }; And I needed to add this:\nconst pluginPDFEmbed = require(\u0026#39;eleventy-plugin-pdfembed\u0026#39;); module.exports = (eleventyConfig) =\u0026gt; { // more stuff here eleventyConfig.addPlugin(pluginPDFEmbed, { key: \u0026#39;\u0026lt;YOUR CREDENTIAL KEY\u0026gt;\u0026#39; }); } But, how and where? After a little research I tried this:\nconst pluginPDFEmbed = require(\u0026#39;eleventy-plugin-pdfembed\u0026#39;); module.exports = function (eleventyConfig) { eleventyConfig.addPassthroughCopy(\u0026#34;./src/css/\u0026#34;); eleventyConfig.addWatchTarget(\u0026#34;./src/css/\u0026#34;); eleventyConfig.addShortcode(\u0026#34;year\u0026#34;, () =\u0026gt; `${new Date().getFullYear()}`); eleventyConfig.addPlugin(pluginPDFEmbed, { key: \u0026#39;\u0026lt;YOUR CREDENTIAL KEY\u0026gt;\u0026#39; }); return { dir: { input: \u0026#34;src\u0026#34;, output: \u0026#34;public\u0026#34;, }, }; }; Woohoo! That syntax looks right, but of course I didn\u0026rsquo;t get any PDF to display because my PDF path is all wrong AND I have yet to replace \u0026lt;YOUR CREDENTIAL KEY\u0026gt; above. It\u0026rsquo;s late so that can wait until tomorrow.\nHow to Restart An Eleventy Site So, I returned to this project today after a git add ., git commit ..., git push sequence last evening. Now my pdfembed plug-in isn\u0026rsquo;t working locally. I wonder why that is? Probably because I didn\u0026rsquo;t properly commit the effects of last night\u0026rsquo;s npm i eleventy-plugin-pdfembed. So, how should I be doing that?\nA quick search of the web turned up evidence of a --save option on the npm install command. The resource I found was What is the \u0026ndash;save option for npm install. The explanation there makes perfect sense, so I\u0026rsquo;m going to run npm i eleventy-plugin-pdfembed --save now to see what happens\u0026hellip;\n╭─mark@Marks-Mac-Mini ~/GitHub/wieting-guild-pages ‹main*› ╰─$ npm i eleventy-plugin-pdfembed --save up to date, audited 354 packages in 613ms 19 packages are looking for funding run `npm fund` for details 7 vulnerabilities (2 moderate, 3 high, 2 critical) To address issues that do not require attention, run: npm audit fix To address all issues (including breaking changes), run: npm audit fix --force Run `npm audit` for details. Hmmm, there was no change in my package.json file, and when I look there I see that the pdfembed dependency was already there. I guess the missing link here is that I didn\u0026rsquo;t npm run build yet? Bingo! That did the trick. So, my proper \u0026ldquo;restart\u0026rdquo; of the local site must include TWO commands:\nnpm run build npm run start Now, back to the problem at hand\u0026hellip;\nResolving the PDF Path and Display The pdfembed plug-in was written to take a full URL pointer to the PDF file, but I\u0026rsquo;d like to know if it works \u0026ldquo;locally\u0026rdquo; as well. So, Eleventy configuration also supports the notion of a passthrough directory as you can see in this line gleaned from our .eleventy.js file:\neleventyConfig.addPassthroughCopy(\u0026#34;./src/css/\u0026#34;); So, I think what I\u0026rsquo;ll do is create a dedicated passthrough directory at ./src/static \u0026ndash; I chose this path because static is the default \u0026ldquo;passthrough\u0026rdquo; directory in Hugo \u0026ndash; and put my PDFs in a subdirectory there. To do this I\u0026rsquo;ll add two more lines to our .eleventy.js file:\neleventyConfig.addPassthroughCopy(\u0026#34;./src/static/\u0026#34;); eleventyConfig.addWatchTarget(\u0026#34;./src/static/\u0026#34;); The second line above instructs the project to \u0026ldquo;keep watch\u0026rdquo; for changes in the contents of ./src/static, and rebuild the project when any changes are detected there.\nNext, I\u0026rsquo;ll create the ./src/static directory and a /pdfs directory beneath it, place copies of all my PDF files there, and experiment with pdfembed shortcode calls like this:\n{% pdfembed \u0026#39;/pdfs/wieting-501c3.pdf\u0026#39; %} {% pdfembed \u0026#39;./pdfs/wieting-501c3.pdf\u0026#39; %} {% pdfembed \u0026#39;./static/pdfs/wieting-501c3.pdf\u0026#39; %} Duh, none of this works. So, I did some inspection of the local site and found this:\nThis application domain (http://localhost:8080) is not authorized to use the provided PDF Embed API Client ID. Well, of course, I haven\u0026rsquo;t supplied a valid key for the .eleventy.js config line that reads eleventyConfig.addPlugin(pluginPDFEmbed, { key: '\u0026lt;YOUR CREDENTIAL KEY\u0026gt;' });!\nThe instructions in Eleventy Plugin: PDFEmbed leads to a free set of credentials link to set that up.\nI setup an Adobe Developer account, created credentials, and tried again. Unfortunately, Adobe is telling me that the viewer needs to be updated, so I opened this GitHub issue to document my approach.\nThe package author very quickly addressed the issue and produced an updated main within just a couple of hours\u0026hellip; AWESOME! So, based on guidance I found in Install NPM Packages from GitHub, I did npm i https://github.com/cfjedimaster/eleventy-plugin-pdfembed to update things locally and then gave it a test. It works! Beautimous!\nProtecting Pages with StatiCrypt CLI Let\u0026rsquo;s jump in with StatiCrypt CLI to see about password protection for the new pages. The guidance I\u0026rsquo;ll follow is How to password protect a static site on Vercel, Netlify, or any JAMStack site as it applies to StatiCrypt CLI and Eleventy. I found that I also needed some guidance from StatiCrypt.\nLocally\u0026hellip; ╭─mark@Marks-Mac-Mini ~/GitHub/wieting-guild-pages ‹main› ╰─$ npm install staticrypt added 3 packages, and audited 357 packages in 2s 19 packages are looking for funding run `npm fund` for details 7 vulnerabilities (2 moderate, 3 high, 2 critical) To address issues that do not require attention, run: npm audit fix To address all issues (including breaking changes), run: npm audit fix --force Run `npm audit` for details. ╭─mark@Marks-Mac-Mini ~/GitHub/wieting-guild-pages ‹main*› ╰─$ npm audit fix added 3 packages, removed 10 packages, changed 6 packages, and audited 350 packages in 2s 20 packages are looking for funding run `npm fund` for details # npm audit report ejs \u0026lt;3.1.7 Severity: critical ejs template injection vulnerability - https://github.com/advisories/GHSA-phwq-j96m-2c2q fix available via `npm audit fix --force` Will install @11ty/eleventy@2.0.0, which is a breaking change node_modules/ejs @11ty/eleventy \u0026lt;=1.0.2 Depends on vulnerable versions of ejs Depends on vulnerable versions of liquidjs Depends on vulnerable versions of markdown-it Depends on vulnerable versions of pug node_modules/@11ty/eleventy liquidjs \u0026lt;10.0.0 Severity: moderate liquidjs may leak properties of a prototype - https://github.com/advisories/GHSA-45rm-2893-5f49 fix available via `npm audit fix --force` Will install @11ty/eleventy@2.0.0, which is a breaking change node_modules/liquidjs markdown-it \u0026lt;12.3.2 Severity: moderate Uncontrolled Resource Consumption in markdown-it - https://github.com/advisories/GHSA-6vfc-qv3f-vr6c fix available via `npm audit fix` node_modules/markdown-it pug \u0026lt;3.0.1 Severity: high Remote code execution via the `pretty` option. - https://github.com/advisories/GHSA-p493-635q-r6gr fix available via `npm audit fix --force` Will install @11ty/eleventy@2.0.0, which is a breaking change node_modules/pug 5 vulnerabilities (2 moderate, 1 high, 2 critical) To address issues that do not require attention, run: npm audit fix To address all issues (including breaking changes), run: npm audit fix --force ╭─mark@Marks-Mac-Mini ~/GitHub/wieting-guild-pages ‹main*› ╰─$ Building in Azure I modified my .github/workflows/azure-static-web-apps-lemon-mushroom-087d78210.yml file to include a app_build_command option as suggested in Build configuration for Azure Static Web Apps. When omitted this defaults to npm run build, but I needed more to enmgage the encryption.\nNow wieting-guild.tamatoledo.com On 2023-03-08T13:05:40-06:00 I added a new Adobe PDF Embed API key to my developer dashboard, and also here in the config of .eleventy.js.\nThis morning I added a CNAME record to the tamatoledo.com domain DNS records at DigitalOcean with this value: lemon-mushroom-087d78210.2.azurestaticapps.net. I also added a custom domain validation record for wieting-guild.tamatoledo.com in the Azure Static Web App settings for this project. It all works!\nI hope you find portions of this very detailed post to be useful. I\u0026rsquo;m sure there will soon be some follow-up to this, but for this installment\u0026hellip; That\u0026rsquo;s a wrap!\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/141-gating-my-content-and-more-parts-34/","tags":["gating","authentication","NetlifyCMS","Wieting","StatiCrypt","Eleventy","11ty","Azure","GitHub","oAuth"],"title":"Gating My Content \u0026 More - Parts 3 and 4"},{"categories":null,"contents":"The task du jour is to begin, and perhaps complete, the process of adding a custom 404 page to Rootstalk.\nThus far I\u0026rsquo;ve found a couple of promising resources to guide the effort:\nCustom 404 Page Custom 404 pages in Hugo done right In particular, I\u0026rsquo;m focusing on \u0026ldquo;Option 2\u0026rdquo; in the \u0026ldquo;\u0026hellip;done right\u0026rdquo; document, and the \u0026ldquo;Azure Static Web App\u0026rdquo; portion of the first document.\nNeed a New Azure API Key? Early in this journey I found that I could not deploy changes to https://thankful-flower-0a2308810.1.azurestaticapps.net from the develop branch of the code because of an invalid or missing API key. I turned to Reset deployment tokens in Azure Static Web Apps to try and remedy that.\nAlas, that didn\u0026rsquo;t fix things. The error in my \u0026ldquo;Build and Deploy Job\u0026rdquo; log in GitHub says\u0026hellip;\nApp Directory Location: \u0026#39;/\u0026#39; was found. Looking for event info The content server has rejected the request with: BadRequest Reason: No matching Static Web App was found or the api key was invalid. I\u0026rsquo;m going to remove the azurestaticwebapp.config.json file and try again. Yes, that worked. So, maybe I put that file in the wrong place? Also, I am able to switch into the main branch of the Rootstalk code, where there is no azurestaticwebapp.config.json, and the deployment workflow is successful there. Time for some more web research.\nDuh! Ok, scratch that last section. It turns out that the GitHub Action that returned the error was an old one desinged to deploy a branch that no longer exists. The custom 404 page has been working all along on https://thankful-flower-0a2308810.1.azurestaticapps.net/. In fact, if you try and open something like https://thankful-flower-0a2308810.1.azurestaticapps.net/bogus, an address that doesn\u0026rsquo;t work, you can see the new custom 404.\nThe Real Problem So, my aim with this is to return a custom 404 page when an external link, one outside of Rootstalk, does NOT work. That is apparently a very different ball of wax, so more research is needed.\nFindings After considerable research, it seems like what I have in mind will not work in a static environment like Hugo. See In HUGO - How to read query parameters in template for more detail.\nSo, what I\u0026rsquo;ve done thus far is to create a new page and a new shortcode in the Rootstalk site.\nThe page at ./static/broken-external-link.html reads like this:\n\u0026lt;h1\u0026gt;Whoops! Page Not Found\u0026lt;/h1\u0026gt; \u0026lt;h2\u0026gt;We looked everywhere for \u0026#34;{{ $dead }}\u0026#34;, but that page can\u0026#39;t be found.\u0026lt;/h2\u0026gt; \u0026lt;p style=\u0026#34;font-weight: 80; color:brown;\u0026#34;\u0026gt;Our latest content is \u0026lt;a href=\u0026#34;/\u0026#34;\u0026gt;on the homepage\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; The new shortcode at ./layouts/shortcodes/broken.html reads like this:\n{{- $s := .Get 1 -}} {{- $link := replaceRE \u0026#34;^https?://([^/]+)\u0026#34; \u0026#34;$1\u0026#34; $s -}} {{- $dead := \u0026#34;dead\u0026#34; -}} \u0026lt;a href=\u0026#34;/broken-external-link.html?{{- (querify $dead $link) | safeURL -}}\u0026#34;\u0026gt;{{- .Get 0 -}}\u0026lt;/a\u0026gt; That shortcode is called using Markdown syntax like this example from Rootstalk\u0026rsquo;s content/past-issues/volume-ii-issue-2/kincaid.md:\n\u0026hellip;He has published stories and poems in The Eclectic, {{% broken \u0026ldquo;Fiction Fix\u0026rdquo; \u0026ldquo;http://fictionfix.net\u0026rdquo; %}} and in the online journal {{% broken \u0026ldquo;deadpaper.org\u0026rdquo; \u0026ldquo;http://www.deadpaper.org\u0026rdquo; %}}\nNote that in order to \u0026ldquo;properly\u0026rdquo; display the block above I had to effectively turn my shortcode references into \u0026ldquo;comments\u0026rdquo; by following the advice found in Hugo - Show ShortCode Markdown in Code Block.\nFortunately, the fourth paragraph in this answer may hold the key. It suggests that\u0026hellip;\nAt this point, you can still use JavaScript (URLSearchParams) to interpret those params and to modify the page on the spot. This means that you\u0026rsquo;re adding complexity to your Hugo site, as well as more load on the clients (browsers). If you really need those query strings and Hugo, this is the way to go.\nI believe this is a static site problem worth solving, for Rootstalk and much more, so let\u0026rsquo;s dive into some JavaScript and URLSearchParams.\nThe Fix My successful implementation of URLSearchParams can be seen in Rootstalk with pertinent source code found in that project\u0026rsquo;s repo and these files:\nstatic/broken-external-link.html layouts/shortcodes/broken.html layouts/shortcodes/broken-endnote.html Any article .md file with a broken external link, such as content/past-issues/volume-i-issue-1/dean.md. Oops, Another Problem The bits of code implemented above work perfectly in my local (via hugo server) instance of Rootstalk, but there\u0026rsquo;s a problem with the broken shortcode when used inside front matter, as is the case in the article titled My Prairie.\nAt the very bottom of that article there are three links in the author\u0026rsquo;s \u0026ldquo;bio\u0026rdquo;, and the last of those links is \u0026ldquo;broken\u0026rdquo;. The author\u0026rsquo;s bio appears in the article front matter as you can see in the example shown here in abridged form:\nazure_dir: rootstalk-2015-fall contributors: - bio: \u0026#39;**Betty Moffett** taught for almost thirty years in Grinnell College’s Writing Lab, where she learned a great deal from her students. Her stories have appeared in a number of journals and magazines, including [The MacGuffin,] (http://www.schoolcraft.edu/a-z-index/the-macguffin#.VqAvnpNVhBc) [The Storyteller,](http://www.thestorytellermagazine.com/) and {{% broken \u0026#34;The Wapsipinicon Almanac\u0026#34; \u0026#34;http://www.wapsialmanac\u0026#34; %}}. She and her husband, Sandy, write songs for and play with The Too Many String Band.\u0026#39; caption: Photo courtesy of Betty Moffett Here the contributors.bio field is a mix of Markdown syntax with a call to the broken shortcode. The snippet of code from layouts/_default/single.html that renders the bio reads like this:\n{{ with .Params.contributors }} {{ range . }} {{ $bio := (index . \u0026#34;bio\u0026#34;) }} ... {{ if not (eq $bio \u0026#34; \u0026#34;) }} {{ $bio | markdownify }} {{ end }} Essentially, the contributors.bio value is filtered using | markdownify and that, as I have come to understand, is the root of this problem. When $bio is filtered using markdownify the rendered bio from our previous example looks like this:\nBetty Moffett taught for almost thirty years in Grinnell College’s Writing Lab, where she learned a great deal from her students. Her stories have appeared in a number of journals and magazines, including The MacGuffin, The Storyteller, and {{% broken \u0026ldquo;The Wapsipinicon Almanac\u0026rdquo; \u0026quot;http://www.wapsialmanac.com\u0026quot; %}}. She and her husband, Sandy, write songs for and play with The Too Many String Band.\nLikewise, if the $bio is not filtered with | markdownify then the Markdown elements including bold text and links don\u0026rsquo;t work, but the broken link does. That output looks like this:\n**Betty Moffett** taught for almost thirty years in Grinnell College’s Writing Lab, where she learned a great deal from her students. Her stories have appeared in a number of journals and magazines, including [The MacGuffin](http://www.schoolcraft.edu/a-z-index/the-macguffin#.VqAvnpNVhBc), [The Storyteller](http://www.thestorytellermagazine.com), and The Wapsipinicon Almanac. She and her husband, Sandy, write songs for and play with The Too Many String Band.\nI found numerous posts involving similar problems when mising shortcodes with Markdown and/or front matter, and many of them tout Hugo .RenderString as the solution. However, I have yet to wrap my head around how .RenderString might be used in this instance. So, I\u0026rsquo;ve simply backed off from using the broken shortcode inside of contributors.bio, and have instead replaced such shortcode calls to straight HTML as you see in our abridged front matter below:\ncontributors: - bio: \u0026#39;**Betty Moffett** taught for almost thirty years in Grinnell College’s Writing Lab, where she learned a great deal from her students. Her stories have appeared in a number of journals and magazines, including [The MacGuffin,](http://www.schoolcraft.edu/a-z-index/the-macguffin#.VqAvnpNVhBc) [The Storyteller,](http://www.thestorytellermagazine.com/) and \u0026lt;a href=\u0026#34;/broken-external-link.html?dead=http://www.wapsialmanac.com\u0026#34;\u0026gt;The Wapsipinicon Almanac\u0026lt;/a\u0026gt;. She and her husband, Sandy, write songs for and play with The Too Many String Band.\u0026#39; Not a very elegant solution, but it works, and you can see a working example of it in the links near the end of My Prairie.\nFailed Again As I was posting this update I came across Questions about .RenderString and the suggestion that replacing {{ $bio | markdownify }} with {{ $.Page.RenderString $bio }} might work. It did not. \u0026#x1f626;\nA Pair of Useful REGEX Replacements I\u0026rsquo;ve created a regular expression with two possible replacment patterns to help make substitutions where broken external links exist:\nExpression for matching: ^\\[(.+)\\]\\((.+)\\)$ The broken shortcode replacment is: {{% broken \u0026quot;$1\u0026quot; \u0026quot;$2\u0026quot; %}}. The equivalent raw HTML replacement is: \u0026lt;a href=\u0026quot;/broken-external-link?dead=$2\u0026quot; target=\u0026quot;_blank\u0026quot;\u0026gt;$1\u0026lt;/a\u0026gt; Use them wisely, and in good health!\nI\u0026rsquo;m sure there will soon be much more here, but for this installment\u0026hellip; That\u0026rsquo;s a wrap!\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/140-adding-a-custom-404-to-hugo/","tags":["Hugo","Rootstalk","custom","404","Azure","API","query","comment","URLSearchParams",".RenderString"],"title":"Adding a Custom 404 Page in Hugo"},{"categories":null,"contents":"A Blended Approach It\u0026rsquo;s a new month, February 2023 that is, and this the first part of follow-up to last month\u0026rsquo;s post in my personal blog, namely Gating My Content. Now, rather than trying to \u0026ldquo;gate\u0026rdquo; some of the content in the Wieting Theatre website, I\u0026rsquo;m going to start a new site, with characteristics listed below, and include only the protected portion of the Wieting\u0026rsquo;s content.\nI call this a \u0026ldquo;blended\u0026rdquo; approach because it will, at least initially, leverage and blend guidance and elements from many of the resources I\u0026rsquo;ve captured at https://www.one-tab.com/page/iyQVdlpSRICO67Mue7Cb_Q.\nThis post originally documented the creation of an 11ty site based on the NEAT Starter Template, and some of the illustrations here may include references to it. However, after the 11ty site was created I found a starter tutorial and site template better suited to my needs. So, I have a new OneTab page which includes A Deep Dive Into Eleventy Static Site Generator with an associated starter/template that I\u0026rsquo;ll be using in place of the NEAT Starter Template.\nFeatures As you\u0026rsquo;ll see in the OneTab page listed above, the new site will feature:\nAn 11ty site following this NEAT Starter Template A Deep Dive Into Eleventy Static Site Generator, Deployed as an Azure Static Web App, Customization to populate the site with Wieting Theatre Guild information, Using StatiCrypt CLI to protect pages, From a GitHub Action, and Perhaps some integration with Netlify CMS in Azure instead of Netlify.com. With Better Documentation As steps are taken, I\u0026rsquo;ll try to capture what I used, and why, along with a detailed history of those steps. In fact, I\u0026rsquo;m going to try using Creating-Better-Documentation to record everything I do on this screen from start-to-finish. Wish me luck.\nThis post will cover features 1 and 2 from the list above. Subsequent posts in this blog will address features 3 through 6.\nIn the figures below, yellow box annotations mark elements that need attention or need to be checked, while red box annotations mark elements that need input of some kind.\nBuilding a Local 11ty Site I started the build process using the NEAT Starter Template Smol-11ty-Starter to create a new local git project. See Figure 1 below.\nFigure 1 \u0026middot; Create a local clone of the Smol-11ty-Starter template In my case, I choose to name my new project wieting-guild-pages rather than using the default name. The full command in my case was: git clone https://github.com/5t3ph/smol-11ty-starter.git wieting-guild-pages.\nFigure 2 \u0026middot; Give the local clone an sppropriate name Change directory (cd) into the new project directory.\nFigure 3 \u0026middot; Change directory into the new project If I intended to create a public code repository I could have \u0026ldquo;forked\u0026rdquo; the Smol-11ty-Starter project and cloned my local from that fork, but the SummittDweller/wieting-guild-pages needed to be private so I used the following process instead. Why? Because you can\u0026rsquo;t create a private repo from a fork.\nSo, I began by removing the local repo\u0026rsquo;s association with Smol-11ty-Starter using rm -fr .git as shown in Figure 4 below.\nFigure 4 \u0026middot; Remove the local repo\u0026rsquo;s .git directory Time to launch VScode. On my Mac that\u0026rsquo;s simply: code ..\nFigure 5 \u0026middot; Launch VSCode I like to have a terminal, attached to my project workspace, running inside VSCode. So, in VSCode that\u0026rsquo;s Command - Shift - P to open the command pallette, then select Terminal: Create New Terminal (In Active Workspace) to open a terminal within the VSCode project as you see below in Figure 6 and Figure 7.\nFigure 6 \u0026middot; Attaching a terminal to my project in VSCode Figure 7 \u0026middot; A project terminal in VSCode Creating a New GitHub Project Next, I returned to my GitHub tab and select the upper-right + drop-down to create a New repository. I gave the new repo a Repository name, Description, chose to make it Private, did NOT elect to Add a README file, and clicked Create repository, as you see in Figure 7 below.\nFigure 8 \u0026middot; Creating a new, private GitHub repo The new repo comes with instructions for proceeding with the project.\nFigure 9 \u0026middot; New repo status and instructions From the project\u0026rsquo;s VSCode terminal I executed the commands as instructed by the new GitHub repo window, beginning with git init.\nFigure 10 \u0026middot; Initialze the local project with git init Then git add . instead of git add README.md as instructed. I do this to add ALL of the project files to staging becase we need to commit everything, not just the README.md file.\nFigure 11 \u0026middot; git add . to stage ALL files for commit Next, I copied and pasted the remaining commands from the instructions into the terminal. The complete command block includes:\ngit commit -m \u0026#34;first commit\u0026#34; git branch -M main git remote add origin https://github.com/SummittDweller/wieting-guild-pages.git git push -u origin main The terminal should respond with output like you see in Figure 13 below.\nFigure 12 \u0026middot; Copy/paste the remaining git commands Figure 13 \u0026middot; Output from git push... Now it was time to make an initial build and run the starter project as instructed in step 2 of the project\u0026rsquo;s Quick Start instructions.\nFigure 14 \u0026middot; Install npm dependencies My install detected a number of vulnerabilities, presumably because of the starter project\u0026rsquo;s age, so I followed the advice provided in the npm install output and ran a npm audit fix command as you see below.\nFigure 15 \u0026middot; Using npm audit fix as suggested Next, continuing the starter project\u0026rsquo;s quick start instructions I executed npm run build to build our first instance of the 11ty project. Note in the output that the command is building the 11ty site in the public directory. This information will be needed later in the process.\nFigure 16 \u0026middot; Build the 11ty project Since the build was a success I continued with the quick start by running npm run start to launch a local instance of the site. As you see in Figure 18, the local site becomes available in our web browser at http://localhost:8080 and multiple links to visit the site are provided.\nFigure 17 \u0026middot; Launch a local instance of the site using npm run start Figure 18 \u0026middot; Follow a provided link to open the site in your browser Our inital local site is now visible in our browser, and it looks good! I used CTRL-C in the VSCode terminal to stop the local site as you see in Figure 20 below.\nFigure 19 \u0026middot; Initial local site Figure 20 \u0026middot; CTRL-C stops the local 11ty web server Creating the Azure Static Web App Next, I returned to my Azure portal to create a new Azure Static Web App from our new repo. The process begins by navigating a browser window to the Azure portal and Azure services, Static Web Apps, and + Create as shown below in Figure 21.\nFigure 21 \u0026middot; Initial steps to create an Azure Static Web App I gave my web app a name, in this case wieting-guild-pages, which matches the name of my GitHub repo. Matching these names is recommended, but NOT a requirement! Other parameters of the app are as shown in Figure 22 below.\nFigure 22 \u0026middot; Web app name and parameters In the lower half of the Create Static Web App screen the required selections for Organization, Repository, Branch and Build presents are as shown in Figure 23. Leave the default values for App location and Api location, and specify an Output location of public. You may recall that in Figure 16 we saw that public is the name of the folder where the 11ty site is built.\nClick Review + create to proceed.\nFigure 23 \u0026middot; Complete static app parameters The Azure window should change to reflect that the new app is being validated and the Create button in the lower-left corner becomes available once validation is complete. Clicking the Create button shown in Figure 24 starts the creation and deployment process.\nFigure 24 \u0026middot; App validation and creation The Azure portal screen for the new app shows ...Deployment is in progress. The deployment could take a minute or two.\nFigure 25 \u0026middot; Deployment is in progress Once the initial deployment is complete the window will change to indicate the progress. An option to Pin to dashboard is displayed and I recommend doing so. Then click Go to resource.\nFigure 26 \u0026middot; Deployment is complete Go to resource opens the app\u0026rsquo;s overview page where you should see key information as marked with yellow boxes in Figure 27 below. Use the URL link on the right to open the new Azure web app site in your browser.\nFigure 27 \u0026middot; New app overview and link The site should open in your browser but it may not display any content. Instead you may see a warning page indicating that the site/app is ready, but has no content. This is because the initial deployment of the app wasn\u0026rsquo;t preceeded by an npm build command. To force a rebuild you should visit the local project in VSCode, make a change to some piece of content, and commit/push that change to GitHub. Each time you complete a change sequence like that the GitHub Action that was created earlier (see the Edit workflow link in Figure 27 above) will automatically build and re-deploy the app.\nPushing Changes to Rebuild the Site I began this part of the workflow by returning to my VSCode project terminal where I did a git pull command to update the local repo with any remote changes. This should pull in one critical addition to the project, the GitHub Action workflow .yml file as you can see in Figure 28 below.\nFigure 28 \u0026middot; git pull remote changes I choose to open the index.md file in the project\u0026rsquo;s ./src directory \u0026ndash; this is essentially the site\u0026rsquo;s \u0026ldquo;home\u0026rdquo; page as markdown content \u0026ndash; where I changed the original title to \u0026ldquo;Wieting Guild Pages\u0026rdquo;.\nFigure 29 \u0026middot; Changing the title: in /src/index.md With the change completed I moved into the VSCode terminal for a typical git add ., git commit -m... and git push command sequence as shown in Figure 30 below.\nFigure 30 \u0026middot; Add, commit and push the changes I immediately switched back to my Azure app\u0026rsquo;s overview screen where I clicked on the GitHub Action runs link under Deployment history. That opens a window showing the GitHub project\u0026rsquo;s Actions tab where a spinning yellow dot in front of the commit name (in this case it was First push to rebuild) indicates that the GitHub Action workflow is underway.\nFigure 31 \u0026middot; Using the GitHub Action runs link to view progress Figure 32 \u0026middot; GitHub Action workflow in progress When the yellow spinning dot turns to a green checkmark, as you see in Figure 33, the GitHub Action is complete and was a success.\nFigure 33 \u0026middot; GitHub Action was successful Now, returning to the app\u0026rsquo;s Azure overview shows a Browse button and an address/link to the newly deployed site on the right. Clicking either of these elements should open the new site.\nFigure 34 \u0026middot; Opening the site from the app\u0026rsquo;s overview screen The edits shown previously in Figure 30 should now be visible in the updated site. Huzzah! 🎉\nFigure 35 \u0026middot; Success! I hope you find portions of this very detailed post to be useful. I\u0026rsquo;m sure there will soon be some follow-up to this, but for this installment\u0026hellip; That\u0026rsquo;s a wrap!\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/139-gating-my-content-and-more/","tags":["gating","authentication","NetlifyCMS","Wieting","StatiCrypt","Eleventy","11ty","Azure","GitHub","oAuth"],"title":"Gating My Content \u0026 More - Parts 1 and 2"},{"categories":null,"contents":"We Need More Cow Bell Screen Capture! On a recent project I found myself following some development guidance provided in Deploying an 11ty Site to Azure Static Web Apps and I really like the work that squalrus did here because there\u0026rsquo;s a nice mix of screen capture images and descriptive text. I think my documentation, at least in the past, has been lacking in images. Time to fix that.\nCommand - Shift - 5 On my Mac I frequently use the command - shift - 5 key sequence to launch dynamic \u0026ndash; think movie, not image \u0026ndash; screen capture. I did just that moments ago, so some of what you\u0026rsquo;ll see below is a result of that maneuver. \u0026#x1f604;\nSo, now when I begin a new bit of development I use command - shift - 5 to open a capture control like you see in the image below, and I capture every keystroke, command, and click as I work. When finished I use command - shift - 5 again to re-open the control, click stop (the square block control shown in the figure below), and presto, I have a new .mov file captured and ready for edting.\nFigure 1 \u0026middot; Use `Command - Shift - 5 to Stop Recording Convert Videos to Frames Embedding a raw .mov file into my documenation is possible, but more often that not the files are HUGE, and they contain LOTS of unnecessary, boring minutes. All I really need is a sequence of images, perhaps with annotations, gleaned from the movie. That\u0026rsquo;s where github.com/SummittDweller/convert_videos_to_frames comes in.\nThe aformentioned Python code repository was forked from github.com/anas-899/convert_videos_to_frames, and I made very few changes to that excellent starting point. Honestly, all that I did was apply my preferred Python project process, namely Proper Python.\nI used the tool to post-process the dynamic screen capture made while building portions of this post. Some of the images generated from that screen capture may appear below, with a bit of explanation.\nConversion Workflow This section will walk us through the workflow I\u0026rsquo;ve developed for turing a .mov into images, and then into figure markdown like this example:\nFigure 2 \u0026middot; Example Figure Syntax Note that while composing this document I found it\u0026rsquo;s best if image files have zero-padded numeric names, so 123.png should be 0123.png. This helps keep the images in numerical order everywhere.\nI made code modifications in github.com/SummittDweller/convert_videos_to_frames to enforce this new naming convention, but some screen images in this document still appear in the old, unpadded naming convention.\n1) Rename the Screen Capture The first step is to change the screen capture .mov filename to something memorable as shown in the next two figures.\nFigure 3 \u0026middot; Rename the Screen Capture Figure 4 \u0026middot; The Renamed Screen Capture 2) Drag the Renamed .mov Into VSCode Drag the renamed .mov file into the VSCode window and the convert_to_video_frames project window as shown below.\nFigure 5 \u0026middot; The Renamed Screen Capture 3) Run the main.py Script Running the main.py script using a command of the form shown below processes the specified .mov file to create a large number of .png image frames. The images are saved in a directory with the same name as the .mov file as shown in the figure below.\nFigure 6 \u0026middot; Running the main.py Script The command used in the above figure was:\npython main.py -p \u0026#34;Creating-Azure-Static-Web-App.mov\u0026#34; -r 50 -s \u0026#34;.png\u0026#34; That command syntax is documented in the convert one video section of the convert_video_to_frames project README.md file.\n4) Delete Unnecessary Images and Add Annotations in VSCode My configuration of VSCode is equipped with a simple image editor extension, so in the next step in my workflow I use that extension to browse through the images. While browsing I delete any images that I don\u0026rsquo;t need. I frequently add simple annotations \u0026ndash; like the red boxes and lines seen in the previous figure \u0026ndash; to those images I want to use.\nSorry, I didn\u0026rsquo;t capture any screen images from this culling and annotation process. \u0026#x1f626;\n5) Upload Necessary Images to Azure Storage Lots of .png images should not be pushed to GitHub, so I typically push the images to Azure Storage and reference them in figure shortcodes like the one shown above in Conversion Workflow.\nI\u0026rsquo;ve established a procedure that works nicely for adding a directory of images to Azure Storage. It looks something like this:\nIn VSCode navigate to the directory containing necessary images, right-click on that directory name, and choose Upload to Azure Storage... from the pop-up menu as illustrated in the figure below. Figure 7 \u0026middot; Right Click on the Directory and Choose Azure Again in VSCode, choose a Storage Account \u0026ndash; this is usually sddocs for me \u0026ndash; from the drop-down list that appears at the top of the window. Figure 8 \u0026middot; Choose a Storage Account In the Select resource type drop-down at the top of the window, select Blob Containers as illustrated below. Figure 9 \u0026middot; Select Blob Containers Resource Type Next, choose a Blob Container \u0026ndash; this is usually documentation for me \u0026ndash; from the Select Blob Container drop-down list that appears at the top of the window as illustrated below. Figure 10 \u0026middot; Choose a Blob Container In the Enter the destination directory... leave the / and press Enter to select it as illustrated below. This will preserve the name of your selected local directory in Azure Storage. Figure 11 \u0026middot; Enter the Destination Directory The Azure: Activity Log screen in VSCode should now reflect the status of the upload, and a pop-up message may appear in the lower-right corner of the window as illustrated below. Figure 12 \u0026middot; Check the Azure Activity Log When the upload is complete the Azure: Activity Log screen in VSCode should indicate this as will the pop-up message in the lower-right corner of the window. Figure 13 \u0026middot; Upload is Complete You can check the upload status using the Azure extension on the left side of the VSCode window. Figure 14 \u0026middot; Check the Upload Using the Azure Extension In the Azure extension navigation pane expand the Resources element, the subordinate subscription element, the Storage accounts element, and Blob Container + directory structure to find the destination directory. Figure 15 \u0026middot; Select the Destination Directory You should be able to navigate and find the uploaded files to confirm that the upload was a success. Figure 16 \u0026middot; Verifying the Upload If needed, you can retrieve the URL of the Azure resource by right-clicking on the filename and choosing the Copy URL element in the drop-down. Read on to understand why having the Azure URL is important! Figure 17 \u0026middot; Copy An Image\u0026rsquo;s URL An Even Better Approach The workflow documented above seems to work very well, but really needed one big improvement before I could comfortably adopt it. Specifically, I needed a script to scan a local directory of frame images like the one shown on the left in the figure below.\nmake-markdown.py Added to convert_videos_to_frames Mackenzie and I created the make-markdown.py Python script inside the github.com/SummittDweller/convert_videos_to_frames project. The figure below illustrates an example of running make-markdown.py on a local directory of images named Better-Documentation/. The specific command in that example was:\npython make-markdown.py -d \u0026#34;Better-Documentation/\u0026#34; -e \u0026#34;.png\u0026#34; -u \u0026#34;https://sddocs.blob.core.windows.net/documentation/Better-Documentation/\u0026#34; Running make-markdown.py with a -h or --help flag prints the script\u0026rsquo;s help text like so:\n(.venv) ╭─mark@Marks-Mac-Mini ~/GitHub/convert_videos_to_frames ‹main*› ╰─$ python make-markdown.py -h usage: make-markdown.py [-h] [-d DIR] [-e EXT] [-u URL] options: -h, --help show this help message and exit -d DIR, --dir DIR path of images directory to generate markdown from -e EXT, --ext EXT the extension of images to process: \u0026#39;jpg\u0026#39;, \u0026#39;png\u0026#39; or \u0026#39;tif\u0026#39; -u URL, --url URL Azure prefix/address to be generated Figure 18 \u0026middot; Running make-markdown.py make-markdown.py Output A successful run of make-markdown.py produces a .md file named after the --dir argument, the name of the frame image directory that is processed. That output .md file contains:\nYAML front matter with information about the --dir and run, and a list of figure references to Azure Storage objects, one line for each frame image found in --dir. The truncated output of the example in the figure above reads like this:\nFigure 19 \u0026middot; Sample make-markdown.py Output I recommend using the .md file generated by make-markdown.py as the start of your documentation! Just be sure to cull and annotate the images in your local --dir before you upload them to Azure, and before you run the make-markdown.py script.\nThat\u0026rsquo;s all for now folks!\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/138-creating-better-documentation/","tags":["documentation","Azure","command-shift-5","screen capture","convert_videos_to_frames","make-markdown.py"],"title":"Creating Better Documentation"},{"categories":null,"contents":"Digital.Grinnell\u0026rsquo;s Islandora lifespan will most likely come to an end this year, or at least in the early part 2024. So, I\u0026rsquo;m adopting a new, lean and mean process for updating it from this point forward. Basically the process will involve backing up the code that\u0026rsquo;s already in place, then using drush up to upgrade the Drupal modules and core if necessary.\nThat process on January 19, 2023, went something like this\u0026hellip;\nvSphere Snapshot In case of catastrophic failure I first elected to open my VPN then a window into VMware® vSphere. Once inside I took a \u0026ldquo;snapshot\u0026rdquo; of the DGDocker1 server to preserve as an emergency backup.\nBackup DG\u0026rsquo;s html Directory Next, to safeguard the Drupal code at a module/file level I made a backup copy of the Apache container\u0026rsquo;s /var/www/html directory and subdirs like so\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~ ╰─$ ssh dgdocker1 ... output removed for clarity ... [islandora@dgdocker1 ~]$ docker exec -it isle-apache-dg bash root@df92a99d657a:/# cd /var/www/html root@df92a99d657a:/var/www/html# cp -fr . /mnt/storage/html-backup/. --verbose ... output removed for clarity ... drush up Next, the all-important drush up command, complete with unabridged output, from inside the Apache container\u0026hellip;\nroot@df92a99d657a:/var/www/html# cd /var/www/html/sites/default/ root@df92a99d657a:/var/www/html/sites/default# drush up Update information last refreshed: Thu, 2023-01-19 11:15 Name Installed Version Proposed version Message Drupal 7.87 7.94 SECURITY UPDATE available Views Bulk Operations (views_bulk_operations) 7.x-3.6 7.x-3.7 Update available Colorbox (colorbox) 7.x-2.15 7.x-2.17 SECURITY UPDATE available Date (date) 7.x-2.12 7.x-2.14 Update available Field Group (field_group) 7.x-1.6 7.x-1.8 Update available Link (link) 7.x-1.9 7.x-1.11 SECURITY UPDATE available SMTP Authentication Support (smtp) 7.x-1.7 7.x-1.9 Update available Views (views) 7.x-3.25 7.x-3.28 Update available NOTE: A security update for the Drupal core is available. Drupal core will be updated after all of the non-core projects are updated. Security and code updates will be made to the following projects: Views Bulk Operations (VBO) [views_bulk_operations-7.x-3.7], Colorbox [colorbox-7.x-2.17], Date [date-7.x-2.14], Field Group [field_group-7.x-1.8], Link [link-7.x-1.11], SMTP Authentication Support [smtp-7.x-1.9], Views (for Drupal 7) [views-7.x-3.28] Note: A backup of your project will be stored to backups directory if it is not managed by a supported version control system. Note: If you have made any modifications to any file that belongs to one of these projects, you will have to migrate those modifications after updating. Do you really want to continue with the update process? (y/n): y Project views_bulk_operations was updated successfully. Installed version is now 7.x-3.7. Backups were saved into the directory /root/drush-backups/digital_grinnell/20230119174513/modules/views_bulk_operations. [ok] Project colorbox was updated successfully. Installed version is now 7.x-2.17. Backups were saved into the directory /root/drush-backups/digital_grinnell/20230119174513/modules/colorbox. [ok] Project date was updated successfully. Installed version is now 7.x-2.14. Backups were saved into the directory /root/drush-backups/digital_grinnell/20230119174513/modules/date. [ok] Project field_group was updated successfully. Installed version is now 7.x-1.8. Backups were saved into the directory /root/drush-backups/digital_grinnell/20230119174513/modules/field_group. [ok] Project link was updated successfully. Installed version is now 7.x-1.11. Backups were saved into the directory /root/drush-backups/digital_grinnell/20230119174513/modules/link. [ok] Project smtp was updated successfully. Installed version is now 7.x-1.9. Backups were saved into the directory /root/drush-backups/digital_grinnell/20230119174513/modules/smtp. [ok] Project views was updated successfully. Installed version is now 7.x-3.28. Backups were saved into the directory /root/drush-backups/digital_grinnell/20230119174513/modules/views. [ok] Code updates will be made to drupal core. WARNING: Updating core will discard any modifications made to Drupal core files, most noteworthy among these are .htaccess and robots.txt. If you have made any modifications to these files, please back them up before updating so that you can re-create your modifications in the updated version of the file. Note: Updating core can potentially break your site. It is NOT recommended to update production sites without prior testing. Do you really want to continue? (y/n): y Project drupal was updated successfully. Installed version is now 7.94. Backups were saved into the directory /root/drush-backups/digital_grinnell/20230119174513/drupal. [ok] System 7085 Remove FLoC-blocking variable. Link 7100 Rebuild the menu cache to make the settings page use the correct permission. Smtp 7104 Add \u0026#34;smtp_verify_peer\u0026#34;, \u0026#34;smtp_verify_peer_name\u0026#34;, \u0026#34;smtp_allow_self_signed\u0026#34; variables based on current running PHP version for most compatibility. Do you wish to run all pending updates? (y/n): y Performed update: system_update_7085 [ok] Performed update: link_update_7100 [ok] Performed update: smtp_update_7104 [ok] \u0026#39;all\u0026#39; cache was cleared. [success] Finished performing updates. [ok] root@df92a99d657a:/var/www/html/sites/default# Comparing New Files to Old The process of updating Drupal core always introduces changes in a couple of files, namely /var/www/html/.htaccess and /var/www/html/robots.txt. It\u0026rsquo;s prudent for check for differences and take steps to preserve critical portions of these files. In this instance that looked like this\u0026hellip;\nroot@df92a99d657a:/var/www/html/sites/default# cd ../../ root@df92a99d657a:/var/www/html# diff .htaccess /mnt/storage/html-backup/.htaccess 6c6 \u0026lt; \u0026lt;FilesMatch \u0026#34;\\.(engine|inc|info|install|make|module|profile|test|po|sh|.*sql|theme|tpl(\\.php)?|xtmpl)(~|\\.sw[op]|\\.bak|\\.orig|\\.save)?$|^(\\.(?!well-known).*|Entries.*|Repository|Root|Tag|Template|composer\\.(json|lock)|web\\.config)$|^#.*#$|\\.php(~|\\.sw[op]|\\.bak|\\.orig|\\.save)$\u0026#34;\u0026gt; --- \u0026gt; \u0026lt;FilesMatch \u0026#34;\\.(engine|inc|info|install|make|module|profile|test|po|sh|.*sql|theme|tpl(\\.php)?|xtmpl)(~|\\.sw[op]|\\.bak|\\.orig|\\.save)?$|^(\\.(?!well-known).*|Entries.*|Repository|Root|Tag|Template|composer\\.(json|lock)|web\\.config)$|^#.*#$|\\.php(~|\\.sw[op]|\\.bak|\\.orig\\.save)$\u0026#34;\u0026gt; 116a117,127 \u0026gt; \u0026gt; ## The following rule lifted from https://www.drupal.org/node/38960 \u0026gt; ## Implemented in April 2021 in order to redirect old object addresses of the form \u0026gt; ## drupal/fedora/repository/grinnell:182, to a proper equivalent form like \u0026gt; ## islandora/object/grinnell:182 \u0026gt; ## \u0026gt; # custom redirects \u0026gt; RewriteRule ^drupal/fedora/repository/(.+)$ https://digital.grinnell.edu/islandora/object/$1 [R=301,L] \u0026gt; # end custom redirects \u0026gt; \u0026gt; 156a168,169 \u0026gt; \u0026gt; SetEnvIf X-Forwarded-Proto https HTTPS=on root@df92a99d657a:/var/www/html# Finding no real difference in the 6c6 block above, I elected to do this with .htaccess\u0026hellip;\nroot@df92a99d657a:/var/www/html# root@df92a99d657a:/var/www/html# mv -f .htaccess .htaccess-update-January-19-2023 root@df92a99d657a:/var/www/html# cp -f /mnt/storage/html-backup/.htaccess . root@df92a99d657a:/var/www/html# diff .htaccess /mnt/storage/html-backup/.htaccess Likewise, looking at /var/www/html/robots.txt I elected to keep the old copy since no important changes had been introduced in the update. Like so\u0026hellip;\nroot@df92a99d657a:/var/www/html# diff robots.txt /mnt/storage/html-backup/robots.txt 51a52 \u0026gt; Disallow: /sites/default/files/webform/ root@df92a99d657a:/var/www/html# cp -f /mnt/storage/html-backup/robots.txt robots.txt root@df92a99d657a:/var/www/html# diff robots.txt /mnt/storage/html-backup/robots.txt Testing the Update First, a high-level look at the results\u0026hellip;\n[islandora@dgdocker1 ~]$ cd ~/ISLE [islandora@dgdocker1 ISLE]$ ll total 12 drwxrwxr-x. 4 islandora islandora 4096 Aug 2 15:04 DEPLOY drwxr-x---. 10 islandora 33 4096 Jan 19 11:51 dg-islandora drwxrwxr-x. 12 islandora islandora 4096 Sep 19 13:13 dg-isle [islandora@dgdocker1 ISLE]$ cd dg-islandora [islandora@dgdocker1 dg-islandora]$ ll total 296 -rw-r--r--. 1 root root 6604 Dec 14 10:08 authorize.php -rw-r--r--. 1 root root 118636 Dec 14 10:08 CHANGELOG.txt -rw-r--r--. 1 root root 1481 Dec 14 10:08 COPYRIGHT.txt -rw-r--r--. 1 root root 720 Dec 14 10:08 cron.php -rw-r--r--. 1 root root 2174 Jan 19 11:45 Digital-Grinnell-Migration-Mitigation-Script.sh -rw-r--r--. 1 root root 53 Jan 19 11:45 google831161f3ec8e9a73.html drwxr-xr-x. 4 root root 4096 Dec 14 10:08 includes -rw-r--r--. 1 root root 529 Dec 14 10:08 index.php -rw-r--r--. 1 root root 1717 Dec 14 10:08 INSTALL.mysql.txt -rw-r--r--. 1 root root 1874 Dec 14 10:08 INSTALL.pgsql.txt -rw-r--r--. 1 root root 722 Dec 14 10:08 install.php -rw-r--r--. 1 root root 1143 Jan 19 11:45 install_solution_packs.sh -rw-r--r--. 1 root root 1298 Dec 14 10:08 INSTALL.sqlite.txt -rw-r--r--. 1 root root 18054 Dec 14 10:08 INSTALL.txt -rw-r--r--. 1 root root 18092 Nov 16 2016 LICENSE.txt -rw-r--r--. 1 root root 8522 Dec 14 10:08 MAINTAINERS.txt -rw-r--r--. 1 root root 3554 Jan 19 11:45 migration_site_vsets.sh drwxr-xr-x. 6 root root 4096 Dec 14 10:08 misc drwxr-xr-x. 42 root root 4096 Dec 14 10:08 modules drwxr-xr-x. 5 root root 66 Dec 14 10:08 profiles -rw-r--r--. 1 root root 1173 Jan 19 11:45 README.md -rw-r--r--. 1 root root 5382 Dec 14 10:08 README.txt -rw-r--r--. 1 root root 2229 Jan 19 11:52 robots.txt drwxr-xr-x. 2 root root 4096 Dec 14 10:08 scripts drwxr-x---. 4 islandora 33 4096 Sep 29 2021 sites drwxr-xr-x. 7 root root 88 Dec 14 10:08 themes -rw-r--r--. 1 root root 19890 Dec 14 10:08 update.php -rw-r--r--. 1 root root 10123 Dec 14 10:08 UPGRADE.txt -rw-r--r--. 1 root root 2774 Dec 14 10:08 web.config -rw-r--r--. 1 root root 417 Dec 14 10:08 xmlrpc.php Then I brought the ISLE stack down and back up again\u0026hellip;\n[islandora@dgdocker1 ISLE]$ cd /home/islandora/ISLE/dg-isle; docker-compose down Stopping isle-images-dg ... done Stopping isle-apache-dg ... done Stopping isle-fedora-dg ... done Stopping isle-solr-dg ... done Stopping isle-proxy-dg ... done Stopping isle-mysql-dg ... done Stopping isle-portainer-dg ... done Removing isle-images-dg ... done Removing isle-apache-dg ... done Removing isle-fedora-dg ... done Removing isle-solr-dg ... done Removing isle-proxy-dg ... done Removing isle-mysql-dg ... done Removing isle-portainer-dg ... done Removing network dg_isle-internal Removing network dg_isle-external [islandora@dgdocker1 dg-isle]$ cd /home/islandora/ISLE/dg-isle; docker-compose up -d Creating network \u0026#34;dg_isle-internal\u0026#34; with the default driver Creating network \u0026#34;dg_isle-external\u0026#34; with the default driver Creating isle-portainer-dg ... done Creating isle-mysql-dg ... done Creating isle-proxy-dg ... done Creating isle-solr-dg ... done Creating isle-fedora-dg ... done Creating isle-apache-dg ... done Creating isle-images-dg ... done [islandora@dgdocker1 dg-isle]$ Checking on https://digital.grinnell.edu showed me that the site was \u0026ldquo;working\u0026rdquo;, but the theme was not properly in play. I\u0026rsquo;ve seen this before and research showed that it was due to Drupal caching of .htaccess settings. Time to flush the cache, so I did that using the link provided on the Digital.Grinnell home page when one is logged in with proper admin permissions.\nThe flush of the cache worked to clean up the theme, so I did a little search and facet testing to demonstrate that these operations were working, and with that I\u0026rsquo;ll declare this update to be DONE!\nThis probably isn\u0026rsquo;t the last time for an Islandora update, but for now\u0026hellip; that\u0026rsquo;s a wrap.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/137-updating-digital.grinnell-one-more-time.md/","tags":["Digital.Grinnell","Islandora","ISLE","drush"],"title":"Updating Digital.Grinnell (in Islandora) One More Time"},{"categories":null,"contents":"This post was written as follow-up to my previous post where I implemented a custom Hugo shortcode, attachments.html, documented in attachments.html below. The implementation of this shortcode required a Hugo Page Bundles content structure and the transition to such a structure is documented below in Page Bundles Structure.\nattachments.html This shortcode, attachments.html, was lifted from Hugo Attachment shortcode, a blog post by Nelis Oostens. Successfull implementation of this shortcode one minor modification (my theme did not have a referenced partial) and conversion of my content/posts from individual Markdown (.md) files to a to Page Bundles Structure, as described below.\nPage Bundles Structure Transitioning this blog to a page bundles organizational strucuture involved running the https://discourse.gohugo.io/t/bash-script-to-convert-hugo-content-files-to-page-bundles/9776 script which I named page-bundles.sh. I was only interested in transforming the content/posts portion of this blog so I ran the script like so:\ncd ~/GitHub/dlad-blog/content/posts ./page-bundles.sh The fact that you are able to read this post is proof that the script worked.\nThe page-bundles.sh script is listed here:\nfor FILE in *.md do # remove the last dot and subsequent chars to name the folder from the .md DIR=\u0026#34;${FILE%.*}\u0026#34; mkdir -p \u0026#34;$DIR\u0026#34; mv \u0026#34;$FILE\u0026#34; \u0026#34;$DIR\u0026#34; done find ./ -iname \u0026#39;*.md\u0026#39; -execdir mv -i \u0026#39;{}\u0026#39; index.md \\; PDF Creation Before wrapping this up, it\u0026rsquo;s worth mentioning that the .pdf attachments you see in my previous post were created from Markdown (.md) files using VSCode. Specifically, I installed and used the Markdown PDF extension for VSCode. It\u0026rsquo;s really cool, so I suspect you\u0026rsquo;ll be seeing more \u0026ldquo;attachments\u0026rdquo; in my posts now that I have it.\nThere will probably be more before long, but for now\u0026hellip; that\u0026rsquo;s a wrap.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/136-hugo-attachments-and-bundles/","tags":["Hugo","attachments","page bundles","VSCode"],"title":"Hugo: Attachments and Bundles"},{"categories":null,"contents":"Portions of this post build on concepts introduced in Managing Azure.\nCATPAW - Computer-Aided Thinking, Primarily about Writing From the CATPAW home screen\u0026hellip;\nIn many ways, CATPAW is an online book about writing style\u0026ndash;a guide to the choices we make in writing that connect us to our readers.\nRather than setting out rules to follow, CATPAW will help you make informed choices in context. The site accomplishes that goal in three ways:\nIt explains the choices writers face. It uses computational tools to help you examine your own writing, letting you see what choices you have already made and what you might want to do differently. It places these choices in the context of advice from other prominent guides to writing. Technical Notes CATPAW is a Python Flask web application that employs a number of Python packages including nltk: the Natural Language Toolkit.\nProject History CATPAW was created by Professor Erik Simpson with initial programming assistance from Alina Guha and myself. Throughout its history, CATPAW development and collaboration has engaged a git workflow with code updates posted to GitHub private repositories.\nInitial Development and Deployment to Azure The first CATPAW code repository is/was https://github.com/alinejg/catpaw. This repo was initially deployed to the web via Azure under a \u0026ldquo;trial\u0026rdquo; account owned by Alina. When the trial subscription ended we took steps to move the deployment from Azure to Reclaim Cloud to avoid accumulating fees on Alina\u0026rsquo;s Azure account.\nThe README.md file from the aforementioned original project documents early development and the move to Reclaim Cloud. That original README.md was exported to a file named README-original.pdf that\u0026rsquo;s stored in our current development repo CATPAW-Azure below. The PDF is available for download in this post\u0026rsquo;s Attachments section for convenience.\nCATPAW Deployment to Reclaim Cloud Deployment to Reclaim Cloud provided us with very few options, and all of them incurred fees throughout the development lifecycle. Fortunately, the fees were not too significant and they were covered with budget and credits secured by the DLAC.\nOur deployment to Reclaim Cloud also encountered two technical challenges:\nDue to the nature of Flask and its built-in webserver, we had to maintain two different versions of the code, one for the local/development websever, and a second copy for any remote/deployed using a wsgi interface. A pair of .sh scripts were created to manually switch between versions. Specific differences between versions are documented in the aforementioned README-original.pdf provided below.\nRecently, the addition of new nltk elements and a pandas.DataFrame object introduced a configuration where the code would run locally, but the modified wsgi version would not successfully deploy to Reclaim Cloud.\nCATPAW-Azure While addressing technical problem #2, above, we discovered that returning to Azure might provide a more flexible and free, or very low-cost, deployment alternative to Reclaim Cloud, and new Azure VSCode extensions could be leveraged to simplify the move. Furthermore, these VSCode extensions provide nearly seemless integration with Azure App Service, and most importantly, there is no need to maintain separate local and wsgi versions of code when deploying to Azure. Yay!\nA search of the web for \u0026ldquo;azure python flask\u0026rdquo; returned a host of promising articles and I choose to create a new CATPAW project repo, https://github.com/Digital-Grinnell/catpaw-azure, and to develop it using the guidance found in Deploying Flask web app on Microsoft Azure., and later in Quickstart: Deploy a Python (Django or Flask) web app to Azure App Service.\nThe original description of https://github.com/Digital-Grinnell/catpaw-azure was:\nA restart of CATPAW work from 2022, this time destined for dev deployment in Azure App Service.\nThe README.md file from the development branch of https://github.com/Digital-Grinnell/catpaw-azure explains some of the repositories\u0026rsquo; early history and it can be downloaded here as README-catpaw-azure.pdf.\npaperclip \u0026nbsp; Attachments README-catpaw-azure.pdf download (151kb) README-original.pdf download (718kb) Pandas vs Polars Also while addressing problem #2 (see above) we found that a successful local build using the Pandas dataframe class could not be successfully deployed to Reclaim Cloud, nor to Azure. The Azure error messages hinted at the platforms inability to \u0026ldquo;build\u0026rdquo; Pandas due to a missing python.h file. Also, the build time in Azure with Pandas included was 10x the time when building the app without Pandas.\nI tried 8 different deployment configurations in hopes of making Pandas work in Azure, but they all failed with the same error. So, rather than perpetuating failures I elected to look for an alternative to Pandas to determine if that was indeed the source of the error. It was.\nUltimately I choose to rewrite a small portion of the app.py code to remove Pandas and replace it with \u0026ldquo;equivalent\u0026rdquo; code using the Polars DataFrame Library. Polars not only works, both locally and in Azure, but its associated build-time is one tenth what Pandas required.\nAt the time of this writing (2023-01-12T12:44:01-06:00) the development branch of catpaw-azure is using Polars, while the main branch still employs the broken Pandas logic. I expect a merge of these branches and adoption of Polars soon.\nThere may be more to come, but for now\u0026hellip; that\u0026rsquo;s a wrap.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/135-migrating-catpaw-development-to-azure/","tags":["migration","CATPAW","Azure","Reclaim Cloud","Python","Flask","VSCode"],"title":"Migrating CATPAW Development to Azure"},{"categories":null,"contents":"This post is essentially a CollectionBuilder-CSV follow-up to Creating a Migration Collection, intended to document the path I\u0026rsquo;ve taken and the decisions I made when creating a first cut of Digital.Grinnell content using the aformentioned CollectionBuilder-CSV.\nCB-CSV_DG-01 With few notable exceptions, everything mentioned below will be visible in a new public GitHub repo at Digital-Grinnell/CB-CSV_DG-01.\nCorresponding Google Sheet One of the exceptions: the project\u0026rsquo;s metadata CSV in a time-stamped tab at https://docs.google.com/spreadsheets/d/1ic4PxHDbuzDrmf4YtauhC4vEQJxt3QSH8bYfLBCM3Gc/.\nOther worksheets/tabs in the Google Sheet contain:\nSheet 1 = the initial imported demo data from the CollectionBuilder-CSV Metadata Template\nmods.csv = original MODS metadata .csv as exported from Digital.Grinnell and described in Creating a Migration Collection.\nCreating the Development Environment The steps required to create a CollectionBuilder-CSV instance are well-documented in the CollectionBuilder Docs. I selected the CollectionBuilder-CSV version of CB, and because my development enviroment will be on a Mac, I consistenly selected Mac-specific options throughout the process, including the use of Homebrew whenever it was an option.\nAn ordered list of documentation sections I followed includes:\nCollectionBuilder Docs Templates CollectionBuilder-CSV GitHub I skipped this section because I am already familiar with GitHub use. Git Setup I skipped this section because Git is already properly configured on my Mac. Get a Text Editor I skipped this section because Visual Studio Code is already installed and includes all of the extensions mentioned in the document. Install Ruby Ruby on Mac - I used all of the recommended commands with Ruby version 3.1.3. Install Jekyll Optional Software Install on Mac Set Up Project Repository I worked through all of the sub-sections here to produce the CB-CSV_DG-01 repository. Most of the work was done using VSCode and it included initial commits and push with project-specific information added to the README.md file. Adding Metadata The next task here involves properly populating the project\u0026rsquo;s metadata CSV in the ready-for-CB tab at https://docs.google.com/spreadsheets/d/1ic4PxHDbuzDrmf4YtauhC4vEQJxt3QSH8bYfLBCM3Gc/. At the time of this writing that tab is just a raw copy of our original MODS metadata .csv as exported from Digital.Grinnell and described in Creating a Migration Collection. Essentially the ready-for-CB tab needs to contain the data that it currently holds, but transformed into a structure matching that of Sheet 1, the initial imported demo data from the CollectionBuilder-CSV Metadata Template.\nUsing Open Refine? My first thought was to use Open Refine to manipulate the CSV structure and data, and I do have this tool installed on all of my Macs in case it is needed. However, I\u0026rsquo;m not a huge fan of Open Refine, in part because it is, in my opinion, Java-based and therefore bloated and cumbersome. My bigger concern is that capturing the transform \u0026ldquo;process\u0026rdquo; isn\u0026rsquo;t a natural thing in Open Refine, and I expect to repeat this same process many times over as we work through each collection of Digital.Grinnell objects. Also, over time I expect to refine and improve the transformation process so I\u0026rsquo;d like to have the logic captured in a repeatable script rather than a GUI-driven tool.\nPython? Of course! My intent is to create a Python script capable of reading and writing Google Sheet data and structures so that I can create, manage, improve, and above all, repeat my transforms. CollectionBuilder is Jekyll-based so it does not involve Hugo, but the Python scripts in my Hugo Front Matter Tools should still provide a good starting point for crafting scripts to help with this.\nUpdate: I\u0026rsquo;m going to pivot the effort described above and take a little different approach. My first efforts in Python produced transform-mods-csv-to-ready-for-CB plus a new Google Sheet at https://docs.google.com/spreadsheets/d/1ic4PxHDbuzDrmf4YtauhC4vEQJxt3QSH8bYfLBCM3Gc/. The head of the README.md file describes where I \u0026ldquo;was\u0026rdquo; going with the effort:\nThis script, evolved from rootstalk-google-sheet-to-front-matter.py from my https://github.com/Digital-Grinnell/hugo-front-matter-tools project, is designed to read all exported MODS records from the mods.csv tab of https://docs.google.com/spreadsheets/d/1ic4PxHDbuzDrmf4YtauhC4vEQJxt3QSH8bYfLBCM3Gc/ and transform that data into a new ready-for-CB tab of the same Google Sheet, but using the column heading/structure of the CollectionBuilder demo Sheet1 tab.\nThe Pivot Essentially, rather than making the ready-for-CB tab conform to CollectionBuilder\u0026rsquo;s out-of-the-box metatdata schema, I\u0026rsquo;m going to make my initial CollectionBuilder configuration conform to the schema reflected in the mods.csv tab of the aforementioned Google Sheet. Wish me luck.\nNot Going to Pivot After All Ok, I changed my mind, again. I started re-strucutring my exported MODS data into a brand new CollectionBuilder configuration as suggested in the \u0026ldquo;Pivot\u0026rdquo;, and realized that my initial approach of scripting the transformation of MODS to initially match CB\u0026rsquo;s out-of-the-box \u0026ldquo;demo\u0026rdquo; schema was a better idea after all. So, I did just that, and this morning I have a working script, transform-mods-csv-to-ready-for-CB and a transformed set of data now in the aformentioned Google Sheet at https://docs.google.com/spreadsheets/d/1ic4PxHDbuzDrmf4YtauhC4vEQJxt3QSH8bYfLBCM3Gc/.\nTime now to reconfig CB-CSV_DG-01 and point it to the new .csv data.\nMetadata Changes With our metadata stored in the latest time-stamped tab at https://docs.google.com/spreadsheets/d/1ic4PxHDbuzDrmf4YtauhC4vEQJxt3QSH8bYfLBCM3Gc/, I exported the Google Sheet to a transformed.csv file and dropped that file into this repo\u0026rsquo;s _data directory as prescribed in the README.md documentation.\nConfig Changes After exporting and depoisting the transformed.csv I incrementally made changes to the project\u0026rsquo;s _config.yml file, and others as described below, regenerating a new CB instance with each change to the config.\nChange Collection Settings: metadata I changed the last line of _config.yml from\u0026hellip;\n########## # COLLECTION SETTINGS # # Set the metadata for your collection (the name of the CSV file in your _data directory that describes the objects in your collection) # Use the filename of your CSV **without** the \u0026#34;.csv\u0026#34; extension! E.g. _data/demo-metadata.csv --\u0026gt; \u0026#34;demo-metadata\u0026#34; metadata: demo-metadata \u0026hellip;to\u0026hellip;\nmetadata: transformed The result wasn\u0026rsquo;t great, it included a host of \u0026ldquo;Notice\u0026rdquo; messages like the one shown below.\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/CB-CSV_DG-01 ‹main●› ╰─$ bundle exec jekyll s Configuration file: /Users/mcfatem/GitHub/CB-CSV_DG-01/_config.yml Source: /Users/mcfatem/GitHub/CB-CSV_DG-01 Destination: /Users/mcfatem/GitHub/CB-CSV_DG-01/_site Incremental build: disabled. Enable with --incremental Generating... Error cb_helpers: Item for featured image with objectid \u0026#39;demo_001\u0026#39; not found in configured metadata \u0026#39;transformed\u0026#39;. Please check \u0026#39;featured-image\u0026#39; in \u0026#39;_data/theme.yml\u0026#39; Notice cb_page_gen: record \u0026#39;0\u0026#39; in \u0026#39;transformed\u0026#39;, \u0026#39;grinnell:10023\u0026#39; is being sanitized to create a valid filename. This may cause issues with links generated on other pages. Please check the naming convention used in \u0026#39;objectid\u0026#39; field. ... Clearly, the objectid values I\u0026rsquo;m writing to the metadata .csv file need to be improved. I\u0026rsquo;m making that change in the transform-mods-csv-to-ready-for-CB script now.\nAfter Sanitizing the objectid Field After the change I got this output\u0026hellip;\nAuto-regeneration: enabled for \u0026#39;/Users/mcfatem/GitHub/CB-CSV_DG-01\u0026#39; Server address: http://127.0.0.1:4000 Server running... press ctrl-c to stop. [2022-12-13 12:52:26] ERROR `/items/grinnell:23517.html\u0026#39; not found. Regenerating: 1 file(s) changed at 2022-12-13 13:01:21 _data/transformed.csv Error cb_helpers: Item for featured image with objectid \u0026#39;demo_001\u0026#39; not found in configured metadata \u0026#39;transformed\u0026#39;. Please check \u0026#39;featured-image\u0026#39; in \u0026#39;_data/theme.yml\u0026#39; ...done in 0.492276 seconds. \u0026hellip;and the output looks better, but there\u0026rsquo;s still an issue with \u0026lsquo;demo_001\u0026rsquo; as a \u0026ldquo;featured item\u0026rdquo;.\nChanging the Home Page (Featured Image) in theme.yml In CB the theme.yml file is home to settings that \u0026ldquo;\u0026hellip;help configure details of individual pages in the website\u0026rdquo;. I made the following changes to that file.\nChanging the last line from this \u0026ldquo;HOME PAGE\u0026rdquo; snippet from\u0026hellip;\n########## # HOME PAGE # # featured image is used in home page banner and in meta markup to represent the collection # use either an objectid (from an item in this collect), a relative location of an image in this repo, or a full url to an image elsewhere featured-image: demo_001 \u0026hellip;to\u0026hellip;\nfeatured-image: grinnell_23345 That change automatically regenerated my local site, but this time with the following error\u0026hellip;\nRegenerating: 1 file(s) changed at 2022-12-13 13:14:09 _data/theme.yml Error cb_helpers: Item for featured image with objectid \u0026#39;grinnell_23345\u0026#39; does not have an image url in metadata. Please check \u0026#39;featured-image\u0026#39; in \u0026#39;_data/theme.yml\u0026#39; and choose an item that has \u0026#39;object_location\u0026#39; or \u0026#39;image_small\u0026#39; ...done in 0.499817 seconds. This is actually indicative of a MUCH bigger issue\u0026hellip; Apparently the object_location values that I\u0026rsquo;m providing \u0026ndash; as links to the original objects in Digital.Grinnell \u0026ndash; are not acceptable. They need to have something like /datastream/OBJ/view appened to them in order to work correctly.\nPointing CB to Digital.Grinnell Storage It\u0026rsquo;s now time to clone the filename function for a new obj function in transform-mods-csv-to-ready-for-CB so that it references exported \u0026ldquo;OBJ\u0026rdquo; objects with URLs from DG like https://digital.grinnell.edu/islandora/object/grinnell:23517/datastream/OBJ/view. I\u0026rsquo;m also going to add a thumbnail function to populate the image_thumb AND image_small metadata columns.\nWith those columns completed the site local site at http://127.0.0.1:4000/ is working, but it\u0026rsquo;s not pretty. One lesson learned\u0026hellip; the featured-image listed in _data/theme.yml MUST have a valid image_small element in order to display correctly. The error message shown above will be present until image_small is resolved.\nNext, Pushing Local Changes to Azure At this time I\u0026rsquo;m going to follow the guidance in Tutorial: Publish a Jekyll site to Azure Static Web Apps to create a shared/visible instance of the new CB site.\nDone. The process was super-simple and the results as-expected. You can see the site from the main branch of https://github.com/Digital-Grinnell/CB-CSV_DG-01 at https://purple-river-002460310.2.azurestaticapps.net/.\nThat address again:\nhttps://purple-river-002460310.2.azurestaticapps.net/\nThe GitHub Action driving the build and deployment of the main branch reads like this:\nname: Azure Static Web Apps CI/CD on: push: branches: - main pull_request: types: [opened, synchronize, reopened, closed] branches: - main jobs: build_and_deploy_job: if: github.event_name == \u0026#39;push\u0026#39; || (github.event_name == \u0026#39;pull_request\u0026#39; \u0026amp;\u0026amp; github.event.action != \u0026#39;closed\u0026#39;) runs-on: ubuntu-latest name: Build and Deploy Job steps: - uses: actions/checkout@v2 with: submodules: true - name: Build And Deploy id: builddeploy uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_PURPLE_RIVER_002460310 }} repo_token: ${{ secrets.GITHUB_TOKEN }} # Used for Github integrations (i.e. PR comments) action: \u0026#34;upload\u0026#34; ###### Repository/Build Configurations - These values can be configured to match your app requirements. ###### # For more information regarding Static Web App workflow configurations, please visit: https://aka.ms/swaworkflowconfig app_location: \u0026#34;/\u0026#34; # App source code path api_location: \u0026#34;\u0026#34; # Api source code path - optional output_location: \u0026#34;_site\u0026#34; # Built app content directory - optional ###### End of Repository/Build Configurations ###### close_pull_request_job: if: github.event_name == \u0026#39;pull_request\u0026#39; \u0026amp;\u0026amp; github.event.action == \u0026#39;closed\u0026#39; runs-on: ubuntu-latest name: Close Pull Request Job steps: - name: Close Pull Request id: closepullrequest uses: Azure/static-web-apps-deploy@v1 with: azure_static_web_apps_api_token: ${{ secrets.AZURE_STATIC_WEB_APPS_API_TOKEN_PURPLE_RIVER_002460310 }} action: \u0026#34;close\u0026#34; We Need a 2nd Azure Instance So, I\u0026rsquo;ve created a new develop branch with it\u0026rsquo;s own GitHub Action and deployment to Azure at:\nhttps://gentle-pond-02af90f10.2.azurestaticapps.net\nIntroducing Oral Histories So the convention in CollectionBuilder is for every value of display_template there should be a corresponding .html template in _layouts/item with the same name, and that template will be used to render the object and control the object\u0026rsquo;s individual page behavior. The documentation also says that any display_template value that does not have a corresponding _layouts/item template will assume a type of item.\nI decided to test that in the new develop branch. So, I first changed the display_template or our \u0026ldquo;grinnell_19423\u0026rdquo; object, an oral history interview, from audio to test. Sure enough, it rendered as an item as promised.\nNext, I copied _layouts/item/audio.html, the audio template, and gave the copy a name of _layouts/item/oral-history.html. I made no changes to the template. Then I altered our transformed.csv data to give \u0026ldquo;grinnell_19423\u0026rdquo; a display_template value of oral-history, matching the name of the new template. Did it work? You betcha! The object is now rendered like an audio object since that\u0026rsquo;s what oral-history.html does.\nKudos to the authors of CollectionBuilder. Well played!\nWill .obj Filename Extensions Work? There\u0026rsquo;s evidence in the few tests I\u0026rsquo;ve run thus far that the extension on the end of a filename makes no difference in how, or if, the object is rendered. Let\u0026rsquo;s test that a little further by changing the extension on a couple of cloned objects stored in Azure to .obj, and altering the transformed.csv file to point to them instead of to Digital.Grinnell. The objects I\u0026rsquo;m going to alter are \u0026ldquo;grinnell_19423\u0026rdquo;, an oral-history type with a .mp3 extension, and \u0026ldquo;grinnell_16934\u0026rdquo;, an image with a .jpg extension. The new URLs for these cloned items are:\nhttps://migrationtestcollection.blob.core.windows.net/migration-test/grinnell_19423_OBJ.obj https://migrationtestcollection.blob.core.windows.net/migration-test/grinnell_16934_OBJ.obj Huzzah, it works! Beautimous!\nThat solves the need for applying proper filename extensions to Digital.Grinnell objects that have none! The only problem with this .obj approach is that downloaded objects might not behave as expected, but we\u0026rsquo;ll cross that bridge when we come to it.\nI\u0026rsquo;m sure there will be more here soon, but for now\u0026hellip; that\u0026rsquo;s a wrap.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/134-creating-a-collectionbuilder-csv-instance-from-our-migration-collection/","tags":["migration","collection","Digital.Grinnell","CollectionBuilder-CSV"],"title":"Creating a CollectionBuilder-CSV Instance from Our Migration Collection"},{"categories":null,"contents":"What follows is a January 2022 excerpt from a piece of Rootstalk project documentation titled Automated-Testing.md\u0026hellip;\nAutomated Testing Today I started a little side-project aimed at helping test or \u0026ldquo;proof\u0026rdquo; the Rootstalk structure and content. I\u0026rsquo;m attempting to use the package/process documented in this GitHub repo.\nI started on the command-line of my MacBook Pro like so:\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹main› ╰─$ gem install html-proofer Fetching yell-2.2.2.gem Fetching nokogiri-1.13.1-arm64-darwin.gem Fetching rainbow-3.1.1.gem Fetching ethon-0.15.0.gem Fetching html-proofer-3.19.3.gem Fetching typhoeus-1.4.0.gem Fetching parallel-1.21.0.gem Successfully installed yell-2.2.2 Successfully installed ethon-0.15.0 Successfully installed typhoeus-1.4.0 Successfully installed rainbow-3.1.1 Successfully installed parallel-1.21.0 Successfully installed nokogiri-1.13.1-arm64-darwin Successfully installed html-proofer-3.19.3 Parsing documentation for yell-2.2.2 Installing ri documentation for yell-2.2.2 Parsing documentation for ethon-0.15.0 Installing ri documentation for ethon-0.15.0 Parsing documentation for typhoeus-1.4.0 Installing ri documentation for typhoeus-1.4.0 Parsing documentation for rainbow-3.1.1 Installing ri documentation for rainbow-3.1.1 Parsing documentation for parallel-1.21.0 Installing ri documentation for parallel-1.21.0 Parsing documentation for nokogiri-1.13.1-arm64-darwin Installing ri documentation for nokogiri-1.13.1-arm64-darwin Parsing documentation for html-proofer-3.19.3 Installing ri documentation for html-proofer-3.19.3 Done installing documentation for yell, ethon, typhoeus, rainbow, parallel, nokogiri, html-proofer after 2 seconds 7 gems installed Then, as suggested in the tool\u0026rsquo;s README.md file\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹main› ╰─$ htmlproofer --help htmlproofer 3.19.3 -- Runs the HTML-Proofer suite on the files in PATH. For more details, see the README. Usage: htmlproofer PATH [options] Options: --allow-missing-href If `true`, does not flag `a` tags missing `href` (this is the default for HTML5). --allow-hash-href If `true`, ignores the `href=\u0026#34;#\u0026#34;` --as-links Assumes that `PATH` is a comma-separated array of links to check. --alt-ignore image1,[image2,...] A comma-separated list of Strings or RegExps containing `img`s whose missing `alt` tags are safe to ignore --assume-extension Automatically add extension (e.g. `.html`) to file paths, to allow extensionless URLs (as supported by Jekyll 3 and GitHub Pages) (default: `false`). --checks-to-ignore check1,[check2,...] A comma-separated list of Strings indicating which checks you do not want to run (default: `[]`) --check-external-hash Checks whether external hashes exist (even if the webpage exists). This slows the checker down (default: `false`). --check-favicon Enables the favicon checker (default: `false`). --check-html Enables HTML validation errors from Nokogumbo (default: `false`). --check-img-http Fails an image if it\u0026#39;s marked as `http` (default: `false`). --check-opengraph Enables the Open Graph checker (default: `false`). --check-sri Check that `\u0026lt;link\u0026gt;` and `\u0026lt;script\u0026gt;` external resources use SRI (default: `false`). --directory-index-file \u0026lt;filename\u0026gt; Sets the file to look for when a link refers to a directory. (default: `index.html`) --disable-external If `true`, does not run the external link checker, which can take a lot of time (default: `false`) --empty-alt-ignore If `true`, ignores images with empty alt tags --error-sort \u0026lt;sort\u0026gt; Defines the sort order for error output. Can be `:path`, `:desc`, or `:status` (default: `:path`). --enforce-https Fails a link if it\u0026#39;s not marked as `https` (default: `false`). --extension \u0026lt;ext\u0026gt; The extension of your HTML files including the dot. (default: `.html`) --external_only Only checks problems with external references --file-ignore file1,[file2,...] A comma-separated list of Strings or RegExps containing file paths that are safe to ignore --http-status-ignore 123,[xxx, ...] A comma-separated list of numbers representing status codes to ignore. --internal-domains domain1,[domain2,...] A comma-separated list of Strings containing domains that will be treated as internal urls. --ignore-empty-mailto If `true`, allows `mailto:` `href`s which do not contain an email address --report-invalid-tags When `check_html` is enabled, HTML markup that is unknown to Nokogumbo are reported as errors (default: `false`) --report-missing-names When `check_html` is enabled, HTML markup that are missing entity names are reported as errors (default: `false`) --report-script-embeds When `check_html` is enabled, `script` tags containing markup are reported as errors (default: `false`) --report-missing-doctype When `check_html` is enabled, HTML markup with missing or out-of-order `DOCTYPE` are reported as errors (default: `false`) --report-eof-tags When `check_html` is enabled, HTML markup with tags that are malformed are reported as errors (default: `false`) --report-mismatched-tags When `check_html` is enabled, HTML markup with mismatched tags are reported as errors (default: `false`) --log-level \u0026lt;level\u0026gt; Sets the logging level, as determined by Yell. One of `:debug`, `:info`, `:warn`, `:error`, or `:fatal`. (default: `:info`) --only-4xx Only reports errors for links that fall within the 4xx status code range --storage-dir PATH Directory where to store the cache log (default: \u0026#34;tmp/.htmlproofer\u0026#34;) --timeframe \u0026lt;time\u0026gt; A string representing the caching timeframe. --typhoeus-config CONFIG JSON-formatted string of Typhoeus config. Will override the html-proofer defaults. --hydra-config CONFIG JSON-formatted string of Hydra config. Will override the html-proofer defaults. --url-ignore link1,[link2,...] A comma-separated list of Strings or RegExps containing URLs that are safe to ignore. It affects all HTML attributes. Note that non-HTTP(S) URIs are always ignored --url-swap re:string,[re:string,...] A comma-separated list containing key-value pairs of `RegExp =\u0026gt; String`. It transforms URLs that match `RegExp` into `String` via `gsub`. The escape sequences `\\:` should be used to produce literal `:`s. --root-dir PATH The absolute path to the directory serving your html-files. -h, --help Show this message -v, --version Print the name and version -t, --trace Show the full backtrace when an error occurs Next\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹main●› ╰─$ hugo Start building sites … hugo v0.87.0+extended darwin/arm64 BuildDate=unknown WARN 2022/01/28 11:48:18 Page.Hugo is deprecated and will be removed in a future release. Use the global hugo function. .File.UniqueID on zero object. Wrap it in if or with: {{ with .File }}{{ .UniqueID }}{{ end }} | EN -------------------+------ Pages | 257 Paginator pages | 10 Non-page files | 172 Static files | 28 Processed images | 0 Aliases | 62 Sitemaps | 1 Cleaned | 0 Total in 773 ms And finally\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹main●› ╰─$ time htmlproofer ./public Running [\u0026#34;ScriptCheck\u0026#34;, \u0026#34;LinkCheck\u0026#34;, \u0026#34;ImageCheck\u0026#34;] on [\u0026#34;./public\u0026#34;] on *.html... Checking 1405 external links... ... * External link https://patch.com/iowa/across-ia/lawsuit-iowa-school-juvenile-offend-ers-misusing-drugs failed: 404 No error * image /images/rootstalk_leaf.svg does not have an alt attribute (line 389) * image https://rootstalk.blob.core.windows.net/rootstalk-2021-spring/grinnell_29963_OBJ.jpg does not have an alt attribute (line 743) * image https://rootstalk.blob.core.windows.net/rootstalk-2021-spring/grinnell_29999_OBJ.jpg does not have an alt attribute (line 132) * linking to internal hash #ref37 that does not exist (line 573) \u0026lt;a href=\u0026#34;#ref37\u0026#34;\u0026gt;\u0026lt;sup\u0026gt;37\u0026lt;/sup\u0026gt;\u0026lt;/a\u0026gt; * linking to internal hash #ref38 that does not exist (line 578) \u0026lt;a href=\u0026#34;#ref38\u0026#34;\u0026gt;\u0026lt;sup\u0026gt;38\u0026lt;/sup\u0026gt;\u0026lt;/a\u0026gt; - ./public/volume-vii-issue-2/taylor/index.html * External link https://rootstalk-archive.grinnell.edu failed: response code 0 means something\u0026#39;s wrong. It\u0026#39;s possible libcurl couldn\u0026#39;t connect to the server or perhaps the request timed out. Sometimes, making too many requests at once also breaks things. Either way, the return message (if any) from the server is: SSL peer certificate or SSH remote key was not OK * External link https://www.census.gov/quickfacts/fact/table/sanmarcoscitytexas/PST120219 failed: 404 No error * image /images/rootstalk_leaf.svg does not have an alt attribute (line 225) * image https://rootstalk.blob.core.windows.net/rootstalk-2021-spring/grinnell_29971_OBJ.jpg does not have an alt attribute (line 132) * image https://rootstalk.blob.core.windows.net/rootstalk-2021-spring/grinnell_29972_OBJ.jpg does not have an alt attribute (line 255) HTML-Proofer found 1005 failures! htmlproofer ./public 7.86s user 1.81s system 29% cpu 32.993 total The output was too big to copy/paste above so I\u0026rsquo;m just showing an abridged version there. Finding no \u0026ldquo;output file\u0026rdquo; option for htmlproofer I elected the following approach in order to get a manageable body of output:\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹main●› ╰─$ time htmlproofer ./public \u0026gt; htmlproofer-28-jan-2022.out 2\u0026gt;\u0026amp;1 htmlproofer ./public \u0026gt; htmlproofer-28-jan-2022.out 2\u0026gt;\u0026amp;1 7.77s user 1.68s system 29% cpu 32.112 total Improving the Workflow The process documented above worked very well, but making the output part of the Rootstalk project repo is a problem, that repo isn\u0026rsquo;t accessible to everyone on the project, but the htmlproofer output really should be! So, I think a couple of improvements are needed:\nThe output should reside in the Rootstalk project documentation container in Azure blob storage, where it will have a consistent and publicly-accessible URL.\nSince the current output is always in a specific file, namely rootstalk-html-proofer.out, that file should contain a timestamp indicating when it was created/updated.\nFirst, An Updated Install If you look closely above you\u0026rsquo;ll see that my original install of htmlproofer was version 3.19.3 and that install is only on my GC MacBook Pro. So before going farther, I\u0026rsquo;m going to attempt to update that installation to version 4.x which is described in more detail at UPGRADING.md. Unfortunately, this document doesn\u0026rsquo;t suggest \u0026ldquo;how\u0026rdquo; to upgrade so I suppose a re-install using README.md is called for.\nNow, per the README.md file\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub ╰─$ gem install html-proofer Fetching zeitwerk-2.6.6.gem Fetching html-proofer-4.4.3.gem Successfully installed zeitwerk-2.6.6 Successfully installed html-proofer-4.4.3 Parsing documentation for zeitwerk-2.6.6 Installing ri documentation for zeitwerk-2.6.6 Parsing documentation for html-proofer-4.4.3 Installing ri documentation for html-proofer-4.4.3 Done installing documentation for zeitwerk, html-proofer after 0 seconds 2 gems installed Hmmm, that still looks like an old version? But this seems to indicate otherwise\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹develop› ╰─$ htmlproofer -v htmlproofer 4.4.3 Getting Help That\u0026rsquo;s easy\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹develop› ╰─$ htmlproofer -h htmlproofer 4.4.3 -- Runs the HTML-Proofer suite on the files in PATH. For more details, see the README. Usage: htmlproofer PATH [options] Options: --allow-hash-href=\u0026lt;true|false\u0026gt; If `true`, assumes `href=\u0026#34;#\u0026#34;` anchors are valid (default: `true`) --allow-missing-href=\u0026lt;true|false\u0026gt; If `true`, does not flag `a` tags missing `href`. In HTML5, this is technically allowed, but could also be human error. (default: `false`) --as-links Assumes that `PATH` is a comma-separated array of links to check. --assume-extension \u0026lt;ext\u0026gt; Automatically add specified extension to files for internal links, to allow extensionless URLs (as supported by most servers) (default: `.html`). --checks check1,[check2,...] A comma-separated list of Strings indicating which checks you want to run (default: `[\u0026#34;Links\u0026#34;, \u0026#34;Images\u0026#34;, \u0026#34;Scripts\u0026#34;]`) --check-external-hash=\u0026lt;true|false\u0026gt; Checks whether external hashes exist (even if the webpage exists) (default: `true`). --check-internal-hash=\u0026lt;true|false\u0026gt; Checks whether internal hashes exist (even if the webpage exists) (default: `true`). --check-sri=\u0026lt;true|false\u0026gt; Check that `\u0026lt;link\u0026gt;` and `\u0026lt;script\u0026gt;` external resources use SRI (default: `false`). --directory-index-file \u0026lt;filename\u0026gt; Sets the file to look for when a link refers to a directory. (default: `index.html`) --disable-external=\u0026lt;true|false\u0026gt; If `true`, does not run the external link checker (default: `false`) --enforce-https=\u0026lt;true|false\u0026gt; Fails a link if it\u0026#39;s not marked as `https` (default: `true`). --extensions ext1,[ext2,...[ A comma-separated list of Strings indicating the file extensions you would like to check (including the dot) (default: `.html`) --ignore-empty-alt=\u0026lt;true|false\u0026gt; If `true`, ignores images with empty/missing alt tags (in other words, `\u0026lt;img alt\u0026gt;` and `\u0026lt;img alt=\u0026#34;\u0026#34;\u0026gt;` are valid; set this to `false` to flag those) (default: `true`) --ignore-empty-mailto=\u0026lt;true|false\u0026gt; If `true`, allows `mailto:` `href`s which do not contain an email address (default: `false`) --ignore-files file1,[file2,...] A comma-separated list of Strings or RegExps containing file paths that are safe to ignore --ignore-missing-alt=\u0026lt;true|false\u0026gt; If `true`, ignores images with missing alt tags (default: `false`) --ignore-status-codes 123,[xxx, ...] A comma-separated list of numbers representing status codes to ignore. --ignore-urls link1,[link2,...] A comma-separated list of Strings or RegExps containing URLs that are safe to ignore. This affects all HTML attributes, such as `alt` tags on images. --log-level \u0026lt;level\u0026gt; Sets the logging level, as determined by Yell. One of `:debug`, `:info`, `:warn`, `:error`, or `:fatal`. (default: `:info`) --only-4xx Only reports errors for links that fall within the 4xx status code range --root-dir PATH The absolute path to the directory serving your html-files. --swap-attributes CONFIG JSON-formatted config that maps element names to the preferred attribute to check (default: `{}`). --swap-urls re:string,[re:string,...] A comma-separated list containing key-value pairs of `RegExp =\u0026gt; String`. It transforms URLs that match `RegExp` into `String` via `gsub`. The escape sequences `\\:` should be used to produce literal `:`s. --typhoeus CONFIG JSON-formatted string of Typhoeus config. Will override the html-proofer defaults. --hydra CONFIG JSON-formatted string of Hydra config. Will override the html-proofer defaults. --parallel CONFIG JSON-formatted string of Parallel config. Will override the html-proofer defaults. --cache CONFIG JSON-formatted string of cache config. Will override the html-proofer defaults. -h, --help Show this message -v, --version Print the name and version -t, --trace Show the full backtrace when an error occurs Running the New htmlproofer So, having read the help documentation I forged ahead and got this\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹develop› ╰─$ htmlproofer ./public Running 3 checks (Links, Scripts, Images) in [\u0026#34;./public\u0026#34;] on *.html files... Checking 2461 external links htmlproofer 4.4.3 | Error: Document tree depth limit exceeded I was unable to overcome that error with any provided options so I tried rebooting my Mac before running it again, and on that occasion it worked. I\u0026rsquo;ve since submitted an \u0026ldquo;Issue\u0026rdquo; with the htmlproofer project to see if others can help work around it.\nWhen I was able to successfully run htmlproofer I got something like this:\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹develop› ╰─$ htmlproofer ./public Running 3 checks (Links, Scripts, Images) in [\u0026#34;./public\u0026#34;] on *.html files... Checking 2461 external links ... * At ./public/volume-vii-issue-2/ross/index.html:585: internally linking to #ref38; the file exists, but the hash \u0026#39;ref38\u0026#39; does not * At ./public/volume-viii-issue-1/page/2/index.html:682: internally linking to .., which does not exist * At ./public/volume-viii-issue-1/page/3/index.html:682: internally linking to .., which does not exist HTML-Proofer found 3374 failures! Using htmlproofer with Docker Because of all the errors I was getting and all the headaches with trying to configure/manage a working ruby environment, I turned to the klakegg/html-proofer project on DockerHub. It works nicely!\nI ran it like so on my M1 MacBook\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk/public ‹develop› ╰─$ docker run --rm -it \\ -v $(pwd):/src \\ klakegg/html-proofer:3.19.2 \\ --allow-hash-href --check-html --empty-alt-ignore WARNING: The requested image\u0026#39;s platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested Running [\u0026#34;ScriptCheck\u0026#34;, \u0026#34;LinkCheck\u0026#34;, \u0026#34;ImageCheck\u0026#34;, \u0026#34;HtmlCheck\u0026#34;] on [\u0026#34;.\u0026#34;] on *.html... Checking 2445 external links... Ran on 430 files! ... - ./volume-viii-issue-1/woodpeckers-of-the-prairie/index.html * External link https://www.linkedin.com/in/chelsea-steinbrecher-hoffmann-82723755 failed: 999 No error HTML-Proofer found 225 failures! And I subsequently copied the rootstalk-html-proofer.out file to Azure blog storage so you\u0026rsquo;ll find it available at https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out. Keep in mind that this version of the output used option flags --allow-hash-href --check-html --empty-alt-ignore to limit the output considerably.\nA Better Docker htmlproofer Run ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk/public ‹develop› ╰─$ time docker run --rm -it \\ -v $(pwd):/src \\ klakegg/html-proofer:3.19.2 \u0026gt; /tmp/rootstalk-html-proofer.out WARNING: The requested image\u0026#39;s platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested docker run --rm -it -v $(pwd):/src klakegg/html-proofer:3.19.2 \u0026gt; 0.12s user 0.08s system 0% cpu 2:10.53 total That run, without any limiting options, returned\u0026hellip; HTML-Proofer found 1383 failures!. It also doesn\u0026rsquo;t contain a timestamp of any kind, nor does it indicate which options were used, so I\u0026rsquo;d like to see if I can easily remedy that with a little more scripting.\nCapture More Info: ./html-proofer.sh To better control and capture relevant output I constructed a small bash script, html-proofer.sh, and you\u0026rsquo;ll find it in the root directory of the Rootstalk project repo. Its initial contents:\n#!/bin/bash ## ## Add options to the end of the COMMAND string to change html-proofer behavior. For a list of available options run: ## docker run --rm -it -v $(pwd):/src klakegg/html-proofer:3.19.2 --help ## Common options might include: ## --allow-hash-href ## --check-html ## --empty-alt-ignore ## hugo # generate a new site cd public # move into the new site\u0026#39;s files COMMAND=\u0026#34;docker run --rm -it -v $(pwd):/src klakegg/html-proofer:3.19.2 --check-html\u0026#34; date \u0026gt; /tmp/rootstalk-html-proofer.out echo $COMMAND \u0026gt;\u0026gt; /tmp/rootstalk-html-proofer.out time $COMMAND | sed -e \u0026#39;s/\\x1b\\[[0-9;]*m//g\u0026#39; \u0026gt;\u0026gt; /tmp/rootstalk-html-proofer.out mv -f /tmp/rootstalk-html-proofer.out ~/Downloads/rootstalk-html-proofer.out Output from running the script should look something like this:\n╭─mark@Marks-Mac-Mini ~/GitHub/rootstalk ‹main*› ╰─$ ./htmlproofer.sh Start building sites … hugo v0.105.0+extended darwin/amd64 BuildDate=unknown WARN 2022/11/08 19:36:19 .File.UniqueID on zero object. Wrap it in if or with: {{ with .File }}{{ .UniqueID }}{{ end }} | EN -------------------+------ Pages | 394 Paginator pages | 23 Non-page files | 114 Static files | 30 Processed images | 0 Aliases | 72 Sitemaps | 1 Cleaned | 0 Total in 1289 ms real\t0m57.285s user\t0m0.068s sys\t0m0.098s The script creates a file, ~/Downloads/rootstalk-html-proofer.out, that should contain easily readable output like this:\nTue Nov 8 19:36:19 CST 2022 docker run --rm -it -v /Users/mark/GitHub/rootstalk/public:/src klakegg/html-proofer:3.19.2 --check-html Running [\u0026#34;HtmlCheck\u0026#34;, \u0026#34;ScriptCheck\u0026#34;, \u0026#34;LinkCheck\u0026#34;, \u0026#34;ImageCheck\u0026#34;] on [\u0026#34;.\u0026#34;] on *.html... Checking 2474 external links... Ran on 413 files! - ./about/index.html * External link https://ojs.grinnell.edu/index.php/prairiejournal/about/submissions#onlineSubmissions failed: 404 No error - ./admin/index.html * image /images/generic-01.png does not have an alt attribute (line 127) - ./admin/tags/index.html * image /images/generic-01.png does not have an alt attribute (line 129) - ./index.html * 254:11: ERROR: Start tag of nonvoid HTML element ends with \u0026#39;/\u0026gt;\u0026#39;, use \u0026#39;\u0026gt;\u0026#39;. \u0026lt;h2\u0026gt;\u0026lt;a href=\u0026#34;/volume-viii-issue-1/\u0026#34; /\u0026gt;Click here for full contents of \u0026#34;Volume VIII, Issue 1\u0026#34;\u0026lt;/a\u0026gt;\u0026lt;/h2\u0026gt; ^ (line 254) * internally linking to .., which does not exist (line 261) ...many lines removed... - ./volume-viii-issue-1/trissell/index.html * External link https://www.alltrails.com/us/iowa/kellogg failed: 403 No error * External link https://www.nass.usda.gov/Publications/AgCensus/2017/Full_Report/Volume_1,_Chapter_2_County_LevelIowa/st19_2_0001_0001.pdf failed: 404 No error * image https://rootstalk.blob.core.windows.net/rootstalk-2022-spring/trissell-7.png does not have an alt attribute (line 210) - ./volume-viii-issue-1/woodpeckers-of-the-prairie/index.html * External link https://www.linkedin.com/in/chelsea-steinbrecher-hoffmann-82723755 failed: 999 No error * image https://rootstalk.blob.core.windows.net/rootstalk-2022-spring/Villatoro-2-volume-iii-issue-1-spring-2022.jpg does not have an alt attribute (line 177) * image https://rootstalk.blob.core.windows.net/rootstalk-2022-spring/Villatoro-8-moffett-volume-viii-issue-1-spring-2022.jpg does not have an alt attribute (line 330) HTML-Proofer found 1426 failures! Again, I moved that ~/Downloads/rootstalk-html-proofer.out file to Azure storage so that it\u0026rsquo;s available at https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out.\nNext Steps Since most of the issues identified in https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out are missing image alt attributes, I think it would be prudent to try and automate the generation of meaningful alt tags. Since all of the Rootstalk images are now stored in Azure Blob Containers, I think it would be prudent to look at things like:\nUsing Artificial Intelligence to Generate Alt Text on Images and Dynamically Generated Alt Text with Azure\u0026rsquo;s Computer Vision API Big fun!\nNot So Much Fun A little research into automatic generation of alt tags leads me to believe it\u0026rsquo;s not going to be worthwhile for Rootstalk after all. Many of our images are relatively complex so effectively training an auto-tag algorithm might take longer than it\u0026rsquo;s worth, and the results are likely to be disappointment. I think it\u0026rsquo;s better if we try to eliminate all the \u0026ldquo;systemic\u0026rdquo; html-proofer issues that we can, and manually deal with the rest, perhaps with the aid of a student worker.\nScripting html-proofer After briefly focusing on eliminating all of the \u0026ldquo;easy\u0026rdquo;, systemic issues that html-proofer flagged, I turned my attention to automating the process of running a new html-proofer. The product of that work is the new html-proofer.sh script that\u0026rsquo;s listed, complete with comments, here:\n#!/bin/bash ## ## Add options to the end of the COMMAND string to change html-proofer behavior. For a list of available options run: ## docker run --rm -it -v $(pwd):/src klakegg/html-proofer:3.19.2 --help ## Common options might include: ## --allow-hash-href ## --check-html ## --empty-alt-ignore ## hugo # generate a new site cd public # move into the new site\u0026#39;s files COMMAND=\u0026#34;docker run --rm -it -v $(pwd):/src klakegg/html-proofer:3.19.2 --check-html\u0026#34; date \u0026gt; /tmp/rootstalk-html-proofer.tmp echo $COMMAND \u0026gt;\u0026gt; /tmp/rootstalk-html-proofer.tmp time $COMMAND | sed -e \u0026#39;s/\\x1b\\[[0-9;]*m//g\u0026#39; \u0026gt;\u0026gt; /tmp/rootstalk-html-proofer.tmp tail -1 /tmp/rootstalk-html-proofer.tmp ## ## I\u0026#39;m unable to effectively control many of the bogus issues reported by html-proofer, things like: ## * internally linking to .., which does not exist (line 682) ## \u0026lt;a href=\u0026#34;..\u0026#34;\u0026gt;Back\u0026lt;/a\u0026gt; ## So, let\u0026#39;s try to implement some `grep` and `sed` commands that will automatically count and remove ## them from the output. ## BOGUS=`grep -c \u0026#39;internally linking to ..,\u0026#39; /tmp/rootstalk-html-proofer.tmp` echo \u0026#34;Removing ${BOGUS} false-negative errors from the output...\u0026#34; sed \u0026#39;/internally linking to \\.\\.,/,+1d\u0026#39; /tmp/rootstalk-html-proofer.tmp \u0026gt; ${HOME}/Downloads/rootstalk-html-proofer.out echo \u0026#34;${BOGUS} false-negative errors were removed from this output.\u0026#34; \u0026gt;\u0026gt; ${HOME}/Downloads/rootstalk-html-proofer.out ## ## Successfully running scripted Azure CLI in a Docker container proved to be virtually ## impossible, perhaps because the CLI login and commands are so \u0026#34;interative\u0026#34;? ## So, the `az` commands that follow will require the Azure CLI be installed on the ## host. See https://learn.microsoft.com/en-us/cli/azure/install-azure-cli-macos?source=recommendations#install-with-homebrew ## for doing that with Homebrew. Also, you should run `az login` before attempting the ## `az storage blob upload...` command shown here. ## az storage blob upload \\ --account-name rootstalk \\ --container-name documentation \\ --name rootstalk-html-proofer.out \\ --file ${HOME}/Downloads/rootstalk-html-proofer.out \\ --overwrite \\ --auth-mode key ## ## The output should be available now for download at https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out ## echo \u0026#34;The output should be available now for download at https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out.\u0026#34; ## The output from my latest run of html-proofer.sh looks like this:\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹develop› ╰─$ ./html-proofer.sh Start building sites … hugo v0.105.0+extended darwin/arm64 BuildDate=unknown WARN 2022/11/11 14:11:30 .File.UniqueID on zero object. Wrap it in if or with: {{ with .File }}{{ .UniqueID }}{{ end }} | EN -------------------+------ Pages | 391 Paginator pages | 23 Non-page files | 24 Static files | 32 Processed images | 0 Aliases | 71 Sitemaps | 1 Cleaned | 0 Total in 558 ms WARNING: The requested image\u0026#39;s platform (linux/amd64) does not match the detected host platform (linux/arm64/v8) and no specific platform was requested real 2m12.174s user 0m0.088s sys 0m0.080s HTML-Proofer found 316 failures! Removing 24 false-negative errors from the output... Argument \u0026#39;--overwrite\u0026#39; is in preview and under development. Reference and support levels: https://aka.ms/CLI_refstatus There are no credentials provided in your command and environment, we will query for account key for your storage account. It is recommended to provide --connection-string, --account-key or --sas-token in your command as credentials. You also can add `--auth-mode login` in your command to use Azure Active Directory (Azure AD) for authorization if your login account is assigned required RBAC roles. For more information about RBAC roles in storage, visit https://docs.microsoft.com/azure/storage/common/storage-auth-aad-rbac-cli. In addition, setting the corresponding environment variables can avoid inputting credentials in your command. Please use --help to get more information about environment variable usage. Finished[#############################################################] 100.0000% { \u0026#34;client_request_id\u0026#34;: \u0026#34;57a1860a-61fd-11ed-a661-9aa213ed91f9\u0026#34;, \u0026#34;content_md5\u0026#34;: \u0026#34;o6WZXPlD/W134hWMYo3XZQ==\u0026#34;, \u0026#34;date\u0026#34;: \u0026#34;2022-11-11T20:13:43+00:00\u0026#34;, \u0026#34;encryption_key_sha256\u0026#34;: null, \u0026#34;encryption_scope\u0026#34;: null, \u0026#34;etag\u0026#34;: \u0026#34;\\\u0026#34;0x8DAC4213C06EEC8\\\u0026#34;\u0026#34;, \u0026#34;lastModified\u0026#34;: \u0026#34;2022-11-11T20:13:44+00:00\u0026#34;, \u0026#34;request_id\u0026#34;: \u0026#34;4fa6aae2-b01e-0070-3c0a-f6f502000000\u0026#34;, \u0026#34;request_server_encrypted\u0026#34;: true, \u0026#34;version\u0026#34;: \u0026#34;2021-06-08\u0026#34;, \u0026#34;version_id\u0026#34;: null } The output should be available now for download at https://rootstalk.blob.core.windows.net/documentation/rootstalk-html-proofer.out. So, 316 failures minus the 24 false-negatives we removed equals 292 failures yet to be examined. Not too bad for a site with seven years of content and 2400+ references to be checked!\nAnd that\u0026rsquo;s a wrap. Until next time, Keep Calm, Carry On, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/133-automated-proofreading-with-htmlproofer/","tags":["Rootstalk","htmlproofer","proofreading"],"title":"Automated Proofreading with `htmlproofer`"},{"categories":null,"contents":"My goal for this afternoon, November 7, 2022, was to find repeat a process last performed almost a year ago on December 22, 2021, to synchronize changes in the development copy of Rootstalk (the main branch https://github.com/Digital-Grinnell/rootstalk) with our production deployment (the main branch of https://github.com/Digital-Grinnell/rootstalk-DO) to DigitalOcean. This should be a simple repeat of the process documented in Sync Rootstalk Production (DigitalOcean) with Dev. As before, I used guidance found in How To Merge Between Two Local Repositories to accomplish this.\nNote: Our staging site cast from the main branch of https://github.com/Digital-Grinnell/rootstalk, an Azure static app, can be accessed via https://icy-tree-020380010.azurestaticapps.net.\nThe Workflow I\u0026rsquo;ve captured the inputs and output of the workflow below. Like before, the process basically involved adding a new local remote named dev to my existing rootstalk-DO local repository, and doing a git fetch of that new remote. Everything in the code block that follows is as-it-was-executed on my Grinnell College MacBook\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹main› ╰─$ git pull Already up to date. ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹main› ╰─$ cd ../rootstalk-DO ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ git pull Already up to date. ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ git remote add dev ../rootstalk error: remote dev already exists. ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ git fetch dev remote: Enumerating objects: 2270, done. remote: Counting objects: 100% (2268/2268), done. remote: Compressing objects: 100% (1472/1472), done. remote: Total 2042 (delta 1503), reused 771 (delta 539), pack-reused 0 Receiving objects: 100% (2042/2042), 11.17 MiB | 14.61 MiB/s, done. Resolving deltas: 100% (1503/1503), completed with 165 local objects. From ../rootstalk * [new branch] 2022-spring -\u0026gt; dev/2022-spring * [new branch] develop -\u0026gt; dev/develop * [new branch] document -\u0026gt; dev/document 4b3e513..2d34f28 main -\u0026gt; dev/main ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ git merge dev/main Merge made by the \u0026#39;ort\u0026#39; strategy. .github/workflows/azure-static-web-apps-delightful-stone-01bd98310.yml | 4 +- .github/workflows/azure-static-web-apps-thankful-flower-0a2308810.yml | 45 + README.md | 45 + assets/sass/_custom.scss | 49 +- config.toml | 23 +- content/_index.html | 4 +- content/about/_index.md | 7 +- content/admin/_index.md | 1 + content/images/headers/peace-rock.jpg | Bin 84108 -\u0026gt; 0 bytes content/images/headers/travel-by-starlight.jpg | Bin 34296 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/andelson-bee.jpg | Bin 531838 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/clayton-bee.jpg | Bin 315767 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/damian-and-tony.jpg | Bin 38545 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/finding-the-lost-duck-1.jpg | Bin 320981 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/finding-the-lost-duck-2.jpg | Bin 301526 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/finding-the-lost-duck-3.jpg | Bin 177709 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/finding-the-lost-duck-4.jpg | Bin 557999 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/fox-grain.jpg | Bin 1530487 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/from-my-table-to-yours-1.jpg | Bin 268054 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/from-my-table-to-yours-2.jpg | Bin 547021 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/hayworth-wetland.jpg | Bin 1191865 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/hazelwood-cemetery-2.jpg | Bin 2029144 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/hazelwood-cemetery.jpg | Bin 337219 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/hernandez-fenceline.jpg | Bin 2719311 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/hooded-merganser.jpg | Bin 1535264 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/janzen-basketball.jpg | Bin 739935 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/janzen-family.jpg | Bin 673144 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/janzen-farm.jpg | Bin 575791 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/janzen-horses.jpg | Bin 289206 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/janzen-oxhide-school.jpg | Bin 1370393 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/janzen-roster.jpg | Bin 230465 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/janzen-scholarship.jpg | Bin 863175 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/janzen-school-bus.jpg | Bin 513591 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/janzen-siblings.jpg | Bin 478142 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/janzen-triplets.jpg | Bin 988310 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/janzen-vigor-dish.jpg | Bin 422872 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/john-lawrence-hanson.jpg | Bin 3527896 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/keith-kozloff-1.jpg | Bin 505404 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/keith-kozloff-2.jpg | Bin 588554 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/keith-kozloff-3.jpg | Bin 665952 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/keith-kozloff-4.jpg | Bin 365534 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/keith-kozloff-5.jpg | Bin 613776 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/keith-kozloff-6.jpg | Bin 364544 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/keith-kozloff-7.jpg | Bin 696227 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/kristine-heykants-1.jpg | Bin 4211427 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/kristine-heykants-2.jpg | Bin 1569044 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/kristine-heykants-3.jpg | Bin 1498524 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/kristine-heykants-4.jpg | Bin 1891155 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/mueller-after.jpg | Bin 697146 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/mueller-before.jpg | Bin 392732 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/mueller-chickens.jpg | Bin 499203 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/mueller-pano.jpg | Bin 325490 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/mural.jpg | Bin 204844 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/peace-rock-byron-2.jpg | Bin 899233 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/peace-rock-byron-john.jpg | Bin 644450 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/peace-rock-byron.jpg | Bin 666094 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/peace-rock-cyclone.jpg | Bin 272603 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/peace-rock-lauren-edwards.jpg | Bin 1039828 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/pieta-brown-1.jpg | Bin 668327 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/pieta-brown-2.jpg | Bin 77390 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/roots-of-stone-1.jpg | Bin 317478 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/roots-of-stone-2.jpg | Bin 874572 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/roots-of-stone-3.jpg | Bin 204256 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/roots-of-stone-4.jpg | Bin 589157 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/roots-of-stone-5.jpg | Bin 427110 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/saunders-loon.jpg | Bin 491834 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/saunders-oriole.jpg | Bin 387016 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/saunders-woodduck.jpg | Bin 679156 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/schoenmaker-starlight.jpg | Bin 564211 -\u0026gt; 0 bytes content/images/volume-iv-issue-2/this-old-house.jpg | Bin 821435 -\u0026gt; 0 bytes content/past-issues/volume-i-issue-1/.pending/andelson.md | 1897 +++++++++++++++++++++++++++++++ content/past-issues/volume-i-issue-1/.pending/crowley-images.md | 1897 +++++++++++++++++++++++++++++++ content/past-issues/volume-i-issue-1/.pending/darrah.md | 1897 +++++++++++++++++++++++++++++++ content/past-issues/volume-i-issue-1/.pending/editorial-staff.md | 1897 +++++++++++++++++++++++++++++++ content/past-issues/volume-i-issue-1/.pending/enshayan.md | 1897 +++++++++++++++++++++++++++++++ content/past-issues/volume-i-issue-1/.pending/ferrell.md | 1897 +++++++++++++++++++++++++++++++ content/past-issues/volume-i-issue-1/.pending/filler-material.md | 1897 +++++++++++++++++++++++++++++++ content/past-issues/volume-i-issue-1/.pending/howe-katz.md | 1897 +++++++++++++++++++++++++++++++ content/past-issues/volume-i-issue-1/.pending/koether.md | 1897 +++++++++++++++++++++++++++++++ content/past-issues/volume-i-issue-1/.pending/larsen-schulte-tyndall.md | 1897 +++++++++++++++++++++++++++++++ content/past-issues/volume-i-issue-1/.pending/mcdonough.md | 1897 +++++++++++++++++++++++++++++++ content/past-issues/volume-i-issue-1/.pending/stone-closeup.md | 1897 +++++++++++++++++++++++++++++++ content/past-issues/volume-i-issue-1/.pending/williams.md | 1897 +++++++++++++++++++++++++++++++ content/past-issues/volume-i-issue-1/_index.md | 11 +- content/past-issues/volume-i-issue-1/andelson-note.md | 37 + content/past-issues/volume-i-issue-1/crowley-closeup.md | 52 + content/past-issues/volume-i-issue-1/dean.md | 73 ++ content/past-issues/volume-i-issue-1/ikerd.md | 118 ++ content/past-issues/volume-i-issue-1/ottenstein.md | 93 ++ content/past-issues/volume-i-issue-1/swander-1.md | 43 + content/past-issues/volume-i-issue-1/swander-2.md | 56 + content/past-issues/volume-i-issue-1/swander-3.md | 49 + content/past-issues/volume-ii-issue-1/.pending/filler-material-UNDONE.md | 1488 +++++++++++++++++++++++++ content/past-issues/volume-ii-issue-1/_index.md | 11 +- content/past-issues/volume-ii-issue-1/editor.md | 38 + content/past-issues/volume-ii-issue-1/farewell-richard.md | 45 + content/past-issues/volume-ii-issue-1/hayworth-photos.md | 43 + content/past-issues/volume-ii-issue-1/heath.md | 57 + content/past-issues/volume-ii-issue-1/kirschenmann.md | 148 +++ content/past-issues/volume-ii-issue-1/lahay.md | 133 +++ content/past-issues/volume-ii-issue-1/mcilrath.md | 199 ++++ content/past-issues/volume-ii-issue-1/moffett-1.md | 47 + content/past-issues/volume-ii-issue-1/moffett-2.md | 94 ++ content/past-issues/volume-ii-issue-1/mutel.md | 136 +++ content/past-issues/volume-ii-issue-1/scanbridge.md | 29 + content/past-issues/volume-ii-issue-1/stowe.md | 122 ++ content/past-issues/volume-ii-issue-1/whittaker.md | 64 ++ content/past-issues/volume-ii-issue-1/wolf.md | 40 + content/past-issues/volume-ii-issue-2/.pending/duncombe-mills.md | 3002 ++++++++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-ii-issue-2/.pending/kuhn.md | 3002 ++++++++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-ii-issue-2/.pending/non-article-media-1.md | 3002 ++++++++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-ii-issue-2/.pending/non-article-media-2.md | 3002 ++++++++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-ii-issue-2/.pending/snow.md | 3002 ++++++++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-ii-issue-2/.pending/weeks.md | 3002 ++++++++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-ii-issue-2/_index.md | 12 +- content/past-issues/volume-ii-issue-2/arena.md | 59 + content/past-issues/volume-ii-issue-2/atmore.md | 112 ++ content/past-issues/volume-ii-issue-2/birds.md | 99 ++ content/past-issues/volume-ii-issue-2/carl.md | 53 + content/past-issues/volume-ii-issue-2/dubbeldbee-kuhn.md | 34 + content/past-issues/volume-ii-issue-2/duncombe-mills.md | 28 + content/past-issues/volume-ii-issue-2/editor.md | 68 ++ content/past-issues/volume-ii-issue-2/harris-love.md | 63 ++ content/past-issues/volume-ii-issue-2/herrnstadt.md | 47 + content/past-issues/volume-ii-issue-2/jiminez.md | 151 +++ content/past-issues/volume-ii-issue-2/johnson.md | 135 +++ content/past-issues/volume-ii-issue-2/kaiser.md | 87 ++ content/past-issues/volume-ii-issue-2/kincaid.md | 297 +++++ content/past-issues/volume-ii-issue-2/kyaruzi.md | 158 +++ content/past-issues/volume-ii-issue-2/snow.md | 85 ++ content/past-issues/volume-ii-issue-2/thomasch-1.md | 41 + content/past-issues/volume-ii-issue-2/thomasch-2.md | 210 ++++ content/past-issues/volume-ii-issue-2/wannamaker.md | 84 ++ content/past-issues/volume-ii-issue-2/water-dance.md | 52 + content/past-issues/volume-ii-issue-2/weeks.md | 280 +++++ content/past-issues/volume-iii-issue-1/.pending/aresty-marek.md | 1 + content/past-issues/volume-iii-issue-1/.pending/birds.md | 1 + content/past-issues/volume-iii-issue-1/.pending/cavanaugh.md | 1 + content/past-issues/volume-iii-issue-1/.pending/evans.md | 1 + content/past-issues/volume-iii-issue-1/.pending/gray.md | 1 + content/past-issues/volume-iii-issue-1/.pending/griffin.md | 1 + content/past-issues/volume-iii-issue-1/.pending/haldy.md | 1 + content/past-issues/volume-iii-issue-1/.pending/lee.md | 1 + content/past-issues/volume-iii-issue-1/.pending/meanders.md | 1 + content/past-issues/volume-iii-issue-1/.pending/melis.md | 1 + content/past-issues/volume-iii-issue-1/.pending/non-article-media.md | 1 + content/past-issues/volume-iii-issue-1/.pending/prindaville.md | 1 + content/past-issues/volume-iii-issue-1/.pending/publisher.md | 1 + content/past-issues/volume-iii-issue-1/.pending/queathem.md | 1 + content/past-issues/volume-iii-issue-1/.pending/saunders.md | 1 + content/past-issues/volume-iii-issue-1/.pending/wiewiora.md | 1 + content/past-issues/volume-iii-issue-1/.pending/woodward.md | 1 + content/past-issues/volume-iii-issue-1/_index.md | 8 +- content/past-issues/volume-iii-issue-1/aresty-marek.md | 64 ++ content/past-issues/volume-iii-issue-1/cavanaugh.md | 64 ++ content/past-issues/volume-iii-issue-1/evans.md | 72 ++ content/past-issues/volume-iii-issue-1/gray.md | 123 +++ content/past-issues/volume-iii-issue-1/griffin.md | 364 ++++++ content/past-issues/volume-iii-issue-1/haldy.md | 65 ++ content/past-issues/volume-iii-issue-1/lee.md | 87 ++ content/past-issues/volume-iii-issue-1/meanders.md | 63 ++ content/past-issues/volume-iii-issue-1/melis.md | 24 + content/past-issues/volume-iii-issue-1/non-article-media.md | 25 + content/past-issues/volume-iii-issue-1/prindaville.md | 35 + content/past-issues/volume-iii-issue-1/publisher.md | 34 + content/past-issues/volume-iii-issue-1/queathem.md | 34 + content/past-issues/volume-iii-issue-1/saunders.md | 78 ++ content/past-issues/volume-iii-issue-1/wiewiora.md | 212 ++++ content/past-issues/volume-iii-issue-1/woodward.md | 25 + content/past-issues/volume-iii-issue-2/.pending/abdulkarim-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/aschittino-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/bergman-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/birds-of-the-prairie-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/brosseau-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/cain-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/editor-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/fellows-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/filler-material.md | 1 + content/past-issues/volume-iii-issue-2/.pending/gaunt-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/hanson-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/maher-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/moffett-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/neems-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/rosburg-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/running-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/schwartz-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/shukla-pending.md | 1 + content/past-issues/volume-iii-issue-2/.pending/stowe-pending.md | 1 + content/past-issues/volume-iii-issue-2/_index.md | 5 +- content/past-issues/volume-iii-issue-2/abdulkarim.md | 1 + content/past-issues/volume-iii-issue-2/aschittino.md | 3 +- content/past-issues/volume-iii-issue-2/bergman.md | 1 + content/past-issues/volume-iii-issue-2/birds-of-the-prairie.md | 39 +- content/past-issues/volume-iii-issue-2/brosseau.md | 221 ++-- content/past-issues/volume-iii-issue-2/cain.md | 15 +- content/past-issues/volume-iii-issue-2/editor.md | 14 +- content/past-issues/volume-iii-issue-2/fellows.md | 65 +- content/past-issues/volume-iii-issue-2/gaunt.md | 1 + content/past-issues/volume-iii-issue-2/hanson.md | 7 +- content/past-issues/volume-iii-issue-2/maher.md | 7 +- content/past-issues/volume-iii-issue-2/moffett.md | 1 + content/past-issues/volume-iii-issue-2/neems.md | 1 + content/past-issues/volume-iii-issue-2/publisher.md | 15 +- content/past-issues/volume-iii-issue-2/rosburg.md | 1 + content/past-issues/volume-iii-issue-2/running.md | 30 +- content/past-issues/volume-iii-issue-2/schwartz.md | 3 +- content/past-issues/volume-iii-issue-2/shukla.md | 3 +- content/past-issues/volume-iii-issue-2/stowe.md | 1 + content/past-issues/volume-iv-issue-1/.not-ready/mutel.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/bernal-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/birds-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/burt-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/chirdon-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/clark-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/dunham-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/evans-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/freeberg-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/jackson-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/kugel-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/lewis-beck-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/luftig-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/moffett-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/munoz-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/mutel-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/publisher-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending-new/rideout-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending/arena.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/atmore.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/birds.md | 2453 +++++++++++++++++++++++------------------ content/past-issues/volume-iv-issue-1/.pending/burt-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending/carl.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/chirdon-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending/chirdon.md | 1 + content/past-issues/volume-iv-issue-1/.pending/clark.md | 2 + content/past-issues/volume-iv-issue-1/.pending/dubbeldbee-kuhn.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/duncombe-mills.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/dunham-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending/editor.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/evans-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending/freeburg.md | 1 + content/past-issues/volume-iv-issue-1/.pending/harris-love.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/herrnstadt.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/jackson-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending/jiminez.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/johnson.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/kaiser.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/kincaid.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/kugel-pending.md | 1 + content/past-issues/volume-iv-issue-1/.pending/kuhn.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/kyaruzi.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/lewis-beck.md | 1 + content/past-issues/volume-iv-issue-1/.pending/luftig.md | 1 + content/past-issues/volume-iv-issue-1/.pending/moffett.md | 1 + content/past-issues/volume-iv-issue-1/.pending/munoz.md | 1 + content/past-issues/volume-iv-issue-1/.pending/mutel.md | 1 + content/past-issues/volume-iv-issue-1/.pending/publisher.md | 1 + content/past-issues/volume-iv-issue-1/.pending/rideout.md | 1 + content/past-issues/volume-iv-issue-1/.pending/snow.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/thomasch-1.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/thomasch-2.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/wannamaker.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/weeks.md | 2484 +++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/_index.md | 5 +- content/past-issues/volume-iv-issue-1/bernal.md | 13 +- content/past-issues/volume-iv-issue-1/birds.md | 9 +- content/past-issues/volume-iv-issue-1/burt-volume-iv-issue-1.md | 1 + content/past-issues/volume-iv-issue-1/chirdon.md | 1 + content/past-issues/volume-iv-issue-1/clark.md | 63 +- content/past-issues/volume-iv-issue-1/dunham.md | 1351 +++-------------------- content/past-issues/volume-iv-issue-1/editor.md | 1 + content/past-issues/volume-iv-issue-1/evans-volume-iv-issue-1.md | 45 +- content/past-issues/volume-iv-issue-1/freeberg.md | 48 +- content/past-issues/volume-iv-issue-1/jackson.md | 19 +- content/past-issues/volume-iv-issue-1/kugel.md | 1 + content/past-issues/volume-iv-issue-1/lewis-beck.md | 14 +- content/past-issues/volume-iv-issue-1/luftig.md | 863 +++------------ content/past-issues/volume-iv-issue-1/moffett.md | 5 +- content/past-issues/volume-iv-issue-1/munoz.md | 19 +- content/past-issues/volume-iv-issue-1/mutel.md | 18 +- content/past-issues/volume-iv-issue-1/publisher.md | 9 +- content/past-issues/volume-iv-issue-1/rideout.md | 3 +- content/past-issues/volume-iv-issue-2/_index.md | 7 +- content/past-issues/volume-iv-issue-2/birds-of-the-prairie-vol-iv-issue-2.md | 11 +- content/past-issues/volume-iv-issue-2/closeup-keith-kozloff.md | 3 +- content/past-issues/volume-iv-issue-2/closeup-kristine-heykants.md | 9 +- content/past-issues/volume-iv-issue-2/editors-note-VolIV_Issue2.md | 10 +- content/past-issues/volume-iv-issue-2/finding-the-lost-duck.md | 393 ++----- content/past-issues/volume-iv-issue-2/from-my-table-to-yours.md | 3 +- content/past-issues/volume-iv-issue-2/gnosis.md | 5 +- content/past-issues/volume-iv-issue-2/growing-up-in-kansas.md | 419 ++----- content/past-issues/volume-iv-issue-2/johansson-his-ambassador.md | 3 +- content/past-issues/volume-iv-issue-2/lac-la-biche-mission.md | 5 +- content/past-issues/volume-iv-issue-2/making-room.md | 5 +- content/past-issues/volume-iv-issue-2/manoylov-peaches-meet-corn.md | 6 +- content/past-issues/volume-iv-issue-2/night-as-controlled-prairie-fire.md | 24 +- content/past-issues/volume-iv-issue-2/ode-to-the-honey-bee.md | 25 +- content/past-issues/volume-iv-issue-2/open-door.md | 16 +- content/past-issues/volume-iv-issue-2/peace-rock.md | 199 +--- content/past-issues/volume-iv-issue-2/pieta-brown-in-concert.md | 38 +- content/past-issues/volume-iv-issue-2/publishers-note-VolIV_Issue2.md | 19 +- content/past-issues/volume-iv-issue-2/roadtrip.md | 613 +++-------- content/past-issues/volume-iv-issue-2/roots-of-stone.md | 316 ++---- content/past-issues/volume-iv-issue-2/roots-talk-podcast-ep-3.md | 20 +- content/past-issues/volume-iv-issue-2/roots-talk-podcast-ep-4.md | 20 +- content/past-issues/volume-iv-issue-2/this-old-house.md | 22 +- content/past-issues/volume-iv-issue-2/travel-by-starlight.md | 15 +- content/past-issues/volume-iv-issue-2/two-poems-bill-graeser.md | 18 +- content/past-issues/volume-iv-issue-2/worthless-rocks.md | 60 - content/past-issues/volume-v-issue-1/.american-bison.md | 3 +- content/past-issues/volume-v-issue-1/.american-mink.md | 1 + content/past-issues/volume-v-issue-1/.virginia-opossum.md | 3 +- content/past-issues/volume-v-issue-1/.white-tailed-deer.md | 3 +- content/past-issues/volume-v-issue-1/_index.md | 7 +- content/past-issues/volume-v-issue-1/a-place-to-call-home.md | 3 +- content/past-issues/volume-v-issue-1/building-the-agricultural-city.md | 9 +- content/past-issues/volume-v-issue-1/cheyenne-bottoms.md | 7 +- content/past-issues/volume-v-issue-1/closeup-regan-golden.md | 7 +- content/past-issues/volume-v-issue-1/community-commons-ecological-restoration.md | 19 +- content/past-issues/volume-v-issue-1/editors-note-VolV_Issue1.md | 1 + content/past-issues/volume-v-issue-1/extraction-roots-energy-plains.md | 7 +- content/past-issues/volume-v-issue-1/first-taste-of-freedom.md | 3 +- content/past-issues/volume-v-issue-1/healing-the-smallest-casualty.md | 1 + content/past-issues/volume-v-issue-1/jumping-into-the-void.md | 5 +- content/past-issues/volume-v-issue-1/little-prairie-on-the-freeway.md | 3 +- content/past-issues/volume-v-issue-1/mammals-of-the-prairie.md | 11 +- content/past-issues/volume-v-issue-1/my-integrated-life-pt-1.md | 39 +- content/past-issues/volume-v-issue-1/on-the-changing-nature-of-the-obituary.md | 9 +- content/past-issues/volume-v-issue-1/publishers-note-VolV-Issue1.md | 7 +- content/past-issues/volume-v-issue-1/roots-talk-podcast-ep-5.md | 1 + content/past-issues/volume-v-issue-1/the-hottest-car-in-town.md | 1 + content/past-issues/volume-v-issue-1/two-poems-john-grey.md | 1 + content/past-issues/volume-v-issue-1/two-poems-richard-luftig.md | 3 +- content/past-issues/volume-v-issue-1/two-poems-rodney-nelson.md | 11 +- content/past-issues/volume-v-issue-1/welcoming-the-world-to-our-farm.md | 1 + content/past-issues/volume-v-issue-1/what-do-you-think-community-is.md | 3 +- content/past-issues/volume-v-issue-1/worthless-rocks.md | 18 +- content/past-issues/volume-v-issue-2/.pending/behar.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/boyce.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/brandt.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/brew.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/brown.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/clarke-curtis.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/cohen.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/curtis.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/editor.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/filler-material.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/goodnature.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/hanson.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/jain.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/lewis-beck.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/mcbee.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/moffett.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/neems.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/podcast.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/publisher.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/sarnat.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/snodgrass.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/stowe.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/.pending/white.md | 2251 +++++++++++++++++++++++++++++++++++++ content/past-issues/volume-v-issue-2/_index.md | 17 + content/past-issues/volume-v-issue-2/behar.md | 47 + content/past-issues/volume-v-issue-2/boyce.md | 99 ++ content/past-issues/volume-v-issue-2/brandt.md | 87 ++ content/past-issues/volume-v-issue-2/brew.md | 86 ++ content/past-issues/volume-v-issue-2/brown.md | 363 ++++++ content/past-issues/volume-v-issue-2/clarke-curtis.md | 226 ++++ content/past-issues/volume-v-issue-2/cohen.md | 45 + content/past-issues/volume-v-issue-2/cover.jpg | Bin 0 -\u0026gt; 758661 bytes content/past-issues/volume-v-issue-2/cover.png | Bin 0 -\u0026gt; 1381926 bytes content/past-issues/volume-v-issue-2/editor.md | 31 + content/past-issues/volume-v-issue-2/filler-material.md | 35 + content/past-issues/volume-v-issue-2/goodnature.md | 44 + content/past-issues/volume-v-issue-2/hanson.md | 65 ++ content/past-issues/volume-v-issue-2/jain.md | 92 ++ content/past-issues/volume-v-issue-2/lewis-beck.md | 79 ++ content/past-issues/volume-v-issue-2/mcbee.md | 34 + content/past-issues/volume-v-issue-2/moffett.md | 52 + content/past-issues/volume-v-issue-2/neems.md | 50 + content/past-issues/volume-v-issue-2/podcast.md | 24 + content/past-issues/volume-v-issue-2/publisher.md | 65 ++ content/past-issues/volume-v-issue-2/sarnat.md | 220 ++++ content/past-issues/volume-v-issue-2/snodgrass.md | 35 + content/past-issues/volume-v-issue-2/stowe.md | 78 ++ content/past-issues/volume-v-issue-2/white.md | 87 ++ content/past-issues/volume-vi-issue-1/Schaefer.md | 9 +- content/past-issues/volume-vi-issue-1/_index.md | 11 +- content/past-issues/volume-vi-issue-1/adair.md | 3 +- content/past-issues/volume-vi-issue-1/andelson.md | 1 + content/past-issues/volume-vi-issue-1/baechtel.md | 3 +- content/past-issues/volume-vi-issue-1/beck.md | 9 +- content/past-issues/volume-vi-issue-1/beisner.md | 1 + content/past-issues/volume-vi-issue-1/brew.md | 1 + content/past-issues/volume-vi-issue-1/brown.md | 59 +- content/past-issues/volume-vi-issue-1/buck.md | 7 +- content/past-issues/volume-vi-issue-1/dean.md | 3 +- content/past-issues/volume-vi-issue-1/drobney.md | 3 +- content/past-issues/volume-vi-issue-1/fatehpuria.md | 3 +- content/past-issues/volume-vi-issue-1/fatehpuria2.md | 7 +- content/past-issues/volume-vi-issue-1/goldberg.md | 1 + content/past-issues/volume-vi-issue-1/hansen.md | 3 +- content/past-issues/volume-vi-issue-1/kim.md | 3 +- content/past-issues/volume-vi-issue-1/kurtz.md | 3 +- content/past-issues/volume-vi-issue-1/moffet.md | 5 +- content/past-issues/volume-vi-issue-1/payne.md | 11 +- content/past-issues/volume-vi-issue-1/perez.md | 1 + content/past-issues/volume-vi-issue-1/potter.md | 1 + content/past-issues/volume-vi-issue-1/segner.md | 7 +- content/past-issues/volume-vi-issue-1/snouffer.md | 7 +- content/past-issues/volume-vi-issue-1/teigland.md | 5 +- content/past-issues/volume-vii-issue-1/_index.md | 11 +- content/past-issues/volume-vii-issue-1/andelson-maya.md | 1 + content/past-issues/volume-vii-issue-1/andelson.md | 5 +- content/past-issues/volume-vii-issue-1/arneson.md | 5 +- content/past-issues/volume-vii-issue-1/baechtel.md | 5 +- content/past-issues/volume-vii-issue-1/carr.md | 1 + content/past-issues/volume-vii-issue-1/commers.md | 1 + content/past-issues/volume-vii-issue-1/devany.md | 5 +- content/past-issues/volume-vii-issue-1/doherty.md | 12 +- content/past-issues/volume-vii-issue-1/getahun.md | 9 +- content/past-issues/volume-vii-issue-1/goodall.md | 1 + content/past-issues/volume-vii-issue-1/keleher.md | 13 +- content/past-issues/volume-vii-issue-1/kreutzian.md | 9 +- content/past-issues/volume-vii-issue-1/meulemans.md | 17 +- content/past-issues/volume-vii-issue-1/miller.md | 17 +- content/past-issues/volume-vii-issue-1/ohlenbusch.md | 5 +- content/past-issues/volume-vii-issue-1/sherpa.md | 1 + content/past-issues/volume-vii-issue-1/tibatemwa.md | 1 + content/past-issues/volume-vii-issue-1/yuan.md | 1 + content/past-issues/volume-vii-issue-2/_index.md | 18 + content/{ =\u0026gt; past-issues}/volume-vii-issue-2/andelson.md | 7 +- content/{ =\u0026gt; past-issues}/volume-vii-issue-2/baechtel.md | 7 +- content/{ =\u0026gt; past-issues}/volume-vii-issue-2/bower.md | 14 +- content/{ =\u0026gt; past-issues}/volume-vii-issue-2/boyce.md | 1 + content/{ =\u0026gt; past-issues}/volume-vii-issue-2/clotfelter.md | 1 + content/{ =\u0026gt; past-issues}/volume-vii-issue-2/cover.png | Bin content/{ =\u0026gt; past-issues}/volume-vii-issue-2/endangered-animals.md | 1 + content/{ =\u0026gt; past-issues}/volume-vii-issue-2/johnson.md | 16 +- content/{ =\u0026gt; past-issues}/volume-vii-issue-2/kouchi.md | 1 + content/{ =\u0026gt; past-issues}/volume-vii-issue-2/ojendyk.md | 11 +- content/{ =\u0026gt; past-issues}/volume-vii-issue-2/ottenstein.md | 9 +- content/{ =\u0026gt; past-issues}/volume-vii-issue-2/rootstalk_leaf.svg | 0 content/{ =\u0026gt; past-issues}/volume-vii-issue-2/ross.md | 7 +- content/{ =\u0026gt; past-issues}/volume-vii-issue-2/taylor.md | 1 + content/script.sh | 14 + content/submit/_index.md | 3 +- content/volume-vii-issue-2/_index.md | 9 - content/volume-viii-issue-1/.pending/_master.md | 2524 ++++++++++++++++++++++++++++++++++++++++++ content/volume-viii-issue-1/_index.md | 17 + content/volume-viii-issue-1/agpoon.md | 74 ++ content/volume-viii-issue-1/bradley.md | 73 ++ content/volume-viii-issue-1/buck.md | 42 + content/volume-viii-issue-1/burchit.md | 76 ++ content/volume-viii-issue-1/burt.md | 79 ++ content/volume-viii-issue-1/chen.md | 63 ++ content/volume-viii-issue-1/cover.png | Bin 0 -\u0026gt; 1185260 bytes content/volume-viii-issue-1/gaddis.md | 123 +++ content/volume-viii-issue-1/henry.md | 162 +++ content/volume-viii-issue-1/hootstein.md | 87 ++ content/volume-viii-issue-1/horan.md | 48 + content/volume-viii-issue-1/kessel.md | 114 ++ content/volume-viii-issue-1/lewis-beck.md | 212 ++++ content/volume-viii-issue-1/macmoran.md | 111 ++ content/volume-viii-issue-1/mcgary-adams-dubow-fay-stindt-schaefer.md | 298 +++++ content/volume-viii-issue-1/munoz.md | 54 + content/volume-viii-issue-1/obrien.md | 136 +++ content/volume-viii-issue-1/publisher.md | 76 ++ content/volume-viii-issue-1/taylor.md | 65 ++ content/volume-viii-issue-1/thompson.md | 127 +++ content/volume-viii-issue-1/trissell.md | 71 ++ content/volume-viii-issue-1/woodpeckers-of-the-prairie.md | 148 +++ ArticleType-and-Tag-Values.md =\u0026gt; documentation/ArticleType-and-Tag-Values.md | 3 + Automated-Testing.md =\u0026gt; documentation/Automated-Testing.md | 0 Git-Workflow-in-Windows.md =\u0026gt; documentation/Git-Workflow-in-Windows.md | 0 Migration-to-Azure.md =\u0026gt; documentation/Migration-to-Azure.md | 0 PUSH-TO-PRODUCTION.md =\u0026gt; documentation/PUSH-TO-PRODUCTION.md | 0 documentation/README.md | 108 ++ STRUCTURE-Rules.md =\u0026gt; documentation/STRUCTURE-Rules.md | 0 Visual-Proofreading.md =\u0026gt; documentation/Visual-Proofreading.md | 0 alina-git-workflow.md =\u0026gt; documentation/alina-git-workflow.md | 0 pdf-to-markdown.md =\u0026gt; documentation/pdf-to-markdown.md | 0 revised-editor-git-workflow.md =\u0026gt; documentation/revised-editor-git-workflow.md | 18 +- documentation/shared-mac-editor-git-workflow.md | 30 + editors-prefatory-text-template.md | 1 + layouts/_default/baseof.html | 1 + layouts/_default/list.html | 22 +- layouts/_default/single.html | 21 +- layouts/shortcodes/audio_azure.html | 2 +- layouts/shortcodes/dropcap.html | 3 + layouts/shortcodes/figure_azure.html | 23 +- layouts/shortcodes/indent-with-attribution.html | 9 + layouts/shortcodes/indent.html | 4 + layouts/shortcodes/video_azure.html | 4 +- link-format.js | 7 + resources/_gen/assets/scss/sass/styles.scss_5bac553973685aab030dcdbdaeaab6f8.content | 2 +- resources/_gen/assets/scss/sass/styles.scss_5bac553973685aab030dcdbdaeaab6f8.json | 2 +- static/announcement.md | 1 + static/images/generic-01.png | Bin 0 -\u0026gt; 163222 bytes themes/rootstalkzen/layouts/partials/meta.html | 2 +- themes/rootstalkzen/layouts/partials/styles.html | 2 +- 499 files changed, 158639 insertions(+), 5269 deletions(-) create mode 100644 .github/workflows/azure-static-web-apps-thankful-flower-0a2308810.yml delete mode 100644 content/images/headers/peace-rock.jpg delete mode 100644 content/images/headers/travel-by-starlight.jpg delete mode 100644 content/images/volume-iv-issue-2/andelson-bee.jpg delete mode 100644 content/images/volume-iv-issue-2/clayton-bee.jpg delete mode 100644 content/images/volume-iv-issue-2/damian-and-tony.jpg delete mode 100644 content/images/volume-iv-issue-2/finding-the-lost-duck-1.jpg delete mode 100644 content/images/volume-iv-issue-2/finding-the-lost-duck-2.jpg delete mode 100644 content/images/volume-iv-issue-2/finding-the-lost-duck-3.jpg delete mode 100644 content/images/volume-iv-issue-2/finding-the-lost-duck-4.jpg delete mode 100644 content/images/volume-iv-issue-2/fox-grain.jpg delete mode 100644 content/images/volume-iv-issue-2/from-my-table-to-yours-1.jpg delete mode 100644 content/images/volume-iv-issue-2/from-my-table-to-yours-2.jpg delete mode 100644 content/images/volume-iv-issue-2/hayworth-wetland.jpg delete mode 100644 content/images/volume-iv-issue-2/hazelwood-cemetery-2.jpg delete mode 100644 content/images/volume-iv-issue-2/hazelwood-cemetery.jpg delete mode 100644 content/images/volume-iv-issue-2/hernandez-fenceline.jpg delete mode 100644 content/images/volume-iv-issue-2/hooded-merganser.jpg delete mode 100644 content/images/volume-iv-issue-2/janzen-basketball.jpg delete mode 100644 content/images/volume-iv-issue-2/janzen-family.jpg delete mode 100644 content/images/volume-iv-issue-2/janzen-farm.jpg delete mode 100644 content/images/volume-iv-issue-2/janzen-horses.jpg delete mode 100644 content/images/volume-iv-issue-2/janzen-oxhide-school.jpg delete mode 100644 content/images/volume-iv-issue-2/janzen-roster.jpg delete mode 100644 content/images/volume-iv-issue-2/janzen-scholarship.jpg delete mode 100644 content/images/volume-iv-issue-2/janzen-school-bus.jpg delete mode 100644 content/images/volume-iv-issue-2/janzen-siblings.jpg delete mode 100644 content/images/volume-iv-issue-2/janzen-triplets.jpg delete mode 100644 content/images/volume-iv-issue-2/janzen-vigor-dish.jpg delete mode 100644 content/images/volume-iv-issue-2/john-lawrence-hanson.jpg delete mode 100644 content/images/volume-iv-issue-2/keith-kozloff-1.jpg delete mode 100644 content/images/volume-iv-issue-2/keith-kozloff-2.jpg delete mode 100644 content/images/volume-iv-issue-2/keith-kozloff-3.jpg delete mode 100644 content/images/volume-iv-issue-2/keith-kozloff-4.jpg delete mode 100644 content/images/volume-iv-issue-2/keith-kozloff-5.jpg delete mode 100644 content/images/volume-iv-issue-2/keith-kozloff-6.jpg delete mode 100644 content/images/volume-iv-issue-2/keith-kozloff-7.jpg delete mode 100644 content/images/volume-iv-issue-2/kristine-heykants-1.jpg delete mode 100644 content/images/volume-iv-issue-2/kristine-heykants-2.jpg delete mode 100644 content/images/volume-iv-issue-2/kristine-heykants-3.jpg delete mode 100644 content/images/volume-iv-issue-2/kristine-heykants-4.jpg delete mode 100644 content/images/volume-iv-issue-2/mueller-after.jpg delete mode 100644 content/images/volume-iv-issue-2/mueller-before.jpg delete mode 100644 content/images/volume-iv-issue-2/mueller-chickens.jpg delete mode 100644 content/images/volume-iv-issue-2/mueller-pano.jpg delete mode 100644 content/images/volume-iv-issue-2/mural.jpg delete mode 100644 content/images/volume-iv-issue-2/peace-rock-byron-2.jpg delete mode 100644 content/images/volume-iv-issue-2/peace-rock-byron-john.jpg delete mode 100644 content/images/volume-iv-issue-2/peace-rock-byron.jpg delete mode 100644 content/images/volume-iv-issue-2/peace-rock-cyclone.jpg delete mode 100644 content/images/volume-iv-issue-2/peace-rock-lauren-edwards.jpg delete mode 100644 content/images/volume-iv-issue-2/pieta-brown-1.jpg delete mode 100644 content/images/volume-iv-issue-2/pieta-brown-2.jpg delete mode 100644 content/images/volume-iv-issue-2/roots-of-stone-1.jpg delete mode 100644 content/images/volume-iv-issue-2/roots-of-stone-2.jpg delete mode 100644 content/images/volume-iv-issue-2/roots-of-stone-3.jpg delete mode 100644 content/images/volume-iv-issue-2/roots-of-stone-4.jpg delete mode 100644 content/images/volume-iv-issue-2/roots-of-stone-5.jpg delete mode 100644 content/images/volume-iv-issue-2/saunders-loon.jpg delete mode 100644 content/images/volume-iv-issue-2/saunders-oriole.jpg delete mode 100644 content/images/volume-iv-issue-2/saunders-woodduck.jpg delete mode 100644 content/images/volume-iv-issue-2/schoenmaker-starlight.jpg delete mode 100644 content/images/volume-iv-issue-2/this-old-house.jpg create mode 100644 content/past-issues/volume-i-issue-1/.pending/andelson.md create mode 100644 content/past-issues/volume-i-issue-1/.pending/crowley-images.md create mode 100644 content/past-issues/volume-i-issue-1/.pending/darrah.md create mode 100644 content/past-issues/volume-i-issue-1/.pending/editorial-staff.md create mode 100644 content/past-issues/volume-i-issue-1/.pending/enshayan.md create mode 100644 content/past-issues/volume-i-issue-1/.pending/ferrell.md create mode 100644 content/past-issues/volume-i-issue-1/.pending/filler-material.md create mode 100644 content/past-issues/volume-i-issue-1/.pending/howe-katz.md create mode 100644 content/past-issues/volume-i-issue-1/.pending/koether.md create mode 100644 content/past-issues/volume-i-issue-1/.pending/larsen-schulte-tyndall.md create mode 100644 content/past-issues/volume-i-issue-1/.pending/mcdonough.md create mode 100644 content/past-issues/volume-i-issue-1/.pending/stone-closeup.md create mode 100644 content/past-issues/volume-i-issue-1/.pending/williams.md create mode 100644 content/past-issues/volume-i-issue-1/andelson-note.md create mode 100644 content/past-issues/volume-i-issue-1/crowley-closeup.md create mode 100644 content/past-issues/volume-i-issue-1/dean.md create mode 100644 content/past-issues/volume-i-issue-1/ikerd.md create mode 100644 content/past-issues/volume-i-issue-1/ottenstein.md create mode 100644 content/past-issues/volume-i-issue-1/swander-1.md create mode 100644 content/past-issues/volume-i-issue-1/swander-2.md create mode 100644 content/past-issues/volume-i-issue-1/swander-3.md create mode 100644 content/past-issues/volume-ii-issue-1/.pending/filler-material-UNDONE.md create mode 100644 content/past-issues/volume-ii-issue-1/editor.md create mode 100644 content/past-issues/volume-ii-issue-1/farewell-richard.md create mode 100644 content/past-issues/volume-ii-issue-1/hayworth-photos.md create mode 100644 content/past-issues/volume-ii-issue-1/heath.md create mode 100644 content/past-issues/volume-ii-issue-1/kirschenmann.md create mode 100644 content/past-issues/volume-ii-issue-1/lahay.md create mode 100644 content/past-issues/volume-ii-issue-1/mcilrath.md create mode 100644 content/past-issues/volume-ii-issue-1/moffett-1.md create mode 100644 content/past-issues/volume-ii-issue-1/moffett-2.md create mode 100644 content/past-issues/volume-ii-issue-1/mutel.md create mode 100644 content/past-issues/volume-ii-issue-1/scanbridge.md create mode 100644 content/past-issues/volume-ii-issue-1/stowe.md create mode 100644 content/past-issues/volume-ii-issue-1/whittaker.md create mode 100644 content/past-issues/volume-ii-issue-1/wolf.md create mode 100644 content/past-issues/volume-ii-issue-2/.pending/duncombe-mills.md create mode 100644 content/past-issues/volume-ii-issue-2/.pending/kuhn.md create mode 100644 content/past-issues/volume-ii-issue-2/.pending/non-article-media-1.md create mode 100644 content/past-issues/volume-ii-issue-2/.pending/non-article-media-2.md create mode 100644 content/past-issues/volume-ii-issue-2/.pending/snow.md create mode 100644 content/past-issues/volume-ii-issue-2/.pending/weeks.md create mode 100644 content/past-issues/volume-ii-issue-2/arena.md create mode 100644 content/past-issues/volume-ii-issue-2/atmore.md create mode 100644 content/past-issues/volume-ii-issue-2/birds.md create mode 100644 content/past-issues/volume-ii-issue-2/carl.md create mode 100644 content/past-issues/volume-ii-issue-2/dubbeldbee-kuhn.md create mode 100644 content/past-issues/volume-ii-issue-2/duncombe-mills.md create mode 100644 content/past-issues/volume-ii-issue-2/editor.md create mode 100644 content/past-issues/volume-ii-issue-2/harris-love.md create mode 100644 content/past-issues/volume-ii-issue-2/herrnstadt.md create mode 100644 content/past-issues/volume-ii-issue-2/jiminez.md create mode 100644 content/past-issues/volume-ii-issue-2/johnson.md create mode 100644 content/past-issues/volume-ii-issue-2/kaiser.md create mode 100644 content/past-issues/volume-ii-issue-2/kincaid.md create mode 100644 content/past-issues/volume-ii-issue-2/kyaruzi.md create mode 100644 content/past-issues/volume-ii-issue-2/snow.md create mode 100644 content/past-issues/volume-ii-issue-2/thomasch-1.md create mode 100644 content/past-issues/volume-ii-issue-2/thomasch-2.md create mode 100644 content/past-issues/volume-ii-issue-2/wannamaker.md create mode 100644 content/past-issues/volume-ii-issue-2/water-dance.md create mode 100644 content/past-issues/volume-ii-issue-2/weeks.md create mode 100644 content/past-issues/volume-iii-issue-1/aresty-marek.md create mode 100644 content/past-issues/volume-iii-issue-1/cavanaugh.md create mode 100644 content/past-issues/volume-iii-issue-1/evans.md create mode 100644 content/past-issues/volume-iii-issue-1/gray.md create mode 100644 content/past-issues/volume-iii-issue-1/griffin.md create mode 100644 content/past-issues/volume-iii-issue-1/haldy.md create mode 100644 content/past-issues/volume-iii-issue-1/lee.md create mode 100644 content/past-issues/volume-iii-issue-1/meanders.md create mode 100644 content/past-issues/volume-iii-issue-1/melis.md create mode 100644 content/past-issues/volume-iii-issue-1/non-article-media.md create mode 100644 content/past-issues/volume-iii-issue-1/prindaville.md create mode 100644 content/past-issues/volume-iii-issue-1/publisher.md create mode 100644 content/past-issues/volume-iii-issue-1/queathem.md create mode 100644 content/past-issues/volume-iii-issue-1/saunders.md create mode 100644 content/past-issues/volume-iii-issue-1/wiewiora.md create mode 100644 content/past-issues/volume-iii-issue-1/woodward.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/abdulkarim.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/aschittino.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/bergman.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/birds-of-the-prairie.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/brosseau.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/cain.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/editor.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/fellows.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/gaunt.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/hanson.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/maher.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/moffett.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/neems.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/publisher.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/rosburg.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/running.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/schwartz.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/shukla.md mode change 100755 =\u0026gt; 100644 content/past-issues/volume-iii-issue-2/stowe.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/arena.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/atmore.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/carl.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/dubbeldbee-kuhn.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/duncombe-mills.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/editor.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/harris-love.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/herrnstadt.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/jiminez.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/johnson.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/kaiser.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/kincaid.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/kuhn.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/kyaruzi.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/snow.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/thomasch-1.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/thomasch-2.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/wannamaker.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/weeks.md delete mode 100644 content/past-issues/volume-iv-issue-2/worthless-rocks.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/behar.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/boyce.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/brandt.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/brew.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/brown.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/clarke-curtis.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/cohen.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/curtis.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/editor.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/filler-material.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/goodnature.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/hanson.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/jain.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/lewis-beck.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/mcbee.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/moffett.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/neems.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/podcast.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/publisher.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/sarnat.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/snodgrass.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/stowe.md create mode 100644 content/past-issues/volume-v-issue-2/.pending/white.md create mode 100644 content/past-issues/volume-v-issue-2/_index.md create mode 100644 content/past-issues/volume-v-issue-2/behar.md create mode 100644 content/past-issues/volume-v-issue-2/boyce.md create mode 100644 content/past-issues/volume-v-issue-2/brandt.md create mode 100644 content/past-issues/volume-v-issue-2/brew.md create mode 100644 content/past-issues/volume-v-issue-2/brown.md create mode 100644 content/past-issues/volume-v-issue-2/clarke-curtis.md create mode 100644 content/past-issues/volume-v-issue-2/cohen.md create mode 100644 content/past-issues/volume-v-issue-2/cover.jpg create mode 100644 content/past-issues/volume-v-issue-2/cover.png create mode 100644 content/past-issues/volume-v-issue-2/editor.md create mode 100644 content/past-issues/volume-v-issue-2/filler-material.md create mode 100644 content/past-issues/volume-v-issue-2/goodnature.md create mode 100644 content/past-issues/volume-v-issue-2/hanson.md create mode 100644 content/past-issues/volume-v-issue-2/jain.md create mode 100644 content/past-issues/volume-v-issue-2/lewis-beck.md create mode 100644 content/past-issues/volume-v-issue-2/mcbee.md create mode 100644 content/past-issues/volume-v-issue-2/moffett.md create mode 100644 content/past-issues/volume-v-issue-2/neems.md create mode 100644 content/past-issues/volume-v-issue-2/podcast.md create mode 100644 content/past-issues/volume-v-issue-2/publisher.md create mode 100644 content/past-issues/volume-v-issue-2/sarnat.md create mode 100644 content/past-issues/volume-v-issue-2/snodgrass.md create mode 100644 content/past-issues/volume-v-issue-2/stowe.md create mode 100644 content/past-issues/volume-v-issue-2/white.md create mode 100644 content/past-issues/volume-vii-issue-2/_index.md rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/andelson.md (98%) rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/baechtel.md (91%) rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/bower.md (95%) rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/boyce.md (97%) rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/clotfelter.md (99%) rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/cover.png (100%) rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/endangered-animals.md (98%) rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/johnson.md (92%) rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/kouchi.md (99%) rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/ojendyk.md (78%) rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/ottenstein.md (89%) rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/rootstalk_leaf.svg (100%) rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/ross.md (99%) rename content/{ =\u0026gt; past-issues}/volume-vii-issue-2/taylor.md (99%) create mode 100644 content/script.sh delete mode 100644 content/volume-vii-issue-2/_index.md create mode 100644 content/volume-viii-issue-1/.pending/_master.md create mode 100644 content/volume-viii-issue-1/_index.md create mode 100644 content/volume-viii-issue-1/agpoon.md create mode 100644 content/volume-viii-issue-1/bradley.md create mode 100644 content/volume-viii-issue-1/buck.md create mode 100644 content/volume-viii-issue-1/burchit.md create mode 100644 content/volume-viii-issue-1/burt.md create mode 100644 content/volume-viii-issue-1/chen.md create mode 100644 content/volume-viii-issue-1/cover.png create mode 100644 content/volume-viii-issue-1/gaddis.md create mode 100644 content/volume-viii-issue-1/henry.md create mode 100644 content/volume-viii-issue-1/hootstein.md create mode 100644 content/volume-viii-issue-1/horan.md create mode 100644 content/volume-viii-issue-1/kessel.md create mode 100644 content/volume-viii-issue-1/lewis-beck.md create mode 100644 content/volume-viii-issue-1/macmoran.md create mode 100644 content/volume-viii-issue-1/mcgary-adams-dubow-fay-stindt-schaefer.md create mode 100644 content/volume-viii-issue-1/munoz.md create mode 100644 content/volume-viii-issue-1/obrien.md create mode 100644 content/volume-viii-issue-1/publisher.md create mode 100644 content/volume-viii-issue-1/taylor.md create mode 100644 content/volume-viii-issue-1/thompson.md create mode 100644 content/volume-viii-issue-1/trissell.md create mode 100644 content/volume-viii-issue-1/woodpeckers-of-the-prairie.md rename ArticleType-and-Tag-Values.md =\u0026gt; documentation/ArticleType-and-Tag-Values.md (95%) rename Automated-Testing.md =\u0026gt; documentation/Automated-Testing.md (100%) rename Git-Workflow-in-Windows.md =\u0026gt; documentation/Git-Workflow-in-Windows.md (100%) rename Migration-to-Azure.md =\u0026gt; documentation/Migration-to-Azure.md (100%) rename PUSH-TO-PRODUCTION.md =\u0026gt; documentation/PUSH-TO-PRODUCTION.md (100%) create mode 100644 documentation/README.md rename STRUCTURE-Rules.md =\u0026gt; documentation/STRUCTURE-Rules.md (100%) rename Visual-Proofreading.md =\u0026gt; documentation/Visual-Proofreading.md (100%) rename alina-git-workflow.md =\u0026gt; documentation/alina-git-workflow.md (100%) rename pdf-to-markdown.md =\u0026gt; documentation/pdf-to-markdown.md (100%) rename revised-editor-git-workflow.md =\u0026gt; documentation/revised-editor-git-workflow.md (63%) create mode 100644 documentation/shared-mac-editor-git-workflow.md create mode 100644 editors-prefatory-text-template.md create mode 100644 layouts/shortcodes/dropcap.html create mode 100644 layouts/shortcodes/indent-with-attribution.html create mode 100644 layouts/shortcodes/indent.html create mode 100644 link-format.js create mode 100644 static/announcement.md create mode 100644 static/images/generic-01.png ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ git status On branch main Your branch is ahead of \u0026#39;origin/main\u0026#39; by 228 commits. (use \u0026#34;git push\u0026#34; to publish your local commits) nothing to commit, working tree clean ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ hugo server Start building sites … hugo v0.101.0+extended darwin/arm64 BuildDate=unknown WARN 2022/11/07 12:59:32 .File.UniqueID on zero object. Wrap it in if or with: {{ with .File }}{{ .UniqueID }}{{ end }} | EN -------------------+------ Pages | 394 Paginator pages | 23 Non-page files | 114 Static files | 29 Processed images | 0 Aliases | 72 Sitemaps | 1 Cleaned | 0 Built in 485 ms Watching for changes in /Users/mcfatem/GitHub/rootstalk-DO/{archetypes,assets,content,layouts,static,themes} Watching for config changes in /Users/mcfatem/GitHub/rootstalk-DO/config.toml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at //localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop At this point I interrupted the process from a year ago to look for any ill-effects in my http://localhost:1313 rendering of the code. I was looking especially at the contents of the config.toml file and the page footer in the rendered site.\nThe local site looks GREAT! There are no longer any issues with the footer that I can see, and About Us properly shows the latest development repo information. Best of all is the appearance of Volume VIII, Issue 1 as the \u0026ldquo;Current Issue\u0026rdquo; from Spring 2022! So all I can say is\u0026hellip; Keep Calm, and Carry On!\n^C% ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ git push Enumerating objects: 2271, done. Counting objects: 100% (2269/2269), done. Delta compression using up to 8 threads Compressing objects: 100% (509/509), done. Writing objects: 100% (2043/2043), 11.17 MiB | 9.76 MiB/s, done. Total 2043 (delta 1503), reused 2042 (delta 1503), pack-reused 0 remote: Resolving deltas: 100% (1503/1503), completed with 165 local objects. To https://github.com/Digital-Grinnell/rootstalk-DO 6a2667b..3f252f9 main -\u0026gt; main ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ The Bad News Unlike the last time, there is NONE! I was worried that my DigitalOcean configuration still deploys to what I thought would be a defunct address, rootstalk.grinnell.rocks, but again that appears to do no harm because Rootstalk appears to be working just as it should.\nThe Good News The good news is simple\u0026hellip; IT WORKED! Rootstalk is now up-to-date, with the minor issue reported above, and the deployment to DigitalOcean was automatic, as intended. The message I see in my DigitalOcean dashboard says:\nNov 07 2022 LIVE Digital-Grinnell\u0026#39;s deployment went live Trigger: Digital-Grinnell pushed 3f252f9 to Digital-Grinnell/rootstalk-DO/main 01:09:55 PM - 4m 0s build And that\u0026rsquo;s a wrap. Until next time, Keep Calm, Carry On, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/132-another-sync-rootstalk-production-do-with-dev/","tags":["Rootstalk","sync","production","DigitalOcean"],"title":"Another Sync to Rootstalk Production (DigitalOcean) with Dev"},{"categories":null,"contents":"I recently created Hugo Front Matter Tools which is described as\u0026hellip;\nA collection of Python scripts desinged to help manage Hugo .md content front matter.\nI already have mechanisms in many projects, like this blog, that help me report the last time ANY content was pushed to GitHub, or the last time a Hugo site was compiled. But it would be nice, especially in the case of Rootstalk, if I could save the last git add date/time into an individual file\u0026rsquo;s front matter. That way the tools mentioned above could leverage and report that valuable information.\nCreating a git Hook While searching for possibilities this morning I ran across this post which I\u0026rsquo;ll repeat here in case the original is ever lost\u0026hellip;\nIt turns out you can run \u0026ldquo;hooks\u0026rdquo; - they are actually handled by another mechanism - when staging files (at git add time):\nhttps://git-scm.com/book/en/v2/Customizing-Git-Git-Attributes#_keyword_expansion\n(scroll down a bit to the \u0026ldquo;smudge\u0026rdquo; and \u0026ldquo;clean\u0026rdquo; diagrams)\nHere is what I understood :\nedit the .gitattributes, and create rules for the files which should trigger a dictionary update: novel.txt filter=updateDict Then, tell Git what the updateDict filter does on smudge (`git checkout`) and clean (`git add`): $ git config --global filter.updateDict.clean countWords.script $ git config --global filter.updateDict.smudge cat My First Hook Here are some of the details surrounding my first git hook\u0026hellip;\nPurpose: To add or update a last_modified_at: front matter field with the current local date/time whenever a git add operation touches a .md (Markdown) file in a specific project.\nProjects: Initially I\u0026rsquo;ll try to implement this on this blog project. If that works, I\u0026rsquo;ll happily apply it to the Rootstalk project.\nImprovements: What follows will only work if the last_modified_at: field already exists in a file\u0026rsquo;s front matter. What happens if we are working with a file that does NOT already have that field?\nFor initial implementation I\u0026rsquo;m going to follow the advice found in Adding last modified timestamps with Git. The shell script in that post reads like this:\n#!/bin/sh # Contents of .git/hooks/pre-commit # Replace `last_modified_at` timestamp with current time git diff --cached --name-status | egrep -i \u0026#34;^(A|M).*\\.(md)$\u0026#34; | while read a b; do cat $b | sed \u0026#34;/---.*/,/---.*/s/^last_modified_at:.*$/last_modified_at: $(date -u \u0026#34;+%Y-%m-%dT%H:%M:%S\u0026#34;)/\u0026#34; \u0026gt; tmp mv tmp $b git add $b done I created the same pre-commit script in this project\u0026rsquo;s .git/hooks/ directory. Now to test it\u0026hellip;\nInitial Test ╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main*› ╰─$ git status On branch main Your branch is up to date with \u0026#39;origin/main\u0026#39;. ╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main*› ╰─$ git add . ╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main*› ╰─$ git commit -m \u0026#34;Testing my pre-commit hook\u0026#34; hint: The \u0026#39;.git/hooks/pre-commit\u0026#39; hook was ignored because it\u0026#39;s not set as executable. hint: You can disable this warning with `git config advice.ignoredHook false`. [main 0b9781d7] Testing my pre-commit hook 3 files changed, 74 insertions(+) create mode 100644 content/posts/131-Creating-a-git-Hook.md So, I changed the pre-commit hook permissions and tried again after adding a bit more to this .md file.\n╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main› ╰─$ git status On branch main Your branch is up to date with \u0026#39;origin/main\u0026#39;. Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: content/posts/131-Creating-a-git-Hook.md no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) ╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main*› ╰─$ git add . ╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main*› ╰─$ git commit -m \u0026#34;2nd test of pre-commit hook\u0026#34; [main 49118af7] 2nd test of pre-commit hook 1 file changed, 23 insertions(+), 1 deletion(-) It worked! The .md file for this post now includes a line of front matter that says last_modified_at: 2022-10-27T16:02:09 in both my local AND GitHub repository versions. Hurrah!\nMore Testing Now, what happens if I git add and git commit more test files including:\ntest1.md - A .md file that has no last_modified_at: front matter key, test2.txt - A .txt file that has an empty last_modified_at: front matter key, and test3.png - A .png image file that, of course, has no last_modified_at: front matter key. ╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main*› ╰─$ git status On branch main Your branch is up to date with \u0026#39;origin/main\u0026#39;. Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git restore \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: content/posts/131-Creating-a-git-Hook.md Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) content/test1.md content/test2.txt content/test3.png no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) ╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main*› ╰─$ git add . ╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main*› ╰─$ git commit -m \u0026#34;Three-part pre-commit hook test\u0026#34; [main 1b20a2a8] Three-part pre-commit hook test 4 files changed, 202 insertions(+), 1 deletion(-) create mode 100644 content/test1.md create mode 100644 content/test2.txt create mode 100644 content/test3.png Looking at the three files (actually, four files including this blog post) and\u0026hellip; BEAUTIMOUS!\nEverything worked as it should!\nNow, let\u0026rsquo;s see if I can improve on the rather cryptic format of the date/time that gets added.\nTo do that, change the git diff... line in .git/hooks/pre-commit to use the TZ timezone setting and remove the -u flag so that we get local time like so:\ngit diff --cached --name-status | egrep -i \u0026#34;^(A|M).*\\.(md)$\u0026#34; | while read a b; do cat $b | sed \u0026#34;/---.*/,/---.*/s/^last_modified_at:.*$/last_modified_at: $(TZ=CST6CDT date \u0026#34;+%F %H.%M %Z\u0026#34;)/\u0026#34; \u0026gt; tmp Notice also that this format does not include ANY colons (I\u0026rsquo;m using a . between hour and minute instead) so there should be no need to quote the value.\n\u0026#x1f941; It works!\nIn fact, it works so well that I\u0026rsquo;m keeping a copy of the pre-commit script here in this blog\u0026rsquo;s repo.\nNow I Need An Install Script The pre-commit script works exceptionally well, but since it lives in each project\u0026rsquo;s /.git/hooks directory, one that is NEVER pushed to the project\u0026rsquo;s remote, it would be nice to have a semi-automatic way of creating it in a repo.\nThis notion got me wondering\u0026hellip; is there already a suitable \u0026ldquo;git hook manager\u0026rdquo; (that is the phrase I Googled) that I can use for this? Yes, I beleive there might very well be one based on these results from my Google search:\nhttps://github.com/aitemr/awesome-git-hooks - Just a list of cool hooks.\nhttps://github.com/evilmartians/lefthook - Interesting, but perhaps more than I need?\nhttps://pre-commit.com/ - Also interesting, but again it\u0026rsquo;s perhaps too much.\nhttps://stackoverflow.com/questions/427207/can-git-hook-scripts-be-managed-along-with-the-repository - A great discussion!\nhttps://githooks.com/ - The discussion above led me here, and from here, anything seems possible!\nhttps://github.com/sds/overcommit - Nice, but again, too complex.\nhttps://github.com/boddenberg-it/.githooker - Maybe, but I found the sparse documentation a little confusing.\nhttps://medium.com/@ripoche.b/using-global-pre-commit-hook-to-prevent-committing-unwanted-code-edbbf957ad12 - My choice because it\u0026rsquo;s relatively simple with no unnecessary dependencies. See below.\nImplementing a Global Pre-Commit Hook Here we go, at 2023-01-20T13:13:17-06:00 I embarked on an effort to implement portions of Using global pre-commit hook to prevent committing unwanted code on my Mac Mini in this dlad-blog project using my existing pre-commit hook.\nMy relevant command history\u0026hellip;\n╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main*›\n╰─$ git \u0026ndash;version\ngit version 2.39.1\n╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main*›\n╰─$ mkdir -p ~/gitconfig/hooks\n╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main*›\n╰─$ mv -f .git/hooks/pre-commit ~/gitconfig/hooks/.\n╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main*›\n╰─$ git config --global --edit\n╭─mark@Marks-Mac-Mini ~/GitHub/dlad-blog ‹main*›\n╰─$ cd ~; pwd\n/Users/mark\nThe last git command above opened my global git configuration file, /Users/mark/.gitconfig in nano, where I added this line to the [core] section:\nhooksPath = /Users/mark/gitconfig/hooks Now it\u0026rsquo;s time to commit all these changes to see if my new global git config works. \u0026#x1f941; Yup! The frontmatter at the top of this .md file just changed to include the current date/time: last_modified_at: 2023-01-20 13.29 CST. Bingo!\nThere might be more to learn here, but for now\u0026hellip; that\u0026rsquo;s a wrap.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/131-creating-a-git-hook/","tags":["git","hook","last_modified_at","pre-commit","clean"],"title":"Creating a `git` Hook"},{"categories":null,"contents":"For the past couple years I/we have been experimenting with moving digital content to Azure, both for storage and as a web app host. The most prominent case is with regard to Rootstalk where Azure currently supports two (recently down from three) static development apps as well as a storage account. All such services are part of a personal* Azure subscription opened under the digital@grinnell.edu email address.\n*I call this a \u0026ldquo;personal\u0026rdquo; account because charges for it are currently billed to my own credit card, a situation that will need to be changed sometime relatively soon. Fortunately, charges thus far have not exceeded $0.15 per month.\nTerminology Azure terminology seems a little odd in places, but it\u0026rsquo;s not too much of a departure from other providers of cloud services. So that we might properly understand things I think it wise to share Microsoft Azure glossary: A dictionary of cloud terminology on the Azure platform. I don\u0026rsquo;t expect these definitions to change, but for easy reference the terms used most often in this document are listed here:\naccount - An account that\u0026rsquo;s used to access and manage an Azure subscription. It\u0026rsquo;s often referred to as an Azure account although an account can be any of these: an existing work, school, or personal Microsoft account. You can also create an account to manage an Azure subscription when you sign up for the free trial.\nportal - The secure web portal used to deploy and manage Azure services.\nresource - An item that is part of your Azure solution. Each Azure service enables you to deploy different types of resources, such as databases or virtual machines.\nresource group - A container in Resource Manager that holds related resources for an application. The resource group can include all of the resources for an application, or only those resources that are logically grouped together. You can decide how you want to allocate resources to resource groups based on what makes the most sense for your organization.\nstorage account - An account that gives you access to the Azure Blob, Queue, Table, and File services in Azure Storage. The storage account name defines the unique namespace for Azure Storage data objects.\nsubscription - A customer\u0026rsquo;s agreement with Microsoft that enables them to obtain Azure services. The subscription pricing and related terms are governed by the offer chosen for the subscription.\nAzure Home - My Portal At present the account\u0026rsquo;s \u0026ldquo;home\u0026rdquo; or portal looks like this:\nFigure 1 \u0026middot; Azure Account Portal As you can see in the image, the portal lists 8 resources including:\n4 static web apps (3 for Rootstalk and one for CollectionBuilder), 2 storage accounts (one for Rootstalk and one for CollectionBuilder), and 2 included \u0026ldquo;Resource group\u0026rdquo; types. Removing a Resource and Renaming a Subscription Before going farther I need to do a little cleanup in the account. Specifically, the rootstalk-gokcebel static web add can be removed, and the subscription name of Azure subscription 1 should be changed to something more meaningful.\nrootstalk-gokcebel Clicking the rootstalk-gokcebel link in the portal took me to the screen shown in the figure below. Since this resource is no longer needed, it was recently replaced by rootstalk-develop, I\u0026rsquo;ll just use the Delete icon to remove it.\nFigure 2 \u0026middot; Deleting a Resource To complete the deletion I had to follow-through when prompted with this confirmation screen you see below. Thankfully, the message was very helpful.\nFigure 3 \u0026middot; Confirming Deletion The rootstalk-gokcebel resource is still present in my portal view minutes after the deletion, but clicking on the resource link now takes me to a \u0026ldquo;Resource not found\u0026rdquo; screen.\nAzure subscription 1 I really needed a better name than Azure subscription 1 for my subscription, so I did some searching on the web and found the blurb shown in the figure below. I also suspect the document it comes from, Change contact information for an Azure billing account, will come in handy later when I hope to transfer ownership of and billing for the account to the Grinnell College Libraries.\nFigure 4 \u0026middot; Confirming Deletion Unfortunately, that guideance did NOT work! So I stumbled through a few screens and eventually got it done. The pages I visited along the way were:\nAll services, Subscriptions, and Azure subscription 1 On that last page I found a Rename icon that took me to a very simple page where I changed the subscription name to mcfatem - Digital@Grinnell.edu Personal Subscription. The name change took only a minute or so to show up in my portal after refreshing my browser. \u0026#x1f604;\nCreating a New Storage Account I have a newly-exported migration-test collection of digital content from Digital.Grinnell and some of the exported files need a web-addressable blob storage home in the cloud. So, I used the guidance provided in Create resource groups and Create a storage account to make a space for that content.\nThe new resource group I named migration-test and in it is a new storage account named migrationtestcollection. I accepted all of the defaults when creating the storage account. Unfortunately, Azure naming rules for storage accounts are VERY restrictive, so this name was the best I could do. \u0026#x1f626;\nEngaging Azure Storage Explorer I\u0026rsquo;ve been using the Azure Storage Explorer* (ASE) app on my MacBook quite a bit lately, so I\u0026rsquo;ll return there to get the new migration-test collection digital objects, the OBJ files, from smb://storage.grinnell.edu/library/allstaff/DG-migration-test network storage to the new migrationtestcollection Azure storage. In the app I tunneled down into the new migrationtestcollection storage account and right-clicked on Blob Containers to create a new one as shown in the figure below.\n*Note that in ASE my account still appears as Azure subscription 1 (Digital@grinnell.edu) no matter how many times I close and re-open the app. \u0026#x1f626;\nFigure 5 \u0026middot; Confirming Deletion The new blob container name is migration-test and I\u0026rsquo;m able to select it and use the Upload menu item to select and upload all of the grinnell_*_OBJ.* files from aforementioned network storage. The process took less than a minute and left me with 29 stored digital objects.\nAt the time of this writing the container looked like the figure below when viewed in Azure Storage Explorer.\nFigure 6 \u0026middot; ASE View of the migration-test Container I also created a new migration-test-metadata blob container and populated it with all of the other files from smb://storage.grinnell.edu/library/allstaff/DG-migration-test. There are now 53 files stored in that container.\nWeb-Addressable Objects All of the content in the new blob containers should be web-addressable, and a right-click on any file will allow you to see/copy the object\u0026rsquo;s URL and/or path. For example:\ngrinnell_10020_OBJ.pdf has a URL of https://migrationtestcollection.blob.core.windows.net/migration-test/grinnell_10020_OBJ.pdf grinnell_10020_MODS.xml has a URL of https://migrationtestcollection.blob.core.windows.net/migration-test-metadata/grinnell_10020_MODS.xml Note that both the storage account name, migrationtestcollection, and the name of the parent blob container is present, and is consistently included in each URL.\nA New Jekyll Site in Azure or Reclaim Cloud? This afternoon I plan to take a big bite out of Tutorial: Publish a Jekyll site to Azure Static Web Apps and maybe a look at Category: jekyll as well.\nThat\u0026rsquo;s a wrap for now. Look for more content here as I continue to expand the role of Azure across the Digital.Grinnell landscape.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/130-managing-azure/","tags":["Azure","storage","blob"],"title":"Managing Azure"},{"categories":null,"contents":"As we continue to look at potential migration paths for our Digital.Grinnell (DG) content, it\u0026rsquo;s become apparent that it would be nice to have a small \u0026ldquo;test\u0026rdquo; or \u0026ldquo;migration\u0026rdquo; collection of objects to play with. The collection should have a small, but diverse, set of objects covering all of the popular content, or CModel, types that we currently have in DG.\nSince Slack now imposes a 90-day lifespan for posts (we are using only free-tier Slack services at this time) I thought I had better create this blog post to capture key parts of a relevant Slack conversation\u0026hellip;\nMark M. September 13, 2022 @ 11:49 AM I modified my iduF MigrateObject command yesterday and now have it working on our staging server, https://isle-stage.grinnell.edu. I’ve successfully created a new `grinnell:migration-test` collection there and have 42 objects shared into it via the new command. I plan to add a few more, up to 50 total objects, this afternoon, but all of this is taking place only in isle-stage thus far. I have a plan to repeat the process in digital.grinnell.edu if all goes well in staging, but before I do that I’d like to push isle-stage as far as I can. That will include at least one export of the objects and metadata, probably using the same process I created for our metadata review in 2020. I might also re-bag all of the objects and hold those bags in a dedicated network storage location apart from our traditional backup storage. Any other ideas/suggestions how I might “export” these objects? Maybe a better question… What target platforms should I plan to provide exports for so that test objects can be ingested as part of migration testing/evaluation? The MigrateObject Command As mentioned above, I elected to use the iduF command named MigrateObject to populate a new collection named grinnell:migration-test. The command\u0026rsquo;s abridged help text says\u0026hellip;\n╭─islandora@dgdockerx ~ ╰─$ docker exec -it isle-apache-dgs bash root@db16bc9ff4c3:/# cd /var/www/html/sites/default/ root@db16bc9ff4c3:/var/www/html/sites/default# drush -u 1 iduF MigrateObject --help ... MigrateObject - Migrates an object from its --source (or --source=None) collection to --replace. See also --share option. [Verified 31-Aug-2018] ... The MigrateObject command was executed a total of 9 times to populate the migration-test collection using the script shown here:\ndrush -u 1 iduF grinnell:1000 MigrateObject --share --source=\u0026#34;grinnell:college-life\u0026#34; --replace=\u0026#34;grinnell:migration-test\u0026#34; drush -u 1 iduF grinnell:5593 MigrateObject --share --source=\u0026#34;grinnell:student-scholarship\u0026#34; --replace=\u0026#34;grinnell:migration-test\u0026#34; drush -u 1 iduF grinnell:10001 MigrateObject --share --source=\u0026#34;grinnell:faculty-scholarship\u0026#34; --replace=\u0026#34;grinnell:migration-test\u0026#34; drush -u 1 iduF grinnell:13273 MigrateObject --share --source=\u0026#34;grinnell:student-scholarship\u0026#34; --replace=\u0026#34;grinnell:migration-test\u0026#34; drush -u 1 iduF grinnell:16394 MigrateObject --share --source=\u0026#34;grinnell:postcards\u0026#34; --replace=\u0026#34;grinnell:migration-test\u0026#34; drush -u 1 iduF grinnell:17289 MigrateObject --share --source=\u0026#34;grinnell:archives-suppressed\u0026#34; --replace=\u0026#34;grinnell:migration-test\u0026#34; drush -u 1 iduF grinnell:19423 MigrateObject --share --source=\u0026#34;grinnell:alumni-oral-histories\u0026#34; --replace=\u0026#34;grinnell:migration-test\u0026#34; drush -u 1 iduF grinnell:23345 MigrateObject --share --source=\u0026#34;grinnell:faculty-scholarship\u0026#34; --replace=\u0026#34;grinnell:migration-test\u0026#34; drush -u 1 iduF grinnell:23517 MigrateObject --share --source=\u0026#34;grinnell:college-life\u0026#34; --replace=\u0026#34;grinnell:migration-test\u0026#34; The following is a simple list of the objects, not including pages and children, and their corresponding CModel.\nPID CModel Details grinnell:1000 islandora:bookCModel A 14-page book grinnell:5593 islandora:sp-audioCModel A very large audio recording grinnell:10001 islandora:compoundCModel A compound parent object with 8 children including some binary CModel objects grinnell:13273 islandora:sp_videoCModel A short video grinnell:16394 islandora:sp_basic_image A JPG basic image grinnell:17289 islandora:sp_web_archive A WARC (web-archive) of a small website grinnell:19423 islandora:oralhistoriesCModel An alumni oral history grinnell:23345 islandora:sp_pdf A multi-page PDF grinnell:23517 islandora:sp_large_image_cmodel A TIFF large image Dealing with the Children Under normal circumstances all of the child objects belonging to a compound or book would be part of the parent\u0026rsquo;s collection(s) so subsequent operations that identify objects by-collection would work on both the parent and it\u0026rsquo;s children. However, our child objects are not yet part of the new migration-test collection, so let\u0026rsquo;s remedy that now.\nThe drush commands I used to migrate the child objects were:\ndrush -u 1 iduF grinnell:1001-1014 MigrateObject --share --source=\u0026#34;grinnell:college-buildings\u0026#34; --replace=\u0026#34;grinnell:migration-test\u0026#34; drush -u 1 iduF grinnell:10020-10027 MigrateObject --share --source=\u0026#34;grinnell:faculty-scholarship\u0026#34; --replace=\u0026#34;grinnell:migration-test\u0026#34; Note: The above examples show why it\u0026rsquo;s always nice to keep a parent object\u0026rsquo;s children together with consecutive PIDs!\nBook Pages Did NOT Migrate Unfortunately, the first of the two commands shown in the previous section did NOT work, presumably because book pages belong ONLY to their parent book object, and not to any \u0026ldquo;collection\u0026rdquo;. So, the MigrateObject command won\u0026rsquo;t work. No worries, we will deal with this wrinkle near the end of this process.\nThe Collection I subsequently visited the new collection at https://isle-stage.grinnell.edu/islandora/object/grinnell:migration-test/manage where I found this summary of the results to confirm:\nFigure 1 \u0026middot; Summary of Objects in grinnell:migration-test Note that the children of grinnell:10001 are NOT listed in this view, but I visited https://isle-stage.grinnell.edu/islandora/object/grinnell%3Amigration-test/manage/collection to confirm that they ARE a part of the new collection.\nMy Bag of Tricks This is not my first rodeo, so I turned to my blog in order to export the new collection. Specifically, I turned to Exporting, Editing \u0026amp; Replacing MODS Datastreams: Updated Technical Details. Only steps 1 and 2 would be necessary. Step 1a and 1b were done long ago on the staging server that I\u0026rsquo;m using, and Step 1c currently isn\u0026rsquo;t even possible (another story) so here\u0026rsquo;s what I did starting with Step 1d, step-by-step.\nStep 1d - Using Drush islandora_datastream_export To execute this step I needed to copy the export.sh script and ri-query.txt from DGDocker1, modify them a little, and make them available inside the Apache container on DGDockerX, something like this:\n╭─islandora@dgdockerx ~ ╰─$ rsync -aruvi islandora@dgdocker1.grinnell.edu:/home/islandora/export.sh . 23 ↵ receiving incremental file list \u0026gt;f+++++++++ export.sh sent 43 bytes received 894 bytes 624.67 bytes/sec total size is 782 speedup is 0.83 ╭─islandora@dgdockerx ~ ╰─$ rsync -aruvi islandora@dgdocker1.grinnell.edu:/home/islandora/ri-query.txt . receiving incremental file list \u0026gt;f+++++++++ ri-query.txt sent 43 bytes received 245 bytes 192.00 bytes/sec total size is 130 speedup is 0.45 Let\u0026rsquo;s have a look at the script and the SPARQL query\u0026hellip;\n╭─islandora@dgdockerx ~ ╰─$ cat export.sh ri-query.txt Apache=isle-apache-dg Target=/utility-scripts # wget https://gist.github.com/McFateM/5bd7e5b0fa5d2928b2799d039a4c0fab/raw/collections.list while read collection do cp -f ri-query.txt query.sparql sed -i \u0026#39;s|COLLECTION|\u0026#39;${collection}\u0026#39;|g\u0026#39; query.sparql docker cp query.sparql ${Apache}:${Target}/${collection}.sparql rm -f query.sparql q=${Target}/${collection}.sparql echo \u0026#34;Processing collection \u0026#39;${collection}\u0026#39;; Query is \u0026#39;${q}\u0026#39;...\u0026#34; docker exec -w ${Target} ${Apache} mkdir -p exported-MODS/${collection} docker exec -w /var/www/html/sites/default/ ${Apache} drush -u 1 islandora_datastream_export --export_target=${Target}/exported-MODS/${collection} --query=${q} --query_type=islandora_datastream_exporter_ri_query --dsid=MODS done \u0026lt; collections.list ╭─islandora@dgdockerx ~ ╰─$ cp ri-query.txt query.sparql ╭─islandora@dgdockerx ~ ╰─$ cat query.sparql SELECT ?pid FROM \u0026lt;#ri\u0026gt; WHERE { ?pid \u0026lt;fedora-rels-ext:isMemberOfCollection\u0026gt; \u0026lt;info:fedora/grinnell:COLLECTION\u0026gt; } OFFSET %offset% \u0026hellip;and make some modifications to our instances to produce these\u0026hellip;\n╭─islandora@dgdockerx ~ ╰─$ cat export.sh Apache=isle-apache-dgs Target=/utility-scripts collection=migration-test # docker cp query.sparql ${Apache}:${Target}/${collection}.sparql q=${Target}/${collection}.sparql echo \u0026#34;Processing collection \u0026#39;${collection}\u0026#39;; Query is \u0026#39;${q}\u0026#39;...\u0026#34; docker exec -w ${Target} ${Apache} mkdir -p exported-MODS/${collection} docker exec -w /var/www/html/sites/default/ ${Apache} drush -u 1 islandora_datastream_export --export_target=${Target}/exported-MODS/${collection} --query=${q} --query_type=islandora_datastream_exporter_ri_query --dsid=MODS ╭─islandora@dgdockerx ~ ╰─$ cat query.sparql SELECT ?pid FROM \u0026lt;#ri\u0026gt; WHERE { ?pid \u0026lt;fedora-rels-ext:isMemberOfCollection\u0026gt; \u0026lt;info:fedora/grinnell:migration-test\u0026gt; } OFFSET %offset% Now, let\u0026rsquo;s run the export.sh script\u0026hellip;\n╭─islandora@dgdockerx ~ ╰─$ source export.sh Processing collection \u0026#39;migration-test\u0026#39;; Query is \u0026#39;/utility-scripts/migration-test.sparql\u0026#39;... Processing results 1 to 10 [ok] Datastream exported succeeded for grinnell:1000. [success] Datastream exported succeeded for grinnell:19423. [success] Datastream exported succeeded for grinnell:17289. [success] Datastream exported succeeded for grinnell:23345. [success] Datastream exported succeeded for grinnell:10001. [success] Datastream exported succeeded for grinnell:10020. [success] Datastream exported succeeded for grinnell:5593. [success] Datastream exported succeeded for grinnell:10024. [success] Datastream exported succeeded for grinnell:16934. [success] Datastream exported succeeded for grinnell:13273. [success] Processing results 11 to 17 [ok] Datastream exported succeeded for grinnell:23517. [success] Datastream exported succeeded for grinnell:10021. [success] Datastream exported succeeded for grinnell:10022. [success] Datastream exported succeeded for grinnell:10025. [success] Datastream exported succeeded for grinnell:10023. [success] Datastream exported succeeded for grinnell:10026. [success] Datastream exported succeeded for grinnell:10027. [success] So, the above results should be found in our Apache container\u0026rsquo;s /utility-scripts/exported-MODS/migration-test/ directory, like so\u0026hellip;\n╭─islandora@dgdockerx ~ ╰─$ docker exec -it isle-apache-dgs bash root@db16bc9ff4c3:/# cd /utility-scripts/exported-MODS/migration-test/ root@db16bc9ff4c3:/utility-scripts/exported-MODS/migration-test# ll total 80 drwxr-xr-x. 2 root root 4096 Oct 18 14:56 ./ drwxr-xr-x. 3 root root 4096 Oct 13 14:41 ../ -rw-r--r--. 1 root root 3339 Oct 18 14:56 grinnell_10001_MODS.xml -rw-r--r--. 1 root root 3003 Oct 18 14:56 grinnell_1000_MODS.xml -rw-r--r--. 1 root root 3482 Oct 18 14:56 grinnell_10020_MODS.xml -rw-r--r--. 1 root root 3475 Oct 18 14:56 grinnell_10021_MODS.xml -rw-r--r--. 1 root root 3503 Oct 18 14:56 grinnell_10022_MODS.xml -rw-r--r--. 1 root root 3496 Oct 18 14:56 grinnell_10023_MODS.xml -rw-r--r--. 1 root root 3482 Oct 18 14:56 grinnell_10024_MODS.xml -rw-r--r--. 1 root root 3483 Oct 18 14:56 grinnell_10025_MODS.xml -rw-r--r--. 1 root root 3488 Oct 18 14:56 grinnell_10026_MODS.xml -rw-r--r--. 1 root root 3526 Oct 18 14:56 grinnell_10027_MODS.xml -rw-r--r--. 1 root root 3696 Oct 18 14:56 grinnell_13273_MODS.xml -rw-r--r--. 1 root root 3099 Oct 18 14:56 grinnell_16934_MODS.xml -rw-r--r--. 1 root root 2168 Oct 18 14:56 grinnell_17289_MODS.xml -rw-r--r--. 1 root root 3425 Oct 18 14:56 grinnell_19423_MODS.xml -rw-r--r--. 1 root root 3580 Oct 18 14:56 grinnell_23345_MODS.xml -rw-r--r--. 1 root root 2047 Oct 18 14:56 grinnell_23517_MODS.xml -rw-r--r--. 1 root root 5308 Oct 18 14:56 grinnell_5593_MODS.xml Note that book pages, like grinnell:1001 through grinnell:1014 have no MODS records so we are not really missing anything here.\nNext, lets copy the MODS .xml files that we have back to the DGDockerX host.\n╭─islandora@dgdockerx ~ ╰─$ mkdir migration-test ╭─islandora@dgdockerx ~ ╰─$ docker cp isle-apache-dgs:/utility-scripts/exported-MODS/migration-test/. migration-test/. ╭─islandora@dgdockerx ~ ╰─$ ll migration-test total 80K drwxrwxr-x. 2 islandora islandora 4.0K Oct 18 11:45 . drwx------. 22 islandora islandora 4.0K Oct 18 11:46 .. -rw-r--r--. 1 islandora islandora 3.3K Oct 18 09:56 grinnell_10001_MODS.xml -rw-r--r--. 1 islandora islandora 3.0K Oct 18 09:56 grinnell_1000_MODS.xml -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10020_MODS.xml -rw-r--r--. 1 islandora islandora 3.4K Oct 18 09:56 grinnell_10021_MODS.xml -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10022_MODS.xml -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10023_MODS.xml -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10024_MODS.xml -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10025_MODS.xml -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10026_MODS.xml -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10027_MODS.xml -rw-r--r--. 1 islandora islandora 3.7K Oct 18 09:56 grinnell_13273_MODS.xml -rw-r--r--. 1 islandora islandora 3.1K Oct 18 09:56 grinnell_16934_MODS.xml -rw-r--r--. 1 islandora islandora 2.2K Oct 18 09:56 grinnell_17289_MODS.xml -rw-r--r--. 1 islandora islandora 3.4K Oct 18 09:56 grinnell_19423_MODS.xml -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_23345_MODS.xml -rw-r--r--. 1 islandora islandora 2.0K Oct 18 09:56 grinnell_23517_MODS.xml -rw-r--r--. 1 islandora islandora 5.2K Oct 18 09:56 grinnell_5593_MODS.xml And since /mnt/storage isn\u0026rsquo;t accessible* let\u0026rsquo;s copy them to my Mac workstation, MA10713, which will be taking the place of iMac 8660 in the next step. So, on my Mac workstation\u0026hellip;\n*Yes, I have an open ticket for ITS to remedy that\u0026hellip;someday, maybe?\n╭─mcfatem@MAC02FK0XXQ05Q ~ ╰─$ mkdir migration-test ╭─mcfatem@MAC02FK0XXQ05Q ~ ╰─$ cd migration-test ╭─mcfatem@MAC02FK0XXQ05Q ~/migration-test ╰─$ rsync -aruvi islandora@dgdockerx.grinnell.edu:/home/islandora/migration-test/. . --progress receiving file list ... 18 files to consider .d..t.... ./ \u0026gt;f..t.... grinnell_10001_MODS.xml 3339 100% 3.18MB/s 0:00:00 (xfer#1, to-check=16/18) \u0026gt;f..t.... grinnell_1000_MODS.xml 3003 100% 2.86MB/s 0:00:00 (xfer#2, to-check=15/18) \u0026gt;f+++++++ grinnell_10020_MODS.xml 3482 100% 1.66MB/s 0:00:00 (xfer#3, to-check=14/18) \u0026gt;f+++++++ grinnell_10021_MODS.xml 3475 100% 1.10MB/s 0:00:00 (xfer#4, to-check=13/18) \u0026gt;f+++++++ grinnell_10022_MODS.xml 3503 100% 855.22kB/s 0:00:00 (xfer#5, to-check=12/18) \u0026gt;f+++++++ grinnell_10023_MODS.xml 3496 100% 682.81kB/s 0:00:00 (xfer#6, to-check=11/18) \u0026gt;f+++++++ grinnell_10024_MODS.xml 3482 100% 566.73kB/s 0:00:00 (xfer#7, to-check=10/18) \u0026gt;f+++++++ grinnell_10025_MODS.xml 3483 100% 485.91kB/s 0:00:00 (xfer#8, to-check=9/18) \u0026gt;f+++++++ grinnell_10026_MODS.xml 3488 100% 486.61kB/s 0:00:00 (xfer#9, to-check=8/18) \u0026gt;f+++++++ grinnell_10027_MODS.xml 3526 100% 430.42kB/s 0:00:00 (xfer#10, to-check=7/18) \u0026gt;f..t.... grinnell_13273_MODS.xml 3696 100% 401.04kB/s 0:00:00 (xfer#11, to-check=6/18) \u0026gt;f..t.... grinnell_16934_MODS.xml 3099 100% 302.64kB/s 0:00:00 (xfer#12, to-check=5/18) \u0026gt;f..t.... grinnell_17289_MODS.xml 2168 100% 176.43kB/s 0:00:00 (xfer#13, to-check=4/18) \u0026gt;f..t.... grinnell_19423_MODS.xml 3425 100% 257.29kB/s 0:00:00 (xfer#14, to-check=3/18) \u0026gt;f..t.... grinnell_23345_MODS.xml 3580 100% 249.72kB/s 0:00:00 (xfer#15, to-check=2/18) \u0026gt;f..t.... grinnell_23517_MODS.xml 2047 100% 142.79kB/s 0:00:00 (xfer#16, to-check=1/18) \u0026gt;f..t.... grinnell_5593_MODS.xml 5308 100% 323.97kB/s 0:00:00 (xfer#17, to-check=0/18) sent 678 bytes received 29316 bytes 19996.00 bytes/sec total size is 57600 speedup is 1.92 ╭─mcfatem@MAC02FK0XXQ05Q ~/migration-test ╰─$ ll total 144 -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.3K Oct 18 09:56 grinnell_10001_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 2.9K Oct 18 09:56 grinnell_1000_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 09:56 grinnell_10020_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 09:56 grinnell_10021_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 09:56 grinnell_10022_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 09:56 grinnell_10023_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 09:56 grinnell_10024_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 09:56 grinnell_10025_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 09:56 grinnell_10026_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 09:56 grinnell_10027_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.6K Oct 18 09:56 grinnell_13273_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.0K Oct 18 09:56 grinnell_16934_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 2.1K Oct 18 09:56 grinnell_17289_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.3K Oct 18 09:56 grinnell_19423_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.5K Oct 18 09:56 grinnell_23345_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 2.0K Oct 18 09:56 grinnell_23517_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 5.2K Oct 18 09:56 grinnell_5593_MODS.xml Finally\u0026hellip; Step 2 - Map-MODS-to-MASTER Python 3 Script iMac 8660 is due to be retired/removed any day now, so I\u0026rsquo;m moving all components of this step to my Mac workstation, MA10713. Since the project is stored in GitHub I was able to do this:\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub ╰─$ git clone https://github.com/DigitalGrinnell/Map-MODS-to-MASTER Cloning into \u0026#39;Map-MODS-to-MASTER\u0026#39;... remote: Enumerating objects: 597, done. remote: Counting objects: 100% (597/597), done. remote: Compressing objects: 100% (528/528), done. remote: Total 597 (delta 49), reused 587 (delta 42), pack-reused 0 Receiving objects: 100% (597/597), 5.07 MiB | 8.50 MiB/s, done. Resolving deltas: 100% (49/49), done. ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub ╰─$ ll total 32 drwxr-xr-x 21 mcfatem GRIN\\Domain Users 672B Oct 12 2021 Digital-Grinnell.github.io drwxr-xr-x 12 mcfatem GRIN\\Domain Users 384B Oct 13 10:20 Map-MODS-to-MASTER ... Houston, We Have a Problem So, the main.py script in the Map-MODS-to-MASTER project does a lot of heavy lifting, and it\u0026rsquo;s got lots of problems as a result. Namely:\nIt\u0026rsquo;s very tightly tied to a couple of //storage directories that I currently can\u0026rsquo;t access, It\u0026rsquo;s meant to process a list of collections with very specific locations, not a single list of .xml documents, and It was written using PyCharm which I hardly use anymore because it promotes bad Python habits. Houston, We Have a Solution Fortunately, the process_collection function inside main.py has all that we need. So, I\u0026rsquo;m going to make a local copy of the project on MA10713 at ~/GitHub/migrate-MODS-xml and keep just the parts of process_collection that I need. It\u0026rsquo;s also worth noting that I plan to do this using VSCode instead of PyCharm and I\u0026rsquo;ll follow Proper Python guidance too.\nmigrate-MODS-xml See the new project\u0026rsquo;s README.md file for details.\nRunning the Script After some necessary modifications the script was easy to run. From start to finish the process involved the command line sequence listed below.\nCopied the MODS .xml directory and files from ~/migration-test to the new project like so: ╭─mcfatem@MAC02FK0XXQ05Q ~ ╰─$ cp -fr migration-test ~/GitHub/migrate-MODS-xml/. Changed working directory to the project and activated the Python virtual environment there: ╭─mcfatem@MAC02FK0XXQ05Q ~ ╰─$ cd ~/GitHub/migrate-MODS-xml ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/migrate-MODS-xml ╰─$ source .venv/bin/activate Changed the working directory to the new copy of the migration-test directory and ran the script there: (.venv) ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/migrate-MODS-xml/migration-test ╰─$ python3 ../main.py Outcomes After the first run of the script the migration-test directory looked like this:\n(.venv) ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/migrate-MODS-xml/migration-test ‹main› ╰─$ ll total 552 -rw-r--r-- 1 mcfatem GRIN\\Domain Users 30K Oct 18 11:55 collection.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.6K Oct 18 11:55 grinnell_10001_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 270B Oct 18 11:55 grinnell_10001_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.3K Oct 18 11:54 grinnell_10001_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.3K Oct 18 11:55 grinnell_1000_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 125B Oct 18 11:55 grinnell_1000_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 2.9K Oct 18 11:54 grinnell_1000_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.3K Oct 18 11:55 grinnell_10020_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 180B Oct 18 11:55 grinnell_10020_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:54 grinnell_10020_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.3K Oct 18 11:55 grinnell_10021_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 180B Oct 18 11:55 grinnell_10021_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:54 grinnell_10021_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:55 grinnell_10022_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 254B Oct 18 11:55 grinnell_10022_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:54 grinnell_10022_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:55 grinnell_10023_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 247B Oct 18 11:55 grinnell_10023_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:54 grinnell_10023_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:55 grinnell_10024_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 247B Oct 18 11:55 grinnell_10024_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:54 grinnell_10024_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:55 grinnell_10025_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 247B Oct 18 11:55 grinnell_10025_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:54 grinnell_10025_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.3K Oct 18 11:55 grinnell_10026_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 180B Oct 18 11:55 grinnell_10026_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:54 grinnell_10026_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:55 grinnell_10027_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 259B Oct 18 11:55 grinnell_10027_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:54 grinnell_10027_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.0K Oct 18 11:55 grinnell_13273_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 225B Oct 18 11:55 grinnell_13273_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.6K Oct 18 11:54 grinnell_13273_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.4K Oct 18 11:55 grinnell_16934_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 495B Oct 18 11:55 grinnell_16934_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.0K Oct 18 11:54 grinnell_16934_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 4.2K Oct 18 11:55 grinnell_17289_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 970B Oct 18 11:55 grinnell_17289_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 2.1K Oct 18 11:54 grinnell_17289_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.5K Oct 18 11:55 grinnell_19423_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 594B Oct 18 11:55 grinnell_19423_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.3K Oct 18 11:54 grinnell_19423_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.2K Oct 18 11:55 grinnell_23345_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 190B Oct 18 11:55 grinnell_23345_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 3.5K Oct 18 11:54 grinnell_23345_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 2.3K Oct 18 11:55 grinnell_23517_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 37B Oct 18 11:55 grinnell_23517_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 2.0K Oct 18 11:54 grinnell_23517_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 4.8K Oct 18 11:55 grinnell_5593_MODS.log -rw-r--r-- 1 mcfatem GRIN\\Domain Users 314B Oct 18 11:55 grinnell_5593_MODS.remainder -rw-r--r-- 1 mcfatem GRIN\\Domain Users 5.2K Oct 18 11:54 grinnell_5593_MODS.xml -rw-r--r-- 1 mcfatem GRIN\\Domain Users 26K Oct 18 11:55 mods.csv Note that there\u0026rsquo;s an overall collection.log file and the all-important mods.csv file which is what we came here for. Also, each object has an original .xml export file, a new .log file, and a .remainder file which lists those elements of the corresponding .xml that were NOT copied into mods.csv.\nThe mods.csv file was to be shared with others who will assist with future migration efforts so I\u0026rsquo;ve copied it to a new shared folder on network storage at //storage/library/all-staff/DG-migration-test. I made the copy using Finder on MA10713 where that folder was mounted using a Connect to Server... specification of smb://storage/library/allstaff.\nExtracting the OBJ Datastreams This step was a piece-of-cake since I\u0026rsquo;d already done this for all of the MODS datastreams. So, I repeated Step 1d - Using Drush islandora_datastream_export changing dsid=MODS to dsid=OBJ, and exported-MODS to exported-OBJ, in the export.sh script. In order to preserve export.sh as-is, I made a new export-OBJ.sh script from it on DGDockerX.\nHaving created the new script with edits specified above, I ran it producing the output you see here.\n╭─islandora@dgdockerx ~ ╰─$ cat export-OBJ.sh Apache=isle-apache-dgs Target=/utility-scripts collection=migration-test # docker cp query.sparql ${Apache}:${Target}/${collection}.sparql q=${Target}/${collection}.sparql echo \u0026#34;Processing collection \u0026#39;${collection}\u0026#39;; Query is \u0026#39;${q}\u0026#39;...\u0026#34; docker exec -w ${Target} ${Apache} mkdir -p exported-OBJ/${collection} docker exec -w /var/www/html/sites/default/ ${Apache} drush -u 1 islandora_datastream_export --export_target=${Target}/exported-OBJ/${collection} --query=${q} --query_type=islandora_datastream_exporter_ri_query --dsid=OBJ ╭─islandora@dgdockerx ~ ╰─$ source export-OBJ.sh Processing collection \u0026#39;migration-test\u0026#39;; Query is \u0026#39;/utility-scripts/migration-test.sparql\u0026#39;... Processing results 1 to 10 [ok] Datastream export failed for grinnell:1000. The object does not [error] contain the OBJ datastream. Datastream exported succeeded for grinnell:19423. [success] Datastream exported succeeded for grinnell:17289. [success] Datastream exported succeeded for grinnell:23345. [success] Datastream export failed for grinnell:10001. The object does not [error] contain the OBJ datastream. Datastream exported succeeded for grinnell:10020. [success] Datastream exported succeeded for grinnell:5593. [success] Datastream exported succeeded for grinnell:10024. [success] Datastream exported succeeded for grinnell:16934. [success] Datastream exported succeeded for grinnell:13273. [success] Processing results 11 to 17 [ok] Datastream exported succeeded for grinnell:23517. [success] Datastream exported succeeded for grinnell:10021. [success] Datastream exported succeeded for grinnell:10022. [success] Datastream exported succeeded for grinnell:10025. [success] Datastream exported succeeded for grinnell:10023. [success] Datastream exported succeeded for grinnell:10026. [success] Datastream exported succeeded for grinnell:10027. [success] As you can see, two of our objects, grinnell:1000 and grinnell:10001 had no OBJ to export; this is because the first is a \u0026ldquo;book\u0026rdquo; object and the other is a compound \u0026ldquo;parent\u0026rdquo;. Those CModel types have no OBJ\u0026rsquo;s of their own, they have only child objects. Also, you may recall that the pages of our book, grinnell:1000 are not part of the migration-test collection, but they have valid OBJ datastreams. So, how can we collect the OBJs of those pages?\nA New SPARQL Query That\u0026rsquo;s how\u0026hellip; we need to modifiy our SPARQL query (currently it exists as /home/islandora/query.sparql on DGDockerX) to return children, in this case \u0026ldquo;pages\u0026rdquo;, of our book object rather than children of a particular collection. The RELS-EXT datastreams of the children we want look something like this:\n\u0026lt;rdf:RDF\u0026gt; \u0026lt;rdf:Description rdf:about=\u0026#34;info:fedora/grinnell:1001\u0026#34;\u0026gt; \u0026lt;hasModel rdf:resource=\u0026#34;info:fedora/islandora:pageCModel\u0026#34;/\u0026gt; \u0026lt;isSequenceNumber rdf:datatype=\u0026#34;http://www.w3.org/2001/XMLSchema#int\u0026#34;\u0026gt;1\u0026lt;/isSequenceNumber\u0026gt; \u0026lt;isPageNumber rdf:datatype=\u0026#34;http://www.w3.org/2001/XMLSchema#int\u0026#34;\u0026gt;1\u0026lt;/isPageNumber\u0026gt; \u0026lt;isPageOf rdf:resource=\u0026#34;info:fedora/grinnell:1000\u0026#34;/\u0026gt; \u0026lt;isSection rdf:datatype=\u0026#34;http://www.w3.org/2001/XMLSchema#int\u0026#34;\u0026gt;1\u0026lt;/isSection\u0026gt; \u0026lt;isMemberOf rdf:resource=\u0026#34;info:fedora/grinnell:1000\u0026#34;/\u0026gt; \u0026lt;islandora:isManageableByUser\u0026gt;fedoraAdmin\u0026lt;/islandora:isManageableByUser\u0026gt; \u0026lt;islandora:isManageableByUser\u0026gt;System Admin\u0026lt;/islandora:isManageableByUser\u0026gt; \u0026lt;islandora:isManageableByRole\u0026gt;Administrator\u0026lt;/islandora:isManageableByRole\u0026gt; \u0026lt;islandora:isManageableByRole\u0026gt;Faulconer Admin\u0026lt;/islandora:isManageableByRole\u0026gt; \u0026lt;islandora:isManageableByRole\u0026gt;Workflow Administrator\u0026lt;/islandora:isManageableByRole\u0026gt; \u0026lt;islandora:isManageableByRole\u0026gt;administrator\u0026lt;/islandora:isManageableByRole\u0026gt; \u0026lt;islandora:hasLanguage\u0026gt;eng\u0026lt;/islandora:hasLanguage\u0026gt; \u0026lt;/rdf:Description\u0026gt; \u0026lt;/rdf:RDF\u0026gt; So I created a new SPARQL query as you see below and executed the new export-OBJ.sh script like so:\n╭─islandora@dgdockerx ~ ╰─$ cat export-OBJ.sh 1 ↵ SELECT ?pid FROM \u0026lt;#ri\u0026gt; WHERE { ?pid \u0026lt;fedora-rels-ext:isMemberOf\u0026gt; \u0026lt;info:fedora/grinnell:1000\u0026gt; } OFFSET %offset% ╭─islandora@dgdockerx ~ ╰─$ source export-OBJ.sh 1 ↵ Processing collection \u0026#39;migration-test\u0026#39;; Query is \u0026#39;/utility-scripts/migration-test.sparql\u0026#39;... Processing results 1 to 10 [ok] Datastream exported succeeded for grinnell:1009. [success] Datastream exported succeeded for grinnell:1008. [success] Datastream exported succeeded for grinnell:1014. [success] Datastream exported succeeded for grinnell:1012. [success] Datastream exported succeeded for grinnell:1007. [success] Datastream exported succeeded for grinnell:1011. [success] Datastream exported succeeded for grinnell:1005. [success] Datastream exported succeeded for grinnell:1013. [success] Datastream exported succeeded for grinnell:1002. [success] Datastream exported succeeded for grinnell:1001. [success] Processing results 11 to 14 [ok] Datastream exported succeeded for grinnell:1003. [success] Datastream exported succeeded for grinnell:1010. [success] Datastream exported succeeded for grinnell:1006. [success] Datastream exported succeeded for grinnell:1004. [success] As before, since /mnt/storage isn\u0026rsquo;t accessible from DGDockerX I copied all of the OBJ datastream files from our Apache container to the DGDockerX host, then to my Mac workstation, MA10713.\n╭─islandora@dgdockerx ~ ╰─$ cd migration-test ╭─islandora@dgdockerx ~/migration-test ╰─$ docker cp isle-apache-dgs:/utility-scripts/exported-OBJ/migration-test/. . ╭─islandora@dgdockerx ~/migration-test ╰─$ ll total 726M drwxrwxr-x. 2 islandora islandora 4.0K Oct 18 12:40 . drwx------. 22 islandora islandora 4.0K Oct 18 12:40 .. -rw-r--r--. 1 islandora islandora 3.3K Oct 18 09:56 grinnell_10001_MODS.xml -rw-r--r--. 1 islandora islandora 3.0K Oct 18 09:56 grinnell_1000_MODS.xml -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1001_OBJ.tiff -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10020_MODS.xml -rw-r--r--. 1 islandora islandora 217K Oct 18 12:07 grinnell_10020_OBJ.pdf -rw-r--r--. 1 islandora islandora 3.4K Oct 18 09:56 grinnell_10021_MODS.xml -rw-r--r--. 1 islandora islandora 62K Oct 18 12:07 grinnell_10021_OBJ.pdf -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10022_MODS.xml -rw-r--r--. 1 islandora islandora 19M Oct 18 12:07 grinnell_10022_OBJ.bin -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10023_MODS.xml -rw-r--r--. 1 islandora islandora 25M Oct 18 12:07 grinnell_10023_OBJ.bin -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10024_MODS.xml -rw-r--r--. 1 islandora islandora 30K Oct 18 12:07 grinnell_10024_OBJ.tar -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10025_MODS.xml -rw-r--r--. 1 islandora islandora 11K Oct 18 12:07 grinnell_10025_OBJ.bin -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10026_MODS.xml -rw-r--r--. 1 islandora islandora 451K Oct 18 12:07 grinnell_10026_OBJ.pdf -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_10027_MODS.xml -rw-r--r--. 1 islandora islandora 485K Oct 18 12:07 grinnell_10027_OBJ.ppt -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1002_OBJ.tiff -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1003_OBJ.tiff -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1004_OBJ.tiff -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1005_OBJ.tiff -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1006_OBJ.tiff -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1007_OBJ.tiff -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1008_OBJ.tiff -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1009_OBJ.tiff -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1010_OBJ.tiff -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1011_OBJ.tiff -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1012_OBJ.tiff -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1013_OBJ.tiff -rw-r--r--. 1 islandora islandora 32M Oct 18 12:31 grinnell_1014_OBJ.tiff -rw-r--r--. 1 islandora islandora 3.7K Oct 18 09:56 grinnell_13273_MODS.xml -rw-r--r--. 1 islandora islandora 36M Oct 18 12:07 grinnell_13273_OBJ.mp4 -rw-r--r--. 1 islandora islandora 3.1K Oct 18 09:56 grinnell_16934_MODS.xml -rw-r--r--. 1 islandora islandora 229K Oct 18 12:07 grinnell_16934_OBJ.jpg -rw-r--r--. 1 islandora islandora 2.2K Oct 18 09:56 grinnell_17289_MODS.xml -rw-r--r--. 1 islandora islandora 5.4M Oct 18 12:07 grinnell_17289_OBJ.warc -rw-r--r--. 1 islandora islandora 3.4K Oct 18 09:56 grinnell_19423_MODS.xml -rw-r--r--. 1 islandora islandora 16M Oct 18 12:07 grinnell_19423_OBJ.mp3 -rw-r--r--. 1 islandora islandora 3.5K Oct 18 09:56 grinnell_23345_MODS.xml -rw-r--r--. 1 islandora islandora 1.2M Oct 18 12:07 grinnell_23345_OBJ.pdf -rw-r--r--. 1 islandora islandora 2.0K Oct 18 09:56 grinnell_23517_MODS.xml -rw-r--r--. 1 islandora islandora 16M Oct 18 12:07 grinnell_23517_OBJ.tiff -rw-r--r--. 1 islandora islandora 5.2K Oct 18 09:56 grinnell_5593_MODS.xml -rw-r--r--. 1 islandora islandora 163M Oct 18 12:07 grinnell_5593_OBJ.mp3 ╭─mcfatem@MAC02FK0XXQ05Q ~ ╰─$ cd migration-test ╭─mcfatem@MAC02FK0XXQ05Q ~/migration-test ╰─$ rsync -aruvi islandora@dgdockerx.grinnell.edu:/home/islandora/migration-test/. . --progress receiving file list ... 47 files to consider .d..t.... ./ \u0026gt;f+++++++ grinnell_1001_OBJ.tiff 33349912 100% 18.41MB/s 0:00:01 (xfer#1, to-check=43/47) \u0026gt;f+++++++ grinnell_10020_OBJ.pdf 222174 100% 293.60kB/s 0:00:00 (xfer#2, to-check=41/47) \u0026gt;f+++++++ grinnell_10021_OBJ.pdf 62947 100% 82.85kB/s 0:00:00 (xfer#3, to-check=39/47) \u0026gt;f+++++++ grinnell_10022_OBJ.bin 19144317 100% 11.38MB/s 0:00:01 (xfer#4, to-check=37/47) \u0026gt;f+++++++ grinnell_10023_OBJ.bin 25843886 100% 14.05MB/s 0:00:01 (xfer#5, to-check=35/47) \u0026gt;f+++++++ grinnell_10024_OBJ.tar 30720 100% 39.68kB/s 0:00:00 (xfer#6, to-check=33/47) \u0026gt;f+++++++ grinnell_10025_OBJ.bin 11032 100% 14.23kB/s 0:00:00 (xfer#7, to-check=31/47) \u0026gt;f+++++++ grinnell_10026_OBJ.pdf 461189 100% 579.64kB/s 0:00:00 (xfer#8, to-check=29/47) \u0026gt;f+++++++ grinnell_10027_OBJ.ppt 496640 100% 607.01kB/s 0:00:00 (xfer#9, to-check=27/47) \u0026gt;f+++++++ grinnell_1002_OBJ.tiff 33356688 100% 13.93MB/s 0:00:02 (xfer#10, to-check=26/47) \u0026gt;f+++++++ grinnell_1003_OBJ.tiff 33349840 100% 17.93MB/s 0:00:01 (xfer#11, to-check=25/47) \u0026gt;f+++++++ grinnell_1004_OBJ.tiff 33354720 100% 14.07MB/s 0:00:02 (xfer#12, to-check=24/47) \u0026gt;f+++++++ grinnell_1005_OBJ.tiff 33350900 100% 18.11MB/s 0:00:01 (xfer#13, to-check=23/47) \u0026gt;f+++++++ grinnell_1006_OBJ.tiff 33352664 100% 14.12MB/s 0:00:02 (xfer#14, to-check=22/47) \u0026gt;f+++++++ grinnell_1007_OBJ.tiff 33353120 100% 18.26MB/s 0:00:01 (xfer#15, to-check=21/47) \u0026gt;f+++++++ grinnell_1008_OBJ.tiff 33352888 100% 14.27MB/s 0:00:02 (xfer#16, to-check=20/47) \u0026gt;f+++++++ grinnell_1009_OBJ.tiff 33353256 100% 18.54MB/s 0:00:01 (xfer#17, to-check=19/47) \u0026gt;f+++++++ grinnell_1010_OBJ.tiff 33349940 100% 14.43MB/s 0:00:02 (xfer#18, to-check=18/47) \u0026gt;f+++++++ grinnell_1011_OBJ.tiff 33351652 100% 18.68MB/s 0:00:01 (xfer#19, to-check=17/47) \u0026gt;f+++++++ grinnell_1012_OBJ.tiff 33353972 100% 14.54MB/s 0:00:02 (xfer#20, to-check=16/47) \u0026gt;f+++++++ grinnell_1013_OBJ.tiff 33352848 100% 18.91MB/s 0:00:01 (xfer#21, to-check=15/47) \u0026gt;f+++++++ grinnell_1014_OBJ.tiff 33349120 100% 14.60MB/s 0:00:02 (xfer#22, to-check=14/47) \u0026gt;f+++++++ grinnell_13273_OBJ.mp4 37367789 100% 19.45MB/s 0:00:01 (xfer#23, to-check=12/47) \u0026gt;f+++++++ grinnell_16934_OBJ.jpg 233941 100% 271.01kB/s 0:00:00 (xfer#24, to-check=10/47) \u0026gt;f+++++++ grinnell_17289_OBJ.warc 5593634 100% 4.89MB/s 0:00:01 (xfer#25, to-check=8/47) \u0026gt;f+++++++ grinnell_19423_OBJ.mp3 16448993 100% 19.13MB/s 0:00:00 (xfer#26, to-check=6/47) \u0026gt;f+++++++ grinnell_23345_OBJ.pdf 1227513 100% 1.34MB/s 0:00:00 (xfer#27, to-check=4/47) \u0026gt;f+++++++ grinnell_23517_OBJ.tiff 16394056 100% 9.75MB/s 0:00:01 (xfer#28, to-check=2/47) \u0026gt;f+++++++ grinnell_5593_OBJ.mp3 169971589 100% 19.76MB/s 0:00:08 (xfer#29, to-check=0/47) sent 660 bytes received 760629901 bytes 21426212.99 bytes/sec total size is 760499540 speedup is 1.00 I subsequently used Finder on my Mac to copy all of these files to //storage/library/all-staff/DG-migration-test.\nAzure Storage Two weeks ago I wrote this\u0026hellip;\n\u0026ldquo;Very soon I hope to have Azure Storage Explorer access to some new network, actually Azure, shares from my MacBook. When that happens I\u0026rsquo;ll take steps to copy all of the above files to a new web-accessible home there, I hope.\u0026rdquo;\nWell, that hasn\u0026rsquo;t happened yet so I\u0026rsquo;ve taken steps to create an Azure Blog Storage container without the benefit of the new ASE capability. The process of copying our migrgation-test collection objects and metadata to Azure is documented in another of my blog posts, Managing Azure.\nMigration Target: JSTOR This morning I imported the mods.csv metadata into Excel where I added two new columns, FILE_OBJECT and FILE_EXTENSION. These columns, intended for use with JSTOR, document the name and extension of the object\u0026rsquo;s content file.\nThe result, mods-JSTOR.xlsx is stored both in Azure and in //storage/library/all-staff/DG-migration-test.\nI\u0026rsquo;m sure there will be more here soon, but for now\u0026hellip; that\u0026rsquo;s a wrap.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/129-creating-a-migration-collection/","tags":["migration","collection","Digital.Grinnell","iduf","JSTOR","Collection Builder"],"title":"Creating a Migration Collection"},{"categories":null,"contents":"This morning I needed to do some bulk \u0026ldquo;find and replace\u0026rdquo; operations in most of my Rootstalk content. My first thought was to write and run a little Python script, but then I wondered what VSCode might bring to the table. Plenty, it brought plenty!\nTo keep this post as brief as possible, I\u0026rsquo;m simply going to reference an instructional video that I created earlier: VSCode-Find-and-Replace.mp4.\nA list of some links mentioned in the video is provided here:\nhttps://linuxpip.org/vscode-regex-replace/ https://itnext.io/vscode-find-and-replace-regex-super-powers-c7f8be0fa80f https://www.educative.io/answers/regex-search-and-replace-with-vs-code https://www.youtube.com/watch?v=xMhKstbdr3k https://remisharrock.fr/post/regex-search-and-replace-visual-studio-code/ If you\u0026rsquo;re looking specifically to install and use the regex-previewer extension check out:\nhttps://www.youtube.com/watch?v=u6Eesv7aqHo https://marketplace.visualstudio.com/items?itemName=chrmarti.regex https://stackoverflow.com/questions/51389446/using-the-vscode-regex-plugin Since Rootstalk code is in a private repo, I\u0026rsquo;m including the contents of my link-format.js file here:\nvar find_period = /\\[(.+)\\]\\((.+)\\)\\./ var replace_period = /[$1.]($2) / var find_comma = /\\[(.+)\\]\\((.+)\\)\\,/ var replace_comma = /[$1,]($2) / ","permalink":"https://static.grinnell.edu/dlad-blog/posts/128-vscode-find-and-replace-using-regex/","tags":["VSCode","regex","find","replace","Rootstalk"],"title":"VSCode Find and Replace Using `regex`"},{"categories":null,"contents":"Lately I\u0026rsquo;ve lamented that all incoming emails to @grinnell.edu addresses pass through a URLDefense / Proofpoint agent that sanitizes all clickable links as a safety/security precaution. In the case of emails automatically dispatched by GitHub the \u0026ldquo;butchering\u0026rdquo; of such messages leaves me with an almost useles notification, one that\u0026rsquo;s so badly bloated that I typically choose to ignore it. Clearly, that\u0026rsquo;s not how notifications are suppsoed to be handled.\nExample of a Butchered Email To help make my point I\u0026rsquo;ll share a small portion of a relatively small email notification below. Yes, I used the term \u0026ldquo;small\u0026rdquo; more than once in that last sentence. Why? Because this is a tiny example compared to some that I\u0026rsquo;ve received lately.\nFrom: Digital Grinnell \u0026lt;noreply@github.com\u0026gt; Date: Wednesday, October 5, 2022 at 10:05 AM To: Digital \u0026lt;Digital@grinnell.edu\u0026gt;, O\u0026#39;Connor, Michael (Mikey) \u0026lt;oconnorm@grinnell.edu\u0026gt; Subject: [Digital-Grinnell/rootstalk] dada8b: Added cover.png for spring-2022 issue Branch: refs/heads/main Home: https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_Digital-2DGrinnell_rootstalk\u0026amp;d=DwICaQ\u0026amp;c=HUrdOLg_tCr0UMeDjWLBOM9lLDRpsndbROGxEKQRFzk\u0026amp;r=D8E-oGNaPT9srWV6jE8UP5unsmKEmmHEH-tzgmjBvLk\u0026amp;m=PxA-gY8Zp47fU9OAwKk5FS-WDskxWx5Ds811Ur96m2fR1d-9W6-SolFNKVJ3ie7F\u0026amp;s=99LzMdHVnq9emX5VBUv5OtRzZ81iaCQmseS437hauKo\u0026amp;e= Commit: dada8bd78f0e4ed646ea4ba4e07f9557ee666363 https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_Digital-2DGrinnell_rootstalk_commit_dada8bd78f0e4ed646ea4ba4e07f9557ee666363\u0026amp;d=DwICaQ\u0026amp;c=HUrdOLg_tCr0UMeDjWLBOM9lLDRpsndbROGxEKQRFzk\u0026amp;r=D8E-oGNaPT9srWV6jE8UP5unsmKEmmHEH-tzgmjBvLk\u0026amp;m=PxA-gY8Zp47fU9OAwKk5FS-WDskxWx5Ds811Ur96m2fR1d-9W6-SolFNKVJ3ie7F\u0026amp;s=XOT9lo9k0F7SsJQ3SmQcxpTdJGOWvjzwrQmffHzZp4A\u0026amp;e= Author: Digital-Grinnell \u0026lt;digital@grinnell.edu\u0026gt; Date: 2022-10-05 (Wed, 05 Oct 2022) Changed paths: A content/volume-viii-issue-1/cover.png Log Message: ----------- Added cover.png for spring-2022 issue Commit: d4d12b4cc67f415b549fd38d294217c232778a5b https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_Digital-2DGrinnell_rootstalk_commit_d4d12b4cc67f415b549fd38d294217c232778a5b\u0026amp;d=DwICaQ\u0026amp;c=HUrdOLg_tCr0UMeDjWLBOM9lLDRpsndbROGxEKQRFzk\u0026amp;r=D8E-oGNaPT9srWV6jE8UP5unsmKEmmHEH-tzgmjBvLk\u0026amp;m=PxA-gY8Zp47fU9OAwKk5FS-WDskxWx5Ds811Ur96m2fR1d-9W6-SolFNKVJ3ie7F\u0026amp;s=Uf6BJsdOR914O0kqZcJQ_xz_qBJUpZBpqDFVIbaD6HM\u0026amp;e= Author: Digital Grinnell \u0026lt;digital@grinnell.edu\u0026gt; Date: 2022-10-05 (Wed, 05 Oct 2022) Changed paths: A content/volume-viii-issue-1/cover.png Log Message: ----------- Merge pull request #29 from Digital-Grinnell/2022-spring Added cover.png for spring-2022 issue Compare: https://urldefense.proofpoint.com/v2/url?u=https-3A__github.com_Digital-2DGrinnell_rootstalk_compare_327c81e44589...d4d12b4cc67f\u0026amp;d=DwICaQ\u0026amp;c=HUrdOLg_tCr0UMeDjWLBOM9lLDRpsndbROGxEKQRFzk\u0026amp;r=D8E-oGNaPT9srWV6jE8UP5unsmKEmmHEH-tzgmjBvLk\u0026amp;m=PxA-gY8Zp47fU9OAwKk5FS-WDskxWx5Ds811Ur96m2fR1d-9W6-SolFNKVJ3ie7F\u0026amp;s=7lxwCBl_cGynsG6a2WnM54-jwgLTD7muzEi4hnR6e98\u0026amp;e= The Slack Alternative The same notification as it appears in the Rootstalk.dev channel in Slack looks like this:\nFigure 1 \u0026middot; Example Notification in Slack Now, that\u0026rsquo;s much better; don\u0026rsquo;t you agree?\nIn the image it\u0026rsquo;s important to note that such notifications always appear in the Apps section of the workspace, and inside the GitHub app specifically.\nInstalling the Integration The agent that makes this kind of GitHub-to-Slack notification possible is itself a GitHub project called integrations/slack. In that repo\u0026rsquo;s README.md file there\u0026rsquo;s a link to Install the GitHub integration for Slack. I successfully followed that link, the process took about 10 minutes, to install the integration into my Slack\u0026rsquo;s Rootstalk.dev workspace. Some Duo authentication was necessary but overall the process was simple and straightforward.\nNo More Bloated Emails from GitHub The project\u0026rsquo;s current Vivero fellow was able to repeat much the same install process on their workstation in about the same amount of time at our weekly Rootstalk check-in meeting this morning. Now both of us receive these notifications so I\u0026rsquo;ve removed our email addresses from the GitHub repo\u0026rsquo;s Settings and Email notifications configuration. Hence, no more bloated notification emails from GitHub!\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/127-github-notifications-to-slack/","tags":["GitHub","Slack","notifications"],"title":"Pushing GitHub Notifications to Slack"},{"categories":null,"contents":"This blog post chronicles portions of a process used to restore and subsequently WARC (the creation of a web archive) a Communication Department website that had been retired. The website content of interest included material describing plans for the recently-completed HSSC and Administration Building projects.\nDuring the restoration and WARC process a .md document named WARC-from-comm.sites.grinnell.edu-clone.md was created and it\u0026rsquo;s contents are presented here.\nCreating a WARC from a Clone of comm.sites.grinnell.edu In July 2022 a new \u0026ldquo;clone\u0026rdquo; of the original comm.sites.grinnell.edu web site project \u0026ndash; a Wordpress copy that contains posts and supporting information regarding campus construction projects including the HSSC and Adminissions Center \u0026ndash; was created. That clone can be found, and administered, from https://comm.sites.grinnell.edu/clone/wp-admin/index.php.\nTurning Redirection Off At present, both the comm.sites.grinnell.edu and my clone at comm.sites.grinnell.edu/clone are redirected to https://www.grinnell.edu/about/leadership/offices-services/institutional-planning/campus-plan. That\u0026rsquo;s not the site that we want to WARC, so I need to turn redirection off in the /clone site so that it can be WARC\u0026rsquo;d, I hope.\nFirst Attempt On the https://comm.sites.grinnell.edu/clone/wp-admin/tools.php?page=redirection.php page I will \u0026ldquo;disable\u0026rdquo; all redirects by selecting all of the listed URLs, then under Bulk Actions I\u0026rsquo;ll choose Disable click the Apply button. Done.\nOutcome Unfortunately, comm.sites.grinnell.edu/clone still redirects to https://www.grinnell.edu/about/leadership/offices-services/institutional-planning/campus-plan. :frown:\nSecond Attempt So, now I\u0026rsquo;m going to delete the old redirection using the same process, but selecting Delete rather than Disable. Done.\nOutcome Nope, still redirected.\nThird Attempt I\u0026rsquo;m going to visit settings and try to point to a new address like comm-clone.sites.grinnell.edu. So, I\u0026rsquo;ll change the Site Address (URL) field from http://comm.sites.grinnell.edu/clone to http://comm-clone.sites.grinnell.edu.\nOutcome Nope, no longer redirected but I got a big SORRY message saying the site could not be found.\nEnlisting Help from DLAC Clearly, my attempts to \u0026ldquo;clone\u0026rdquo; the old comm site in a form that could be successfully archived had failed, so I turned to the Digital Liberal Arts Collaborative (DLAC) and their Reclaim Hosting admin powers.\nThe next section of this document is a thread of emails captured as a PDF document and subsequently converted to Markdown format for publication here.\nDLAC Email Thread Thread elements are in reverse-chronological order.\nSubject: Re: email\nDate: Friday, July 29, 2022 at 3:09:11 PM Central Daylight Time\nFrom: Pelzel, Morris\nTo: Rodrigues, Elizabeth, McFate, Mark\nOK, should be ready to go \u0026hellip; https://dg-dev.sites.grinnell.edu.\nMo\nDr. Morris Pelzel\nFrom: Rodrigues, Elizabeth rodrigue8@grinnell.edu\nSent: Friday, July 29, 2022 11:07 AM\nTo: Pelzel, Morris pelzelmo@grinnell.edu; McFate, Mark mcfatem@grinnell.edu\nSubject: Re: email\nThanks, Mo, and I\u0026rsquo;m sorry about the jargon. A WARC is a web archive file format created through a process of crawling a site.\nIf we could clone the site to dg-dev directly, I think that would be our best bet for a next thing to try. Basically, we want to be able to crawl the site as it was originally published in wordpress.\nElizabeth Rodrigues, PhD\nFrom: Pelzel, Morris pelzelmo@grinnell.edu\nSent: Friday, July 29, 2022 11:04 AM\nTo: Rodrigues, Elizabeth rodrigue8@grinnell.edu; McFate, Mark mcfatem@grinnell.edu\nSubject: Re: email\nHi Mark and Liz,\nI\u0026rsquo;m back in town and taking a look at this. I\u0026rsquo;m trying to get clear for myself exactly what it is that you want to do, so it may be best for us to meet in person sometime next week to sort things out. When you refer to WP \u0026ldquo;modules\u0026rdquo; Mark, I assume you mean plug-ins?\nIn general, we handle redirects, backups, restorations, migrations, and the like, in cPanel, and not in WordPress itself. It\u0026rsquo;s just cleaner and simpler to do it that way.\nPerhaps the issue is that we set up the clone as a subdirectory instead of a subdomain. As a subdirectory, the clone remains part of the original domain, so the redirect cannot be removed. If we instead created it as a subdomain, then it would appear in the list of domains in the cPanel Domains module, and we could then remove the redirects for that subdomain.\nBut would it not be easier just to clone the site directly in dg- dev.sites.grinnell.edu? We should be able to clone a WP site from one cPanel account (comms) into another (dg-dev). Then we should be able to turn off any redirects.\nLet me know if I am on the right track here.\nAlso \u0026hellip; I do not know (and perhaps do not need to know) what WARC is.\nThanks,\nMo\nFrom: Rodrigues, Elizabeth rodrigue8@grinnell.edu\nSent: Wednesday, July 27, 2022 4:50 PM\nTo: McFate, Mark mcfatem@grinnell.edu; Pelzel, Morris pelzelmo@grinnell.edu\nSubject: Re: email\nAnd I\u0026rsquo;d add that the pain point here is the redirect that Comm currently has set up. It doesn\u0026rsquo;t appear to be changeable from within the cloned copy, and when Mark tried reconstructing the site on his own subdomain using Updraft, the homepage worked but all the links still pointed back to the cloned comm site with the apparently baked in redirect.\nIs getting a copy with no redirect possible? Or does comm have to stop the redirect from within their own cPanel long enough for us to copy it?\nBy redirect, I mean comm.sites.grinnell.edu now redirects to https://www.grinnell.edu/about/leadership/offices-services/institutional-planning/campus-plan. We have confirmed that the WP site has unique content, and on top of that, WARCing the redirected address leads to WARCing the whole college site\u0026hellip;as we learned.\nThanks for any insight you have!\nLiz\nFrom: McFate, Mark mcfatem@grinnell.edu\nSent: Wednesday, July 27, 2022 2:57 PM\nTo: Pelzel, Morris pelzelmo@grinnell.edu\nCc: Rodrigues, Elizabeth rodrigue8@grinnell.edu\nSubject: Re: email\nGood afternoon, Mo.\nI’ve been waiting on some ITS changes to DG today and turned my attention back to comm.sites.grinnell.edu/clone for a bit. In that site’s wp-admin I tried turning off, then deleting, the “Redirection” module, but that had no effect.\nSo, I tried changing the site’s “sekngs” to have it resolve to a different URL, and that didn’t work. Then I tried changing it to resolve to my new https://dg- dev.sites.grinnell.edu address, but that also failed.\nLiz suggested trying the “Updraft” module to migrate the site and provided an article with guidance. Once I’d completed the prescribed backup process, I tried to restore the backup into dg-dev.sites.grinnell.edu, but was warned that the free version of “Updraft” is for “backup only”, and not to be used for “migration”. The migration add-on costs extra, or one must purchase Updraft Premium. 8^(\nWell, I didn’t like that answer so I proceeded with the restoration anyway. The outcome was interesting\u0026hellip; I got a copy of the old comm.sites.grinnell.edu home page at https://dg-dev.sites.grinnell.edu, but all of the navigation was still redirected to their new site, and some nav elements didn’t work at all.\nSo, that was not a site that I can WARC as intended.\nThe other effect of restoring from backup was that I lost access to dg-dev.sites.grinnell.edu/wp-admin, since that address always asked me to login and then took me back to comm.sites.grinnell.edu/wp-admin again. So, I opened the cPanel for dg-dev.sites.grinnell.edu and uninstalled WordPress, and have since re-installed a pristine copy and I have wp-admin access there once again.\nThrough all of this we looked at different means of properly “migrating” the old WordPress site at comm.sites.grinnell.edu/wp-admin to my new domain at dg-dev.sites.grinnell.edu, but everything I’ve found so far suggests that there is no easy DIY process, there are only $$$$ options available. Even Reclaim’s own discussion about migration suggests the same\u0026hellip;\nhttps://reclaimhostig.com/migration-assistance/.\nSo, I’m wondering if you have a recommendation for me\u0026hellip;. How can we easily get the WordPress content that’s in comm.sites.grinnell.edu migrated to dg-dev.sites.grinnell.edu?\nThanks for any advice you can offer. Take care.\n-Mark M.\nFrom: McFate, Mark mcfatem@grinnell.edu\nDate: Monday, July 25, 2022 at 10:44 AM\nTo: Pelzel, Morris pelzelmo@grinnell.edu\nSubject: Re: email\nOk, thanks Mo. No worries, and no rush. Take care.\n-Mark M.\nFrom: Pelzel, Morris pelzelmo@grinnell.edu\nDate: Monday, July 25, 2022 at 10:42 AM\nTo: McFate, Mark mcfatem@grinnell.edu\nSubject: email\nMark,\nI\u0026rsquo;m setting up the domain you requested. If you just received an email about your password, please ignore it\u0026hellip;I accidentally left a check box checked (that should have been unchecked).\nI\u0026rsquo;ll send you more information in a moment.\nMo\nAttempting to WARC https://dg-dev.sites.grinnell.edu DLAC was able to properly clone the old comm site into my https://dg-dev.sites.grinnell.edu Wordpress space, without redirection, so my hope was restored. I set about creating a WARC of that site\u0026hellip;\nFirst wget from My MacBook Pro wget --warc-file=living-and-learning-community-web-archive --recursive --level=5 --warc-cdx --page-requisites --html-extension --convert-links --execute robots=off --directory-prefix=. -x /solr-search --wait=10 --random-wait https://dg-dev.sites.grinnell.edu/ FINISHED --2022-08-01 12:08:09-- Total wall clock time: 18m 19s Downloaded: 94 files, 11M in 1m 13s (156 KB/s) Second wget from iMac wget --warc-file=living-and-learning-community-web-archive --recursive --level=10 --warc-cdx --page-requisites --html-extension --convert-links --execute robots=off --directory-prefix=. -x /solr-search --wait=10 --random-wait https://dg-dev.sites.grinnell.edu/ FINISHED --2022-08-01 14:04:25-- Total wall clock time: 16m 13s Downloaded: 94 files, 11M in 7.3s (1.54 MB/s) Outcome Since both wget operations returned 94 files it\u0026rsquo;s safe to assume that constitutes a complete archive.\nOn the iMac the process produced the following .cdx index and .warc.gz compressed archive\u0026hellip;\n╭─markmcfate@MAD25W812UJ1G9 ~ ‹ruby-2.3.0› ╰─$ ls -alh living* -rw-r--r-- 1 markmcfate staff 35K Aug 1 14:04 living-and-learning-community-web-archive.cdx -rw-r--r-- 1 markmcfate staff 9.1M Aug 1 14:04 living-and-learning-community-web-archive.warc.gz WARC is NOT Complete Unfortunately, the WARC mentioned above is woefully incomplete because the WordPress reconstruction of the old site is also incomplete. A lot of the old content regarding projects like the HSSC, Adminssions Center, and campus Landscaping were still \u0026ldquo;published\u0026rdquo; in the site, but excluded from navigation so the wget... command used to produce the WARC was unable to \u0026ldquo;find\u0026rdquo; them.\nI enlisted the help of Donna D., an original author of the site, to reassemble things as best we could. Now that that\u0026rsquo;s done (8-Aug-2022) I\u0026rsquo;m kicking off a new WARC process on iMac 8660, like so\u0026hellip;\n╭─markmcfate@MAD25W812UJ1G9 ~/dg-dev.sites.grinnell.edu ‹ruby-2.3.0› ╰─$ time wget --warc-file=living-and-learning-community-web-archive --recursive --level=10 --warc-cdx --page-requisites --html-extension --convert-links --execute robots=off --directory-prefix=. -x /solr-search --wait=10 --random-wait https://dg-dev.sites.grinnell.edu/ Opening WARC file ‘living-and-learning-community-web-archive.warc.gz’. /solr-search: Scheme missing. --2022-09-08 13:00:47-- https://dg-dev.sites.grinnell.edu/ Resolving dg-dev.sites.grinnell.edu (dg-dev.sites.grinnell.edu)... 165.227.97.167 Connecting to dg-dev.sites.grinnell.edu (dg-dev.sites.grinnell.edu)|165.227.97.167|:443... connected. HTTP request sent, awaiting response... 200 OK Length: unspecified [text/html] Saving to: ‘./dg-dev.sites.grinnell.edu/index.html’ 0K .......... .......... .......... .......... .......... 314K 50K .......... .......... .......... .......... .... 6.78M=0.2s 2022-09-08 13:00:48 (569 KB/s) - ‘./dg-dev.sites.grinnell.edu/index.html’ saved [96460] ... Converting links in ./dg-dev.sites.grinnell.edu/wp-content/themes/twentyseventeen/assets/css/blocks.css?ver=20220524.css... nothing to do. Converting links in ./dg-dev.sites.grinnell.edu/wp-content/plugins/cool-timeline/assets/css/ctl_styles.min.css?ver=2.4.4.css... nothing to do. Converting links in ./dg-dev.sites.grinnell.edu/wp-content/plugins/cool-timeline/assets/css/prettyPhoto.css?ver=2.4.4.css... 100. 37-63 Converted links in 198 files in 2.6 seconds. wget --warc-file=living-and-learning-community-web-archive --recursive 30.23s user 31.62s system 0% cpu 2:59:10.63 total Success! The output from the above operation includes /Users/markmcfate/dg-dev.sites.grinnell.edu/living-and-learning-community-web-archive.warc.gz and a corresponding .cdx file both stored on iMac 8660. Both files have also been copied to my OneDrive so the .gz file also exists at /Users/markmcfate/Library/CloudStorage/OneDrive-GrinnellCollege/iMac-Home-Folder-07-Sep-2022/living-and-learning-community-web-archive.warc.gz.\nMoving WARCs to //Storage I\u0026rsquo;ve striked the mention of OneDrive above because on my Macs I just don\u0026rsquo;t trust OneDrive anymore. Today I created a new WARCs folder in my OneDrive, or at least I thought I did, in order to consolidate my storage of WARC archives. Well, the folder structure that I see in my OneDrive on iMac 8660 doesn\u0026rsquo;t look the same as on my GC MacBook, MA01713. So, like I said, I just don\u0026rsquo;t trust it.\nI do have a reliable home for WARCs in //Storage, the college\u0026rsquo;s age-old network storage, so that\u0026rsquo;s where I\u0026rsquo;m going to put these precious files, at least for now. So I\u0026rsquo;ve mounted //Storage/Library/mcfatem on my MacBook as verified below\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q /Volumes/Library/mcfatem ╰─$ pwd /Volumes/Library/mcfatem Note: I keep a mount link in Finder on every Mac I have, it reads something like this under the Go | Connect to Server... menu: smb://storage/library/mcfatem.\nCapturning a WARC of https://rootstalk-archive.grinnell.edu While working on this WARC process I discovered that a very old website, https://rootstalk-archive.grinnell.edu, was still \u0026ldquo;active\u0026rdquo; (although the site certificates were now invalid) but overdue to be retired. So, I assumed it would be a good idea to capture a WARC. Due to the expired certificate I used this command on iMac 8660 to capture the site:\ntime wget --warc-file=rootstalk-archive-WARC --recursive --level=10 --warc-cdx --page-requisites --html-extension --convert-links --execute robots=off --directory-prefix=. -x /solr-search --wait=10 --random-wait --no-check-certificate https://rootstalk-archive.grinnell.edu/ The result is a pair of files, almost 2.5 GB in size, named:\n-rwx------@ 1 markmcfate staff 1.0M Sep 14 23:48 rootstalk-archive-WARC.cdx -rwx------@ 1 markmcfate staff 2.4G Sep 14 23:48 rootstalk-archive-WARC.warc.gz //Storage WARC Contents All of the aforementioned WARC capture files, and more, are now stored in //Storage as shown below\u0026hellip;\n╭─markmcfate@MAD25W812UJ1G9 /Volumes/mcfatem/warcs ‹ruby-2.3.0› ╰─$ ls -alh total 6481568 drwx------+ 1 markmcfate GRIN\\Domain Users 16K Sep 16 10:07 . drwx------+ 1 markmcfate GRIN\\Domain Users 16K Jul 13 10:23 .. -rwx------@ 1 markmcfate staff 361K Sep 8 15:59 living-and-learning-community-web-archive.cdx -rwx------@ 1 markmcfate staff 556M Sep 8 15:59 living-and-learning-community-web-archive.warc.gz -rwx------+ 1 markmcfate staff 74K Oct 27 2021 mime-and-me.warc.cdx -rwx------+ 1 markmcfate staff 101M Oct 27 2021 mime-and-me.warc.warc.gz -rwx------@ 1 markmcfate staff 1.0M Sep 14 23:48 rootstalk-archive-WARC.cdx -rwx------@ 1 markmcfate staff 2.4G Sep 14 23:48 rootstalk-archive-WARC.warc.gz -rwx------@ 1 markmcfate staff 621K Sep 14 14:36 wget-log Verified Using https://replayweb.page I turned to ReplayWeb in order to confirm the validity of the two \u0026ldquo;new\u0026rdquo; WARCs listed above. Using that tool I was able to successfully load and subsequently browse both of the most recently captured WARCs, specifically\u0026hellip;\nhttps://replayweb.page/?source=file%3A%2F%2Fliving-and-learning-community-web-archive.warc.gz#view=resources\u0026amp;urlSearchType=prefix\u0026amp;url=https%3A%2F%2Fdg-dev.sites.grinnell.edu%2F\u0026amp;ts=20220908180047, and https://replayweb.page/?source=file%3A%2F%2Frootstalk-archive-WARC.warc.gz#view=resources\u0026amp;urlSearchType=prefix\u0026amp;url=https%3A%2F%2Frootstalk-archive.grinnell.edu\u0026amp;ts=20220914203341 Note that, because of its size, the ReplayWeb rendering of rootstalk-archive-WARC.warc.gz takes a very, very, very long time to load and even longer to render!\nUpdate After the COVID-19 pandemic was declared a thing of the past in June 2023, Grinnell College made the decision to archive the coronavirus portion of its website, https://www.grinnell.edu/campus-life/campus-living/health-wellness/coronavirus/.\nMy first attempts to capture that site caught way too much stuff, presumably because the coronavirus page includes a global menu that opens up all of https://www.grinnell.edu. So, I added the --no-parent option to my wget command, but in that form the command didn\u0026rsquo;t catch much at all. I found it necessary to also drop the trailing slash at the end of https://www.grinnell.edu/campus-life/campus-living/health-wellness/coronavirus/ from my original command, and then the capture looked reasonable.\nSo, the wget that I ultimately used was this:\nmcfatem@MAD25W812UJ1G9 ~ % wget --warc-file=coronavirus-pages-web-archive --recursive --level=10 --warc-cdx --page-requisites --html-extension --convert-links --execute robots=off --directory-prefix=. -x /solr-search --wait=10 --random-wait --no-parent https://www.grinnell.edu/campus-life/campus-living/health-wellness/coronavirus Note that there\u0026rsquo;s no trailing slash on the https://... specification!\nLike the WARCs that came before, this capture has been copied to //Storage/Library/mcfatem/warcs/ for safe-keeping.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/126-creating-a-warc-from-a-wordpress-clone/","tags":["Reclaim Hosting","Wordpress","WARC","web archive","comm.sites.grinnell.edu","//Storage","ReplayWeb.page","rootstalk-archive"],"title":"Creating a WARC from a Wordpress Clone"},{"categories":null,"contents":"This blog post will be used to chronicle a process I\u0026rsquo;m using to rebuild Digital.Grinnell in Legacy Islandora using ISLE. This process was triggered by ITS\u0026rsquo; intent to move DG\u0026rsquo;s FEDORA repository to a new server. That new server is currently mounted on node DGDockerX, my staging server, as /mnt/datastage and it contains a copy of DG\u0026rsquo;s production FEDORA repository made on or about June 15, 2020.\nPORTABLE-DG For starters I\u0026rsquo;m going be doing lots of \u0026ldquo;local\u0026rdquo; ISLE work up-front so I\u0026rsquo;m creating a USB drive backup/copy of /mnt/datastage. The USB drive is named PORTABLE-DG. In order to populate it I first attached the drive to my MacBook Pro (which is now woefully short on USB \u0026lsquo;A\u0026rsquo; adapters) and started a series of rsync processes while that MacBook is connected to the campus network in Grinnell.\nNote that I had hoped to control this process from the iMac (asset tag: 8660) that\u0026rsquo;s still in my Burling office, but it\u0026rsquo;s not behaving well for the last couple of weeks so that\u0026rsquo;s a no-go.\nUsing rsync On my MacBook Pro with an SSH connection and terminal open to DGDockerX I started with this\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~ ╰─$ cd /Volumes/PORTABLE-DG/FEDORA ╭─mcfatem@MAC02FK0XXQ05Q /Volumes/PORTABLE-DG/FEDORA ╰─$ rsync -aruvi islandora@dgdockerx.grinnell.edu:/mnt/datastage/datastreamStore . --progress That operation completely tied up my MacBook Pro, so once I got control of my iMac again I moved PORTABLE-DG over there, opened a Screen Sharing connection to the iMac\u0026hellip; and died a little (more) inside. While I can open the Screen Sharing connection again, presumably because both Macs are on campus Ethernet, the iMac won\u0026rsquo;t even recognize the USB drive. So, I might as well be back home where systems and peripherals all work\u0026hellip; sometimes very slowly.\nSo I need two things from the same machine\u0026hellip; a FAST connection to campus network storage, and a working large-capacity external drive. The only way I can get that now is via my MacBook Pro, sitting in my office with an Ethernet connection, and PORTABLE-DG plugged into it. Problem is I can\u0026rsquo;t use the MacBook Pro for ANYTHING else while this is running, and I can\u0026rsquo;t disconnect it for as long as the rsync operations take\u0026hellip; and that could be a couple of days.\nIf I go home with the MacBook Pro it will mean pulling almost 1 TB of data from campus storage over my internet connection at home, and that could easily take a week.\nAnd that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/125-rebuiding-isle-for-digital.grinnell/","tags":["ISLE","Digital.Grinnell"],"title":"Rebuilding ISLE for Digital.Grinnell"},{"categories":null,"contents":"A couple of months ago the Screen Sharing connection to the iMac in my campus office stopped working, presumably because ITS no longer allowed that connection. No matter, rather than working to get that restored I thought I\u0026rsquo;d dry using a Virtual Desktop connection to a Windows VM. I also had some need to work with Windows in other capacities so this seemed like a nice dual-purpose solution. It worked! Now I\u0026rsquo;m trying to remember and document how it works.\nSo, I\u0026rsquo;ve learned that I can connect to my virtual desktop in one of two ways, with or without VPN.\nWithout VPN To open my virtual desktop without a VPN connection I simply have to visit https://remotehorizon.grinnell.edu and login using my usual GC credentials. I haven\u0026rsquo;t fully tested yet, but I don\u0026rsquo;t think this link will work if I am on the VPN at the time.\nThe really nice thing about this is that I can use Apple\u0026rsquo;s new Universal Control feature to share my mouse and keyboard across multiple devices, but only when not using the VPN!\nWith a VPN Connection To open my virtual desktop with an active VPN connection I simply have to visit https://horizon.grinnell.edu and login using my usual GC credentials. I have verified that this link does NOT work if VPN is not active.\nDocumentation I haven\u0026rsquo;t perused it all, yet, but there is helpful guidance available in Sharepoint.\nChoosing the Library VDI When horizon.grinnell.edu or remotehorizon.grinnell.edu opens in our browser we should see a window like this:\nFigure 1 \u0026middot; VDI Client Window In that window we simply click on the Library VM icon since it is our only choice.\nConnecting to DGDocker1 The Library selection above opens a Windows desktop where I typically search for terminal, or just term, in the provided Search box. That search should return a link to the Windows Command Prompt application which I then use to login to host DGDocker1 as user islandora, like so:\nFigure 2 \u0026middot; Windows Command Prompt Login to DGDocker1 In the Command Prompt the following sequence of docker and drush commands is typical:\nFigure 3 \u0026middot; Running an ihcQ Report And that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/124-using-the-library-virtual-desktop/","tags":["Virtual Desktop","VDI","Universal Control"],"title":"Using the Library Virtual Desktop"},{"categories":null,"contents":"This might just be my shortest post ever in this blog, at least for now. At this early date it\u0026rsquo;s just a link to the Atom No More? blog post in my personal blog.\nThe note above indicates that this post superseeds 085-remote-atom, but there are other Atom-related posts that are also impacted.\nSome of these include:\nposts/033-adding-lastmod-date/ As time passes I\u0026rsquo;ll document here any work-specific changes I make to my new VSCode environments.\nReplacing atom . with code . The personal blog post mentioned above, Atom No More?, includes a procedure I used to implement code . to launch VSCode from a terminal window on any of my Mac workstations. As of this writing, June 22, 2022, I have successfully implmented this change on all of my Grinnell College workstations.\nReplacing Remote Atom Some time ago I documented my setup of Remote Atom which gave me the ability to type open an SSH tunnel from any workstation to a remote Linux host, like DGDocker1 or DGDockerX, and locally use my Atom editor to make and save changes to individual files remotely. The command was simple, ratom \u0026lt;filename\u0026gt; would do the trick.\nVSCode can be setup to do the same kind of thing and I\u0026rsquo;m following guidance at Remote Development using SSH to do just that, starting with DGDockerX, the Docker node where we build and run the staging version of Digital.Grinnell.\nIt works, and the setup was easier than with Atom, and perhaps better in the end? We shall see, but so far it\u0026rsquo;s looking very good.\nDGDockerX To begin this process I installed the necessary Remote Development extemsion pack. That was easy. Next, I opened my VPN connection to campus, as required for SSH, and then inside VSCode I did as I was told\u0026hellip;\n⇧⌘P to open the command pallet, entered Remote-SSH: Connect to Host... to initiate the command, selected Add New SSH Host..., and added DGDockerX using ssh islandora@dgdockerx.grinnell.edu -A When prompted to save the above confituration I did so into my workstation\u0026rsquo;s ~/.ssh/config file. Now, to open a new connection to DGDockerX I just repeat the first two steps above, then pick dgdockerx.grinnell.edu from the list provided. That\u0026rsquo;s it. Once that is done I can open the VSCode explorer and navigate through the remote host as needed.\nDGDocker1 The process was exactly as above, so now when I want to edit files on DGDocker1, I just do this in VSCode\u0026hellip;\n⇧⌘P to open the command pallet, entered Remote-SSH: Connect to Host... to initiate the command, select dgdocker1.grinnell.edu from the pull-down list that\u0026rsquo;s presented. And that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/123-migrating-workflows-from-atom-to-vscode/","tags":["Atom","VSCode","workflow"],"title":"Migrating Workflows from Atom to VSCode"},{"categories":null,"contents":"For now this is just a place-holder document that I hope to populate soon.\nAnd that\u0026rsquo;s a wrap. Until next time, be safe! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/122-moving-bags-to-cloud-storage/","tags":["SMB","NFS","Bagit"],"title":"Moving Bags to Cloud Storage"},{"categories":null,"contents":"Digital.Grinnell features a fair number of transcribed digital oral histories. Most are interviews conducted with Grinnelleans as they return to campus annually for alumni Reunion or Grinnell\u0026rsquo;s Multicultural Reunion.\nWhat follows is largely sharable \u0026ldquo;public\u0026rdquo; content lifted from a \u0026ldquo;private\u0026rdquo; GitHub repository at https://github.com/McFateM/OHScribe, the README.md and WORKFLOW.md files in particular.\nOHScribe! https://ohscribe.us.reclaim.cloud/\nNote: This link is the new production home for OHScribe! as of March 2022. It is now hosted on Reclaim Cloud. The old address, https://ohscribe.grinnell.edu, is no longer in service.\nOHScribe! code now resides in a private repository so a copy of its shared documents are also provided as public gists. They are:\nREADME.md, DEV_HISTORY.md, and WORKFLOW.md. This is a Python 3 and Flask web application designed to transform oral history transcripts, presumably created using InqScribe, into XML suitable for ingest into the Islandora Oral Histories Solution Pack to populate a TRANSCRIPT datastream and its derivatives. Islandora Oral Histories and the aforementioned solution pack are commonly referred to as IOH throughout this document.\nOHScribe! is intended to be used as part of a broader IOH transcription workflow developed at Grinnell College. The workflow documented in the section titled The Digital.Grinnell Oral History Workflow may be of significant interest to individuals tasked with transcribing IOH audio recordings. The aforementioned workflow document now includes a link to an 11.5 minute long training video.\nFormatted IOH Example The aforementioned workflow, application, and accompanying CSS (provided below) are intended to deliver oral histories that look something like this:\nNote that the names of speakers appear in different colors in the video window captions, and speaker names appear in bold in the indexed transcript below the video. Each speaker and corresponding text appears on a new line, and captions are superimposed over a thumbnail image of the speaker(s).\nInqScribe IOH Transcription Workflow A detailed description of the workflow intended for use with OHScribe! is provided in our WORKFLOW.md document.\nWorkflow Output The workflow documented in the aforementioned WORKFLOW.md should produce an XML formatted transcript which resembles the structure of the following example.\nNote that a sample of transcript XML, suitable for testing, now exists in the OHScribe! codebase at https://github.com/DigitalGrinnell/OHScribe/blob/reclaim/test/sample-from-InqScribe.xml.\n\u0026lt;transcript\u0026gt; \u0026lt;prologue/\u0026gt; \u0026lt;scene id=\u0026#34;1\u0026#34; in=\u0026#34;00:00:00.21\u0026#34; out=\u0026#34;00:00:12.07\u0026#34;\u0026gt;\u0026lt;speaker\u0026gt; Heather Riggs \u0026lt;/speaker\u0026gt; Heather | Okay, so.. Yeah, just before we start, if you could each go around and say your name, your class year, and where you live now, just for the microphone.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;2\u0026#34; in=\u0026#34;00:00:12.08\u0026#34; out=\u0026#34;00:00:18.02\u0026#34;\u0026gt;\u0026lt;speaker\u0026gt; Margo Gray \u0026lt;/speaker\u0026gt; Margo | Cool. I’m Margo Gray of the class of 2005, and, what else am I saying?\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;3\u0026#34; in=\u0026#34;00:00:18.03\u0026#34; out=\u0026#34;00:00:19.07\u0026#34;\u0026gt;Heather | Your home. \u0026lt;speaker\u0026gt; Maggie Montanaro \u0026lt;/speaker\u0026gt; Maggie | Where you live.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;4\u0026#34; in=\u0026#34;00:00:19.08\u0026#34; out=\u0026#34;00:00:21.14\u0026#34;\u0026gt;Margo | I live in Chicago, Illinois.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;5\u0026#34; in=\u0026#34;00:00:21.15\u0026#34; out=\u0026#34;00:00:26.07\u0026#34;\u0026gt;\u0026lt;speaker\u0026gt; Jenny Noyce \u0026lt;/speaker\u0026gt; Jenny | My name is Jenny Noyce, the class of 2005 and I live in Oakland, California.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;6\u0026#34; in=\u0026#34;00:00:26.08\u0026#34; out=\u0026#34;00:00:32.11\u0026#34;\u0026gt;Maggie | I’m Maggie Montanaro, also class of 2005, and I live in Avignon, France.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;7\u0026#34; in=\u0026#34;00:00:32.12\u0026#34; out=\u0026#34;00:00:39.17\u0026#34;\u0026gt;Heather | Wow. So, what are your strongest memories of Grinnell?\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;8\u0026#34; in=\u0026#34;00:00:39.18\u0026#34; out=\u0026#34;00:00:45.05\u0026#34;\u0026gt;Maggie | Harris Parties.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;9\u0026#34; in=\u0026#34;00:00:45.06\u0026#34; out=\u0026#34;00:00:53.00\u0026#34;\u0026gt;Jenny | Mud sliding in the rain. Maggie | Yeah, mud wrestling. Mud sliding on Mac Field. Lots and lots of work.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;10\u0026#34; in=\u0026#34;00:00:53.01\u0026#34; out=\u0026#34;00:01:30.27\u0026#34;\u0026gt;Margo | I guess I remember people, like, I still am in touch with a lot of people from Grinnell and yeah. So I don’t, I mean I don’t have like these really specific memories of like meeting people, but just mostly, like this whole sort of like pool of memories of times when I was hanging out with people or working with people or, yeah. Building the sort of, you don’t think of it when you’re there, it’s not like, \u0026#34;Ah, I’m building connections to last me!\u0026#34; You’re just like, \u0026#34;I’m hanging out with my friends.\u0026#34; But those sort of things tend to last. Maggie | Lots of good hanging out. Margo | Yes.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;11\u0026#34; in=\u0026#34;00:01:30.28\u0026#34; out=\u0026#34;00:02:01.00\u0026#34;\u0026gt;Heather | What kind of Harris parties did you have? Like themed... Maggie | All the, I assume they still have them, the hall ones like the Haines Underwear Ball, the Mary B. James, Disco... what else? Lots of just themed… Jenny | They started a fetish party. Maggie | Really? They still have fetish? Jenny | I never went to that one. Maybe I was too close-minded. Maggie | Yeah.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;12\u0026#34; in=\u0026#34;00:02:01.01\u0026#34; out=\u0026#34;00:02:18.14\u0026#34;\u0026gt;Heather | What are your first memories of Grinnell? Key parts of this exported transcript example are described in the sections below.\n\u0026lt;scene\u0026gt; Tags \u0026lt;scene\u0026gt; tags enclose data indicating who is speaking along with transcribed text from the recording. \u0026lt;scene\u0026gt; tags also include attributes indicating the position or scene\u0026rsquo;s id, in and out times provided in hours-minutes-seconds, or hh:mm:ss.s notation, and one or more optional \u0026lt;speaker\u0026gt; tags.\n\u0026lt;scene id=\u0026#34;1\u0026#34; in=\u0026#34;00:00:00.21\u0026#34; out=\u0026#34;00:00:12.07\u0026#34;\u0026gt; \u0026lt;speaker\u0026gt; Heather Riggs \u0026lt;/speaker\u0026gt; Heather | Okay, so.. Yeah, just before we start, if you could each go around and say your name, your class year, and where you live now, just for the microphone. \u0026lt;/scene\u0026gt; The example \u0026lt;scene\u0026gt; tag shown above carries an id of \u0026lsquo;1\u0026rsquo;, it\u0026rsquo;s the first scene in the transcript, with in and out times provided in hours-minutes-seconds, or hh:mm:ss.s notation. The in and out attributes indicate when the transcribed text is heard in the audio stream. This scene also encloses a \u0026lt;speaker\u0026gt; tag which, in this case, identifies the speaker \u0026ldquo;Heather\u0026rdquo;, as an individual with a full name of \u0026ldquo;Heather Riggs\u0026rdquo;.\nThe text of this scene, a single sentence, opens with a required speaker ID, the name of the speaker, Heather, followed by a pipe (|) delimiter.\nThis demonstrates an important rule\u0026hellip; Each line of transcribed text MUST begin with a speaker ID, a given/first name, or single word identifying the speaker, followed by a pipe delimiter.\n\u0026lt;scene id=\u0026#34;11\u0026#34; in=\u0026#34;00:01:30.28\u0026#34; out=\u0026#34;00:02:01.00\u0026#34;\u0026gt; Heather | What kind of Harris parties did you have? Like themed... Maggie | All the, I assume they still have them, the hall ones like the Haines Underwear Ball, the Mary B. James, Disco... what else? Lots of just themed… Jenny | They started a fetish party. Maggie | Really? They still have fetish? Jenny | I never went to that one. Maybe I was too close-minded. Maggie | Yeah. \u0026lt;/scene\u0026gt; The example \u0026lt;scene\u0026gt; tag shown above, with an id of \u0026lsquo;11\u0026rsquo;, includes in and out time attributes, and seven lines of transcribed text. Each line begins with a speaker ID, in this case the given/first name of the speaker followed by a pipe (|) delimiter. Note that this scene has NO \u0026lt;speaker\u0026gt; tags because the speakers: Heather, Maggie, and Jenny; all have corresponding \u0026lt;speaker\u0026gt; tags inside previous scenes.\n\u0026lt;speaker\u0026gt; Tags \u0026lt;speaker\u0026gt; tags are used to identify each speaker in a transcript by providing their full name, and associating the given/first name portion of their full name with subsequent speaker ID prefixes.\nAt least one \u0026lt;speaker\u0026gt; tag must appear for each speaker ID used in the transcript, and a speaker\u0026rsquo;s tag must appear BEFORE any/all corresponding speaker IDs.\nA \u0026lt;speaker\u0026gt; must occur only within an enclosing \u0026lt;scene\u0026gt; tag.\nSpeaker ID As previously mentioned, a speaker ID identifies a speaker by their given/first name or some single-word identifying term, like \u0026ldquo;Interviewer\u0026quot;or \u0026ldquo;Interviewee\u0026rdquo;. Each speaker ID consists of a given/first name, or other single-word identifier, followed by a pipe (|) delimiter.\nSpaces around pipe delimiters are recommended, but not required.\nXML for Ingest into IOH The Islandora Oral Histories (IOH) solution pack expects a TRANSCRIPT datastream of \u0026lt;cues\u0026gt; formatted like the following for successful ingest.\n\u0026lt;cues\u0026gt; \u0026lt;cue\u0026gt; \u0026lt;speaker\u0026gt;Heather Riggs\u0026lt;/speaker\u0026gt; \u0026lt;start\u0026gt;0.21\u0026lt;/start\u0026gt; \u0026lt;end\u0026gt;12.07\u0026lt;/end\u0026gt; \u0026lt;transcript\u0026gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_1\u0026#39;\u0026amp;gt;Heather: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; Okay, so.. Yeah, just before we start, if you could each go around and say your name, your class year, and where you live now, just for the microphone.\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026lt;/transcript\u0026gt; \u0026lt;/cue\u0026gt; \u0026lt;cue\u0026gt; \u0026lt;speaker\u0026gt;Margo Gray\u0026lt;/speaker\u0026gt; \u0026lt;start\u0026gt;12.08\u0026lt;/start\u0026gt; \u0026lt;end\u0026gt;18.02\u0026lt;/end\u0026gt; \u0026lt;transcript\u0026gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_2\u0026#39;\u0026amp;gt;Margo: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; Cool. I\u0026amp;#8217;m Margo Gray of the class of 2005, and, what else am I saying?\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026lt;/transcript\u0026gt; \u0026lt;/cue\u0026gt; \u0026lt;cue\u0026gt; \u0026lt;speaker\u0026gt;Heather Riggs \u0026amp;amp; Maggie Montanaro\u0026lt;/speaker\u0026gt; \u0026lt;start\u0026gt;18.03\u0026lt;/start\u0026gt; \u0026lt;end\u0026gt;19.07\u0026lt;/end\u0026gt; \u0026lt;transcript\u0026gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_1\u0026#39;\u0026amp;gt;Heather: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; Your home.\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_3\u0026#39;\u0026amp;gt;Maggie: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; Where you live.\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026lt;/transcript\u0026gt; \u0026lt;/cue\u0026gt; \u0026lt;cue\u0026gt; \u0026lt;speaker\u0026gt;Margo Gray\u0026lt;/speaker\u0026gt; \u0026lt;start\u0026gt;19.08\u0026lt;/start\u0026gt; \u0026lt;end\u0026gt;21.14\u0026lt;/end\u0026gt; \u0026lt;transcript\u0026gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_2\u0026#39;\u0026amp;gt;Margo: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; I live in Chicago, Illinois.\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026lt;/transcript\u0026gt; \u0026lt;/cue\u0026gt; \u0026lt;cue\u0026gt; \u0026lt;speaker\u0026gt;Jenny Noyce\u0026lt;/speaker\u0026gt; \u0026lt;start\u0026gt;21.15\u0026lt;/start\u0026gt; \u0026lt;end\u0026gt;26.07\u0026lt;/end\u0026gt; \u0026lt;transcript\u0026gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_4\u0026#39;\u0026amp;gt;Jenny: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; My name is Jenny Noyce, the class of 2005 and I live in Oakland, California.\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026lt;/transcript\u0026gt; \u0026lt;/cue\u0026gt; \u0026lt;cue\u0026gt; \u0026lt;speaker\u0026gt;Maggie Montanaro\u0026lt;/speaker\u0026gt; \u0026lt;start\u0026gt;26.08\u0026lt;/start\u0026gt; \u0026lt;end\u0026gt;32.11\u0026lt;/end\u0026gt; \u0026lt;transcript\u0026gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_3\u0026#39;\u0026amp;gt;Maggie: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; I\u0026amp;#8217;m Maggie Montanaro, also class of 2005, and I live in Avignon, France.\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026lt;/transcript\u0026gt; \u0026lt;/cue\u0026gt; In this format each \u0026lt;cue\u0026gt; tag within the enclosing \u0026lt;cues\u0026gt; tag represents one \u0026lt;scene\u0026gt; from the previously documented workflow output. Like a \u0026lt;scene\u0026gt;, each \u0026lt;cue\u0026gt; identifies a speaker, a start and end time, and one or more lines of transcribed text.\nOHScribe! can be used to transform output from the documented workflow into this format for ingest.\nRunning OHScribe! OHScribe! is accessible at https://ohscribe.us.reclaim.cloud and should run in any web browser. It permits a user to upload an XML file (presumably this is output from the aforementioned workflow), and if successful, it provides an output file in IOH-compatible XML format as a download.\nThis feature is no longer necessary in Reclaim Cloud. Since upload and download of content is provided the site will present the user with a required login screen like this:\nInterested users of OHScribe! should [request credentials via email to digital@grinnell.edu](mailto:digital@grinnell.edu?subject=OHScribe Credentials).\nUploading XML Once authorized, OHScribe! presents the user with a file upload form like so:\nSelecting the Browse... button will open a file-selection window on the local host. The user should select a single .XML transcript file for upload and click the Upload key to send it to the OHScribe! server.\nA successful file upload produces Main/Control screen like this:\nNote the message/status portion of the window just above the Main / Control Screen title. The message here in green print indicates a successful upload.\nTypical/Recommended Use Once an XML file has been successfully uploaded for processing, the user is presented with a six individual or single actions, or a seventh option to Do All of the Above. Users should ALWAYS choose Do All of the Above unless there are special circumstances and they have been instructed otherwise.\nTypical use of OHScribe! follows these steps.\nIn the Upload an XML File screen click the Browse button and navigate to a transcript XML file prepared in and exported from InqScribe. This action opens the selected XML file for processing with the path to the file reflected in the box at the top of the GUI. In the Main / Control Screen window click the Do All of the Above button. This inokes the six actions documented below. If successful it will transform the XML exported from InqScribe into the XML form required for IOH ingest, giving the user an option to download the transformed file. Click the Download button. This action converts \\\u0026lt;start\u0026gt; and \\\u0026lt;end\u0026gt; values from hours:minutes:seconds notation to the decimal seconds notation required for IOH. The changes are saved directly in the selected **IOH-** file. This file should be suitable for ingest into IOH. Single Actions OHScribe! divides the transformation of a transcript into six ordered, individual steps represented by the following single Action choices:\nClean-Up the XML - This action checks that the uploaded file has a .xml extension and subsequently parses the XML to verify its validity.\nTransform XML to IOH - This action transforms the valid XML creating a \u0026lt;cues\u0026gt; and enclosed \u0026lt;cue\u0026gt; tags from \u0026lt;scene\u0026gt; tags.\nConvert hh:mm:ss to Seconds - This action converts all of the in and out time codes from hours-minutes-seconds (hh:mm:ss.s) notation to necessary \u0026lt;start\u0026gt; and \u0026lt;end\u0026gt; tags expressed as the number of seconds measured from the beginning of the transcribed audio or video stream.\nFormat Speaker Tags - This action transforms all of the speaker tags and speaker IDs from the original transcription into IOH-formatted speaker tags.\nAnalyze Cue Times - This final action checks each of the \u0026lt;cue\u0026gt; tag contents against a fixed target of 10 lines. Cues with more than 10 lines of captioning may overwhelm or even overflow the underlying image or video content.\nNote that when a single action is performed the user must take steps to download intermediate results and upload those back to OHScribe! in order to perform the next step. Since this can become rather tedious, the option to Do All of the Above is recommended.\nA single action can be performed by clicking the radio button corresponding to the desired action, and clicking the Do Single Action button near the bottom of the Main / Control Screen as shown below.\nAction Results Results of a Do All of the Above action typically include status output in a box at the top of the window, a Message box explaining the outcome, and a Download your Output! button with instructions. The window typically looks something like this:\nSingle actions produce slightly different results which may also include Details of the output, and Guidance for follow-up actions as shown below.\nErrors Processing errors are generally presented with red text appearing in the status box at the top of the window, like so:\nUnexpected or unresolvable errors encountered in OHScribe! should be [reported to the author via email to digital@grinnell.edu](mailto:digital@grinnell.edu?subject=OHScribe Error) and/or added to the Issue queue at https://github.com/DigitalGrinnell/OHScribe/issues.\nCSS Required for Speaker Formatting To take advantage of the script\u0026rsquo;s \u0026ldquo;speaker formatting\u0026rdquo; capabilities you must add the following CSS, or something very similar, to the theme of the site where Islandora Oral Histories are displayed. This CSS produces coloring and formatting like that shown in the example above.\n/* Color, display and font additions for Oral Histories */ div.tier.active span { font-weight: bold; } div.tier.active span, div.tier.active span span.oh_speaker_text { color: black !important; } div.tier.active span span.oh_speaker_text { font-weight: normal !important; } span.oh_speaker_text { color: #ffff00 !important; /* yellow */ } span.oh_speaker_1 { display: block; color: #00ffff; /* aqua */ } span.oh_speaker_2 { display: block; color: #80ff00; /* bright green */ } span.oh_speaker_3 { display: block; color: #ff0000; /* bright red */ } span.oh_speaker_4 { display: block; color: #ff00ff; /* fuchsia */ } span.oh_speaker_5 { display: block; color: #ffbf00; /* orange */ } The Digital.Grinnell Oral History Workflow InqScribe IOH Transcription Workflow Grinnell College employs the transcription workflow described here when preparing oral histories for ingest into Digital Grinnell. This workflow includes a commercially available software tool called InqScribe and at Grinnell transcribers also frequently use a VEC USB Footpedal to help control playback of audio to be transcribed.\nTraining Video An 11.5 minute long is available to reinforce the concepts presented below.\nThe video moves very quickly, compressing a 2-hour transcription session down into 11.5 minutes. You may find it necessary to slow the playback down, or rewind and repeat portions of the video, using the controls available in your browser.\nWorkflow Description A typical transcription session generally involves the following steps\u0026hellip;\nOHScribe creates a new cue every time it encounters a timecode, so every timecode should be followed immediately by a newline, speaker name and pipe character. For areas of the recording that are dense with speaker changes, no timecode is needed to transition to the next speaker, i.e. the transcriber can represent a change in speaker by entering a newline, the speaker name and pipe character to start the next speaker\u0026rsquo;s dialogue. This will result in a cue that has mutliple speakers.\nInqScribe Snippets and Triggers (Shortcuts) InqScribe allows a transcriber to define and use Snippets, short bits of frequently-repeated text, with associated triggers or keyboard Shortcuts that make it easy to quickly add key elements to a transcript. The following are samples of Snippets and their corresponding Triggers/Shortcuts used in conjunction with our workflow.\nThe above image is an example of a Snippet we refer to as a \u0026lsquo;Speaker Timecode\u0026rsquo;. When triggered, this snippet will insert:\nA timecode, the ${TIME} variable portion of the snippet, and A name identifying the speaker providing the transcribed text that will follow. In this example the speaker\u0026rsquo;s name is \u0026lsquo;Darrell\u0026rsquo; and that name is followed by a REQUIRED space and a pipe character, the vertical bar, in the portion that reads Darrell | . There is a space after the pipe character so as to allow the thranscriber to simply press the triger and then immediately start to type the dialogue into InqScribe. OHScribe does not currently correctly parse a pipe character from other text unless it is surrounded by spaces on either side.\nIf a speaker has a double first name, the name will need to be hyphentated because OHScribe will only identify one word/name, separated from others by spaces, as the trigger name for the speaker. The first name in the \u0026lt;speaker\u0026gt; FirstName LastName \u0026lt;/speaker\u0026gt; tag MUST match the name that follows after the timecode.\nNote also that in this example our Speaker Timecode snippet is named Darrell Fisher and it is assigned to trigger KP1 which has a corresponding keyboard shortcut. The name of the timecode snippet is not important, and can be left vague/general, as with the Interviewer example.\nAny additional speakers can be represented in the same way by selecting \u0026lsquo;Add\u0026rsquo; and then filling in the correct information similarly to the example above.\nEach time a new speaker is introduced, there must be \u0026lt;speaker\u0026gt; FirstName LastName \u0026lt;/speaker\u0026gt; line added between the timecode and the FirstName | . Each speaker should only have one instance of speaker tags in the InqScribe file.\nThe second image, immediately above, is an example of a Snippet we refer to as a \u0026lsquo;Raw Timecode\u0026rsquo;. When triggered, this snippet will insert:\nA timecode, the value of the ${TIME} variable referenced in the snippet, and nothing else. Note that a Raw Timecode has no associated speaker name as it\u0026rsquo;s intended to be used when the speaker name is unknown, or when there isn\u0026rsquo;t time during transcription to pause for identification of the next speaker.\nThis example Raw Timecode snippet is named {$TIME} and it is assigned to the Enter trigger which generally corresponds to the Enter or Return key on the keyboard.\nExport to XML Once the transcription and timecodes are in place, save the InqScribe file and export it to an XML file by selecting File -\u0026gt; Export -\u0026gt; XML.\nAnd that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/121-new-oral-history-transcription-workflow/","tags":["Digital.Grinnell","Oral History","OHScribe","InqScribe","workflow"],"title":"New Oral History Transcription Workflow"},{"categories":null,"contents":"The addition of scholar profiles from LASIR, specifically the module\u0026rsquo;s introduction of /mods/identifier[@type='u1'] and /mods/identifier[@type='u2'] fields, has caused a few problems in Digital.Grinnell. Perhaps the most sinister of these\u0026hellip; these fields are transformed into DC or Dublin Core elements that wreak havoc with our OAI export and subsequent import into Primo VE.\nOAI Exports While on the subject of OAI, it\u0026rsquo;s worth noting here that we can query to see the OAI that Digital.Grinnell exported by visiting a URL like: https://digital.grinnell.edu/oai2?verb=ListRecords\u0026amp;metadataPrefix=oai_dc\u0026amp;from=2022-02-15.\nNote the from= parameter at the end of the address. Specifying a date here will show us what was exported on the specified date (and since that date?).\nA Possible Solution? I\u0026rsquo;ve confirmed that if all \u0026lt;dc:identifier\u0026gt; elements containing u1:* or u2:* values are removed from an object\u0026rsquo;s DC datastream, the object\u0026rsquo;s behavior in Digital.Grinnell is not impacted, and the objects\u0026rsquo; import to Primo appear to be successful.\nIdentifying XML elements based on their \u0026ldquo;value\u0026rdquo; can be tricky, but I found that an xpath query like *[contains(.,’u1:’) or contains(.,'u2:')] works nicely.\nFixing the MODS-to-DC Transform? Unfortunately, this is NOT an attractive option because our XSLT is so complex. To be honest, I think ALL XSLT is too complex! I hate XSLT with a passion. The transform that Digital.Grinnell uses can be found in two places on DGDocker1:\n/var/www/html/sites/all/modules/islandora/dg7/xslt/mods_to_dc_grinnell.xsl, and /var/www/html/sites/all/modules/islandora/islandora_xml_forms/builder/transforms/mods_to_dc_grinnell.xsl Yes, these two files are IDENTICAL, but a necessary evil due the way that the islandora_xml_forms module is built. I hate that module almost as much as I hate XSLT, maybe more. \u0026#x1f626;\nThe XSLT that we apply for MODS-to-DC transform reads like this:\n\u0026lt;xsl:template match=\u0026#34;mods:identifier\u0026#34;\u0026gt; \u0026lt;dc:identifier\u0026gt; \u0026lt;xsl:variable name=\u0026#34;type\u0026#34; select=\u0026#34;translate(@type,\u0026#39;ABCDEFGHIJKLMNOPQRSTUVWXYZ\u0026#39;,\u0026#39;abcdefghijklmnopqrstuvwxyz\u0026#39;)\u0026#34;/\u0026gt; \u0026lt;xsl:choose\u0026gt; \u0026lt;!-- 2.0: added identifier type attribute to output, if it is present--\u0026gt; \u0026lt;xsl:when test=\u0026#34;contains(.,\u0026#39;:\u0026#39;)\u0026#34;\u0026gt; \u0026lt;xsl:value-of select=\u0026#34;.\u0026#34;/\u0026gt; \u0026lt;/xsl:when\u0026gt; \u0026lt;xsl:when test=\u0026#34;@type\u0026#34;\u0026gt; \u0026lt;xsl:value-of select=\u0026#34;$type\u0026#34;/\u0026gt;: \u0026lt;xsl:value-of select=\u0026#34;.\u0026#34;/\u0026gt; \u0026lt;/xsl:when\u0026gt; \u0026lt;xsl:when test=\u0026#34;contains (\u0026#39;isbn issn uri doi lccn\u0026#39;, $type)\u0026#34;\u0026gt; \u0026lt;xsl:value-of select=\u0026#34;$type\u0026#34;/\u0026gt;: \u0026lt;xsl:value-of select=\u0026#34;.\u0026#34;/\u0026gt; \u0026lt;/xsl:when\u0026gt; \u0026lt;xsl:otherwise\u0026gt; \u0026lt;xsl:value-of select=\u0026#34;.\u0026#34;/\u0026gt; \u0026lt;/xsl:otherwise\u0026gt; \u0026lt;/xsl:choose\u0026gt; \u0026lt;/dc:identifier\u0026gt; \u0026lt;/xsl:template\u0026gt; A Viable Solution What I\u0026rsquo;m going to discuss here isn\u0026rsquo;t an ideal fix because every time we ingest a new object with u1: and/or u2: MODS identifiers, our transform will put them in the object\u0026rsquo;s corresponding DC record. Like I said, not ideal. But there is a relatively easy way to remove what the transforms deposit.\nFor this purpose I\u0026rsquo;ve resurrected an old, broken drush iduF command. So a command of the form drush -u 1 iduF grinnell:31898-31902 PurgeElements --dsid=DC --xpath=\u0026quot;*[contains(.,'u1:') or contains(.,'u2:')]\u0026quot; --verbose can be used to strip the DC object(s) of its/their offending element(s). The command is PurgeElements and the --xpath clause shown above is CRITICAL. As with all drush iduF commands, the --verbose parameter is optional, and --dyrRun is also available for testing.\nThat command is worth highlighting one more time.\ndrush -u 1 iduF grinnell:31898-31902 PurgeElements --dsid=DC --xpath=\u0026#34;*[contains(.,\u0026#39;u1:\u0026#39;) or contains(.,\u0026#39;u2:\u0026#39;)]\u0026#34; --verbose An XML Namespace Issue? It\u0026rsquo;s not been confirmed just yet, but there is speculation that the real root of the problem here stems from the apparent existence of srw_dc namespace references that appear in DC records only when identifier u1: or u2: elements exist. You may get some sense of what these references look like on OAI exports from the screen capture shared below.\nFigure 1 \u0026middot; Problematic OAI Export Sample If this is indeed the root of the problem, then my prayer (\u0026#x1f64f;) is that removing all unnecessary \u0026lt;dc:identifier\u0026gt; u1: and u2: elements will make the problem go away. \u0026#x1f340;\nAnd that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/120-problems-with-dcidentifer-elements/","tags":["iduF","OAI","DC","Dublin Core","identifier","scholar profile","PurgeElements"],"title":"Problems with dc:identifier Elements"},{"categories":null,"contents":"Note to self: IHC is back in DG production! What\u0026rsquo;s IHC, you ask?\nIHC is my Islandora Health Check drush command/module. Since this is a note to myself, I\u0026rsquo;m not going to get into a lot of detail here, just posting some reminders so I can remember how it works and how it is used.\nWhat Is It? IHC is a module that provides drush commands that can be used to collect (drush ihcC), analyze (drush ihcA), and subsequently report (drush ihcR) key object info from DG\u0026rsquo;s FEDORA repository.\nAt its core, the module consists of three parts:\nihcCollect (alias ihcC) collects information by walking through a user-specified range of PIDs and making calls to PHP functions, peering into and harvesting information from those objects\u0026rsquo; XML datastreams. ihcAnalyze (alias ihcA), the analysis command, can be used to analyze the data collected by ihcC. It looks at values for specific fields and for trends in the data, then applies special color or text formatting codes to highlight that data or trend. ihcReport (alias ihcR), the reporting command, is then used to output the collected and analyzed data into a readable form. Before ISLE the reports were written directly into Excel format. Since the Excel libraries I used years ago are no longer available, due to security concerns, I made the ihcR command output data into a simple .csv format. History IHC is a module (on DGDocker1 it lives in /var/www/html/sites/all/modules/islandora/ihc) that I wrote several years ago, but one that was broken with the introduction of ISLE in about 2018. In February 2022 I managed to re-work some of the code to bring it back-to-life in DG\u0026rsquo;s ISLE environment.\nihcQuick - A \u0026ldquo;Quick\u0026rdquo; Report Running a series of ihcC, ihcA, then ihcR commands is a hassle. So, I created the drush ihcQuick (alias ihcQ) command, and it is how I recommend generating an IHC report now. A typical run of ihcQ from DG\u0026rsquo;s production node, DGDocker1, looks like this (truncated here for readability):\n[islandora@dgdocker1 ~]$ docker exec -it isle-apache-dg bash root@7f4311ed759c:/# cd /var/www/html/sites/default/ root@7f4311ed759c:/var/www/html/sites/default# drush -u 1 ihcQ grinnell:1-50000 --verbose Executing: mysql --defaults-extra-file=/tmp/drush_FURANC --database=digital_grinnell --host=mysql --silent \u0026lt; /tmp/drush_lhQcPD Executing: mysql --defaults-extra-file=/tmp/drush_Va3GiE --database=digital_grinnell --host=mysql --silent \u0026lt; /tmp/drush_Fode6D icu_drush_prep will consider only objects modified after 2000-01-01T00:00:00Z. [status] Starting operation for PID \u0026#39;grinnell:1\u0026#39; and --repeat=\u0026#39;49999\u0026#39; at 11:29:05. [status] Fetching all valid object PIDs in the specified range. [status] Completed fetch of 22947 valid object PIDs. [status] Progress: ihcCollect icu_Connect: Connection to Fedora repository as \u0026#39;System Admin\u0026#39; is complete. [status] pid is: grinnell:45 [status] pid is: grinnell:47 [status] pid is: grinnell:49 [status] pid is: grinnell:50 [status] ... As you can see above, it begins by shelling in to DG\u0026rsquo;s isle-apache-dg container, moving into the /var/www/html/sites/default directory, and executing the drush command of the form: drush -u 1 ihcQ grinnell:1-50000 --verbose.\nThis command will run a \u0026ldquo;quick\u0026rdquo; report for PIDs in the grinnell namespace with numeric IDs in the range 1 to 50000, and it will generate --verbose output.\nIn order to capture a complete picture or an ihcQ run I\u0026rsquo;m going to omit the --verbose option and vastly shrink the number of objects to report on, like so:\nroot@7f4311ed759c:/var/www/html/sites/default# drush -u 1 ihcQ grinnell:1-50 icu_drush_prep will consider only objects modified after 2000-01-01T00:00:00Z. [status] Starting operation for PID \u0026#39;grinnell:1\u0026#39; and --repeat=\u0026#39;49\u0026#39; at 11:35:39. [status] Fetching all valid object PIDs in the specified range. [status] Completed fetch of 4 valid object PIDs. [status] Progress: ihcCollect icu_Connect: Connection to Fedora repository as \u0026#39;System Admin\u0026#39; is complete. [status] [=================================================================================] 100% Completed ihcCollect at 11:35:50. Use \u0026#39;drush ihcA\u0026#39; to analyze and \u0026#39;drush ihcR\u0026#39; [status] to report it. icu_drush_prep will consider only objects modified after 2000-01-01T00:00:00Z. [status] Starting operation for PID \u0026#39;grinnell:1\u0026#39; and --repeat=\u0026#39;49\u0026#39; at 11:35:50. [status] Fetching all valid object PIDs in the specified range. [status] Completed fetch of 4 valid object PIDs. [status] Progress: ihcAnalyze [=================================================================================] 100% Completed ihcAnalyze at 11:36:00. Use \u0026#39;drush ihcR\u0026#39; to report it. [ok] icu_drush_prep will consider only objects modified after 2000-01-01T00:00:00Z. [status] Starting operation for PID \u0026#39;grinnell:1\u0026#39; and --repeat=\u0026#39;49\u0026#39; at 11:36:00. [status] Fetching all valid object PIDs in the specified range. [status] Completed fetch of 4 valid object PIDs. [status] .csv file path is: \u0026#39;public://ihcQ_grinnell_ih.csv\u0026#39;. [status] Progress: ihcReport [=================================================================================] 100% As you can see above, the ihcQ command did the following:\nCollected data by invoking an ihcC, or ihcCollect, operation; Analyzed the data via ihcA, or ihcAnalyze; and Reported the data using ihcR, or ihcReport. Near the end of the output you can see that a new public://ihcQ_grinnell_ih.csv was generated.\nDownloading IHC Reports Since the IHC report is a .csv file it\u0026rsquo;s not much use sitting in the public:// folder on the server, so I use the Islandora Health Check command in the Management menu on DG\u0026rsquo;s home page to facilitate download of these reports. Clicking that command link in my browser brings me to https://digital.grinnell.edu/ihc where I\u0026rsquo;m presented with a list of _ih.csv files to download.\nOnce I\u0026rsquo;ve selected and downloaded one of these files I engage a Python utility of my own creation to interpret the color and format data that ihcAnalyze generated and produce a visually effective Excel output file.\nConvert .csv to Excel with Formatting On my Mac I\u0026rsquo;m able to run my format-csv-to-excel Python script like so:\n╭─mcfatem@MAC02FK0XXQ05Q ~/Downloads ╰─$ python3 /Users/mcfatem/GitHub/format-csv-to-excel/main.py --verbose --dg ihcQ_grinnell_ih.csv Verbose output selected. Special Digital.Grinnell processing is selected. ihcQ_grinnell_ih.xlsx has been created. The Result The results, in Excel, look something like the figure you see below.\nFigure 1 \u0026middot; Example Excel from ihcQ And that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/119-ihc-is-back/","tags":["IHC","Islandora Health Check","format-csv-to-excel"],"title":"IHC is Back!"},{"categories":null,"contents":"Recently, I have been catching up on processing some pending metadata review edits in DG\u0026rsquo;s Faculty Scholarship and Student Scholarship collections. Since these two collections are relatively \u0026ldquo;fluid\u0026rdquo;, they get frequent additions and metadata changes, I was worried that introducing bulk edits could negatively impact some objects. So I set out to find, or create, a tool that would help me evaluate the impact of these edits.\nIslandora Pretty Text Diff I was fortunate to find a very nice tool all ready-to-go, the Islandora Pretty Text Diff module, specifically islandora_pretty_text_diff. I installed and enabled this module on DGDocker1 in early February 2022.\nIt can be used by any DG admin by examining an object\u0026rsquo;s datastream versions, https://digital.grinnell.edu/islandora/object/grinnell:10001/manage/datastream, for example. In a manage/datastreams window like the example an admin can now select the version history of any text datastream, MODS is the most likely choice, and in the ensuing window, like https://digital.grinnell.edu/islandora/object/grinnell:10001/datastreams/MODS/version, there is an option at the bottom of the list/window to select a pair of versions to compare, then click View Diff.\nIn my example the bottom of the window looked like the small image captured below.\nFigure 1 \u0026middot; Example MODS Versions Window for grinnell:10001 From that window I elected to select two of the most recent MODS versions as you can see in the screen capture posted below.\nFigure 2 \u0026middot; Example MODS Versions Window - Two Versions Selected After selecting the two MODS version I simply clicked on the View Diff button to open the window shown below.\nFigure 3 \u0026middot; Example Version Diff Display The single line highlighted in green indicates the only difference between the two versions was the \u0026ldquo;addition\u0026rdquo;, green changes represent additions, of a \u0026lt;copyrightDate\u0026gt;2014\u0026lt;/copyrightDate\u0026gt; field and value. Note that anything \u0026ldquo;removed\u0026rdquo; from by the version changes would be similarly highlighted in red.\nThe Problem with This Approach Many of the operations that can generate new datastream versions will deposit more than one new version per operation. For instance, running a single islandora_datastream_replace command (see Exporting, Editing \u0026amp; Replacing MODS Datastreams: Updated Technical Details) can generate as many as three new versions of an object\u0026rsquo;s MODS record. You can get a sense of the outcome looking at the portion of the MODS version history displayed below for grinnell:10001.\nFigure 4 \u0026middot; Example Version History Note how many versions of the MODS datastream were created at the same instant in time, or only a few seconds apart!\nA New Drush IDU Command To try and deal with the issue of multiple versions per operation, I crafted a new IDU (Islandora Drush Utilities) command, DiffDatastreamVersions, with example syntax like:\ndrush -u 1 iduF grinnell:10000-10001 DiffDatastreamVersions --dsid=MODS This command will examine the creation dates/times of all of the --dsid datastream versions and select the current version along with the most recent version that is at least one minute older than the current. This helps avoid comparing \u0026ldquo;intermediate\u0026rdquo; versions from the same operation.\nThe above command produced the following output:\nroot@7f4311ed759c:/var/www/html/sites/default# drush -u 1 iduF grinnell:10000-10001 DiffDatastreamVersions --dsid=MODS Ok, iduF command \u0026#39;DiffDatastreamVersions\u0026#39; was verified on 10-Feb-2022. [status] iduCommon_drush_prep will consider only objects modified with a yyyy-mm-dd local time matching 2*. [status] Starting operation for PID \u0026#39;grinnell:10000\u0026#39; and --repeat=\u0026#39;1\u0026#39; at 16:01:12. [status] Fetching all valid object PIDs in the specified range. [status] Completed fetch of 2 valid object PIDs from Solr. [status] Progress: iduFix - DiffDatastreamVersions iduCommon_Connect: Connection to Fedora repository as \u0026#39;System Admin\u0026#39; is complete. [status] Version diff written to \u0026#39;public://grinnell-10000_MODS.diff\u0026#39; Version diff written to \u0026#39;public://grinnell-10001_MODS.diff\u0026#39; ========\u0026gt; ] 50% [=====================================================================================================] 100% Completed 2 \u0026#39;iduFix - DiffDatastreamVersions\u0026#39; operations at 16:01:22. [status] Note that two .diff files were created, public://grinnell-10000_MODS.diff and public://grinnell-10001_MODS.diff.\nThe first of these files, grinnell-10000_MODS.diff, was empty, signifying that the two version of MODS were identical. The second file, grinnell-10001_MODS.diff, contained the following:\n--- /tmp/grinnell-10001_2022-02-10-16-40-43.MODS\t2022-02-14 22:01:22.883776814 +0000 +++ /tmp/grinnell-10001_2022-02-10-18-14-27.MODS\t2022-02-14 22:01:22.884776813 +0000 @@ -20,6 +20,7 @@ \u0026lt;dateCreated\u0026gt;2014\u0026lt;/dateCreated\u0026gt; \u0026lt;publisher\u0026gt;Grinnell College\u0026lt;/publisher\u0026gt; \u0026lt;dateOther\u0026gt;41933\u0026lt;/dateOther\u0026gt; + \u0026lt;copyrightDate\u0026gt;2014\u0026lt;/copyrightDate\u0026gt; \u0026lt;/originInfo\u0026gt; \u0026lt;physicalDescription\u0026gt; \u0026lt;extent\u0026gt;8 objects\u0026lt;/extent\u0026gt; In this \u0026ldquo;standard\u0026rdquo; diff, or \u0026ldquo;patch\u0026rdquo; format, a single + in the left margin indicates that the line was \u0026ldquo;added\u0026rdquo; during the creation of the most recent datastream version. Likewise, any lines removed or superseded during the creation of the new version would be proceeded by a -, but there were none in this example.\nA more complete example, using grinnell:10010, is shown below.\nroot@7f4311ed759c:/var/www/html/sites/default/files# drush -u 1 iduF grinnell:10010 DiffDatastreamVersions --dsid=MODS Ok, iduF command \u0026#39;DiffDatastreamVersions\u0026#39; was verified on 10-Feb-2022. [status] iduCommon_drush_prep will consider only objects modified with a yyyy-mm-dd local time matching 2*. [status] Starting operation for PID \u0026#39;grinnell:10010\u0026#39; and --repeat=\u0026#39;0\u0026#39; at 16:09:49. [status] Fetching all valid object PIDs in the specified range. [status] Completed fetch of 1 valid object PIDs from Solr. [status] Progress: iduFix - DiffDatastreamVersions iduCommon_Connect: Connection to Fedora repository as \u0026#39;System Admin\u0026#39; is complete. [status] Version diff written to \u0026#39;public://grinnell-10010_MODS.diff\u0026#39; [===================================================================================================] 100% Completed 1 \u0026#39;iduFix - DiffDatastreamVersions\u0026#39; operations at 16:09:59. [status] The result:\nroot@7f4311ed759c:/var/www/html/sites/default/files# cat grinnell-10010_MODS.diff --- /tmp/grinnell-10010_2022-02-09-21-24-57.MODS\t2022-02-14 22:09:59.181745351 +0000 +++ /tmp/grinnell-10010_2022-02-10-16-29-49.MODS\t2022-02-14 22:09:59.181745351 +0000 @@ -1,71 +1,70 @@ \u0026lt;?xml version=\u0026#34;1.0\u0026#34;?\u0026gt; -\u0026lt;mods xmlns=\u0026#34;http://www.loc.gov/mods/v3\u0026#34; xmlns:mods=\u0026#34;http://www.loc.gov/mods/v3\u0026#34; xmlns:xlink=\u0026#34;http://www.w3.org/1999/xlink\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;\u0026gt; - \u0026lt;name type=\u0026#34;personal\u0026#34; xlink:href=\u0026#34;grinnell-auth:193\u0026#34;\u0026gt; - \u0026lt;namePart type=\u0026#34;given\u0026#34;\u0026gt;Henry\u0026lt;/namePart\u0026gt; - \u0026lt;namePart type=\u0026#34;family\u0026#34;\u0026gt;Walker\u0026lt;/namePart\u0026gt; - \u0026lt;namePart\u0026gt;Walker, Henry M., 1947-\u0026lt;/namePart\u0026gt; +\u0026lt;mods xmlns=\u0026#34;http://www.loc.gov/mods/v3\u0026#34; xmlns:mods=\u0026#34;http://www.loc.gov/mods/v3\u0026#34; xmlns:xlink=\u0026#34;http://www.w3.org/1999/xlink\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://www.loc.gov/mods/v3 http://www.loc.gov/standards/mods/v3/mods-3-4.xsd\u0026#34;\u0026gt; + \u0026lt;titleInfo\u0026gt; + \u0026lt;title\u0026gt;ACM Works of Dr. Henry M. Walker\u0026lt;/title\u0026gt; + \u0026lt;/titleInfo\u0026gt; + \u0026lt;titleInfo type=\u0026#34;alternative\u0026#34;\u0026gt; + \u0026lt;title\u0026gt;Publications of Henry M. Walker indexed by the ACM Digital Library or from other sources\u0026lt;/title\u0026gt; + \u0026lt;/titleInfo\u0026gt; + \u0026lt;name type=\u0026#34;personal\u0026#34;\u0026gt; + \u0026lt;namePart\u0026gt;Walker, Henry M., 1947- (Faculty/Staff)\u0026lt;/namePart\u0026gt; \u0026lt;role\u0026gt; \u0026lt;roleTerm type=\u0026#34;text\u0026#34;\u0026gt;creator\u0026lt;/roleTerm\u0026gt; \u0026lt;/role\u0026gt; \u0026lt;/name\u0026gt; - \u0026lt;titleInfo\u0026gt; - \u0026lt;title\u0026gt;ACM Works of Dr. Henry M. Walker\u0026lt;/title\u0026gt; - \u0026lt;/titleInfo\u0026gt; \u0026lt;name type=\u0026#34;corporate\u0026#34;\u0026gt; + \u0026lt;namePart\u0026gt;Grinnell College. Computer Science\u0026lt;/namePart\u0026gt; \u0026lt;role\u0026gt; \u0026lt;roleTerm type=\u0026#34;text\u0026#34;\u0026gt;supporting host\u0026lt;/roleTerm\u0026gt; \u0026lt;/role\u0026gt; - \u0026lt;namePart\u0026gt;Grinnell College. Computer Science.\u0026lt;/namePart\u0026gt; \u0026lt;/name\u0026gt; - \u0026lt;abstract\u0026gt;This image is a screen capture (taken February 4, 2015) of a web page listing several ACM (Association for Computing Machinery) copyrighted publications of Dr. Henry M. Walker, Grinnell College. Associated metadata includes the text of all pertinent links presented in Dr. Walker\u0026#39;s page.\u0026lt;/abstract\u0026gt; + \u0026lt;abstract\u0026gt;Screen capture (taken February 4, 2015) of a web page listing several ACM (Association for Computing Machinery) copyrighted publications of Dr. Henry M. Walker, Grinnell College.\u0026lt;/abstract\u0026gt; \u0026lt;originInfo\u0026gt; - \u0026lt;dateCreated\u0026gt;1981\u0026lt;/dateCreated\u0026gt; - \u0026lt;edition\u0026gt;current\u0026lt;/edition\u0026gt; - \u0026lt;edition\u0026gt;current\u0026lt;/edition\u0026gt; + \u0026lt;dateCreated\u0026gt;2015\u0026lt;/dateCreated\u0026gt; + \u0026lt;dateOther displayLabel=\u0026#34;Date Issued\u0026#34;\u0026gt;1981-present\u0026lt;/dateOther\u0026gt; \u0026lt;publisher\u0026gt;ACM\u0026lt;/publisher\u0026gt; \u0026lt;/originInfo\u0026gt; - \u0026lt;typeOfResource\u0026gt;text\u0026lt;/typeOfResource\u0026gt; - \u0026lt;genre\u0026gt;web site\u0026lt;/genre\u0026gt; - \u0026lt;physicalDescription\u0026gt; - \u0026lt;digitalOrigin\u0026gt;born digital\u0026lt;/digitalOrigin\u0026gt; - \u0026lt;extent displayLabel=\u0026#34;Digital Extent\u0026#34;\u0026gt;1 sheet\u0026lt;/extent\u0026gt; - \u0026lt;internetMediaType\u0026gt;image/jpeg\u0026lt;/internetMediaType\u0026gt; - \u0026lt;/physicalDescription\u0026gt; - \u0026lt;note displayLabel=\u0026#34;Date Issued\u0026#34; type=\u0026#34;creation/production credits\u0026#34;\u0026gt;1981-present\u0026lt;/note\u0026gt; - \u0026lt;language\u0026gt; - \u0026lt;languageTerm type=\u0026#34;text\u0026#34;\u0026gt;English\u0026lt;/languageTerm\u0026gt; - \u0026lt;languageTerm type=\u0026#34;code\u0026#34;\u0026gt;eng\u0026lt;/languageTerm\u0026gt; - \u0026lt;/language\u0026gt; + \u0026lt;note type=\u0026#34;description\u0026#34;\u0026gt;This image is a screen capture (taken February 4, 2015) of a web page listing several ACM (Association for Computing Machinery) copyrighted publications of Dr. Henry M. Walker, Grinnell College. Associated metadata includes the text of all pertinent links presented in Dr. Walker\u0026#39;s page.\u0026lt;/note\u0026gt; \u0026lt;subject authority=\u0026#34;lcsh\u0026#34;\u0026gt; \u0026lt;topic\u0026gt;Computer science\u0026lt;/topic\u0026gt; + \u0026lt;topic\u0026gt;Bibliography\u0026lt;/topic\u0026gt; \u0026lt;/subject\u0026gt; \u0026lt;subject authority=\u0026#34;lcsh\u0026#34;\u0026gt; \u0026lt;temporal\u0026gt;20th century\u0026lt;/temporal\u0026gt; \u0026lt;/subject\u0026gt; - \u0026lt;relatedItem type=\u0026#34;isPartOf\u0026#34;\u0026gt; + \u0026lt;relatedItem type=\u0026#34;host\u0026#34;\u0026gt; \u0026lt;titleInfo\u0026gt; - \u0026lt;title\u0026gt;Digital Grinnell\u0026lt;/title\u0026gt; + \u0026lt;title\u0026gt;Faculty Scholarship\u0026lt;/title\u0026gt; \u0026lt;/titleInfo\u0026gt; - \u0026lt;identifier type=\u0026#34;uri\u0026#34;\u0026gt;grinnell:173\u0026lt;/identifier\u0026gt; \u0026lt;/relatedItem\u0026gt; - \u0026lt;relatedItem displayLabel=\u0026#34;ACM Page\u0026#34; type=\u0026#34;isPartOf\u0026#34; xlink:href=\u0026#34;http://www.cs.grinnell.edu/~walker/acm-publications.html\u0026#34;\u0026gt; + \u0026lt;relatedItem type=\u0026#34;host\u0026#34;\u0026gt; \u0026lt;titleInfo\u0026gt; - \u0026lt;title\u0026gt;Faculty Scholarship\u0026lt;/title\u0026gt; + \u0026lt;title\u0026gt;Scholarship at Grinnell\u0026lt;/title\u0026gt; \u0026lt;/titleInfo\u0026gt; \u0026lt;/relatedItem\u0026gt; - \u0026lt;relatedItem type=\u0026#34;constituent\u0026#34;\u0026gt; + \u0026lt;relatedItem type=\u0026#34;host\u0026#34; xlink:href=\u0026#34;https://digital.grinnell.edu\u0026#34;\u0026gt; \u0026lt;titleInfo\u0026gt; \u0026lt;title\u0026gt;Digital Grinnell\u0026lt;/title\u0026gt; \u0026lt;/titleInfo\u0026gt; \u0026lt;/relatedItem\u0026gt; - \u0026lt;identifier type=\u0026#34;hdl\u0026#34;\u0026gt;http://hdl.handle.net/11084/10010\u0026lt;/identifier\u0026gt; + \u0026lt;genre authorityURI=\u0026#34;https://id.loc.gov/authorities/genreForms.html\u0026#34;\u0026gt;Bibliographies\u0026lt;/genre\u0026gt; + \u0026lt;physicalDescription\u0026gt; + \u0026lt;extent\u0026gt;1 sheet\u0026lt;/extent\u0026gt; + \u0026lt;internetMediaType\u0026gt;image/jpeg\u0026lt;/internetMediaType\u0026gt; + \u0026lt;digitalOrigin\u0026gt;born digital\u0026lt;/digitalOrigin\u0026gt; + \u0026lt;/physicalDescription\u0026gt; + \u0026lt;classification authoritiy=\u0026#34;lcc\u0026#34; type=\u0026#34;mixed\u0026#34;\u0026gt;QA76\u0026lt;/classification\u0026gt; + \u0026lt;language\u0026gt; + \u0026lt;languageTerm type=\u0026#34;text\u0026#34;\u0026gt;English\u0026lt;/languageTerm\u0026gt; + \u0026lt;languageTerm authority=\u0026#34;iso639-2b\u0026#34; type=\u0026#34;code\u0026#34;\u0026gt;eng\u0026lt;/languageTerm\u0026gt; + \u0026lt;/language\u0026gt; \u0026lt;identifier type=\u0026#34;local\u0026#34;\u0026gt;grinnell:10010\u0026lt;/identifier\u0026gt; + \u0026lt;identifier type=\u0026#34;hdl\u0026#34;\u0026gt;http://hdl.handle.net/11084/10010\u0026lt;/identifier\u0026gt; \u0026lt;accessCondition type=\u0026#34;useAndReproduction\u0026#34;\u0026gt;Copyright to this work is held by the ACM and by the author(s), in accordance with United States copyright law (USC 17). Readers of this work have certain rights as defined by the law, including but not limited to fair use (17 USC 107 et seq.).\u0026lt;/accessCondition\u0026gt; - \u0026lt;mods:extension\u0026gt; - \u0026lt;mods:creators\u0026gt;Walker, Henry M., 1947-\u0026lt;/mods:creators\u0026gt; - \u0026lt;mods:primarySort\u0026gt;99\u0026lt;/mods:primarySort\u0026gt; - \u0026lt;mods:CModel\u0026gt;islandora:sp_basic_image\u0026lt;/mods:CModel\u0026gt; - \u0026lt;/mods:extension\u0026gt; - \u0026lt;identifier type=\u0026#34;u1\u0026#34;\u0026gt;walker\u0026lt;/identifier\u0026gt; - \u0026lt;identifier type=\u0026#34;u2\u0026#34;\u0026gt;CSC\u0026lt;/identifier\u0026gt; + \u0026lt;extension\u0026gt; + \u0026lt;dg_importIndex\u0026gt;4\u0026lt;/dg_importIndex\u0026gt; + \u0026lt;primarySort\u0026gt;99\u0026lt;/primarySort\u0026gt; + \u0026lt;dg_parentObject\u0026gt;grinnell:faculty-scholarship\u0026lt;/dg_parentObject\u0026gt; + \u0026lt;CModel\u0026gt;islandora:sp_basic_image\u0026lt;/CModel\u0026gt; + \u0026lt;/extension\u0026gt; As you can see, there were numerous differences and they can be very difficult to effectively visualize in this form. Have no fear, there\u0026rsquo;s a remedy for this too. Read on.\nNavigation Menu: Islandora Drush Utilities With the addition of the aforementioned drush DiffDatastreamVersions command comes a new Navigation menu selection for Digital.Grinnell admins. If you are an admin, on the DG home page look to the right under Navigation for the Islandora Drush Utilities menu selection. If you click that link DG will open a page where you\u0026rsquo;ll be presented with a list of all the .diff files that have been produced by the drush DiffDatastreamVersions commands. The window will look something like this:\nFigure 5 \u0026middot; New IDU Navigation Menu Item In this instance I selected the grinnell-10010_MODS.diff file and clicked Submit. On my MacBook I have .diff downloads set to automatically open in my Atom text editing environment and a portion of the result looks like what you see below.\nFigure 6 \u0026middot; Visualizing Differences in Atom As was the case above, additions to the data appear underlined in green, while portions that are removed or superseded appear in red. This may still be difficult to interpret in cases like this example where there are numerous changes from version-to-version, but it\u0026rsquo;s also a definite improvement. Enjoy!\nAnd that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/118-new-idu-diffdatastreamversions-command/","tags":["IDU","Islandora Drush Utilities","diff","Datastream","version","islandora_pretty_text_diff","islandora_datastream_replace"],"title":"IDU DiffDatastreamVersions Command"},{"categories":null,"contents":"Today\u0026rsquo;s endeavor\u0026hellip; begin the process of moving hundreds of Rootstalk digital objects out of Digital.Grinnell to Azure storage.\nDigital.Grinnell Objects Over the last couple of years I have deposited some 477 digital objects, mostly JPEG images and PDFs, into Digital.Grinnell so they could be \u0026ldquo;served\u0026rdquo; up for Rootstalk. I did this because at the time Digital.Grinnell\u0026rsquo;s FEDORA repository was available and relatively easy to use. Unfortunately, DG isn\u0026rsquo;t as reliable or responsive as it once was since its FEDORA is due to be retired in a year or two. Also, the objects had to be made available to the public so they are \u0026ldquo;visible\u0026rdquo; to Rootstalk, but that also makes them discoverable by the public, even though they are essentially out-of-context in DG.\nFirst Step - Download All the Objects This would seem like a simple thing, download a bunch of digital objects from a public repo; easy, right? It\u0026rsquo;s simple enough for a few objects, but doing this for 400+ objects via the public web interface would require a awful lot of clicking and could take a week or more!\nIslandora Datastream Exporter to the Rescue At the onset of the COVID-19 pandemic in 2020 I managed to install and successfully implement the Islandora Datastream Exporter module and corresponding drush commands. So, I choose to revisit it again for this effort.\nLooking back at my notes from 2020, specifically Exporting, Editing \u0026amp; Replacing MODS Datastreams: Updated Technical Details, I found guidance that helped a great deal. Based on that guidance I crafted a SPARQL query text file named query.txt and parked a copy of it in the /var/www/html/sites/default/files directory of the Apache container on node DGDocker1.\nThe Query The contents of query.txt was:\nSELECT ?pid FROM \u0026lt;#ri\u0026gt; WHERE { ?pid \u0026lt;fedora-rels-ext:isMemberOfCollection\u0026gt; \u0026lt;info:fedora/grinnell:rootstalk\u0026gt; } OFFSET %offset% I copied that file from DGDocker1:/home/islandora/query.txt into the Apache container using:\ndocker cp /home/islandora/query.txt isle-apache-dg:/var/www/html/sites/default/files/query.txt Invoked via Drush To engage this query I executed the following drush command from within the Apache container\u0026rsquo;s /var/www/html/sites/default directory:\nroot@7611e19c4663:/var/www/html/sites/default# drush -u 1 islandora_datastream_export --export_target=/tmp --dsid=OBJ --query=/var/www/html/sites/default/files/query.txt --query_type=islandora_datastream_exporter_ri_query Results The abridged output of the command looked like this:\nProcessing results 1 to 10 [ok] Datastream exported succeeded for grinnell:28575. [success] Datastream export failed for grinnell:28471. The object does not contain the OBJ datastream. [error] Datastream export failed for grinnell:28464. The object does not contain the OBJ datastream. [error] Datastream export failed for grinnell:28401. The object does not contain the OBJ datastream. [error] Datastream export failed for grinnell:28287. The object does not contain the OBJ datastream. [error] Datastream exported succeeded for grinnell:29595. [success] Datastream exported succeeded for grinnell:29561. [success] Datastream exported succeeded for grinnell:29548. [success] Datastream exported succeeded for grinnell:29641. [success] Datastream exported succeeded for grinnell:29647. [success] Processing results 11 to 20 [ok] ... The entire operation took about 5 minutes and ultimately produced a series of files in the Apache container\u0026rsquo;s /tmp directory with names of the form: grinnell_\u0026lt;PID\u0026gt;_OBJ.\u0026lt;extension\u0026gt;. In this format \u0026lt;PID\u0026gt; is the numeric portion of the object\u0026rsquo;s PID, and \u0026lt;extension\u0026gt; indicates the object type, such as jpg, pdf, mp3 and so on.\nThe complete list of 385 exported files is:\n-rw-r--r--. 1 islandora islandora 189529191 Jan 24 14:14 grinnell_28047_OBJ.pdf -rw-r--r--. 1 islandora islandora 53042574 Jan 24 14:13 grinnell_28048_OBJ.pdf -rw-r--r--. 1 islandora islandora 168264 Jan 24 14:13 grinnell_28269_OBJ.jpg -rw-r--r--. 1 islandora islandora 402055 Jan 24 14:14 grinnell_28270_OBJ.jpg -rw-r--r--. 1 islandora islandora 40413 Jan 24 14:14 grinnell_28271_OBJ.jpg -rw-r--r--. 1 islandora islandora 182145 Jan 24 14:13 grinnell_28272_OBJ.jpg -rw-r--r--. 1 islandora islandora 6415449 Jan 24 14:13 grinnell_28273_OBJ.mp3 -rw-r--r--. 1 islandora islandora 9727484 Jan 24 14:14 grinnell_28279_OBJ.pdf -rw-r--r--. 1 islandora islandora 12126841 Jan 24 14:13 grinnell_28280_OBJ.pdf -rw-r--r--. 1 islandora islandora 32999945 Jan 24 14:14 grinnell_28281_OBJ.pdf -rw-r--r--. 1 islandora islandora 60368718 Jan 24 14:14 grinnell_28282_OBJ.pdf -rw-r--r--. 1 islandora islandora 165232161 Jan 24 14:14 grinnell_28283_OBJ.pdf -rw-r--r--. 1 islandora islandora 45447390 Jan 24 14:14 grinnell_28284_OBJ.pdf -rw-r--r--. 1 islandora islandora 79899993 Jan 24 14:14 grinnell_28285_OBJ.pdf -rw-r--r--. 1 islandora islandora 189529191 Jan 24 14:14 grinnell_28286_OBJ.pdf -rw-r--r--. 1 islandora islandora 265445 Jan 24 14:14 grinnell_28479_OBJ.pdf -rw-r--r--. 1 islandora islandora 2011631 Jan 24 14:13 grinnell_28480_OBJ.pdf -rw-r--r--. 1 islandora islandora 1459883 Jan 24 14:13 grinnell_28481_OBJ.pdf -rw-r--r--. 1 islandora islandora 361093 Jan 24 14:14 grinnell_28482_OBJ.pdf -rw-r--r--. 1 islandora islandora 569211 Jan 24 14:14 grinnell_28483_OBJ.pdf -rw-r--r--. 1 islandora islandora 345109 Jan 24 14:14 grinnell_28484_OBJ.pdf -rw-r--r--. 1 islandora islandora 553469 Jan 24 14:14 grinnell_28485_OBJ.pdf -rw-r--r--. 1 islandora islandora 1123278 Jan 24 14:13 grinnell_28486_OBJ.pdf -rw-r--r--. 1 islandora islandora 745100 Jan 24 14:13 grinnell_28487_OBJ.pdf -rw-r--r--. 1 islandora islandora 599550 Jan 24 14:14 grinnell_28488_OBJ.pdf -rw-r--r--. 1 islandora islandora 1118135 Jan 24 14:13 grinnell_28489_OBJ.pdf -rw-r--r--. 1 islandora islandora 784343 Jan 24 14:13 grinnell_28490_OBJ.pdf -rw-r--r--. 1 islandora islandora 1200440 Jan 24 14:13 grinnell_28491_OBJ.pdf -rw-r--r--. 1 islandora islandora 58333365 Jan 24 14:13 grinnell_28492_OBJ.pdf -rw-r--r--. 1 islandora islandora 989470 Jan 24 14:13 grinnell_28493_OBJ.pdf -rw-r--r--. 1 islandora islandora 1218880 Jan 24 14:13 grinnell_28494_OBJ.pdf -rw-r--r--. 1 islandora islandora 556383 Jan 24 14:13 grinnell_28495_OBJ.pdf -rw-r--r--. 1 islandora islandora 2139710 Jan 24 14:13 grinnell_28496_OBJ.pdf -rw-r--r--. 1 islandora islandora 905503 Jan 24 14:14 grinnell_28497_OBJ.pdf -rw-r--r--. 1 islandora islandora 288485490 Jan 24 14:14 grinnell_28498_OBJ.pdf -rw-r--r--. 1 islandora islandora 1032147 Jan 24 14:14 grinnell_28499_OBJ.pdf -rw-r--r--. 1 islandora islandora 1233748 Jan 24 14:14 grinnell_28500_OBJ.jpg -rw-r--r--. 1 islandora islandora 1278758 Jan 24 14:14 grinnell_28501_OBJ.jpg -rw-r--r--. 1 islandora islandora 1537364 Jan 24 14:14 grinnell_28502_OBJ.jpg -rw-r--r--. 1 islandora islandora 3992245 Jan 24 14:14 grinnell_28503_OBJ.jpg -rw-r--r--. 1 islandora islandora 29239041 Jan 24 14:14 grinnell_28504_OBJ.jpg -rw-r--r--. 1 islandora islandora 4692821 Jan 24 14:13 grinnell_28505_OBJ.jpg -rw-r--r--. 1 islandora islandora 2807762 Jan 24 14:14 grinnell_28506_OBJ.jpg -rw-r--r--. 1 islandora islandora 4175998 Jan 24 14:13 grinnell_28507_OBJ.jpg -rw-r--r--. 1 islandora islandora 1235389 Jan 24 14:14 grinnell_28508_OBJ.jpg -rw-r--r--. 1 islandora islandora 60089 Jan 24 14:14 grinnell_28509_OBJ.jpg -rw-r--r--. 1 islandora islandora 3890455 Jan 24 14:14 grinnell_28510_OBJ.jpg -rw-r--r--. 1 islandora islandora 155792 Jan 24 14:14 grinnell_28511_OBJ.jpg -rw-r--r--. 1 islandora islandora 4066227 Jan 24 14:13 grinnell_28512_OBJ.jpg -rw-r--r--. 1 islandora islandora 747320 Jan 24 14:13 grinnell_28513_OBJ.jpg -rw-r--r--. 1 islandora islandora 863793 Jan 24 14:14 grinnell_28514_OBJ.jpg -rw-r--r--. 1 islandora islandora 219187 Jan 24 14:14 grinnell_28515_OBJ.jpg -rw-r--r--. 1 islandora islandora 173334 Jan 24 14:14 grinnell_28516_OBJ.jpg -rw-r--r--. 1 islandora islandora 5864773 Jan 24 14:14 grinnell_28517_OBJ.jpg -rw-r--r--. 1 islandora islandora 3694400 Jan 24 14:14 grinnell_28518_OBJ.jpg -rw-r--r--. 1 islandora islandora 1055157 Jan 24 14:14 grinnell_28519_OBJ.jpg -rw-r--r--. 1 islandora islandora 3564515 Jan 24 14:13 grinnell_28520_OBJ.jpg -rw-r--r--. 1 islandora islandora 5873511 Jan 24 14:14 grinnell_28521_OBJ.jpg -rw-r--r--. 1 islandora islandora 5252623 Jan 24 14:14 grinnell_28522_OBJ.jpg -rw-r--r--. 1 islandora islandora 5403727 Jan 24 14:14 grinnell_28523_OBJ.jpg -rw-r--r--. 1 islandora islandora 8419857 Jan 24 14:14 grinnell_28524_OBJ.jpg -rw-r--r--. 1 islandora islandora 2940094 Jan 24 14:13 grinnell_28525_OBJ.jpg -rw-r--r--. 1 islandora islandora 4066325 Jan 24 14:14 grinnell_28526_OBJ.jpg -rw-r--r--. 1 islandora islandora 1626294 Jan 24 14:13 grinnell_28527_OBJ.jpg -rw-r--r--. 1 islandora islandora 1899558 Jan 24 14:14 grinnell_28528_OBJ.jpg -rw-r--r--. 1 islandora islandora 3688802 Jan 24 14:13 grinnell_28529_OBJ.jpg -rw-r--r--. 1 islandora islandora 224078 Jan 24 14:14 grinnell_28530_OBJ.jpg -rw-r--r--. 1 islandora islandora 4771307 Jan 24 14:14 grinnell_28531_OBJ.jpg -rw-r--r--. 1 islandora islandora 5428927 Jan 24 14:13 grinnell_28532_OBJ.jpg -rw-r--r--. 1 islandora islandora 1475134 Jan 24 14:14 grinnell_28533_OBJ.jpg -rw-r--r--. 1 islandora islandora 691223 Jan 24 14:14 grinnell_28534_OBJ.jpg -rw-r--r--. 1 islandora islandora 1060982 Jan 24 14:14 grinnell_28535_OBJ.jpg -rw-r--r--. 1 islandora islandora 985012 Jan 24 14:13 grinnell_28536_OBJ.jpg -rw-r--r--. 1 islandora islandora 2917311 Jan 24 14:13 grinnell_28537_OBJ.jpg -rw-r--r--. 1 islandora islandora 10698112 Jan 24 14:14 grinnell_28538_OBJ.jpg -rw-r--r--. 1 islandora islandora 214384 Jan 24 14:14 grinnell_28539_OBJ.jpg -rw-r--r--. 1 islandora islandora 413505 Jan 24 14:13 grinnell_28540_OBJ.jpg -rw-r--r--. 1 islandora islandora 261332 Jan 24 14:14 grinnell_28541_OBJ.jpg -rw-r--r--. 1 islandora islandora 247717 Jan 24 14:14 grinnell_28542_OBJ.jpg -rw-r--r--. 1 islandora islandora 3004933 Jan 24 14:14 grinnell_28543_OBJ.jpg -rw-r--r--. 1 islandora islandora 264530 Jan 24 14:14 grinnell_28544_OBJ.jpg -rw-r--r--. 1 islandora islandora 224951 Jan 24 14:14 grinnell_28545_OBJ.jpg -rw-r--r--. 1 islandora islandora 813840 Jan 24 14:13 grinnell_28546_OBJ.jpg -rw-r--r--. 1 islandora islandora 879137 Jan 24 14:13 grinnell_28547_OBJ.jpg -rw-r--r--. 1 islandora islandora 1428387 Jan 24 14:14 grinnell_28548_OBJ.jpg -rw-r--r--. 1 islandora islandora 1263554 Jan 24 14:13 grinnell_28549_OBJ.jpg -rw-r--r--. 1 islandora islandora 631142 Jan 24 14:14 grinnell_28550_OBJ.jpg -rw-r--r--. 1 islandora islandora 5551881 Jan 24 14:14 grinnell_28551_OBJ.jpg -rw-r--r--. 1 islandora islandora 7622250 Jan 24 14:14 grinnell_28552_OBJ.jpg -rw-r--r--. 1 islandora islandora 4990577 Jan 24 14:14 grinnell_28553_OBJ.jpg -rw-r--r--. 1 islandora islandora 2783439 Jan 24 14:14 grinnell_28554_OBJ.jpg -rw-r--r--. 1 islandora islandora 283291 Jan 24 14:14 grinnell_28555_OBJ.jpg -rw-r--r--. 1 islandora islandora 1181684 Jan 24 14:14 grinnell_28556_OBJ.jpg -rw-r--r--. 1 islandora islandora 12989497 Jan 24 14:14 grinnell_28557_OBJ.jpg -rw-r--r--. 1 islandora islandora 7109537 Jan 24 14:13 grinnell_28558_OBJ.jpg -rw-r--r--. 1 islandora islandora 1560014 Jan 24 14:14 grinnell_28559_OBJ.jpg -rw-r--r--. 1 islandora islandora 4765653 Jan 24 14:13 grinnell_28560_OBJ.jpg -rw-r--r--. 1 islandora islandora 2471008 Jan 24 14:14 grinnell_28561_OBJ.jpg -rw-r--r--. 1 islandora islandora 272052 Jan 24 14:14 grinnell_28562_OBJ.jpg -rw-r--r--. 1 islandora islandora 112147 Jan 24 14:14 grinnell_28563_OBJ.jpg -rw-r--r--. 1 islandora islandora 5280648 Jan 24 14:14 grinnell_28564_OBJ.jpg -rw-r--r--. 1 islandora islandora 877880 Jan 24 14:14 grinnell_28565_OBJ.jpg -rw-r--r--. 1 islandora islandora 1342770 Jan 24 14:14 grinnell_28566_OBJ.jpg -rw-r--r--. 1 islandora islandora 723041 Jan 24 14:13 grinnell_28567_OBJ.jpg -rw-r--r--. 1 islandora islandora 1187536 Jan 24 14:14 grinnell_28568_OBJ.jpg -rw-r--r--. 1 islandora islandora 4678584 Jan 24 14:13 grinnell_28569_OBJ.jpg -rw-r--r--. 1 islandora islandora 511099 Jan 24 14:14 grinnell_28570_OBJ.jpg -rw-r--r--. 1 islandora islandora 3998823 Jan 24 14:14 grinnell_28571_OBJ.jpg -rw-r--r--. 1 islandora islandora 4741153 Jan 24 14:14 grinnell_28572_OBJ.jpg -rw-r--r--. 1 islandora islandora 11276920 Jan 24 14:13 grinnell_28573_OBJ.jpg -rw-r--r--. 1 islandora islandora 219601 Jan 24 14:14 grinnell_28574_OBJ.jpg -rw-r--r--. 1 islandora islandora 233057 Jan 24 14:13 grinnell_28575_OBJ.jpg -rw-r--r--. 1 islandora islandora 2596685 Jan 24 14:14 grinnell_28576_OBJ.jpg -rw-r--r--. 1 islandora islandora 1088314 Jan 24 14:13 grinnell_28577_OBJ.jpg -rw-r--r--. 1 islandora islandora 960350 Jan 24 14:13 grinnell_28578_OBJ.jpg -rw-r--r--. 1 islandora islandora 145333 Jan 24 14:14 grinnell_28579_OBJ.jpg -rw-r--r--. 1 islandora islandora 1665649 Jan 24 14:14 grinnell_28580_OBJ.jpg -rw-r--r--. 1 islandora islandora 127494 Jan 24 14:14 grinnell_28581_OBJ.jpg -rw-r--r--. 1 islandora islandora 967060 Jan 24 14:13 grinnell_28582_OBJ.jpg -rw-r--r--. 1 islandora islandora 6212429 Jan 24 14:14 grinnell_28583_OBJ.jpg -rw-r--r--. 1 islandora islandora 65834 Jan 24 14:13 grinnell_28584_OBJ.jpg -rw-r--r--. 1 islandora islandora 562790 Jan 24 14:14 grinnell_28585_OBJ.jpg -rw-r--r--. 1 islandora islandora 210500 Jan 24 14:14 grinnell_28586_OBJ.jpg -rw-r--r--. 1 islandora islandora 121443 Jan 24 14:13 grinnell_28587_OBJ.jpg -rw-r--r--. 1 islandora islandora 3281372 Jan 24 14:14 grinnell_28588_OBJ.jpg -rw-r--r--. 1 islandora islandora 262220 Jan 24 14:13 grinnell_28589_OBJ.jpg -rw-r--r--. 1 islandora islandora 6153646 Jan 24 14:14 grinnell_28590_OBJ.jpg -rw-r--r--. 1 islandora islandora 1284628 Jan 24 14:14 grinnell_28591_OBJ.jpg -rw-r--r--. 1 islandora islandora 14118040 Jan 24 14:14 grinnell_28592_OBJ.jpg -rw-r--r--. 1 islandora islandora 70560 Jan 24 14:14 grinnell_28593_OBJ.jpg -rw-r--r--. 1 islandora islandora 484276 Jan 24 14:13 grinnell_28594_OBJ.jpg -rw-r--r--. 1 islandora islandora 428055 Jan 24 14:14 grinnell_28595_OBJ.jpg -rw-r--r--. 1 islandora islandora 8888853 Jan 24 14:14 grinnell_28596_OBJ.jpg -rw-r--r--. 1 islandora islandora 854220 Jan 24 14:14 grinnell_28597_OBJ.jpg -rw-r--r--. 1 islandora islandora 5521699 Jan 24 14:14 grinnell_28598_OBJ.jpg -rw-r--r--. 1 islandora islandora 1197383 Jan 24 14:14 grinnell_28599_OBJ.jpg -rw-r--r--. 1 islandora islandora 9394607 Jan 24 14:13 grinnell_28600_OBJ.jpg -rw-r--r--. 1 islandora islandora 5395750 Jan 24 14:14 grinnell_28601_OBJ.jpg -rw-r--r--. 1 islandora islandora 5220523 Jan 24 14:14 grinnell_28602_OBJ.jpg -rw-r--r--. 1 islandora islandora 3540877 Jan 24 14:14 grinnell_28603_OBJ.jpg -rw-r--r--. 1 islandora islandora 579768 Jan 24 14:13 grinnell_28604_OBJ.jpg -rw-r--r--. 1 islandora islandora 2896806 Jan 24 14:13 grinnell_28605_OBJ.jpg -rw-r--r--. 1 islandora islandora 3050759 Jan 24 14:14 grinnell_28606_OBJ.jpg -rw-r--r--. 1 islandora islandora 2554044 Jan 24 14:14 grinnell_28607_OBJ.jpg -rw-r--r--. 1 islandora islandora 556946 Jan 24 14:14 grinnell_28608_OBJ.jpg -rw-r--r--. 1 islandora islandora 24781 Jan 24 14:14 grinnell_28609_OBJ.jpg -rw-r--r--. 1 islandora islandora 1398411 Jan 24 14:13 grinnell_28610_OBJ.jpg -rw-r--r--. 1 islandora islandora 3974308 Jan 24 14:13 grinnell_28611_OBJ.jpg -rw-r--r--. 1 islandora islandora 57152190 Jan 24 14:14 grinnell_28743_OBJ.mp4 -rw-r--r--. 1 islandora islandora 215027175 Jan 24 14:13 grinnell_28744_OBJ.mp4 -rw-r--r--. 1 islandora islandora 3644620 Jan 24 14:13 grinnell_28771_OBJ.jpg -rw-r--r--. 1 islandora islandora 2927416 Jan 24 14:14 grinnell_28772_OBJ.jpg -rw-r--r--. 1 islandora islandora 2356352 Jan 24 14:14 grinnell_28773_OBJ.jpg -rw-r--r--. 1 islandora islandora 1723094 Jan 24 14:14 grinnell_28774_OBJ.jpg -rw-r--r--. 1 islandora islandora 57152190 Jan 24 14:13 grinnell_29432_OBJ.mp4 -rw-r--r--. 1 islandora islandora 36176455 Jan 24 14:13 grinnell_29539_OBJ.pdf -rw-r--r--. 1 islandora islandora 1038414 Jan 24 14:14 grinnell_29540_OBJ.png -rw-r--r--. 1 islandora islandora 58008 Jan 24 14:13 grinnell_29541_OBJ.jpg -rw-r--r--. 1 islandora islandora 1342770 Jan 24 14:14 grinnell_29542_OBJ.jpg -rw-r--r--. 1 islandora islandora 1776219 Jan 24 14:13 grinnell_29543_OBJ.jpg -rw-r--r--. 1 islandora islandora 1276257 Jan 24 14:14 grinnell_29544_OBJ.jpg -rw-r--r--. 1 islandora islandora 660354 Jan 24 14:14 grinnell_29546_OBJ.jpg -rw-r--r--. 1 islandora islandora 29363953 Jan 24 14:14 grinnell_29547_OBJ.mp3 -rw-r--r--. 1 islandora islandora 1106625 Jan 24 14:13 grinnell_29548_OBJ.jpg -rw-r--r--. 1 islandora islandora 53385 Jan 24 14:13 grinnell_29549_OBJ.jpg -rw-r--r--. 1 islandora islandora 2221876 Jan 24 14:13 grinnell_29550_OBJ.jpg -rw-r--r--. 1 islandora islandora 5654154 Jan 24 14:13 grinnell_29551_OBJ.png -rw-r--r--. 1 islandora islandora 4480841 Jan 24 14:14 grinnell_29552_OBJ.png -rw-r--r--. 1 islandora islandora 446463 Jan 24 14:13 grinnell_29553_OBJ.jpg -rw-r--r--. 1 islandora islandora 1458312 Jan 24 14:13 grinnell_29554_OBJ.jpg -rw-r--r--. 1 islandora islandora 18413 Jan 24 14:13 grinnell_29556_OBJ.jpg -rw-r--r--. 1 islandora islandora 44960 Jan 24 14:14 grinnell_29557_OBJ.png -rw-r--r--. 1 islandora islandora 176080 Jan 24 14:13 grinnell_29558_OBJ.jpg -rw-r--r--. 1 islandora islandora 95087 Jan 24 14:14 grinnell_29559_OBJ.png -rw-r--r--. 1 islandora islandora 293734 Jan 24 14:13 grinnell_29560_OBJ.jpg -rw-r--r--. 1 islandora islandora 77891 Jan 24 14:13 grinnell_29561_OBJ.jpg -rw-r--r--. 1 islandora islandora 145603 Jan 24 14:14 grinnell_29562_OBJ.jpg -rw-r--r--. 1 islandora islandora 60765 Jan 24 14:13 grinnell_29563_OBJ.png -rw-r--r--. 1 islandora islandora 1304503 Jan 24 14:14 grinnell_29564_OBJ.png -rw-r--r--. 1 islandora islandora 496411 Jan 24 14:14 grinnell_29565_OBJ.jpg -rw-r--r--. 1 islandora islandora 2680272 Jan 24 14:14 grinnell_29566_OBJ.jpg -rw-r--r--. 1 islandora islandora 3347257 Jan 24 14:13 grinnell_29567_OBJ.jpg -rw-r--r--. 1 islandora islandora 1233120 Jan 24 14:14 grinnell_29568_OBJ.jpg -rw-r--r--. 1 islandora islandora 2832162 Jan 24 14:13 grinnell_29569_OBJ.jpg -rw-r--r--. 1 islandora islandora 3632645 Jan 24 14:14 grinnell_29570_OBJ.jpg -rw-r--r--. 1 islandora islandora 3483326 Jan 24 14:14 grinnell_29571_OBJ.jpg -rw-r--r--. 1 islandora islandora 3667752 Jan 24 14:13 grinnell_29572_OBJ.jpg -rw-r--r--. 1 islandora islandora 2628051 Jan 24 14:14 grinnell_29573_OBJ.jpg -rw-r--r--. 1 islandora islandora 183546 Jan 24 14:14 grinnell_29574_OBJ.jpg -rw-r--r--. 1 islandora islandora 39872 Jan 24 14:13 grinnell_29575_OBJ.jpg -rw-r--r--. 1 islandora islandora 115714 Jan 24 14:14 grinnell_29576_OBJ.jpg -rw-r--r--. 1 islandora islandora 170528 Jan 24 14:14 grinnell_29577_OBJ.jpg -rw-r--r--. 1 islandora islandora 1471118 Jan 24 14:14 grinnell_29578_OBJ.jpg -rw-r--r--. 1 islandora islandora 685359 Jan 24 14:14 grinnell_29579_OBJ.jpg -rw-r--r--. 1 islandora islandora 5536862 Jan 24 14:14 grinnell_29580_OBJ.png -rw-r--r--. 1 islandora islandora 2484265 Jan 24 14:14 grinnell_29581_OBJ.jpg -rw-r--r--. 1 islandora islandora 814620 Jan 24 14:14 grinnell_29582_OBJ.jpg -rw-r--r--. 1 islandora islandora 158536 Jan 24 14:14 grinnell_29583_OBJ.jpg -rw-r--r--. 1 islandora islandora 3606487 Jan 24 14:13 grinnell_29584_OBJ.mp3 -rw-r--r--. 1 islandora islandora 713351 Jan 24 14:13 grinnell_29585_OBJ.png -rw-r--r--. 1 islandora islandora 701049 Jan 24 14:14 grinnell_29586_OBJ.png -rw-r--r--. 1 islandora islandora 1240492 Jan 24 14:14 grinnell_29587_OBJ.png -rw-r--r--. 1 islandora islandora 2416253 Jan 24 14:13 grinnell_29588_OBJ.jpg -rw-r--r--. 1 islandora islandora 1112190 Jan 24 14:14 grinnell_29589_OBJ.png -rw-r--r--. 1 islandora islandora 27081 Jan 24 14:13 grinnell_29590_OBJ.jpg -rw-r--r--. 1 islandora islandora 1119347 Jan 24 14:14 grinnell_29591_OBJ.png -rw-r--r--. 1 islandora islandora 186698 Jan 24 14:14 grinnell_29592_OBJ.jpg -rw-r--r--. 1 islandora islandora 10037 Jan 24 14:14 grinnell_29593_OBJ.jpg -rw-r--r--. 1 islandora islandora 1765488 Jan 24 14:14 grinnell_29594_OBJ.jpg -rw-r--r--. 1 islandora islandora 495776 Jan 24 14:13 grinnell_29595_OBJ.jpg -rw-r--r--. 1 islandora islandora 729579 Jan 24 14:14 grinnell_29596_OBJ.jpg -rw-r--r--. 1 islandora islandora 1098258 Jan 24 14:14 grinnell_29597_OBJ.jpg -rw-r--r--. 1 islandora islandora 501682 Jan 24 14:14 grinnell_29599_OBJ.jpg -rw-r--r--. 1 islandora islandora 946809 Jan 24 14:14 grinnell_29600_OBJ.jpg -rw-r--r--. 1 islandora islandora 200609 Jan 24 14:14 grinnell_29601_OBJ.jpg -rw-r--r--. 1 islandora islandora 495197 Jan 24 14:14 grinnell_29602_OBJ.jpg -rw-r--r--. 1 islandora islandora 941181 Jan 24 14:14 grinnell_29603_OBJ.jpg -rw-r--r--. 1 islandora islandora 1021954 Jan 24 14:14 grinnell_29604_OBJ.mp3 -rw-r--r--. 1 islandora islandora 198719 Jan 24 14:14 grinnell_29605_OBJ.jpg -rw-r--r--. 1 islandora islandora 739537 Jan 24 14:13 grinnell_29606_OBJ.jpg -rw-r--r--. 1 islandora islandora 1992991 Jan 24 14:13 grinnell_29612_OBJ.jpg -rw-r--r--. 1 islandora islandora 696587 Jan 24 14:14 grinnell_29613_OBJ.jpg -rw-r--r--. 1 islandora islandora 1840034 Jan 24 14:14 grinnell_29614_OBJ.jpg -rw-r--r--. 1 islandora islandora 622656 Jan 24 14:14 grinnell_29615_OBJ.jpg -rw-r--r--. 1 islandora islandora 17886716 Jan 24 14:13 grinnell_29616_OBJ.jpg -rw-r--r--. 1 islandora islandora 5122519 Jan 24 14:14 grinnell_29617_OBJ.jpg -rw-r--r--. 1 islandora islandora 2017009 Jan 24 14:14 grinnell_29618_OBJ.jpg -rw-r--r--. 1 islandora islandora 4204300 Jan 24 14:13 grinnell_29619_OBJ.jpg -rw-r--r--. 1 islandora islandora 577832 Jan 24 14:14 grinnell_29620_OBJ.jpg -rw-r--r--. 1 islandora islandora 937652 Jan 24 14:14 grinnell_29621_OBJ.jpg -rw-r--r--. 1 islandora islandora 2396914 Jan 24 14:14 grinnell_29622_OBJ.jpg -rw-r--r--. 1 islandora islandora 23052 Jan 24 14:14 grinnell_29623_OBJ.jpg -rw-r--r--. 1 islandora islandora 4917737 Jan 24 14:14 grinnell_29624_OBJ.jpg -rw-r--r--. 1 islandora islandora 782889 Jan 24 14:14 grinnell_29625_OBJ.jpg -rw-r--r--. 1 islandora islandora 1671519 Jan 24 14:14 grinnell_29627_OBJ.jpg -rw-r--r--. 1 islandora islandora 3706466 Jan 24 14:14 grinnell_29628_OBJ.jpg -rw-r--r--. 1 islandora islandora 412724 Jan 24 14:13 grinnell_29629_OBJ.jpg -rw-r--r--. 1 islandora islandora 323358816 Jan 24 14:13 grinnell_29632_OBJ.mp4 -rw-r--r--. 1 islandora islandora 33359673 Jan 24 14:14 grinnell_29633_OBJ.mp4 -rw-r--r--. 1 islandora islandora 8838186 Jan 24 14:13 grinnell_29638_OBJ.jpg -rw-r--r--. 1 islandora islandora 217621 Jan 24 14:14 grinnell_29639_OBJ.jpg -rw-r--r--. 1 islandora islandora 2184474 Jan 24 14:14 grinnell_29640_OBJ.jpg -rw-r--r--. 1 islandora islandora 7919140 Jan 24 14:13 grinnell_29641_OBJ.jpg -rw-r--r--. 1 islandora islandora 173325 Jan 24 14:14 grinnell_29642_OBJ.jpg -rw-r--r--. 1 islandora islandora 559959 Jan 24 14:13 grinnell_29643_OBJ.jpg -rw-r--r--. 1 islandora islandora 169325 Jan 24 14:13 grinnell_29644_OBJ.jpg -rw-r--r--. 1 islandora islandora 1683146 Jan 24 14:14 grinnell_29645_OBJ.png -rw-r--r--. 1 islandora islandora 3436024 Jan 24 14:14 grinnell_29646_OBJ.jpg -rw-r--r--. 1 islandora islandora 851840 Jan 24 14:13 grinnell_29647_OBJ.jpg -rw-r--r--. 1 islandora islandora 13633772 Jan 24 14:14 grinnell_29648_OBJ.jpg -rw-r--r--. 1 islandora islandora 14439157 Jan 24 14:14 grinnell_29649_OBJ.jpg -rw-r--r--. 1 islandora islandora 2417870 Jan 24 14:14 grinnell_29650_OBJ.jpg -rw-r--r--. 1 islandora islandora 1814392 Jan 24 14:13 grinnell_29651_OBJ.jpg -rw-r--r--. 1 islandora islandora 454503 Jan 24 14:14 grinnell_29652_OBJ.jpg -rw-r--r--. 1 islandora islandora 22508355 Jan 24 14:14 grinnell_29653_OBJ.jpg -rw-r--r--. 1 islandora islandora 24676426 Jan 24 14:14 grinnell_29654_OBJ.jpg -rw-r--r--. 1 islandora islandora 20417653 Jan 24 14:14 grinnell_29655_OBJ.jpg -rw-r--r--. 1 islandora islandora 6694759 Jan 24 14:14 grinnell_29656_OBJ.jpg -rw-r--r--. 1 islandora islandora 6573281 Jan 24 14:14 grinnell_29657_OBJ.jpg -rw-r--r--. 1 islandora islandora 7518511 Jan 24 14:14 grinnell_29658_OBJ.jpg -rw-r--r--. 1 islandora islandora 8350328 Jan 24 14:13 grinnell_29659_OBJ.jpg -rw-r--r--. 1 islandora islandora 7322834 Jan 24 14:14 grinnell_29660_OBJ.jpg -rw-r--r--. 1 islandora islandora 7207438 Jan 24 14:13 grinnell_29661_OBJ.jpg -rw-r--r--. 1 islandora islandora 1100220 Jan 24 14:13 grinnell_29662_OBJ.jpg -rw-r--r--. 1 islandora islandora 508157 Jan 24 14:14 grinnell_29663_OBJ.jpg -rw-r--r--. 1 islandora islandora 2071947 Jan 24 14:14 grinnell_29664_OBJ.jpg -rw-r--r--. 1 islandora islandora 2759519 Jan 24 14:14 grinnell_29665_OBJ.jpg -rw-r--r--. 1 islandora islandora 363051 Jan 24 14:14 grinnell_29666_OBJ.jpg -rw-r--r--. 1 islandora islandora 912831 Jan 24 14:14 grinnell_29667_OBJ.png -rw-r--r--. 1 islandora islandora 471499 Jan 24 14:14 grinnell_29668_OBJ.png -rw-r--r--. 1 islandora islandora 665821 Jan 24 14:14 grinnell_29669_OBJ.png -rw-r--r--. 1 islandora islandora 135748 Jan 24 14:14 grinnell_29670_OBJ.jpg -rw-r--r--. 1 islandora islandora 164252 Jan 24 14:14 grinnell_29672_OBJ.jpg -rw-r--r--. 1 islandora islandora 4399381 Jan 24 14:14 grinnell_29673_OBJ.jpg -rw-r--r--. 1 islandora islandora 4337360 Jan 24 14:14 grinnell_29674_OBJ.jpg -rw-r--r--. 1 islandora islandora 1585421 Jan 24 14:14 grinnell_29675_OBJ.png -rw-r--r--. 1 islandora islandora 575147 Jan 24 14:14 grinnell_29676_OBJ.jpg -rw-r--r--. 1 islandora islandora 431125 Jan 24 14:14 grinnell_29677_OBJ.jpg -rw-r--r--. 1 islandora islandora 258614 Jan 24 14:14 grinnell_29678_OBJ.jpg -rw-r--r--. 1 islandora islandora 154479 Jan 24 14:14 grinnell_29679_OBJ.jpg -rw-r--r--. 1 islandora islandora 314038 Jan 24 14:14 grinnell_29680_OBJ.jpg -rw-r--r--. 1 islandora islandora 1665650 Jan 24 14:14 grinnell_29681_OBJ.jpg -rw-r--r--. 1 islandora islandora 659540 Jan 24 14:14 grinnell_29682_OBJ.jpg -rw-r--r--. 1 islandora islandora 862621 Jan 24 14:14 grinnell_29694_OBJ.jpg -rw-r--r--. 1 islandora islandora 10973119 Jan 24 14:14 grinnell_29706_OBJ.jpg -rw-r--r--. 1 islandora islandora 265414 Jan 24 14:13 grinnell_29707_OBJ.jpg -rw-r--r--. 1 islandora islandora 9387200 Jan 24 14:13 grinnell_29708_OBJ.jpg -rw-r--r--. 1 islandora islandora 4705731 Jan 24 14:14 grinnell_29709_OBJ.png -rw-r--r--. 1 islandora islandora 2778937 Jan 24 14:14 grinnell_29710_OBJ.jpg -rw-r--r--. 1 islandora islandora 791528 Jan 24 14:14 grinnell_29711_OBJ.jpg -rw-r--r--. 1 islandora islandora 547133 Jan 24 14:14 grinnell_29712_OBJ.jpg -rw-r--r--. 1 islandora islandora 757581 Jan 24 14:14 grinnell_29713_OBJ.jpg -rw-r--r--. 1 islandora islandora 253847 Jan 24 14:14 grinnell_29714_OBJ.jpg -rw-r--r--. 1 islandora islandora 312735 Jan 24 14:14 grinnell_29715_OBJ.jpg -rw-r--r--. 1 islandora islandora 733347 Jan 24 14:14 grinnell_29716_OBJ.png -rw-r--r--. 1 islandora islandora 538614 Jan 24 14:14 grinnell_29717_OBJ.png -rw-r--r--. 1 islandora islandora 4322467 Jan 24 14:14 grinnell_29891_OBJ.jpg -rw-r--r--. 1 islandora islandora 4503431 Jan 24 14:14 grinnell_29892_OBJ.jpg -rw-r--r--. 1 islandora islandora 4565245 Jan 24 14:14 grinnell_29893_OBJ.jpg -rw-r--r--. 1 islandora islandora 2553837 Jan 24 14:13 grinnell_29894_OBJ.jpg -rw-r--r--. 1 islandora islandora 2989592 Jan 24 14:13 grinnell_29895_OBJ.jpg -rw-r--r--. 1 islandora islandora 1083196 Jan 24 14:14 grinnell_29896_OBJ.jpg -rw-r--r--. 1 islandora islandora 1342770 Jan 24 14:14 grinnell_29897_OBJ.jpg -rw-r--r--. 1 islandora islandora 3135291 Jan 24 14:14 grinnell_29898_OBJ.pdf -rw-r--r--. 1 islandora islandora 32332233 Jan 24 14:14 grinnell_29899_OBJ.pdf -rw-r--r--. 1 islandora islandora 1655225 Jan 24 14:14 grinnell_29900_OBJ.jpg -rw-r--r--. 1 islandora islandora 2610725 Jan 24 14:13 grinnell_29901_OBJ.jpg -rw-r--r--. 1 islandora islandora 2627201 Jan 24 14:14 grinnell_29902_OBJ.jpg -rw-r--r--. 1 islandora islandora 82443 Jan 24 14:14 grinnell_29903_OBJ.jpg -rw-r--r--. 1 islandora islandora 448352 Jan 24 14:14 grinnell_29904_OBJ.pdf -rw-r--r--. 1 islandora islandora 356461 Jan 24 14:13 grinnell_29905_OBJ.jpg -rw-r--r--. 1 islandora islandora 197721 Jan 24 14:14 grinnell_29906_OBJ.jpg -rw-r--r--. 1 islandora islandora 213512 Jan 24 14:14 grinnell_29907_OBJ.jpg -rw-r--r--. 1 islandora islandora 232116 Jan 24 14:14 grinnell_29908_OBJ.jpg -rw-r--r--. 1 islandora islandora 199208 Jan 24 14:14 grinnell_29909_OBJ.jpg -rw-r--r--. 1 islandora islandora 257597 Jan 24 14:14 grinnell_29910_OBJ.jpg -rw-r--r--. 1 islandora islandora 475470 Jan 24 14:14 grinnell_29911_OBJ.jpg -rw-r--r--. 1 islandora islandora 572720 Jan 24 14:14 grinnell_29912_OBJ.pdf -rw-r--r--. 1 islandora islandora 2094880 Jan 24 14:14 grinnell_29913_OBJ.jpg -rw-r--r--. 1 islandora islandora 175535 Jan 24 14:14 grinnell_29914_OBJ.jpg -rw-r--r--. 1 islandora islandora 225942 Jan 24 14:14 grinnell_29915_OBJ.jpg -rw-r--r--. 1 islandora islandora 523212 Jan 24 14:14 grinnell_29916_OBJ.jpg -rw-r--r--. 1 islandora islandora 1144732 Jan 24 14:13 grinnell_29917_OBJ.jpg -rw-r--r--. 1 islandora islandora 1144371 Jan 24 14:14 grinnell_29918_OBJ.pdf -rw-r--r--. 1 islandora islandora 103825 Jan 24 14:13 grinnell_29919_OBJ.jpg -rw-r--r--. 1 islandora islandora 98025 Jan 24 14:14 grinnell_29920_OBJ.jpg -rw-r--r--. 1 islandora islandora 327135 Jan 24 14:14 grinnell_29921_OBJ.jpg -rw-r--r--. 1 islandora islandora 312303 Jan 24 14:14 grinnell_29922_OBJ.jpg -rw-r--r--. 1 islandora islandora 304871 Jan 24 14:13 grinnell_29923_OBJ.jpg -rw-r--r--. 1 islandora islandora 5174838 Jan 24 14:13 grinnell_29924_OBJ.jpg -rw-r--r--. 1 islandora islandora 476889 Jan 24 14:14 grinnell_29925_OBJ.pdf -rw-r--r--. 1 islandora islandora 259385 Jan 24 14:14 grinnell_29926_OBJ.pdf -rw-r--r--. 1 islandora islandora 485865 Jan 24 14:13 grinnell_29927_OBJ.jpg -rw-r--r--. 1 islandora islandora 5923442 Jan 24 14:13 grinnell_29928_OBJ.jpg -rw-r--r--. 1 islandora islandora 3748237 Jan 24 14:14 grinnell_29929_OBJ.jpg -rw-r--r--. 1 islandora islandora 129223 Jan 24 14:14 grinnell_29930_OBJ.jpg -rw-r--r--. 1 islandora islandora 974964 Jan 24 14:14 grinnell_29931_OBJ.pdf -rw-r--r--. 1 islandora islandora 200122 Jan 24 14:14 grinnell_29932_OBJ.jpg -rw-r--r--. 1 islandora islandora 2941639 Jan 24 14:14 grinnell_29933_OBJ.jpg -rw-r--r--. 1 islandora islandora 762691 Jan 24 14:14 grinnell_29934_OBJ.pdf -rw-r--r--. 1 islandora islandora 660118 Jan 24 14:14 grinnell_29935_OBJ.jpg -rw-r--r--. 1 islandora islandora 113295 Jan 24 14:14 grinnell_29936_OBJ.jpg -rw-r--r--. 1 islandora islandora 313891 Jan 24 14:14 grinnell_29937_OBJ.jpg -rw-r--r--. 1 islandora islandora 312598 Jan 24 14:14 grinnell_29938_OBJ.jpg -rw-r--r--. 1 islandora islandora 235405 Jan 24 14:13 grinnell_29939_OBJ.jpg -rw-r--r--. 1 islandora islandora 226884 Jan 24 14:13 grinnell_29940_OBJ.jpg -rw-r--r--. 1 islandora islandora 197907 Jan 24 14:14 grinnell_29941_OBJ.jpg -rw-r--r--. 1 islandora islandora 1080193 Jan 24 14:14 grinnell_29942_OBJ.pdf -rw-r--r--. 1 islandora islandora 383448 Jan 24 14:14 grinnell_29943_OBJ.pdf -rw-r--r--. 1 islandora islandora 20893613 Jan 24 14:13 grinnell_29944_OBJ.jpg -rw-r--r--. 1 islandora islandora 19886578 Jan 24 14:14 grinnell_29945_OBJ.jpg -rw-r--r--. 1 islandora islandora 20512936 Jan 24 14:14 grinnell_29946_OBJ.jpg -rw-r--r--. 1 islandora islandora 500146 Jan 24 14:14 grinnell_29947_OBJ.jpg -rw-r--r--. 1 islandora islandora 23448497 Jan 24 14:14 grinnell_29948_OBJ.jpg -rw-r--r--. 1 islandora islandora 23288954 Jan 24 14:14 grinnell_29949_OBJ.jpg -rw-r--r--. 1 islandora islandora 13714481 Jan 24 14:14 grinnell_29950_OBJ.jpg -rw-r--r--. 1 islandora islandora 16059786 Jan 24 14:14 grinnell_29951_OBJ.jpg -rw-r--r--. 1 islandora islandora 21244099 Jan 24 14:14 grinnell_29952_OBJ.jpg -rw-r--r--. 1 islandora islandora 27823934 Jan 24 14:14 grinnell_29953_OBJ.jpg -rw-r--r--. 1 islandora islandora 15356115 Jan 24 14:13 grinnell_29954_OBJ.jpg -rw-r--r--. 1 islandora islandora 17418047 Jan 24 14:14 grinnell_29955_OBJ.jpg -rw-r--r--. 1 islandora islandora 16084117 Jan 24 14:14 grinnell_29956_OBJ.jpg -rw-r--r--. 1 islandora islandora 14767707 Jan 24 14:13 grinnell_29957_OBJ.jpg -rw-r--r--. 1 islandora islandora 23264211 Jan 24 14:14 grinnell_29958_OBJ.pdf -rw-r--r--. 1 islandora islandora 573091 Jan 24 14:13 grinnell_29959_OBJ.pdf -rw-r--r--. 1 islandora islandora 664199 Jan 24 14:14 grinnell_29960_OBJ.png -rw-r--r--. 1 islandora islandora 16564 Jan 24 14:14 grinnell_29961_OBJ.jpg -rw-r--r--. 1 islandora islandora 50323 Jan 24 14:14 grinnell_29962_OBJ.jpg -rw-r--r--. 1 islandora islandora 154960 Jan 24 14:13 grinnell_29963_OBJ.jpg -rw-r--r--. 1 islandora islandora 54601 Jan 24 14:14 grinnell_29964_OBJ.jpg -rw-r--r--. 1 islandora islandora 257685 Jan 24 14:14 grinnell_29965_OBJ.jpg -rw-r--r--. 1 islandora islandora 30762 Jan 24 14:13 grinnell_29966_OBJ.jpg -rw-r--r--. 1 islandora islandora 1048423 Jan 24 14:14 grinnell_29967_OBJ.pdf -rw-r--r--. 1 islandora islandora 5685915 Jan 24 14:14 grinnell_29968_OBJ.jpg -rw-r--r--. 1 islandora islandora 6447406 Jan 24 14:14 grinnell_29969_OBJ.jpg -rw-r--r--. 1 islandora islandora 6300238 Jan 24 14:14 grinnell_29970_OBJ.jpg -rw-r--r--. 1 islandora islandora 390766 Jan 24 14:14 grinnell_29971_OBJ.jpg -rw-r--r--. 1 islandora islandora 1863608 Jan 24 14:13 grinnell_29972_OBJ.jpg -rw-r--r--. 1 islandora islandora 892697 Jan 24 14:14 grinnell_29973_OBJ.jpg -rw-r--r--. 1 islandora islandora 1824278 Jan 24 14:14 grinnell_29974_OBJ.pdf -rw-r--r--. 1 islandora islandora 166963 Jan 24 14:14 grinnell_29975_OBJ.png -rw-r--r--. 1 islandora islandora 32394967 Jan 24 14:14 grinnell_29976_OBJ.pdf -rw-r--r--. 1 islandora islandora 288904 Jan 24 14:14 grinnell_29990_OBJ.jpg -rw-r--r--. 1 islandora islandora 8733861 Jan 24 14:13 grinnell_29997_OBJ.jpg -rw-r--r--. 1 islandora islandora 132374 Jan 24 14:14 grinnell_29999_OBJ.jpg The /tmp directory inside the Apache container is purged of old files every hour, so quickly I copied them all to a safe directory on DGDocker1 like so:\n[islandora@dgdocker1 ~]$ docker cp isle-apache-dg:/tmp/. /home/islandora/. Errors So, there were 477 Rootstalk objects listed in DG, why did we only export 385? The errors reported above lend a clue\u0026hellip; many of the Rootstalk objects are children of a compound object parent, and the parent object has no OBJ datastream of its own. As a result, 92 Rootstalk objects were not exported and they should not be missed.\nAnd that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/117-export-rootstalk-objs-from-dg/","tags":["Rootstalk","islandora_datastream_exporter","export"],"title":"Export Rootstalk OBJs from Digital.Grinnell"},{"categories":null,"contents":"My goal for this morning, December 22, 2021, was to find a process I could reliably use to synchronize changes in the development copy of Rootstalk (the main branch https://github.com/Digital-Grinnell/rootstalk) with our production deployment (the main branch of https://github.com/Digital-Grinnell/rootstalk-DO) to DigitalOcean. I used guidance found in How To Merge Between Two Local Repositories to accomplish this with mixed results.\nNote: Our staging site cast from the main branch of https://github.com/Digital-Grinnell/rootstalk, an Azure static app, can be accessed via https://icy-tree-020380010.azurestaticapps.net.\nThe Workflow I\u0026rsquo;ve captured the inputs and output of the workflow below. The process basically involved adding a new local remote named dev to my existing rootstalk-DO local repository, and doing a git fetch of that new remote. Everything in the code block that follows is as-it-was-executed on my Grinnell College MacBook\u0026hellip;\n╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ cd ../rootstalk ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹main› ╰─$ git pull Already up to date. ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk ‹main› ╰─$ cd ../rootstalk-DO ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ git remote add dev ../rootstalk ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ git fetch dev remote: Enumerating objects: 664, done. remote: Counting objects: 100% (664/664), done. remote: Compressing objects: 100% (616/616), done. remote: Total 624 (delta 471), reused 1 (delta 0), pack-reused 0 Receiving objects: 100% (624/624), 23.39 MiB | 30.87 MiB/s, done. Resolving deltas: 100% (471/471), completed with 30 local objects. From ../rootstalk * [new branch] gokcebel -\u0026gt; dev/gokcebel * [new branch] main -\u0026gt; dev/main * [new branch] mcfate -\u0026gt; dev/mcfate * [new branch] sep-23-mcfate -\u0026gt; dev/sep-23-mcfate * [new branch] v4i2 -\u0026gt; dev/v4i2 ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ git merge dev/main Removing content/past-issues/volume-vii-issue-1/rootstalk_leaf.svg:Zone.Identifier Removing content/past-issues/volume-vi-issue-1/rootstalk_leaf.svg:Zone.Identifier Removing content/images/rootstalk_leaf.svg:Zone.Identifier Removing assets/sass/_custom.scss:Zone.Identifier Removing Workflow Text.pdf:Zone.Identifier Removing Workflow Diagrams.zip:Zone.Identifier Merge made by the \u0026#39;recursive\u0026#39; strategy. .github/workflows/azure-static-web-apps-delightful-stone-01bd98310.yml | 45 + .github/workflows/azure-static-web-apps-icy-tree-020380010.yml | 45 + .hugo_build.lock | 0 Git-Workflow-in-Windows.md | 119 +++ Meeting Notes.docx =\u0026gt; Meeting-Notes.docx | Bin Workflow Diagrams.zip:Zone.Identifier | 4 - Workflow Text.pdf:Zone.Identifier | 4 - Workflow-Diagrams/1.png | Bin 0 -\u0026gt; 247700 bytes Workflow-Diagrams/10.png | Bin 0 -\u0026gt; 1892560 bytes Workflow-Diagrams/11.png | Bin 0 -\u0026gt; 970248 bytes Workflow-Diagrams/2.png | Bin 0 -\u0026gt; 1313699 bytes Workflow-Diagrams/3.png | Bin 0 -\u0026gt; 1060465 bytes Workflow-Diagrams/4.png | Bin 0 -\u0026gt; 1020743 bytes Workflow-Diagrams/5.png | Bin 0 -\u0026gt; 1419989 bytes Workflow-Diagrams/6.png | Bin 0 -\u0026gt; 1372473 bytes Workflow-Diagrams/7.png | Bin 0 -\u0026gt; 1251133 bytes Workflow-Diagrams/8.png | Bin 0 -\u0026gt; 1438089 bytes Workflow-Diagrams/9.png | Bin 0 -\u0026gt; 1233239 bytes Workflow Text.pdf =\u0026gt; Workflow-Text.pdf | Bin alina-git-workflow.md | 10 +- assets/sass/_custom.scss | 6 + assets/sass/_custom.scss:Zone.Identifier | 3 - complex-pdf-to-markdown.py | 273 ++++++ config.toml | 4 +- content/images/rootstalk_leaf.svg:Zone.Identifier | 3 - content/past-issues/volume-iv-issue-1/.not-ready/mutel.md | 31 + content/past-issues/volume-iv-issue-1/.pending-new/bernal-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/birds-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/burt-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/chirdon-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/clark-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/dunham-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/evans-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/freeberg-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/jackson-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/kugel-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/lewis-beck-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/luftig-pending.md | 738 +++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/moffett-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/munoz-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/mutel-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending-new/publisher-pending.md | 25 + content/past-issues/volume-iv-issue-1/.pending-new/rideout-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/birds.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/burt-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/chirdon-pending.md | 103 +++ content/past-issues/volume-iv-issue-1/.pending/chirdon.md | 103 +++ content/past-issues/volume-iv-issue-1/.pending/clark.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/dunham-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/evans-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/freeburg.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/jackson-pending.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/kugel-pending.md | 28 + content/past-issues/volume-iv-issue-1/.pending/lewis-beck.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/luftig.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/moffett.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/munoz.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/mutel.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/publisher.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/.pending/rideout.md | 2185 ++++++++++++++++++++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/2017-fall.yml | 22 + content/past-issues/volume-iv-issue-1/_index.md | 6 +- content/past-issues/volume-iv-issue-1/bernal.md | 67 ++ content/past-issues/volume-iv-issue-1/birds.md | 70 ++ content/past-issues/volume-iv-issue-1/burt-volume-iv-issue-1.md | 38 + content/past-issues/volume-iv-issue-1/chirdon.md | 103 +++ content/past-issues/volume-iv-issue-1/clark.md | 73 ++ content/past-issues/volume-iv-issue-1/dunham.md | 1333 +++++++++++++++++++++++++++ content/past-issues/volume-iv-issue-1/editor.md | 33 + content/past-issues/volume-iv-issue-1/evans-volume-iv-issue-1.md | 122 +++ Workflow Diagrams.zip =\u0026gt; content/past-issues/volume-iv-issue-1/evans-volume-iv-issue-1.pdf | Bin 13221882 -\u0026gt; 11043734 bytes content/past-issues/volume-iv-issue-1/freeberg.md | 93 ++ content/past-issues/volume-iv-issue-1/jackson.md | 105 +++ content/past-issues/volume-iv-issue-1/kugel.md | 19 + content/past-issues/volume-iv-issue-1/lewis-beck.md | 132 +++ content/past-issues/volume-iv-issue-1/luftig.md | 738 +++++++++++++++ content/past-issues/volume-iv-issue-1/moffett.md | 109 +++ content/past-issues/volume-iv-issue-1/munoz.md | 26 + content/past-issues/volume-iv-issue-1/mutel.md | 32 + content/past-issues/volume-iv-issue-1/publisher.md | 25 + content/past-issues/volume-iv-issue-1/rideout.md | 69 ++ content/past-issues/volume-iv-issue-2/_index.md | 2 +- content/past-issues/volume-iv-issue-2/birds-of-the-prairie-vol-iv-issue-2.md | 21 +- content/past-issues/volume-iv-issue-2/closeup-keith-kozloff.md | 23 +- content/past-issues/volume-iv-issue-2/closeup-kristine-heykants.md | 19 +- content/past-issues/volume-iv-issue-2/editors-note-VolIV_Issue2.md | 5 +- content/past-issues/volume-iv-issue-2/finding-the-lost-duck.md | 13 +- content/past-issues/volume-iv-issue-2/from-my-table-to-yours.md | 13 +- content/past-issues/volume-iv-issue-2/gnosis.md | 7 +- content/past-issues/volume-iv-issue-2/growing-up-in-kansas.md | 31 +- content/past-issues/volume-iv-issue-2/johansson-his-ambassador.md | 860 +++++++++++++++++ .../past-issues/volume-iv-issue-2/{his-ambassador.md =\u0026gt; johansson-his-ambassador.md.save} | 11 +- content/past-issues/volume-iv-issue-2/lac-la-biche-mission.md | 13 +- content/past-issues/volume-iv-issue-2/making-room.md | 9 +- .../past-issues/volume-iv-issue-2/{peaches-meet-corn.md =\u0026gt; manoylov-peaches-meet-corn.md} | 51 +- content/past-issues/volume-iv-issue-2/two-poems-bill-graeser.md | 8 +- content/past-issues/volume-vi-issue-1/rootstalk_leaf.svg:Zone.Identifier | 3 - content/past-issues/volume-vii-issue-1/rootstalk_leaf.svg:Zone.Identifier | 3 - layouts/_default/list.html | 22 +- layouts/_default/single.html | 60 +- layouts/index.html | 26 +- layouts/shortcodes/audio_azure.html | 7 + layouts/shortcodes/figure_azure.html | 44 + layouts/shortcodes/video_azure.html | 38 + pdf-to-markdown.py | 58 ++ resources/_gen/assets/scss/sass/styles.scss_5bac553973685aab030dcdbdaeaab6f8.content | 2 +- resources/_gen/assets/scss/sass/styles.scss_5bac553973685aab030dcdbdaeaab6f8.json | 2 +- 107 files changed, 69337 insertions(+), 138 deletions(-) create mode 100644 .github/workflows/azure-static-web-apps-delightful-stone-01bd98310.yml create mode 100644 .github/workflows/azure-static-web-apps-icy-tree-020380010.yml create mode 100644 .hugo_build.lock create mode 100644 Git-Workflow-in-Windows.md rename Meeting Notes.docx =\u0026gt; Meeting-Notes.docx (100%) delete mode 100644 Workflow Diagrams.zip:Zone.Identifier delete mode 100644 Workflow Text.pdf:Zone.Identifier create mode 100644 Workflow-Diagrams/1.png create mode 100644 Workflow-Diagrams/10.png create mode 100644 Workflow-Diagrams/11.png create mode 100644 Workflow-Diagrams/2.png create mode 100644 Workflow-Diagrams/3.png create mode 100644 Workflow-Diagrams/4.png create mode 100644 Workflow-Diagrams/5.png create mode 100644 Workflow-Diagrams/6.png create mode 100644 Workflow-Diagrams/7.png create mode 100644 Workflow-Diagrams/8.png create mode 100644 Workflow-Diagrams/9.png rename Workflow Text.pdf =\u0026gt; Workflow-Text.pdf (100%) delete mode 100644 assets/sass/_custom.scss:Zone.Identifier create mode 100644 complex-pdf-to-markdown.py delete mode 100644 content/images/rootstalk_leaf.svg:Zone.Identifier create mode 100644 content/past-issues/volume-iv-issue-1/.not-ready/mutel.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/bernal-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/birds-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/burt-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/chirdon-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/clark-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/dunham-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/evans-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/freeberg-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/jackson-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/kugel-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/lewis-beck-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/luftig-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/moffett-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/munoz-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/mutel-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/publisher-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending-new/rideout-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/birds.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/burt-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/chirdon-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/chirdon.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/clark.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/dunham-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/evans-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/freeburg.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/jackson-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/kugel-pending.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/lewis-beck.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/luftig.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/moffett.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/munoz.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/mutel.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/publisher.md create mode 100644 content/past-issues/volume-iv-issue-1/.pending/rideout.md create mode 100755 content/past-issues/volume-iv-issue-1/2017-fall.yml create mode 100644 content/past-issues/volume-iv-issue-1/bernal.md create mode 100644 content/past-issues/volume-iv-issue-1/birds.md create mode 100644 content/past-issues/volume-iv-issue-1/burt-volume-iv-issue-1.md create mode 100644 content/past-issues/volume-iv-issue-1/chirdon.md create mode 100644 content/past-issues/volume-iv-issue-1/clark.md create mode 100644 content/past-issues/volume-iv-issue-1/dunham.md create mode 100644 content/past-issues/volume-iv-issue-1/editor.md create mode 100644 content/past-issues/volume-iv-issue-1/evans-volume-iv-issue-1.md rename Workflow Diagrams.zip =\u0026gt; content/past-issues/volume-iv-issue-1/evans-volume-iv-issue-1.pdf (53%) mode change 100644 =\u0026gt; 100755 create mode 100644 content/past-issues/volume-iv-issue-1/freeberg.md create mode 100644 content/past-issues/volume-iv-issue-1/jackson.md create mode 100644 content/past-issues/volume-iv-issue-1/kugel.md create mode 100644 content/past-issues/volume-iv-issue-1/lewis-beck.md create mode 100644 content/past-issues/volume-iv-issue-1/luftig.md create mode 100644 content/past-issues/volume-iv-issue-1/moffett.md create mode 100644 content/past-issues/volume-iv-issue-1/munoz.md create mode 100644 content/past-issues/volume-iv-issue-1/mutel.md create mode 100644 content/past-issues/volume-iv-issue-1/publisher.md create mode 100644 content/past-issues/volume-iv-issue-1/rideout.md create mode 100644 content/past-issues/volume-iv-issue-2/johansson-his-ambassador.md rename content/past-issues/volume-iv-issue-2/{his-ambassador.md =\u0026gt; johansson-his-ambassador.md.save} (99%) rename content/past-issues/volume-iv-issue-2/{peaches-meet-corn.md =\u0026gt; manoylov-peaches-meet-corn.md} (76%) delete mode 100644 content/past-issues/volume-vi-issue-1/rootstalk_leaf.svg:Zone.Identifier delete mode 100644 content/past-issues/volume-vii-issue-1/rootstalk_leaf.svg:Zone.Identifier create mode 100644 layouts/shortcodes/audio_azure.html create mode 100644 layouts/shortcodes/figure_azure.html create mode 100644 layouts/shortcodes/video_azure.html create mode 100644 pdf-to-markdown.py ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ git status On branch main Your branch is ahead of \u0026#39;origin/main\u0026#39; by 87 commits. (use \u0026#34;git push\u0026#34; to publish your local commits) nothing to commit, working tree clean ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ hugo server Start building sites … hugo v0.87.0+extended darwin/arm64 BuildDate=unknown WARN 2021/12/22 11:43:28 Page.Hugo is deprecated and will be removed in a future release. Use the global hugo function. .File.UniqueID on zero object. Wrap it in if or with: {{ with .File }}{{ .UniqueID }}{{ end }} | EN -------------------+------ Pages | 256 Paginator pages | 10 Non-page files | 171 Static files | 27 Processed images | 0 Aliases | 62 Sitemaps | 1 Cleaned | 0 Built in 605 ms Watching for changes in /Users/mcfatem/GitHub/rootstalk-DO/{archetypes,assets,content,layouts,static,themes} Watching for config changes in /Users/mcfatem/GitHub/rootstalk-DO/config.toml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at //localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop ^C% ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ git push Enumerating objects: 702, done. Counting objects: 100% (664/664), done. Delta compression using up to 8 threads Compressing objects: 100% (145/145), done. Writing objects: 100% (624/624), 23.39 MiB | 1.31 MiB/s, done. Total 624 (delta 472), reused 622 (delta 471), pack-reused 0 remote: Resolving deltas: 100% (472/472), completed with 30 local objects. To https://github.com/Digital-Grinnell/rootstalk-DO 90aa079..7a1c223 main -\u0026gt; main ╭─mcfatem@MAC02FK0XXQ05Q ~/GitHub/rootstalk-DO ‹main› ╰─$ The Bad News The bad news here is two-fold:\nI got the following email from Azure indicating a failed deployment. Why? Well, because a GitHub Action from the rootstalk remote, one that\u0026rsquo;s intended only to run in that repository\u0026rsquo;s Azure environment, got executed here in the new rootstalk-DO repository. Fortunately, this does NO HARM. [Digital-Grinnell/rootstalk-DO] Run failed: Azure Static Web Apps CI/CD - main (7a1c223) The production instance of Rootstalk now displays a © Rootstalk - from 'main' footer at the bottom of every page. Why? Well, because the config.toml file from rootstalk found its way into rootstalk-DO and it defines the footer to display that way. That display was intended to help us determine which branch of the rootstalk code is being displayed in development. Perhaps I need to investigate a way to populate that config.toml variable at compile-time? That would be sweet!\nThe Good News The good news is simple\u0026hellip; IT WORKED! Rootstalk is now up-to-date, with the minor issue reported above, and the deployment to DigitalOcean was automatic, as intended. The message I see in my DigitalOcean dashboard says:\nRecent Activity Dec 22 2021 LIVE Digital-Grinnell\u0026#39;s deployment went live Trigger: Digital-Grinnell pushed 7a1c223 to Digital-Grinnell/rootstalk-DO/main 11:45:15 AM And that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/116-sync-rootstalk-production-do-with-dev/","tags":["Rootstalk","sync","production","DigitalOcean"],"title":"Sync Rootstalk Production (DigitalOcean) with Dev"},{"categories":null,"contents":" Attention: On 21-May-2020 an optional, but recommended, sixth step was added to this workflow in the form of a new _Drush_ command: _islandora\\_mods_post\\_processing_, an addition to my previous work in [islandora_mods_via_twig](https://github.com/DigitalGrinnell/islandora_mods_via_twig). See my new post, [Islandora MODS Post Processing](/posts/075-islandora-mods-post-processing/) for complete details. Attention: In November 2021 a recommended seventh step was added to this workflow. That addition is documented in the final section of this document. A 7-Step Workflow This document is follow-up, with technical details, to Exporting, Editing \u0026amp; Replacing MODS Datastreams, post 069, in my blog. In case you missed it, the aforementioned post was written specifically for metadata editors working on the 2020 Grinnell College Libraries review of Digital Grinnell MODS metadata.\nAttention: This document uses a shorthand ./ in place of the frequently referenced //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/ directory. For example, ./social-justice is equivalent to the Social Justice collection sub-directory at //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/social-justice.\nBriefly, the seven steps in this workflow are:\nExport of all grinnell:* MODS datastreams using drush islandora_datastream_export. This step, last performed on April 14, 2020, was responsible for creating all of the grinnell_\u0026lt;PID\u0026gt;_MODS.xml exports found in ./\u0026lt;collection-PID\u0026gt;.\nExecute my Map-MODS-to-MASTER Python 3 script on iMac MA8660 to create a mods.tsv file for each collection, along with associated grinnell_\u0026lt;PID\u0026gt;_MODS.log and grinnell_\u0026lt;PID\u0026gt;_MODS.remainder files for each object. The resultant ./\u0026lt;collection-PID\u0026gt;/mods.tsv files are tab-seperated-value (.tsv) files, and they are key to this process.\nEdit the MODS .tsv files. Refer Exporting, Editing, \u0026amp; Replacing MODS Datastreams for details and guidance.\nUse drush islandora_mods_via_twig in each ready-for-update collection to generate new .xml MODS datastream files. For a specified collection, this command will find and read the ./\u0026lt;collection-PID\u0026gt;/mods-imvt.tsv and create one ./\u0026lt;collection-PID\u0026gt;/ready-for-datastream-replace/grinnell_\u0026lt;PID\u0026gt;_MODS.xml file for each object.\nExecute the drush islandora_datastream_replace command once for each collection. This command will process each ./\u0026lt;collection-PID\u0026gt;/ready-for-datastream-replace/grinnell_\u0026lt;PID\u0026gt;_MODS.xml file and replace the corresponding object\u0026rsquo;s MODS datastream with the contents of the .xml file. The digital_grinnell branch version of the islandora_datastream_replace command also performs an implicit update of the object\u0026rsquo;s \u0026ldquo;Title\u0026rdquo;, a transform of the new MODS to DC (Dublin Core), and a re-indexing of the new metadata in Solr.\nExecute an optional follow-up drush command as documented in Islandora MODS Post Processing. This portion of the workflow will help to reduce duplication of effort for objects that are shared between two or more collections.\nConfigure and run the main.py script as described in the README.md file at my reduce-MODS-remainders repository. This portion of the workflow will analyze all of the *.remainders files left behind by worflow Step 2 for objects in a given collection.\nThe remainder of this document provides technical details, frequently in the form of command lines used to build and use the aforementioned tools.\nStep 1a - Installation of Drush islandora_datastream_export and islandora_datastream_replace Commands To help implement this process efficiently and effectively I first turned to Exporting, Editing, \u0026amp; Replacing MODS Datastreams, a workflow developed by the good folks at The California Historical Society. I initiated the workflow by installing two Drush tools on my local/development instance of ISLE on my Mac workstation.\nThe command line process in my local host/workstation terminal looked like this:\nApache=isle-apache-ld docker exec -w /var/www/html/sites/all/modules/islandora/ ${Apache} git clone https://github.com/Islandora-Labs/islandora_datastream_exporter.git --recursive docker exec -w /var/www/html/sites/all/modules/islandora/ ${Apache} git clone https://github.com/pc37utn/islandora_datastream_replace.git --recursive docker exec -w /var/www/html/sites/all/modules/islandora/ ${Apache} chown -R islandora:www-data * docker exec -w /var/www/html/sites/default ${Apache} drush en islandora_datastream_exporter islandora_datastream_replace -y docker exec -w /var/www/html/sites/default ${Apache} drush cc drush -y Local tests of these commands were successful so I proceeded to install them in the production instance of Digital Grinnell at dgdocker1.grinnell.edu. Before doing that I needed to change the definition of Apache to reflect the production instance of our Apache container, like so Apache=isle-apache-dg.\nCreated a Fork of Islandora Datastream Replace I also chose to \u0026ldquo;fork\u0026rdquo; the islandora_datastream_replace project so that I could do a little Digital.Grinnell customization of it. The fork I\u0026rsquo;m working with is here and my work is limited to the digital_grinnell branch of that fork.\nIn the digital_grinnell branch I modified the behavior of the islandora_datastream_replace command so that it implicitly performs an UpdateFromMODS operation that lives in our idu, or Islandora Drush Utilities module. The UpdateFromMODS, performed immediately after each datastream replace operation does the following:\nUpdates the object \u0026ldquo;Title\u0026rdquo;, one of its properties, to match the new value of /mods:mods/mods:titleInfo[not(@type)]/mods:title. Invokes the iduF DCTransform operation which runs the default XSLT transform of the new MODS to DC (Dublin Core) and creates a new \u0026ldquo;DC\u0026rdquo; datastream for the object. The iduF DCTransform operation also concludes with an implicit iduF IndexSolr operation to ensure that the new object metadata is properly indexed in Solr. Step 1b - Installation of Drush islandora_datastream_export and islandora_datastream_replace Commands in Production To install the commands in production I opened a terminal to dgdocker1.grinnell.edu as user islandora and executed the following commands there:\nApache=isle-apache-dg docker exec -w /var/www/html/sites/all/modules/islandora/ ${Apache} git clone https://github.com/Islandora-Labs/islandora_datastream_exporter.git --recursive docker exec -w /var/www/html/sites/all/modules/islandora/ ${Apache} git clone https://github.com/DigitalGrinnell/islandora_datastream_replace.git --recursive docker exec -w /var/www/html/sites/all/modules/islandora/ ${Apache} chown -R islandora:www-data * docker exec -w /var/www/html/sites/all/modules/islandora/islandora_datastream_replace ${Apache} git checkout -b digital_grinnell docker exec -w /var/www/html/sites/default ${Apache} drush en islandora_datastream_exporter islandora_datastream_replace -y docker exec -w /var/www/html/sites/default ${Apache} drush cc drush -y Step 1c - Mounting //STORAGE to DGDocker1 Attention! This step, and some that come later, will require that the network storage path //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1 be accessible to our production instance of Digital.Grinnell. To make that possible I had to run this sequence on DGDocker1:\ndocker exec -it isle-apache-dg bash mount -t cifs -o username=mcfatem /storage.grinnell.edu/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1 /mnt/metadata-review /mnt/metadata-review\nStep 1d - Using Drush islandora_datastream_export Unfortunately, the islandora_datastream_export results in my local test were woefully incomplete\u0026hellip; NONE of the child objects with a compound parent were exported. I\u0026rsquo;m still not entirely sure why child obejcts were omitted since the query I used should have captured all objects. In testing I did find that this seems to be a flaw in the islandora_datastream_export command, and specifically in its implementation of any Solr query.\nFortunately, the aforementioned command also has a SPARQL query option, and after some trial-and-error I got it to work properly. To do so I created an export.sh bash script, shown below, and used it on dgdocker1.grinnell.edu like so:\ndocker exec -it isle-apache-dg bash source export.sh The export.sh script is:\nApache=isle-apache-dg Target=/utility-scripts # wget https://gist.github.com/McFateM/5bd7e5b0fa5d2928b2799d039a4c0fab/raw/collections.list while read collection do cp -f ri-query.txt query.sparql sed -i \u0026#39;s|COLLECTION|\u0026#39;${collection}\u0026#39;|g\u0026#39; query.sparql docker cp query.sparql ${Apache}:${Target}/${collection}.sparql rm -f query.sparql q=${Target}/${collection}.sparql echo Processing collection \u0026#39;${collection}\u0026#39;; Query is \u0026#39;${q}\u0026#39;... docker exec -w ${Target} ${Apache} mkdir -p /mnt/metadata-review/${collection} docker exec -w /var/www/html/sites/default/ ${Apache} drush -u 1 islandora_datastream_export --export_target=/mnt/metadata-review/${collection} --query=${q} --query_type=islandora_datastream_exporter_ri_query --dsid=MODS done \u0026lt; collections.list In the case of the Digital Grinnell social-justice collection, for example, this script produced 32 .xml files, the correct number. Each collection\u0026rsquo;s set of exported .xml files can be found in the collection-specific subdirectory of //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/ and all have filenames of the form: grinnell_\u0026lt;PID\u0026gt;_MODS.xml. Note that objects which have no MODS datastream were not exported.\nStep 2 - Map-MODS-to-MASTER Python 3 Script The Map-MODS-to-MASTER script was developed, in Python 3, on iMac MA8660 at ~/GitHub/Map-MODS-to-MASTER to facilitate generation of mods.tsv and accompanying .log files for each Digital Grinnell collection from the .xml files found in subdirectories of //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/.\nThe Map-MODS-to-MASTER project can be found in the master branch of https://github.com/DigitalGrinnell/Map-MODS-to-MASTER. I choose to execute it using PyCharm from iMac MA8660 since the directory holding all of the .xml files and folders is already mapped to /Volumes/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1 on that iMac. Note that this //STORAGE location was choosen because the ./ALLSTAFF directory, and its subordinates, are accessible to all staff in the Grinnell College Libraries.\nIt should not be necessary to run this script ever again\u0026hellip;NEVER. However, if it becomes necessary to look back at this code and process, details can be found in Map-MODS-to-MASTER. Note: If it should ever become necessary to repeat the Map-MODS-to-MASTER process it might be wise to look at replacing the Python 3 script with a new Drush command, maybe islandora_map_mods_to_master, written in PHP and installed directly into the production instance of Digital.Grinnell.\nStep 3 - Editing the MODS .tsv Files Please refer to Refer to Exporting, Editing, \u0026amp; Replacing MODS Datastreams, post 069 in my blog, for details and guidance.\nStep 4 - Run drush islandora_mods_via_twig As each individual collection mods-imvt.tsv file is made ready-for-update, it will be necessary to run a drush islandora_mods_via_twig command to process the .tsv data. Running --help with that command produces:\n[islandora@dgdocker1 ~]$ docker exec -it isle-apache-dg bash root@122092fe8182:/# cd /var/www/html/sites/default/ root@122092fe8182:/var/www/html/sites/default# drush -u 1 islandora_mods_via_twig --help Generate MODS .xml files from the mods-imvt.tsv file for a specified collection. Examples: drush -u 1 islandora_mods_via_twig social-justice Process ../social-justice/mods-imvt.tsv, for example. Arguments: collection The name of the collection to be processed. Defaults to \u0026#34;social-justice\u0026#34;. Aliases: imvt So, my command sequence to run islandora_mods_via_twig for the \u0026ldquo;Social Justice\u0026rdquo; collection, as an example, was:\n[islandora@dgdocker1 ~]$ docker exec -it isle-apache-dg bash root@122092fe8182:/# cd /var/www/html/sites/default/ root@122092fe8182:/var/www/html/sites/default# drush -u 1 islandora_mods_via_twig social-justice When the islandora_mods_via_twig command is run, it processes the corresponding //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/\u0026lt;collection-PID\u0026gt;/mods-imvt.tsv file and creates one //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/\u0026lt;collection-PID\u0026gt;/ready-for-datastream-replace/grinnell_\u0026lt;PID\u0026gt;_MODS.xml file for each object.\nStep 5 - Run drush islandora_datastream_replace The whole point of this entire process is to get us back to this point with a set of reviewed and modified .xml files in a //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/\u0026lt;collection-PID\u0026gt;/ready-for-datastream-replace/ collection-specific subdirectory so that we can replace existing object MODS datastreams with new data, and we use the drush islandora_datastream_replace command to do this.\nRunning --help for the aformentioned command produced this:\nroot@122092fe8182:/var/www/html/sites/default# drush -u 1 islandora_datastream_replace --help Replaces a datastream in all objects given a file list in a directory. Examples: drush -u 1 islandora_datastream_replace --source=/mnt/metadata-review/social-justice/ready-for-datastream-replace --dsid=MODS --namespace=grinnell Replacing MODS datastream for objects in --source using the digital_grinnell branch of code. Options: --dsid The datastream id of the datastream. Required. --namespace The namespace of the pids. Required. --source The directory to get the datastreams and pid# from. Required. Aliases: idre It\u0026rsquo;s worth noting that this command looks for any files named MODS in whatever ABSOLUTE directory is named with the --source parameter. The command shown below was executed inside the Apache container, isle-apache-dg, on node DGDocker1, in order to process Digital Grinnell\u0026rsquo;s social-justice collection.\nroot@122092fe8182:drush -u 1 islandora_datastream_replace --source=/mnt/metadata-review/social-justice/ready-for-datastream-replace --dsid=MODS --namespace=grinnell The same command could have been executed directly from node DGDocker1 like so:\ndocker exec isle-apache-dg drush -u 1 -w /var/www/html/sites/default drush -u 1 islandora_datastream_replace --source=mnt/metadata-review/social-justice/ready-for-datastream-replace --dsid=MODS --namespace=grinnell Step 6 - Reduce Duplicates As mentioned in the summary above, this is an optional but recommended follow-up drush command described in Islandora MODS Post Processing.\nStep 7 - Analyze and Restore \u0026ldquo;remainder\u0026rdquo; Fields As mentioned in the summary above, this step is designed to analyze all of the *.remainder files left behind by workflow Step 2 for objects in a given collection, and provide commands to restore legitimate elements after review. Follow instructions found in the README.md file at my reduce-MODS-remainders repository.\nAnd that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/115-exporting-editing-replacing-mods-datastreams-updated-technical-details/","tags":["MODS","Export","Replace","Twig","Islandora Multi-Importer","islandora_datastream_export","islandora_datastream_replace","islandora_mods_via_twig","Map-MODS-to-MASTER","islandora_mods_post_processing","reduce-MODS-remainders"],"title":"Exporting, Editing \u0026 Replacing MODS Datastreams: Updated Technical Details"},{"categories":null,"contents":"Digital.Grinnell relies on two different metadata XSL \u0026ldquo;transforms\u0026rdquo; to convert a cataloger\u0026rsquo;s MODS descriptive data into a modified MODS record and a corresponding Dublin Core record.\nSelf-Transforms The first transform type can be thought of as a \u0026ldquo;self-transform\u0026rdquo; because it accepts a MODS input and produces a modified MODS output; there is no change in schema, just changes in the data and its order.\nMODS-to-DC Transforms All other transforms relevant to this document are \u0026ldquo;MODS-to-DC\u0026rdquo; transforms. They accept a valid MODS record and output a corresponding, valid record under the DC schema.\nMetadata Transform Operations Metadata transformations in Digital.Grinnell take place in three possible scenarios:\nWhen a new object is created or an existing object is modified using one of DG\u0026rsquo;s input \u0026ldquo;Forms\u0026rdquo;. When one or more new or existing objects are processed in-bulk using IMI, the Islandora Multi-Importer module. When the system admin invokes an IDU, or Islandora Drush Utilities, action like SelfTransform. DG\u0026rsquo;s Available Transforms A recent survey of DG\u0026rsquo;s staging instance found the following transforms, or .xsl files, relevant to MODS and DC records. The first list includes relevant .xsl files from the the ./sites/all/modules/islandora/ path. The second list includes relevant .xsl files from all other paths.\nTransforms From ./sites/all/modules/islandora/ Path Purpose ./islandora_importer/xsl/mods_to_dc.xsl Unknown. ./islandora_batch/transforms/mods_to_dc.xsl Unknown. ./islandora_mods_display/xsl/mods_display.xsl See MODS display module below. ./islandora_mods_display/xsl/mods_display_compound_parent.xsl See MODS Display Module below. ./islandora_oai/transforms/mods_to_dc_oai.xsl Unknown. Part of Islandora\u0026rsquo;s OAI export module. ./islandora/xml/strip_newlines_and_whitespace.xsl Unknown. ./islandora/xml/transforms/mods_to_dc.xsl Apparently, this is the default MODS-to-DC transform shipped with Islandora\u0026rsquo;s core module? ./islandora_multi_importer/xslt/mods_to_dc.xsl See IMI Transforms below. ./islandora_multi_importer/xslt/islandora_cleanup_mods_extended_strict.xsl See IMI Transforms below. ./islandora_xml_forms/builder/transforms/mods_to_dc.xsl See Forms Builder below. ./islandora_xml_forms/builder/self_transforms/islandora_cleanup_mods_extended.xsl See Forms Builder below. ./islandora_xml_forms/builder/self_transforms/cleanup_mods.xsl See Forms Builder below. ./islandora_xml_forms/builder/self_transforms/islandora_cleanup_mods_extended_strict.xsl See Forms Builder below. ./islandora_xml_forms/tests/islandora_solution_pack_test/xsl/self_transform.xsl Unknown. Part of the \u0026ldquo;test\u0026rdquo; soloution pack. ./islandora_xml_forms/tests/islandora_solution_pack_test/xsl/mods_to_dc_custom.xsl Unknown. Part of the \u0026ldquo;test\u0026rdquo; soloution pack. ./dg7/xslt/cleanup_mods_and_reorder.xsl See DG7 Custom Transforms below. ./dg7/xslt/mods_to_dc_grinnell.xsl See DG7 Custom Transforms below. Transforms From All Other Paths Path Purpose ./sites/default/files/cleanup_mods.xsl public:// files. See Custom public:// Files below. ./sites/default/files/reorder_mods.xsl public:// files. See Custom public:// Files below. MODS Display Module Digital.Grinnell uses a DG-specific fork of the Islandora MODS Display module to display metadata on individual object pages like the sample shown here.\nFigure 1 \u0026middot; Sample MODS Metadata Display from grinnell:11451 The two transforms listed as part of this module are for display only. These transforms are engaged \u0026ldquo;on-demand\u0026rdquo; when MODS metadata is to be displayed; the output from these transforms is never saved. They transform an object\u0026rsquo;s stored MODS datastream into a display like that shown in the same above.\nThe ./islandora_mods_display/xsl/mods_display.xsl is a customized DG-specific copy of a default transform provided by the module. In most cases this is the only transform that exists as part of the module; however, in Digital.Grinnell we have introduced a mechanism that treats compound objects a little differtently than all others. That\u0026rsquo;s where the module customization and the ./islandora_mods_display/xsl/mods_display_compound_parent.xsl transform come into play.\nWhen DG\u0026rsquo;s custom islandora_mods_display module encounters an object which is a \u0026ldquo;child\u0026rdquo; of a compound \u0026ldquo;parent\u0026rdquo; object, it engages both transforms to remove most data which is \u0026ldquo;redundant\u0026rdquo; between the \u0026ldquo;parent\u0026rdquo; and its \u0026ldquo;child\u0026rdquo;. The display is split into two sections:\nThe top section shows data specific to the child*, and The bottom section appears below a Group Record sub-heading and shows data that is specific to the parent*, or common to both the parent and child. *The Creator and Title elements of BOTH the parent and child are always shown in BOTH sections of the display.\nGroup Record Sub-heading Not Displayed During evaluation of some objects I found that some compound parent objects were not displaying the Group Record sub-heading mentioned above. I devised a quick fix for such compound parents as a simple drush iduF command of the form:\ndrush -u 1 iduF grinnell:12423 AddXML --title=\u0026quot;mods:CModel\u0026quot; --xpath=\u0026quot;/mods:mods/mods:extension\u0026quot; --contents=\u0026quot;islandora:compoundCModel\u0026quot; --dsid=MODS\nThat particular command produced this output\u0026hellip;\nroot@b15318351296:/var/www/html/sites/default# drush -u 1 iduF grinnell:12423 AddXML --title=\u0026#34;mods:CModel\u0026#34; --xpath=\u0026#34;/mods:mods/mods:extension\u0026#34; --contents=\u0026#34;islandora:compoundCModel\u0026#34; --dsid=MODS Ok, iduF command \u0026#39;AddXML\u0026#39; was verified on 3-Dec-2021. [status] icu_drush_prep will consider only objects modified with a yyyy-mm-dd local time matching 2*. [status] Starting operation for PID \u0026#39;grinnell:12423\u0026#39; and --repeat=\u0026#39;0\u0026#39; at 12:20:06. [status] Fetching all valid object PIDs in the specified range. [status] Completed fetch of 1 valid object PIDs from Solr. [status] Progress: iduFix - AddXML icu_Connect: Connection to Fedora repository as \u0026#39;System Admin\u0026#39; is complete. [status] [==============================================================================================================================================================================================================] 100% Completed 1 \u0026#39;iduFix - AddXML\u0026#39; operations at 12:20:16. [status] Forms Builder In this context the \u0026ldquo;forms builder\u0026rdquo; refers to the Islandora XML Forms module. I hate to say it, but this module is an admin nightmare and always has been. I\u0026rsquo;ve found the forms builder difficult to use and impossible to master, there are just too many undocumented or poorly-documented \u0026ldquo;features\u0026rdquo;. Forms are made to be customized but they live in the Drupal database where there\u0026rsquo;s no version control. Even worse, the user interface provided to associate transforms with a form only makes available those transforms that reside within the module at ./islandora_xml_forms/builder/transforms/ and ./islandora_xml_forms/builder/self_transforms/. The effect is a module that\u0026rsquo;s intended to be customized, is painful to manage, and with no reasonable means of enforcing version control for necessary customizations!\nThe transforms associated with the forms builder above are ./islandora_xml_forms/builder/transforms/mods_to_dc.xsl, ./islandora_xml_forms/builder/self_transforms/islandora_cleanup_mods_extended.xsl, ./islandora_xml_forms/builder/self_transforms/cleanup_mods.xsl, and ./islandora_xml_forms/builder/self_transforms/islandora_cleanup_mods_extended_strict.xsl are all un-customized \u0026ldquo;default\u0026rdquo; forms that ship with the Islandora XML Forms module.\nIt is my belief that the two \u0026ldquo;custom\u0026rdquo; transforms currently found in the ./dg7/xslt/ directory, namely cleanup_mods_and_reorder.xsl and mods_to_dc_grinnell.xsl, should be copied to ./islandora_xml_forms/builder/self_transforms/cleanup_mods_and_reorder.xsl and ./islandora_xml_forms/builder/transforms/mods_to_dc_grinnell.xsl, respectively. Once these have been copied into the islandora_xml_forms module path they should be associated with Digital.Grinnell\u0026rsquo;s latest XML form for all content model types.\nI Was Wrong, Again A short time ago I put my hypothesis (see the annotation just above) to the test. Specifically, I applied the two XSL transforms from ./dg7/xslt/ to a new XML form and ingested a new test object. The results in terms of MODS were NOT good. In order to capture the history of this testing process I also created a new GitHub repo, https://https://github.com/Digital-Grinnell/mods-reordering-notes-mystery.git.\ngrinnell:20259 and test:22591, test:22592, etc. The subjects of this section are grinnell:20259, an object that is missing one of the MODS note fields that an editor specified, test:22591, and test:22592, new copies of that same object ingested and subsequently modified for testing purposes using a series of operations.\ngrinnell:20259 was one of many objects found to be \u0026ldquo;missing\u0026rdquo; a MODS note field that was originally input by an object editor but later disappeared from the object\u0026rsquo;s MODS display.\nHistory of grinnell:20259 I found evidence of changes made to grinnell:20259 in /mnt/metadata-review/phpp-dcl/ where there are a number of files named grinnell_20259_MODS... as you can see in this directory listing from dgdocker1:\n[root@dgdocker1 phpp-dcl]# ls -alh grinnell_20259* -rwxr-xr-x. 1 root root 3.4K Apr 14 2020 grinnell_20259_MODS.log -rwxr-xr-x. 1 root root 291 Apr 14 2020 grinnell_20259_MODS.remainder -rwxr-xr-x. 1 root root 2.7K Apr 8 2020 grinnell_20259_MODS.xml The grinnell_20259_MODS.log file pinpoints why the notes element went missing\u0026hellip;\n[root@dgdocker1 phpp-dcl]# cat grinnell_20259_MODS.log Object PID: grinnell:20259 14-Apr-2020 16:55 Warning: Unexpected structure detected in the data. The element could not be processed. Unexpected Element: {\u0026#34;primarySort\u0026#34;: \u0026#34; ==\u0026gt; Primary_Sort\u0026#34;, \u0026#34;dg_importIndex\u0026#34;: \u0026#34; ==\u0026gt; Import_Index\u0026#34;} Warning: Unexpected structure detected in the data. The element could not be processed. Unexpected Element: \u0026#34;290 of 592 slides from the Imagine Grinnell 2000 collection have been added to the Poweshiek History Preservation Project. A physical copy and TIFF images of all the slides can be found at Drake Community Library Archives in Grinnell, Iowa.\u0026#34; Warning: Unexpected structure detected in the data. The element could not be processed. Unexpected Element: \u0026#34;290 of 592 slides from the Imagine Grinnell 2000 collection have been added to the Poweshiek History Preservation Project. A physical copy and TIFF images of all the slides can be found at Drake Community Library Archives in Grinnell, Iowa.\u0026#34; Warning: Unexpected structure detected in the data. The element could not be processed. Unexpected Element: {\u0026#34;dateCreated\u0026#34;: \u0026#34; ==\u0026gt; Index_Date\u0026#34;, \u0026#34;dateIssued\u0026#34;: \u0026#34; ==\u0026gt; Date_Issued\u0026#34;, \u0026#34;publisher\u0026#34;: \u0026#34; ==\u0026gt; Publisher\u0026#34;} Warning: Unexpected structure detected in the data. The element could not be processed. Unexpected Element: {\u0026#34;digitalOrigin\u0026#34;: \u0026#34; ==\u0026gt; Digital_Origin\u0026#34;, \u0026#34;extent\u0026#34;: \u0026#34; ==\u0026gt; Extent\u0026#34;, \u0026#34;internetMediaType\u0026#34;: \u0026#34; ==\u0026gt; MIME_Type\u0026#34;} Warning: Unexpected structure detected in the data. The element could not be processed. Unexpected Element: {\u0026#34;@authority\u0026#34;: \u0026#34;lcsh\u0026#34;, \u0026#34;geographic\u0026#34;: \u0026#34; ==\u0026gt; Subjects_Geographic\u0026#34;} Warning: Unexpected structure detected in the data. The element could not be processed. Unexpected Element: {\u0026#34;@authority\u0026#34;: \u0026#34;lcsh\u0026#34;, \u0026#34;geographic\u0026#34;: \u0026#34; ==\u0026gt; Subjects_Geographic\u0026#34;} Remaining elements are: { \u0026#34;mods\u0026#34;: { \u0026#34;@xmlns\u0026#34;: \u0026#34;http://www.loc.gov/mods/v3\u0026#34;, \u0026#34;@xmlns:mods\u0026#34;: \u0026#34;http://www.loc.gov/mods/v3\u0026#34;, \u0026#34;@xmlns:xlink\u0026#34;: \u0026#34;http://www.w3.org/1999/xlink\u0026#34;, \u0026#34;@xmlns:xsi\u0026#34;: \u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;, \u0026#34;abstract\u0026#34;: \u0026#34; ==\u0026gt; Abstract\u0026#34;, \u0026#34;accessCondition\u0026#34;: \u0026#34; ==\u0026gt; Access_Condition\u0026#34;, \u0026#34;extension\u0026#34;: { \u0026#34;dg_importIndex\u0026#34;: \u0026#34; ==\u0026gt; Import_Index\u0026#34;, \u0026#34;primarySort\u0026#34;: \u0026#34; ==\u0026gt; Primary_Sort\u0026#34; }, \u0026#34;genre\u0026#34;: \u0026#34; ==\u0026gt; Genre~AuthorityURI\u0026#34;, \u0026#34;identifier\u0026#34;: [ \u0026#34; ==\u0026gt; Local_Identifier\u0026#34;, \u0026#34; ==\u0026gt; Handle\u0026#34; ], \u0026#34;language\u0026#34;: { \u0026#34;languageTerm\u0026#34;: \u0026#34; ==\u0026gt; Language_Names~Codes\u0026#34; }, \u0026#34;name\u0026#34;: \u0026#34; ==\u0026gt; Corporate_Names~Roles\u0026#34;, \u0026#34;note\u0026#34;: [ \u0026#34; ==\u0026gt; Public_Notes~Types\u0026#34;, \u0026#34;290 of 592 slides from the Imagine Grinnell 2000 collection have been added to the Poweshiek History Preservation Project. A physical copy and TIFF images of all the slides can be found at Drake Community Library Archives in Grinnell, Iowa.\u0026#34; ], \u0026#34;originInfo\u0026#34;: { \u0026#34;dateCreated\u0026#34;: \u0026#34; ==\u0026gt; Index_Date\u0026#34;, \u0026#34;dateIssued\u0026#34;: \u0026#34; ==\u0026gt; Date_Issued\u0026#34;, \u0026#34;publisher\u0026#34;: \u0026#34; ==\u0026gt; Publisher\u0026#34; }, \u0026#34;physicalDescription\u0026#34;: { \u0026#34;digitalOrigin\u0026#34;: \u0026#34; ==\u0026gt; Digital_Origin\u0026#34;, \u0026#34;extent\u0026#34;: \u0026#34; ==\u0026gt; Extent\u0026#34;, \u0026#34;internetMediaType\u0026#34;: \u0026#34; ==\u0026gt; MIME_Type\u0026#34; }, \u0026#34;relatedItem\u0026#34;: [ \u0026#34; ==\u0026gt; Related_Items~Types\u0026#34;, \u0026#34; ==\u0026gt; Related_Items~Types\u0026#34;, \u0026#34; ==\u0026gt; Related_Items~Types\u0026#34; ], \u0026#34;subject\u0026#34;: [ \u0026#34; ==\u0026gt; LCSH_Subjects\u0026#34;, { \u0026#34;@authority\u0026#34;: \u0026#34;lcsh\u0026#34;, \u0026#34;geographic\u0026#34;: \u0026#34; ==\u0026gt; Subjects_Geographic\u0026#34; }, \u0026#34; ==\u0026gt; Keywords\u0026#34; ], \u0026#34;titleInfo\u0026#34;: \u0026#34; ==\u0026gt; Title\u0026#34;, \u0026#34;typeOfResource\u0026#34;: \u0026#34; ==\u0026gt; Type_of_Resource~AuthorityURI\u0026#34; } The presence of \u0026ldquo;leftover\u0026rdquo; data in the above log is an indication of a problem in the process.\n\u0026#34;note\u0026#34;: [ \u0026#34; ==\u0026gt; Public_Notes~Types\u0026#34;, \u0026#34;290 of 592 slides from the Imagine Grinnell 2000 collection have been added to the Poweshiek History Preservation Project. A physical copy and TIFF images of all the slides can be found at Drake Community Library Archives in Grinnell, Iowa.\u0026#34; ], It\u0026rsquo;s acceptable and expected that some \u0026ldquo;labels\u0026rdquo; will be leftover after the record is processed, but there should never be any \u0026ldquo;data\u0026rdquo; left behind. This is also reflected in the contents of grinnell_20259_MODS.remainder which shows:\n[root@dgdocker1 phpp-dcl]# cat grinnell_20259_MODS.remainder {\u0026#34;note\u0026#34;: [\u0026#34;290 of 592 slides from the Imagine Grinnell 2000 collection have been added to the Poweshiek History Preservation Project. A physical copy and TIFF images of all the slides can be found at Drake Community Library Archives in Grinnell, Iowa.\u0026#34;], \u0026#34;subject\u0026#34;: [{\u0026#34;@authority\u0026#34;: \u0026#34;lcsh\u0026#34; The outcomes noted above point to a deficiency in the logic of the Map-MODS-to-MASTER Python 3 script that was used to map MODS to a new mods.tsv file on April 14, 2020.\nTime for a\u0026hellip; break.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/114-digital.grinnell-transforms/","tags":["XSLT","XSL","transforms","mods","dc","Islandora MODS Display","Group Record"],"title":"Digital.Grinnell Transforms"},{"categories":null,"contents":"The Grinnell College Digital Library Application Developer\u0026rsquo;s Blog I\u0026rsquo;m getting really lazy in my old age, so what I\u0026rsquo;ve posted here is just the meager README.md file from my blog\u0026rsquo;s new GitHub repo.\nThis project, my Grinnell College Digital Library Application Developer\u0026rsquo;s blog, is no longer a Docker \u0026ldquo;Multi-Stage\u0026rdquo; build.\nGitHub Pages I successfully moved this blog to GitHub Pages in October 2021, after creating instances of it on DigitalOcean and Azure. GH Pages, specifically https://static.grinnell.edu/dlad-blog/ seems like the right home for it, finally.\nResources I\u0026rsquo;ve create a OneTab of resources that I used here: https://www.one-tab.com/page/Pm1eXBmxS8KOe7PjCt_DLg. Enjoy! And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/113-blog-migration-details/","tags":["blog","migration","Hugo","GitHub Pages"],"title":"Blog Migration Details"},{"categories":null,"contents":"Pertinent Resources This section simply tabluates the posts and documentation used to effect migration of all sites from the Grinnell College static host to GitHub Pages.\nResource Address Hugo: Host on GitHub https://gohugo.io/hosting-and-deployment/hosting-on-github/ GitHub: Getting started with GitHub Pages https://docs.github.com/en/pages/getting-started-with-github-pages GitHub: Creating a GitHub Pages site https://docs.github.com/en/pages/getting-started-with-github-pages/creating-a-github-pages-site GitHub: Managing a custom domain for your GitHub Pages site https://docs.github.com/en/pages/configuring-a-custom-domain-for-your-github-pages-site/managing-a-custom-domain-for-your-github-pages-site#configuring-a-subdomain Customizations Do NOT Set publishDir = \u0026quot;docs\u0026quot; Do NOT change the publishDir parameter in your configuration, if you even have one! The default public setting is correct.\ngh-pages.yml Host on GitHub directs us to create a new .github/workflows/gh-pages.yml file in each project. This file directs GitHub to build a Hugo site each time a triggering event, like a \u0026ldquo;push\u0026rdquo;, takes place.\nThe document specifies the following contents plus a few additions of my own:\nname: github pages on: push: branches: - main # Set a branch to deploy pull_request: jobs: deploy: runs-on: ubuntu-20.04 steps: - uses: actions/checkout@v2 with: submodules: true # Fetch Hugo themes (true OR recursive) fetch-depth: 0 # Fetch all history for .GitInfo and .Lastmod - uses: szenius/set-timezone@v1.0 # per https://github.com/marketplace/actions/set-timezone with: timezoneLinux: \u0026#34;America/Chicago\u0026#34; - name: Setup Hugo uses: peaceiris/actions-hugo@v2 with: hugo-version: \u0026#39;latest\u0026#39; extended: true - name: Build run: hugo --minify - name: Deploy uses: peaceiris/actions-gh-pages@v3 if: github.ref == \u0026#39;refs/heads/main\u0026#39; with: github_token: ${{ secrets.GITHUB_TOKEN }} publish_dir: ./public GitHub Pages Settings It\u0026rsquo;s not documented well, but important to note that this workflow will create a new gh-pages branch of your repo and the root of that branch is what you should publish! Pay attention to those settings in the figure below!\nTo complete the process of creating a GitHub Pages site you\u0026rsquo;ll need to visit your repository\u0026rsquo;s GitHub Pages Settings page and make selections like you see in the figure below. The example below is taken from https://github.com/Digital-Grinnell/Digital-Grinnell.github.io/settings/pages.\nFigure 1 \u0026middot; GH Pages Settings Completed Migrations CNAME Assigned New GH Address GitHub Repo Old Address https://static.grinnell.edu/ https://digital-grinnell.github.io/ https://github.com/Digital-Grinnell/Digital-Grinnell.github.io https://static.grinnell.edu https://static.grinnell.edu/dlad-blog/ https://digital-grinnell.github.io/dlad-blog/ https://github.com/Digital-Grinnell/dlad-blog https://static.grinnell.edu/blogs-McFateM And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/112-moving-static-sites-to-github-pages/","tags":["static","Hugo","GitHub Pages"],"title":"Moving Static Sites to GitHub Pages"},{"categories":null,"contents":"A short time ago I moved this blog from DigitalOcean to Azure, and along the way I discovered that my search feature wasn\u0026rsquo;t working properly. That old search mechanism used Fuse, which has NO dependencies, but that old scheme used a Hugo theme component that I found difficult to properly maintain. So, as this blog was moving to Azure I elected to try something a little different with Fuse and found this gist to help get it done.\nAs of this writing, the new search is limited to just finding tag references, and sometimes a search will return a 404 error because of a bad path reference. If you try to search and get the 404 error have a look at the returned URL and if it reads like .../search/search?search-query... then you\u0026rsquo;ve got one too many instances of the term search. Remove that first /search term, including the slash in front of it, and hit return. The search should now return valid results.\nEven if it appears that search is working, know that I\u0026rsquo;m still working on the feature and hope to make some improvements soon, like full-text search instead of just the tags or keyword search you now see.\nThat\u0026rsquo;s all\u0026hellip; for now.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/110-searching-my-blog-using-fuse/","tags":["static","search","Fuse","fuse.js"],"title":"Searching This Blog Using Fuse"},{"categories":null,"contents":" Attention: This is an updated copy of post 107 gleaned from the ISLE project\u0026rsquo;s update.md document. ONLY the headings from the original document and the annotations which are specific to Digital.Grinnell appear in this document!\nNote: This update procedure was NOT performed \u0026ldquo;locally\u0026rdquo;, as recommended, due to substantial errors encountered in my last attempt to update. Rather than diving down that rabbit hole, again, I elected to attempt this update on our staging server, DGDockerX on 2021-Sep-27.\nUpdate ISLE to the Latest Release Important Information # stop the docker service $ sudo service docker stop # download the latest docker binary and replace the current outdated docker # DEPRECATED WAY TO UPGRADE DOCKER: $ sudo wget https://get.docker.com/builds/Linux/x86_64/docker-latest -O /usr/bin/docker $ sudo yum update docker-engine # start the docker service $ sudo service docker start # check the version $ sudo docker version # check the images and containers $ sudo docker images $ sudo docker ps $ sudo docker ps -a Update Local (personal computer) As mentioned above, this work is being conducted in staging rather than local, on DGDockerX in the /home/islandora/ISLE directory and the dg-isle and dg-islandora directories there.\nMy new branch is named isle-update-v1.5.11.\nOutput from git pull follows.\n╭─islandora@dgdockerx ~/ISLE/dg-isle ‹isle-update-v1.5.11› ╰─$ git pull icg-upstream main From https://github.com/Islandora-Collaboration-Group/ISLE * branch main -\u0026gt; FETCH_HEAD Auto-merging docs/install/host-software-dependencies.md Auto-merging docker-compose.staging.yml Auto-merging docker-compose.production.yml Auto-merging docker-compose.local.yml Merge made by the \u0026#39;recursive\u0026#39; strategy. docker-compose.demo.yml | 14 +++++++------- docker-compose.local.yml | 16 ++++++++-------- docker-compose.production.yml | 16 ++++++++-------- docker-compose.staging.yml | 14 +++++++------- docker-compose.test.yml | 16 ++++++++-------- docs/cookbook-recipes/migrate-dbs-utf8.md | 6 +++--- docs/install/host-software-dependencies.md | 22 +++++++++++++++++++++- docs/install/install-same-server.md | 45 +++++++++++++++++++++++++++++++++++++++++++++ docs/release-notes/release-1-5-10.md | 64 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ docs/release-notes/release-1-5-11.md | 65 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ docs/release-notes/release-1-5-8.md | 70 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ docs/release-notes/release-1-5-9.md | 65 +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ mkdocs.yml | 7 ++++++- 13 files changed, 377 insertions(+), 43 deletions(-) create mode 100644 docs/install/install-same-server.md create mode 100644 docs/release-notes/release-1-5-10.md create mode 100644 docs/release-notes/release-1-5-11.md create mode 100644 docs/release-notes/release-1-5-8.md create mode 100644 docs/release-notes/release-1-5-9.md There were NO merge conflicts so ratom (see Remote Atom) was not needed.\nThere really should be an additional step here since you must edit, or at least check your .env file! I changed mine to read as follows:\nCOMPOSE_PROJECT_NAME=dgs BASE_DOMAIN=isle-stage.grinnell.edu CONTAINER_SHORT_ID=dgs COMPOSE_FILE=docker-compose.staging.yml With an active VPN connection, I am happy to report that the staging site is now working at https://isle-stage.grinnell.edu.\nPreliminary tests look good, and a search for \u0026ldquo;Ley\u0026rdquo; returns a very short list of objects that do exist in my DG-STAGING test repository.\nI wasn\u0026rsquo;t able to execute the sequence as presented above. Instead, I did this:\n╭─islandora@dgdockerx ~/ISLE/dg-isle ╰─$ git add . ╭─islandora@dgdockerx ~/ISLE/dg-isle ╰─$ git commit -m \u0026#34;Updated and working on DGDockerX\u0026#34; ╭─islandora@dgdockerx ~/ISLE/dg-isle ╰─$ git push origin isle-update-v1.5.11 Hold on, there\u0026rsquo;s one more significant part of the update process that\u0026rsquo;s not covered here, updating the Drupal and Islandora code that is not technically part of ISLE, but probably should be?\nMy process for updating that went something like this:\n╭─islandora@dgdockerx ~ ╰─$ cd ~/ISLE/dg-islandora ╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹main› ╰─$ git pull ╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹main› ╰─$ git checkout -b update-sept-27 ╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹update-sept-27*› ╰─$ docker exec -it isle-apache-dgs bash root@e97835142685:/# cd /var/www/html/sites/default/ root@e97835142685:/var/www/html/sites/default# rm -fr files/webform/* root@e97835142685:/var/www/html/sites/default# drush up The drush up command was invoked to pull in all of the latest updates to Drupal v7, and there were many, including an update to Drupal\u0026rsquo;s core, now at v7.82. Since the core got updated, I had to eventually revert changes to /var/www/html/.htaccess since those updates typically remove a line that ISLE requires to function properly. I confirmed that the changes made by drush up did indeed make their way into ~/ISLE/dg-islandora by checking git status. I managed to revert the changes to .htaccess using sudo git checkout -- .htaccess.\nAfter making those changes I found it necessary to restart the stack using:\n╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹update-sept-27*› ╰─$ cd ~/DG-STAGING ╭─islandora@dgdockerx ~/DG-STAGING ‹main*› ╰─$ ./RESTART.sh This time I did NOT have to run several of the ./Update-*.sh scripts in ~/DG-STAGING in order to get everything working properly. So, I pushed my dg-islandora changes to Github using:\n╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹update-sept-27*› ╰─$ git add --all . ╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹update-sept-27*› ╰─$ git commit -m \u0026#34;Updates from DGDockerX\u0026#34; [update-sept-27 e27a34c] Updates from DGDockerX 183 files changed, 954 insertions(+), 666 deletions(-) mode change 100644 =\u0026gt; 100755 scripts/drupal.sh mode change 100644 =\u0026gt; 100755 scripts/password-hash.sh mode change 100644 =\u0026gt; 100755 scripts/run-tests.sh rewrite sites/all/modules/contrib/views/README.txt (70%) ╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹update-sept-27› ╰─$ git pull origin main Username for \u0026#39;https://github.com\u0026#39;: Digital-Grinnell Password for \u0026#39;https://Digital-Grinnell@github.com\u0026#39;: xxxxxxxxxxxxxxx From https://github.com/Digital-Grinnell/dg-islandora * branch main -\u0026gt; FETCH_HEAD Already up-to-date. At this point I did another cd ~/DG-STAGING; ./RESTART.sh just to ensure things still work properly. Another ./Update-PERMISSIONS.sh was also needed.\nThen\u0026hellip;\n╭─islandora@dgdockerx ~/DG-STAGING ‹main› ╰─$ cd ~/ISLE/dg-islandora ╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹update-sept-27› ╰─$ git push origin update-sept-27 Username for \u0026#39;https://github.com\u0026#39;: Digital-Grinnell Password for \u0026#39;https://Digital-Grinnell@github.com\u0026#39;: xxxxxxxxxxxxxxx Counting objects: 558, done. Delta compression using up to 4 threads. Compressing objects: 100% (288/288), done. Writing objects: 100% (292/292), 35.30 KiB | 0 bytes/s, done. Total 292 (delta 253), reused 0 (delta 0) remote: Resolving deltas: 100% (253/253), completed with 227 local objects. remote: remote: Create a pull request for \u0026#39;update-sept-27\u0026#39; on GitHub by visiting: remote: https://github.com/Digital-Grinnell/dg-islandora/pull/new/update-sept-27 remote: To https://github.com/Digital-Grinnell/dg-islandora * [new branch] update-sept-27 -\u0026gt; update-sept-27 Update Staging Server Ok, at this point I\u0026rsquo;ve already got our staging server up-to-date, including the addition of my purge-tmp-files script successfully added to the Apache container\u0026rsquo;s /etc/cron.hourly directory, but the ownership on that script is islandora:islandora and it needs to be root:root. So, I need to change that from inside the isle-apache-dgs container like so: root@bc83fc8bb3c3:/etc/cron.hourly# chown root:root purge-tmp-files.\nMy next steps will be to backup the production database and /var/www/html/sites/default/files, merge all of the dg-isle and dg-islandora repository changes into their respective main branches, and repeat all of my implementation and testing in staging again. For production backup look at ~/ISLE/DEPLOY/dump-production.sh on DGDocker1 for guidance!\nI used the ~/ISLE/DG-STAGING/rsync-from-STORAGE.sh and ~/ISLE/DG-STAGING/RESTART.sh scripts on DGDockerX to restart the stack after capturing the production database and /var/www/html/sites/default/files contents. It worked nicely except for a warning about permissions on public://backup_migrate/manual. I reran the ~/ISLE/DG-STAGING/Update-PERMISSIONS.sh script and the warning went away.\nI subsequently performed a git add .; git commit -m \u0026quot;message\u0026quot;; git push origin update-sept-27 command sequence from both the ~/ISLE/dg-islandora and a similar sequence, but with branch isle-update-v1.5.11 in ~/ISLE/dg-isle repositories to commit all changes. I subsequently opened pull requests to merge both back to the corresponding main branches of their respective repositories.\nNext update will be to our production server, DGDocker1, where I hope to repeat this process to-the-letter.\nWorking Here !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! Add a note about making sure we hold on to, and use, the old acme.json file that\u0026rsquo;s in ~/ISLE/dg-isle/config/proxy/! Update Production Server When you are confident that your Staging installation is working as expected:\nRepeat the same above \u0026ldquo;Update Staging Server\u0026rdquo; process but do so on your Production server environment. Additional Resources Please post questions to the public Islandora ISLE Google group, or subscribe to receive emails.\nOne hiccup in the production update\u0026hellip; I encountered this error when trying to initiate an IMI import:\nRuntimeException: Unable to create the cache directory (/var/www/private/ed). in Twig\\Cache\\FilesystemCache-\u0026gt;write() (line 57 of /var/www/html/sites/all/modules/islandora/islandora_multi_importer/vendor/twig/twig/src/Cache/FilesystemCache.php). The problem appeared to be ownership of the /var/www/public directory in the Apache container. The public folder was owned by islandora:islandora so I opened a shell into Apache and did cd /var/www/; chown -R islandora:www-data public. That seems to have fixed the problem nicely.\nEnd of: Update ISLE to the Latest Release And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/111-updating-isle-v1.5.11/","tags":["ISLE","update","v1.5.11"],"title":"Updating Digital.Grinnell to ISLE v1.5.11"},{"categories":null,"contents":"On Thursday, September 9, 2021, I discovered that Azure, Microsoft\u0026rsquo;s offering of host/cloud services for Open Source developers, and a favored partner of Grinnell\u0026rsquo;s IT department, had come up with a Static Web App deployment scheme that allegedly would rival what I\u0026rsquo;ve been using at DigitalOcean. So, I first tried to migrate my very simple Static Landing Page Hugo static site to a new home on Azure. The process wasn\u0026rsquo;t quite as slick and easy as at DigitalOcean, but Azure does nicely leverage GitHub Actions, and the process wasn\u0026rsquo;t too difficult to grasp.\nAs of this writing I\u0026rsquo;m attempting to follow-up by moving my professional blog, formerly https://static.grinnell.edu/blogs-McFateM/, to a new Azure home.\nMoving my blog turned out to be just as easy as the landing page. The entire process took just an hour or so. Some cleanup of hard-coded links and similar data is still pending, but for the most part the experience has been almost as good as what I did at DigitalOcean.\nMy First Azure Account Encouraged by the ITS Department, I registered my first Azure account under my mcfatem@grinnell.edu email address on March 20, 2020. That account was registered under the college\u0026rsquo;s existing \u0026ldquo;home\u0026rdquo; organization named Grinnell College, also identified as GRINCO.ONMICROSOFT.COM. Because my first year of \u0026ldquo;free\u0026rdquo; services under that account had long expired, in September 2021 I elected to open a new account as indicated below.\nA New Azure Account and Portal Nothing of any consequence was ever created under my mcfatem@grinnell.edu Azure account, the services I needed were not available back in early 2020. So, that account still exists but has nothing of value in it at this time.\nAs mentioned above, on 2021-09-09, I created a new Azure account, also under the Grinnell College organization umbrella. This account is registered to my digital@grinnell.edu email address. Unlike my earlier account, this one has a Portal with some actual content.\nStatic Landing Page The sole occupant, so far, of my Azure Portal is named static-landing-page and clicking on its link and toggling on the JSON view (a handy feature) shows these details:\n{ \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/609af5e3-a5d8-4ff9-968f-6524767a4dbe/resourceGroups/Static.Grinnell.edu-Resources/providers/Microsoft.Web/staticSites/static-landing-page\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;static-landing-page\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Web/staticSites\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;Central US\u0026#34;, \u0026#34;tags\u0026#34;: {}, \u0026#34;properties\u0026#34;: { \u0026#34;defaultHostname\u0026#34;: \u0026#34;victorious-river-0bf860d10.azurestaticapps.net\u0026#34;, \u0026#34;repositoryUrl\u0026#34;: \u0026#34;https://github.com/Digital-Grinnell/static-landing-page\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;customDomains\u0026#34;: [], \u0026#34;privateEndpointConnections\u0026#34;: [], \u0026#34;stagingEnvironmentPolicy\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;allowConfigFileUpdates\u0026#34;: true, \u0026#34;contentDistributionEndpoint\u0026#34;: \u0026#34;https://content-dm1.infrastructure.azurestaticapps.net\u0026#34;, \u0026#34;keyVaultReferenceIdentity\u0026#34;: \u0026#34;SystemAssigned\u0026#34;, \u0026#34;userProvidedFunctionApps\u0026#34;: [], \u0026#34;provider\u0026#34;: \u0026#34;GitHub\u0026#34;, \u0026#34;enterpriseGradeCdnStatus\u0026#34;: \u0026#34;Disabled\u0026#34;, \u0026#34;publicNetworkAccess\u0026#34;: null }, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Free\u0026#34;, \u0026#34;tier\u0026#34;: \u0026#34;Free\u0026#34; } } As you can see above, this app responds to a canonical URL of https://victorious-river-0bf860d10.azurestaticapps.net and its built from a GitHub repo at https://github.com/Digital-Grinnell/static-landing-page. That is a private repository.\nYou may also note that when this JSON structure was dumped the site had NO \u0026ldquo;customDomains\u0026rdquo;. Before long I hope that ITS can alias our old https://static.grinnell.edu address to point here.\nThis Blog on Azure Next step, posting and hosting this blog in my new Azure account. As soon as that is done I\u0026rsquo;ll be back here to provide some details\u0026hellip;\nDone. So, here are the details, again in JSON format:\n{ \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/609af5e3-a5d8-4ff9-968f-6524767a4dbe/resourceGroups/Static.Grinnell.edu-Resources/providers/Microsoft.Web/staticSites/dlad-blog\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;dlad-blog\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Web/staticSites\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;Central US\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;defaultHostname\u0026#34;: \u0026#34;ashy-hill-086e62810.azurestaticapps.net\u0026#34;, \u0026#34;repositoryUrl\u0026#34;: \u0026#34;https://github.com/Digital-Grinnell/dlad-blog\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;customDomains\u0026#34;: [], \u0026#34;privateEndpointConnections\u0026#34;: [], \u0026#34;stagingEnvironmentPolicy\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;allowConfigFileUpdates\u0026#34;: true, \u0026#34;contentDistributionEndpoint\u0026#34;: \u0026#34;https://content-dm1.infrastructure.azurestaticapps.net\u0026#34;, \u0026#34;keyVaultReferenceIdentity\u0026#34;: \u0026#34;SystemAssigned\u0026#34;, \u0026#34;userProvidedFunctionApps\u0026#34;: [], \u0026#34;provider\u0026#34;: \u0026#34;GitHub\u0026#34;, \u0026#34;enterpriseGradeCdnStatus\u0026#34;: \u0026#34;Disabled\u0026#34;, \u0026#34;publicNetworkAccess\u0026#34;: null }, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Free\u0026#34;, \u0026#34;tier\u0026#34;: \u0026#34;Free\u0026#34; } } Please note the name and location of the GitHub repo this newest edition is built from: https://github.com/Digital-Grinnell/dlad-blog. This is also a private repository. The only problem I encountered when I moved my repo to this location was that all my posts inherited new commit dates, and all have the same commit date, so my automatic sorting by commit date (see the enableGitInfo = true setting in config.toml) went bonkers. I\u0026rsquo;ve since disabled that sort until I can get my repository commit history back to what it was before the move.\nImportant Notes! So, the deployment of my blog took a few iterations that do not all appear here. In my last deployment I found the key to making things work the first time, and I captured some screens along the way.\nFirst, in order to create a new Static Web App you have to \u0026ldquo;find\u0026rdquo; the right form. Use the search feature to do so, like this:\nFigure 1 \u0026middot; Specifying an Azure Static Web App Then, in the case of a Hugo static site, after you have identified your project repository you need to select Hugo from a list. Unlike DigitalOcean, this is by-no-means \u0026ldquo;automatic\u0026rdquo;, so be sure you set your Build Presets to \u0026ldquo;Hugo\u0026rdquo;, like so:\nFigure 2 \u0026middot; The Key to Hugo The other \u0026ldquo;App\u0026rdquo;, \u0026ldquo;API\u0026rdquo;, and \u0026ldquo;Output\u0026rdquo; fields can all be left to the defaults as shown above.\nVAF - Public Facing This is the public-facing, aka not-the-kiosk version of Visualizing Abolition and Freedom (VAF). Note that it is built from the public-facing branch of https://github.com/Digital-Grinnell/vaf!\n{ \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/609af5e3-a5d8-4ff9-968f-6524767a4dbe/resourceGroups/Static.Grinnell.edu-Resources/providers/Microsoft.Web/staticSites/vaf-public-facing\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;vaf-public-facing\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Web/staticSites\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;Central US\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;defaultHostname\u0026#34;: \u0026#34;victorious-field-065e3d110.azurestaticapps.net\u0026#34;, \u0026#34;repositoryUrl\u0026#34;: \u0026#34;https://github.com/Digital-Grinnell/vaf\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;public-facing\u0026#34;, \u0026#34;customDomains\u0026#34;: [], \u0026#34;privateEndpointConnections\u0026#34;: [], \u0026#34;stagingEnvironmentPolicy\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;allowConfigFileUpdates\u0026#34;: true, \u0026#34;contentDistributionEndpoint\u0026#34;: \u0026#34;https://content-dm1.infrastructure.azurestaticapps.net\u0026#34;, \u0026#34;keyVaultReferenceIdentity\u0026#34;: \u0026#34;SystemAssigned\u0026#34;, \u0026#34;userProvidedFunctionApps\u0026#34;: [], \u0026#34;provider\u0026#34;: \u0026#34;GitHub\u0026#34;, \u0026#34;enterpriseGradeCdnStatus\u0026#34;: \u0026#34;Disabled\u0026#34;, \u0026#34;publicNetworkAccess\u0026#34;: null }, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Free\u0026#34;, \u0026#34;tier\u0026#34;: \u0026#34;Free\u0026#34; } } VAF - Kiosk Version This is the kiosk version of Visualizing Abolition and Freedom (VAF), not the public-facing internet web site. Note that it is built from the main branch of https://github.com/Digital-Grinnell/vaf!\n{ \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/609af5e3-a5d8-4ff9-968f-6524767a4dbe/resourceGroups/Static.Grinnell.edu-Resources/providers/Microsoft.Web/staticSites/vaf-kiosk\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;vaf-kiosk\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Web/staticSites\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;Central US\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;defaultHostname\u0026#34;: \u0026#34;green-beach-045641410.azurestaticapps.net\u0026#34;, \u0026#34;repositoryUrl\u0026#34;: \u0026#34;https://github.com/Digital-Grinnell/vaf\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;customDomains\u0026#34;: [], \u0026#34;privateEndpointConnections\u0026#34;: [], \u0026#34;stagingEnvironmentPolicy\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;allowConfigFileUpdates\u0026#34;: true, \u0026#34;contentDistributionEndpoint\u0026#34;: \u0026#34;https://content-dm1.infrastructure.azurestaticapps.net\u0026#34;, \u0026#34;keyVaultReferenceIdentity\u0026#34;: \u0026#34;SystemAssigned\u0026#34;, \u0026#34;userProvidedFunctionApps\u0026#34;: [], \u0026#34;provider\u0026#34;: \u0026#34;GitHub\u0026#34;, \u0026#34;enterpriseGradeCdnStatus\u0026#34;: \u0026#34;Disabled\u0026#34;, \u0026#34;publicNetworkAccess\u0026#34;: null }, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Free\u0026#34;, \u0026#34;tier\u0026#34;: \u0026#34;Free\u0026#34; } } Rootstalk Last, but not least, there\u0026rsquo;s Rootstalk! Note that it is built from the main branch of a new GitHub repo at https://github.com/Digital-Grinnell/rootstalk! Note that it should be given a .grinnell.edu address of rootstalk-DEV for now\u0026hellip; I\u0026rsquo;m not quite ready to make it the public-facing edition of the digital magazine just yet.\n{ \u0026#34;id\u0026#34;: \u0026#34;/subscriptions/609af5e3-a5d8-4ff9-968f-6524767a4dbe/resourceGroups/Static.Grinnell.edu-Resources/providers/Microsoft.Web/staticSites/rootstalk\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;rootstalk\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;Microsoft.Web/staticSites\u0026#34;, \u0026#34;location\u0026#34;: \u0026#34;Central US\u0026#34;, \u0026#34;properties\u0026#34;: { \u0026#34;defaultHostname\u0026#34;: \u0026#34;icy-tree-020380010.azurestaticapps.net\u0026#34;, \u0026#34;repositoryUrl\u0026#34;: \u0026#34;https://github.com/Digital-Grinnell/rootstalk\u0026#34;, \u0026#34;branch\u0026#34;: \u0026#34;main\u0026#34;, \u0026#34;customDomains\u0026#34;: [], \u0026#34;privateEndpointConnections\u0026#34;: [], \u0026#34;stagingEnvironmentPolicy\u0026#34;: \u0026#34;Enabled\u0026#34;, \u0026#34;allowConfigFileUpdates\u0026#34;: true, \u0026#34;contentDistributionEndpoint\u0026#34;: \u0026#34;https://content-dm1.infrastructure.azurestaticapps.net\u0026#34;, \u0026#34;keyVaultReferenceIdentity\u0026#34;: \u0026#34;SystemAssigned\u0026#34;, \u0026#34;userProvidedFunctionApps\u0026#34;: [], \u0026#34;provider\u0026#34;: \u0026#34;GitHub\u0026#34;, \u0026#34;enterpriseGradeCdnStatus\u0026#34;: \u0026#34;Disabled\u0026#34;, \u0026#34;publicNetworkAccess\u0026#34;: null }, \u0026#34;sku\u0026#34;: { \u0026#34;name\u0026#34;: \u0026#34;Free\u0026#34;, \u0026#34;tier\u0026#34;: \u0026#34;Free\u0026#34; } } Summary of Aliases This table from https://gist.github.com/Digital-Grinnell/707c5c30b2a046e638b1e645a796838d#gistcomment-3893281 might come in handy\u0026hellip;\nSite\u0026rsquo;s Azure Address .grinnell.edu Alias https://victorious-field-065e3d110.azurestaticapps.net vaf.grinnell.edu https://green-beach-045641410.azurestaticapps.net vaf-kiosk.grinnell.edu https://ashy-hill-086e62810.azurestaticapps.net dlad-blog.grinnell.edu https://victorious-river-0bf860d10.azurestaticapps.net static.grinnell.edu https://icy-tree-020380010.azurestaticapps.net rootstalk-DEV.grinnell.edu That\u0026rsquo;s all\u0026hellip; for now.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/109-moving-static-sites-to-azure/","tags":["static","Azure"],"title":"Moving Static Sites to Azure"},{"categories":null,"contents":"On Thursday, August 26, 2021, updates to a new iPad destined for the VAF (Visualizing Abolition and Freedom) installing in the Grinnell\u0026rsquo;s HSSC were completed. As of this writing the iPad has not been re-installed, but Facilities Management has been contacted to schedule that event soon.\nNew DigitalOcean Deployment Due to small differences between the aspect ratio and resolution of the old versus new devices, the kiosk site had to be re-designed. Changes were also necessary to help ensure that users of the kiosk could not \u0026ldquo;escape\u0026rdquo; from the VAF screens and cause havoc by surfing the internet. As a result, a new private https://github.com/Digital-Grinnell/vafvaf-kiosk repository was created and eventually deployed for kiosk use ONLY via DigitalOcean\u0026rsquo;s App platform. The new kiosk site is deployed to https://vaf-kiosk-2021-xjmpc.ondigitalocean.app/.\nNote that the README.md document in the new private repo is woefully outdated.\nNew Analytics It should also be noted that the new deployment includes an updated Google Analytics v4 tracking code, and that analytics are available, with valid login credentials, at https://analytics.google.com/analytics/web/?authuser=1#/p284309057/reports/reportinghub.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/108-updates-to-vaf-kiosk/","tags":["VAF","kiosk","update","Google Analytics","DigitalOcean"],"title":"Updates to VAF-Kiosk"},{"categories":null,"contents":" Attention: This is an annotated copy of the ISLE project\u0026rsquo;s update.md document. Annotations specific to Digital.Grinnell appear in specially formatted blocks like this one\nThere was one universal change made here in the original text, specifically all references to master branches have been changed to main to correlate with updated names of the repositories involved in this process.\nNote: This update procedure was first performed \u0026ldquo;locally\u0026rdquo;, as recommended on 2021-June-22 when I attempted it on my Grinnell College MacBook Pro, MA10713, serial number C02FK0XXQ05Q. **The process failed miserably due to numerous errors, ultimately including\u0026hellip;\nWARNING: Module xdebug ini file doesn\u0026#39;t exist under /etc/php/7.1/mods-available AH00558: apache2: Could not reliably determine the server\u0026#39;s fully qualified domain name, using 172.22.0.7. Set the \u0026#39;ServerName\u0026#39; directive globally to suppress this message [Wed Jun 23 00:39:38.868649 2021] [core:emerg] [pid 72604] (95)Operation not supported: AH00023: Couldn\u0026#39;t create the proxy mutex [Wed Jun 23 00:39:38.870388 2021] [proxy:crit] [pid 72604] (95)Operation not supported: AH02478: failed to create proxy mutex AH00016: Configuration Failed Action \u0026#39;-D FOREGROUND\u0026#39; failed. The images-ld container also crashed with lots of Java trash, as Java is apt to do:\n[s6-init] making user provided files available at /var/run/s6/etc...exited 0. [s6-init] ensuring user provided files have correct perms...exited 0. [fix-attrs.d] applying ownership \u0026amp; permissions fixes... [fix-attrs.d] 01-tomcat: applying... [fix-attrs.d] 01-tomcat: exited 1. [fix-attrs.d] 11-cantaloupe: applying... [fix-attrs.d] 11-cantaloupe: exited 0. [fix-attrs.d] done. [cont-init.d] executing container initialization scripts... [cont-init.d] 01-cantaloupe-config: executing... 2021-06-23T00:25:09Z b0db6274e175 /usr/local/bin/confd[2461]: INFO Backend set to env 2021-06-23T00:25:09Z b0db6274e175 /usr/local/bin/confd[2461]: INFO Starting confd 2021-06-23T00:25:09Z b0db6274e175 /usr/local/bin/confd[2461]: INFO Backend source(s) set to 2021-06-23T00:25:09Z b0db6274e175 /usr/local/bin/confd[2461]: INFO /usr/local/cantaloupe/cantaloupe.properties has md5sum 1a486ec62cc9c046b9d61a4d3972cc91 should be 9f84eb40bf531b1714cc616f20e60341 2021-06-23T00:25:09Z b0db6274e175 /usr/local/bin/confd[2461]: INFO Target config /usr/local/cantaloupe/cantaloupe.properties out of sync 2021-06-23T00:25:09Z b0db6274e175 /usr/local/bin/confd[2461]: INFO Target config /usr/local/cantaloupe/cantaloupe.properties has been updated 2021-06-23T00:25:09Z b0db6274e175 /usr/local/bin/confd[2461]: INFO /usr/local/tomcat/conf/logging.properties has mode -rwxr-xr-x should be -rw-r--r-- 2021-06-23T00:25:09Z b0db6274e175 /usr/local/bin/confd[2461]: INFO /usr/local/tomcat/conf/logging.properties has md5sum 8edf0889dd7a263984094de9bea3770b should be 943111bb86645471250c7b35ed695ab0 2021-06-23T00:25:09Z b0db6274e175 /usr/local/bin/confd[2461]: INFO Target config /usr/local/tomcat/conf/logging.properties out of sync 2021-06-23T00:25:09Z b0db6274e175 /usr/local/bin/confd[2461]: INFO Target config /usr/local/tomcat/conf/logging.properties has been updated 2021-06-23T00:25:09Z b0db6274e175 /usr/local/bin/confd[2461]: INFO /usr/local/tomcat/conf/tomcat-users.xml has md5sum d04005f593cfc6db810a79766bbf7917 should be 64b2ab19d2455cfd3a36fa5d374baed3 2021-06-23T00:25:09Z b0db6274e175 /usr/local/bin/confd[2461]: INFO Target config /usr/local/tomcat/conf/tomcat-users.xml out of sync 2021-06-23T00:25:09Z b0db6274e175 /usr/local/bin/confd[2461]: INFO Target config /usr/local/tomcat/conf/tomcat-users.xml has been updated [cont-init.d] 01-cantaloupe-config: exited 0. [cont-init.d] done. [services.d] starting services Starting Tomcat Privileged. [services.d] done. # # A fatal error has been detected by the Java Runtime Environment: # # SIGILL (0x4) at pc=0x000000400ba12b21, pid=2588, tid=0x00000040ea42c700 # # JRE version: OpenJDK Runtime Environment (8.0_292-b10) (build 1.8.0_292-b10) # Java VM: OpenJDK 64-Bit Server VM (25.292-b10 mixed mode linux-amd64 compressed oops) # Problematic frame: # J 1791 C1 org.apache.catalina.startup.ContextConfig.processAnnotationsStream(Ljava/io/InputStream;Lorg/apache/tomcat/util/descriptor/web/WebXml;ZLjava/util/Map;)V (38 bytes) @ 0x000000400ba12b21 [0x000000400ba12ac0+0x61] # # Failed to write core dump. Core dumps have been disabled. To enable core dumping, try \u0026#34;ulimit -c unlimited\u0026#34; before starting Java again # # An error report file with more information is saved as: # /run/s6/services/tomcat/hs_err_pid2588.log Compiled method (c1) 25034 1791 3 org.apache.catalina.startup.ContextConfig::processAnnotationsStream (38 bytes) total in heap [0x000000400ba128d0,0x000000400ba13a40] = 4464 relocation [0x000000400ba129f8,0x000000400ba12ab8] = 192 main code [0x000000400ba12ac0,0x000000400ba13440] = 2432 stub code [0x000000400ba13440,0x000000400ba13518] = 216 oops [0x000000400ba13518,0x000000400ba13520] = 8 metadata [0x000000400ba13520,0x000000400ba13558] = 56 scopes data [0x000000400ba13558,0x000000400ba13908] = 944 scopes pcs [0x000000400ba13908,0x000000400ba13a38] = 304 dependencies [0x000000400ba13a38,0x000000400ba13a40] = 8 # # If you would like to submit a bug report, please visit: # https://github.com/AdoptOpenJDK/openjdk-support/issues # qemu: uncaught target signal 6 (Aborted) - core dumped Aborted [cont-finish.d] executing container finish scripts... [cont-finish.d] done. [s6-finish] waiting for services. [s6-finish] sending all processes the TERM signal. [s6-finish] sending all processes the KILL signal and exiting. Rather than diving down this rabbit hole I elected to attempt this update on our staging server, DGDockerX.\nUpdate ISLE to the Latest Release Update an existing ISLE installation to install the newest improvements and security updates. This process is intended to be backwards compatible with your existing ISLE site.\nWe strongly recommend that you begin the update process on your Local environment so that you may test and troubleshoot before proceeding to update your Staging and Production environments.\nImportant Information These instructions assume you have already installed either the Local ISLE Installation: New Site or the Local ISLE Installation: Migrate Existing Islandora Site on your Local personal computer and are using that described git workflow. Please test these updates on your Local and Staging environments before updating your Production server. Always read the Release Notes for any version(s) newer than that which you are currently running. Docker Desktop Update: If Docker prompts that updates are available for your personal computer, please follow these steps: Go to your Local ISLE site: docker-compose down Install the new updated version(s) of Docker Desktop. Go to your Local ISLE site: docker-compose up -d # stop the docker service $ sudo service docker stop # download the latest docker binary and replace the current outdated docker # DEPRECATED WAY TO UPGRADE DOCKER: $ sudo wget https://get.docker.com/builds/Linux/x86_64/docker-latest -O /usr/bin/docker $ sudo yum update docker-engine # start the docker service $ sudo service docker start # check the version $ sudo docker version # check the images and containers $ sudo docker images $ sudo docker ps $ sudo docker ps -a Update Local (personal computer) As mentioned above, this work is being conducted in staging rather than local, on DGDockerX in the /home/islandora/ISLE directory and the dg-isle and dg-islandora directories there.\nOn your Local (personal computer) server, open a terminal (Windows: open Git Bash) and navigate to your Local ISLE repository (this contains the \u0026ldquo;docker-compose.local.yml\u0026rdquo; file):\nExample: cd /path/to/your/repository Stop the existing ISLE containers:\ndocker-compose down Check your git remotes:\ngit remote -v If you do not have a remote named \u0026ldquo;icg-upstream\u0026rdquo; then create one:\ngit remote add icg-upstream https://github.com/Islandora-Collaboration-Group/ISLE.git Run a git fetch from the ICG upstream:\ngit fetch icg-upstream Checkout a new git branch as a precaution for performing the update on your project. This way your \u0026ldquo;main\u0026rdquo; branch stays safe and untouched until you have tested thoroughly and are ready to merge in changes from the recent update. You may select any name for your new local branch.\nExample: git checkout -b isle-update-numberhere My new branch is named isle-update-1.5.7.\nPull down the ICG ISLE \u0026ldquo;main\u0026rdquo; branch into your Local project\u0026rsquo;s new \u0026ldquo;isle-update-numberhere\u0026rdquo; branch: git pull icg-upstream main I pulled icg-upstream main rather than master.\nIn your ISLE directory, you may view the newest release of ISLE code by entering: ls -lha Now that you have pulled down the latest code, there are likely to be conflicts between your existing code and the newer code. Run this command to determine if there are git merge conflicts: git status Output from git pull follows.\n╭─islandora@dgdockerx ~/ISLE/dg-isle ‹isle-update-1.5.7› ╰─$ git pull icg-upstream main From https://github.com/Islandora-Collaboration-Group/ISLE * branch main -\u0026gt; FETCH_HEAD Auto-merging staging.env Auto-merging scripts/apache/migration_site_vsets.sh CONFLICT (content): Merge conflict in scripts/apache/migration_site_vsets.sh Auto-merging production.env Auto-merging local.env Removing docs/specifications/supported-software-matrix.md Removing docs/specifications/supported-drupal-modules-matrix.md Auto-merging docs/install/install-staging-migrate.md Auto-merging docs/install/install-local-migrate.md CONFLICT (content): Merge conflict in docs/install/install-local-migrate.md Auto-merging docs/install/host-software-dependencies.md Removing docs/install/_obsolete_install-server.md Removing docs/contributor-docs/build-guide.md Removing docs/contributor-docs/build-guide-v111.md Removing docs/contributor-docs/ansible-guide.md Removing docs/contributor-docs/ISLE-v.1.1.2-buildnotes.md Removing docs/appendices/user-story-new-site.md Removing docs/appendices/user-story-migration-site.md Removing docs/appendices/user-story-demo-site.md Auto-merging docker-compose.staging.yml Auto-merging docker-compose.production.yml Auto-merging docker-compose.local.yml Auto-merging config/proxy/traefik.staging.toml CONFLICT (content): Merge conflict in config/proxy/traefik.staging.toml Auto-merging config/proxy/traefik.production.toml CONFLICT (content): Merge conflict in config/proxy/traefik.production.toml Auto-merging config/proxy/traefik.local.toml Automatic merge failed; fix conflicts and then commit the result. If there are any merge conflicts, then use a text editor (or IDE) to resolve them. (The Atom text editor offers green and red buttons to facilitate this process.) Some releases will have more merge conflicts than others. Carefully progress through this process of resolving merge conflicts. Changes will usually include but are not limited to: new configuration files new ISLE services optional or otherwise new ISLE docker image tags new comments new documentation I used ratom (see Remote Atom) to resolve the 4 merge conflicts.\nAfter all merge conflicts are resolved, then add and commit your edits to your Local environment:\nExample: git add \u0026lt;changedfileshere\u0026gt; Example: git commit -m \u0026quot;ISLE update from version #X to version #Y\u0026quot; Optional: If you want to backup this new branch to your origin, then run this command: (Ultimately after testing on your Local, you\u0026rsquo;ll merge to main and then deploy the new code to your Staging and Production environments.)\nExample: git push origin isle-update-numberhere Using the same open terminal, ensure you are in the root of your ISLE project directory:\nExample: cd /path/to/your/repository Download the new ISLE docker images to the Local (personal computer):\ndocker-compose pull There really should be an additional step here since you must edit, or at least check your .env file! I changed mine to read as follows:\nCOMPOSE_PROJECT_NAME=dgs BASE_DOMAIN=isle-stage.grinnell.edu CONTAINER_SHORT_ID=dgs COMPOSE_FILE=docker-compose.staging.yml Run the new docker containers (and new code) on your Local environment:\ndocker-compose up -d In your web browser, enter the URL of your Local site:\nExample: https://yourprojectnamehere.localdomain With an active VPN connection, I am happy to report that the staging site is now working at https://isle-stage.grinnell.edu. However, I\u0026rsquo;m getting tired or accepting the self-signed certs used in staging, so I found and executed How to disable Firefox: “Warning: Potential Security Risk Ahead” forever.\nQuality control (QC) the Local site and ensure the following: The site appears and functions as it did prior to these updates. You can ingest test objects without any issue. You can modify existing object data without any issue. All services are functional and without apparent ERROR warnings in the browser log console output. Example: In Chrome, press the F12 button to open the Console, then select the \u0026ldquo;Console\u0026rdquo; tab. Preliminary tests look good, although a search for \u0026ldquo;Ley\u0026rdquo; still returns a number of objects that do not exist in my DG-STAGING test repository. I\u0026rsquo;m going to attempt to re-index FGS and Solr to see if I can fix that now.\nNope, the Fedora and Solr update scripts did not resolve this issue. Next up, http://dgdockerx.grinnell.edu:8081/fedoragsearch/rest and login as fgsAdmin. That opens the Admin Client for Fedora Generic Search Service where I can hope to empty the index and rebuild. Nope, the createEmpty option returns a message telling me to createEmpty: Stop solr, remove the index dir, restart solr. So I opened a shell into the isle-solr-dgs container and did root@d7e1538334f6:/usr/local/solr/collection1/data# rm -fr index. Then I tried the rebuild scripts again, but this also did NOT work.\nUltimately, after deleting the aforementioned directory, I had to bring the stack down and back up again (using ~/DG-STAGING/destroy.sh and ~/DG-STAGING/RESTART.sh), then rebuild Fedora and update Solr. Now I have an accurate, abbreviated Solr return list!\nWhen you have completed testing and have no further adjustments to make, switch from your \u0026ldquo;isle-update-numberhere\u0026rdquo; branch of code to your \u0026ldquo;main\u0026rdquo; branch:\ngit checkout main Merge your \u0026ldquo;isle-update-numberhere\u0026rdquo; branch of code to \u0026ldquo;main\u0026rdquo;.\nExample: git merge isle-update-numberhere Push this code to your online git provider ISLE\ngit push -u origin main This will take 2-5 minutes depending on your internet speed. I wasn\u0026rsquo;t able to execute the sequence as presented above. Instead, I did this:\n╭─islandora@dgdockerx ~/ISLE/dg-isle ╰─$ git add . ╭─islandora@dgdockerx ~/ISLE/dg-isle ╰─$ git commit -m \u0026#34;Updated and working on DGDockerX\u0026#34; ╭─islandora@dgdockerx ~/ISLE/dg-isle ╰─$ git push origin isle-update-1.5.7 You now have the current ISLE project code checked into git; this will be the foundation for making changes to your Staging and Production servers. During my updates to DGDockerX I had network interruptions that left processes on the remote holding on to port 52698, the port that I need to make ratom work. I found this simple means of freeing up that port\u0026hellip;\n╭─islandora@dgdockerx ~ ╰─$ sudo netstat -plant | grep 52698 tcp 0 0 127.0.0.1:52698 0.0.0.0:* LISTEN 121885/sshd: island tcp6 0 0 ::1:52698 :::* LISTEN 121885/sshd: island tcp6 0 0 ::1:41724 ::1:52698 ESTABLISHED 24103/bash tcp6 0 0 ::1:52698 ::1:41724 ESTABLISHED 121885/sshd: island tcp6 0 0 ::1:52698 ::1:41382 ESTABLISHED 121885/sshd: island tcp6 0 0 ::1:41400 ::1:52698 ESTABLISHED 122118/bash tcp6 0 0 ::1:41610 ::1:52698 ESTABLISHED 23600/bash tcp6 0 0 ::1:41382 ::1:52698 ESTABLISHED 122040/bash tcp6 0 0 ::1:52698 ::1:41400 ESTABLISHED 121885/sshd: island tcp6 0 0 ::1:52698 ::1:41610 ESTABLISHED 121885/sshd: island ╭─islandora@dgdockerx ~ ╰─$ sudo kill -9 121885 Hold on, there\u0026rsquo;s one more significant part of the update process that\u0026rsquo;s not covered here, updating the Drupal and Islandora code that is not technically part of ISLE, but probably should be?\nMy process for updating that went something like this:\n╭─islandora@dgdockerx ~ ╰─$ cd ~/ISLE/dg-islandora ╭─islandora@dgdockerx ~/ISLE/dg-islandora ╰─$ git checkout -b update-june-23 ╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹update-june-23*› ╰─$ docker exec -it isle-apache-dgs bash root@03df8d9c975a:/# cd /var/www/html/sites/default/ root@03df8d9c975a:/var/www/html/sites/default# drush up The drush up command was invoked to pull in all of the latest updates to Drupal v7, and there were many, including an update to Drupal\u0026rsquo;s core. Since the core got updated, I had to eventually revert changes to /var/www/html/.htaccess since those updates typically remove a line that ISLE requires to function properly. I confirmed that the changes made by drush up did indeed make their way into ~/ISLE/dg-islandora by checking git status. I managed to revert the changes to .htaccess using git checkout -- .htaccess.\nAfter making those changes I found it necessary to restart the stack using:\n╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹update-june-23*› ╰─$ cd ~/DG-STAGING ╭─islandora@dgdockerx ~/DG-STAGING ‹main*› ╰─$ ./RESTART.sh I subsequently had to run several of the ./Update-*.sh scripts in ~/DG-STAGING in order to get everything working properly. Once everything was working properly I pushed my dg-islandora changes to Github using:\n╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹update-june-23*› ╰─$ git add --all . ╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹update-june-23*› ╰─$ git commit -m \u0026#34;Updates from DGDockerX\u0026#34; [update-june-23 f8e3cd8] Updates from DGDockerX 232 files changed, 3498 insertions(+), 1283 deletions(-) rewrite modules/simpletest/files/css_test_files/comment_hacks.css.optimized.css (100%) create mode 100644 modules/simpletest/files/css_test_files/quotes.css create mode 100644 modules/simpletest/files/css_test_files/quotes.css.optimized.css create mode 100644 modules/simpletest/files/css_test_files/quotes.css.unoptimized.css mode change 100755 =\u0026gt; 100644 sites/all/libraries/tuque/tests/README.md mode change 100755 =\u0026gt; 100644 sites/all/libraries/tuque/tests/scripts/travis_setup.sh create mode 100644 sites/all/modules/contrib/link/link.admin.inc rewrite sites/all/modules/contrib/link/link.info (61%) rename sites/all/modules/contrib/link/tests/{link.test =\u0026gt; LinkBaseTestClass.test} (85%) create mode 100644 sites/all/modules/contrib/link/tests/LinkConvertInternalPathsTest.test create mode 100644 sites/all/modules/contrib/link/tests/LinkDefaultProtocolTest.test rename sites/all/modules/contrib/link/tests/{link.entity_token.test =\u0026gt; LinkEntityTokenTest.test} (98%) rename sites/all/modules/contrib/link/tests/{link.attribute.test =\u0026gt; LinkFieldAttributesTest.test} (96%) rename sites/all/modules/contrib/link/tests/{link.crud_browser.test =\u0026gt; LinkFieldCrudTest.test} (87%) rename sites/all/modules/contrib/link/tests/{link.validate.test =\u0026gt; LinkFieldValidateTest.test} (60%) rename sites/all/modules/contrib/link/tests/{link.multilingual.test =\u0026gt; LinkPathPrefixesTest.test} (98%) create mode 100644 sites/all/modules/contrib/link/tests/LinkSanitizeTest.test rename sites/all/modules/contrib/link/tests/{link.token.test =\u0026gt; LinkTokenTest.test} (99%) create mode 100644 sites/all/modules/contrib/link/tests/LinkUnitTestCase.test create mode 100644 sites/all/modules/contrib/link/tests/LinkValidationApiTest.test delete mode 100644 sites/all/modules/contrib/link/tests/link.crud.test ╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹update-june-23› ╰─$ git pull origin main Username for \u0026#39;https://github.com\u0026#39;: digital@grinnell.edu Password for \u0026#39;https://digital@grinnell.edu@github.com\u0026#39;: xxxxxxxxxxxxxxxxxxxxxxxx From https://github.com/Digital-Grinnell/dg-islandora * branch main -\u0026gt; FETCH_HEAD Merge made by the \u0026#39;recursive\u0026#39; strategy. .htaccess | 13 +++++++-- sites/all/modules/islandora/idu/idu.drush.inc | 133 ++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++-- 2 files changed, 142 insertions(+), 4 deletions(-) At this point I did another cd ~/DG-STAGING; ./RESTART.sh just to ensure things still work properly. Another ./Update-PERMISSIONS.sh was also needed.\nThen\u0026hellip;\n╭─islandora@dgdockerx ~/ISLE/dg-islandora ‹update-june-23› ╰─$ git push origin update-june-23 Username for \u0026#39;https://github.com\u0026#39;: digital@grinnell.edu Password for \u0026#39;https://digital@grinnell.edu@github.com\u0026#39;: xxxxxxxxxxxxxxx Counting objects: 660, done. Delta compression using up to 4 threads. Compressing objects: 100% (348/348), done. Writing objects: 100% (352/352), 102.29 KiB | 0 bytes/s, done. Total 352 (delta 288), reused 1 (delta 0) remote: Resolving deltas: 100% (288/288), completed with 261 local objects. remote: remote: Create a pull request for \u0026#39;update-june-23\u0026#39; on GitHub by visiting: remote: https://github.com/Digital-Grinnell/dg-islandora/pull/new/update-june-23 remote: To https://github.com/Digital-Grinnell/dg-islandora * [new branch] update-june-23 -\u0026gt; update-june-23 Update Staging Server Ok, at this point I\u0026rsquo;ve already got our staging server up-to-date, including the addtion of my purge-tmp-files script successfully added to the Apache container\u0026rsquo;s /etc/cron.hourly directory. So my next steps will be to backup the production database and /var/www/html/sites/default/files, merge all of the dg-isle and dg-islandora repository changes into their respective main branches, and repeat all of my implementation and testing in staging again.\nSSH into your Staging ISLE Host Server:\nExample: ssh islandora@yourstagingserver.institution.edu Example: cd /opt/yourprojecthere Stop the existing ISLE containers:\ndocker-compose down Update the Docker files via git:\ngit pull origin main You must now again fix the .env file as you did in the On Remote Staging - Edit the \u0026quot;.env\u0026quot; File to Change to the Staging Environment step of either the Staging ISLE Installation: New Site or the Staging ISLE Installation: Migrate Existing Islandora Site instructions. As described, this step is a multi-step, involved process that allows an end-user to make appropriate changes to the .env and then commit it locally to git. This local commit will never be pushed back to the git repository and this is critical because it allows future ISLE updates and/or configuration changes. In other words, you are restoring what you had in the .env to the Staging settings in case they are overwritten.\nDownload the new ISLE docker images to the remote Staging environment:\ndocker-compose pull This may take a few minutes depending on your network connection Run the new docker containers (and new code) on your Staging environment:\ndocker-compose up -d The new containers should start up and your Staging Islandora site should be available, without any loss of existing content.\nI used the /home/islandora/DG-STAGING/RESTART.sh script on DGDockerX to restart the stack after capturing the production database and /var/www/html/sites/default/files contents. It worked nicely! It did require one run of the /home/islandora/DG-STAGING/Update-PERMISSIONS.sh script to get everything set.\nNext update will be to our production server, DGDocker1, where I hope to repeat this process to-the-letter.\nQC the Staging site and ensure the following:\nThe site appears and functions as it did prior to these updates. You can ingest test objects without any issue. You can modify existing object data without any issue. All services are functional and without apparent ERROR warnings in the browser log console output. We recommend that you observe the above Staging installation for a few days or a week.\nUpdate Production Server When you are confident that your Staging installation is working as expected:\nRepeat the same above \u0026ldquo;Update Staging Server\u0026rdquo; process but do so on your Production server environment. Additional Resources Please post questions to the public Islandora ISLE Google group, or subscribe to receive emails.\nOne hiccup in the production update\u0026hellip; I encountered this error when trying to initiate an IMI import:\nRuntimeException: Unable to create the cache directory (/var/www/private/ed). in Twig\\Cache\\FilesystemCache-\u0026gt;write() (line 57 of /var/www/html/sites/all/modules/islandora/islandora_multi_importer/vendor/twig/twig/src/Cache/FilesystemCache.php). The problem appeared to be ownership of the /var/www/public directory in the Apache container. The public folder was owned by islandora:islandora so I opened a shell into Apache and did cd /var/www/; chown -R islandora:www-data public. That seems to have fixed the problem nicely.\nEnd of: Update ISLE to the Latest Release And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/107-updating-digital-grinnell-in-isle/","tags":["ISLE","update","ratom",52698,"disable security","public"],"title":"Updating Digital.Grinnell in ISLE"},{"categories":null,"contents":" Digital.Grinnell and a handful of other servers/sites that I deal with crashed on the morning of Thursday, May 16. Really bad timing! I have yet to figure out what happened to trigger the tsunami, but it happened shortly before the college announced successful cut-over to a new emergency alert system. Related? I dunno.\nIn any case, on Friday afternoon (yesterday) I got a steroid shot for a nasty sinus condition and my doctor warned that I might not be able to sleep well. I didn\u0026rsquo;t believe her at the time, but at about 4 AM this morning the prophecy came true.\nWhat follows is my posting to Slack from the ensuing early-morning sleepless rant.\nMark M. 4:15 AM It’s 4 AM and I can’t sleep, again. Too many concerns about DGDocker1 and whatever the hell took it and other servers down on Thursday, coupled with the untimely death of our HDL server. I’m hoping an hour or two of fiddling with things will help resolve this, or at least get things working again, so I can get this out of my head for now.\nUnfortunately, I think my first move is to bring DGDocker1 down, apply pending updates, and bring it back. However, that means backing it up first in case of catastrophic failure, and that requires using the vSphere Client, a start-to-finish process that will take at least 30 minutes…once I get it started.\nI said unfortunately above because it literally took me 20 minutes to get my MacBook up and running again, connected via VPN, and ready to open the vSphere Client. When I finally did I was greeted with the security warnings shown below.\nSo, it appears that the vSphere Client https cert is not valid, or self-signed. I can understand why that might be allowed as an exception to our security rules since this client requires VPN or on-campus access, but it’s still a little troubling that we might make an “exception” for the UI into all our server assets. Maybe I’m just extra sensitive to this because it’s now 4:14 AM.\nAnyway, vSphere is finally open so I’m going to start that backup. Wish me luck.\nMark M. 4:23 AM Grrrrrrr…… the clucking VPN died halfway through the backup process. Now 4:23 AM. Starting over.\n4:27 AM Discovered that all of my NFS mounts are also dead. Not surprised based on what I saw on Thursday morning. I should have seen this coming.\nMark M. 4:35 AM Got things re-mounted and am removing the old, failed snapshots so I can start a new vSphere snapshot now.\nMark M. 4:58 AM In the meantime I think I’ve come up with a new name for GUAVA, er, you know what I mean. Most developers probably associate GUAVA with coffee, and that might make sense for some folks who love coffee since I think that’s central to the origin of the name…back in 1995. It will come as no surprise that I hate coffee. Never liked it, can’t imagine that I ever will. Begs the question… which came first, my dislike of coffee or my utter loathing of all things GUAVA? Probably the coffee, but it’s all ancient history so who really knows?\n4:40 AM Back to the new name train of thought since removing the old snapshots is now 87% complete.\nSo I think GUAVA is a rabbit of some kind. Clearly it lives in a hole, a rabbit hole I think, at least that’s what this feels like because I’m having a difficult time remembering why I started down this path. Fortunately, I can look back in this Slack thread for reminders. Gotta love Slack! Naturally, it is NOT based on GUAVA. Coincidence? Nope, it’s modern, not 20th century software.\n4:44 AM Removal of the failed backup is complete, so time to start another backup. VPN don’t fail me now! \u0026#x1f91e;\nOh ya, the name…. So the taxonomic name for rabbit is Oryctolagus cuniculus, but that’s a lot to remember, and there are so many different kinds of rabbits, as I’m learning now. GUAVA was created by Oracle and Sun Microsystems (are either of them still in business? does anybody care?) way back in the day. I believe they were headquartered in Silicon Valley, so it makes sense that GUAVA is a Californian rabbit, a particular breed of domestic rabbit.\nWell, taxonomic names only get longer as we get more specific, so this ain’t gonna help. :white_frowning_face: But wait, Californian rabbits are mostly white with gray/black tips, so, owners of this breed commonly name them “Snowball”. That sounds like a perfect name to me, it fits! I’ll try and remember to write it as SNOWBALL just to help folks remember that we’re talking about rabbits, er, I mean GUAVA, er, I mean… you know what I mean.\n4:57 AM and the snapshot is done! Now give me a minute to look back in this thread and see how this SNOWBALL (see, it works!) started…\nOk, here we go…\n5:00 AM With a snapshot in hand I’ve taken DG down. docker-compose down\nBack to vSphere and I’m going to reboot DGDocker1.\n5:01\nRestart the guest OS\n5:06 Ok, so that killed the VPN, for good reason this time. Back at it. sudo yum update. Only 253 packages that require updates. The SNOWBALL is getting bigger. Gotta love progress, aka complexity… NOT.\n5:08 AM Almost there, completed update of 252 packages. Unfortunately that means package cleanup comes next, and it’s really boring. All of this makes me wonder… Isn’t all of this the kind of thing you’d think ITS would take care of? Must be late, what am I thinking? That ain’t never gonna happen.\n5:10 AM Update is done. Now, \u0026#x1f91e; and hope there’s enough disk space to complete sudo yum upgrade.\n5:11 Made it! One more vSphere restart just for good measure.\n5:13 So far, so good. Reconnect VPN and restoring the NFS mounts…\nMark M. 5:18 AM Hmmm, 2 out of 3 ain’t bad. Unfortunately, that 3rd one is our bagit storage, our backup failsafe. Not so good. I’m seeing this:\n[root@dgdocker1 islandora]# mount -t nfs -o username=mcfatem 132.161.252.72:/nas/LibArchive /libarchive mount.nfs: an incorrect mount option was specified It’s always worked in the past. :white_frowning_face: I guess I’ll proceed without and hope for the best, and open another ticket when all this is done. What was I doing here again?\n5:20 Lets just hope this works…\n[islandora@dgdocker1 ~]$ cd /opt/ISLE/dg-isle [islandora@dgdocker1 dg-isle]$ [islandora@dgdocker1 dg-isle]$ docker-compose up -d Creating network \u0026#34;dg_isle-internal\u0026#34; with the default driver Creating network \u0026#34;dg_isle-external\u0026#34; with the default driver Creating isle-proxy-dg ... done Creating isle-mysql-dg ... done Creating isle-portainer-dg ... done Creating isle-solr-dg ... done Creating isle-fedora-dg ... done Creating isle-apache-dg ... done Creating isle-images-dg ... done [islandora@dgdocker1 dg-isle]$ Looking good. Now, is DG back?\n5:20 AM Not yet. Wait for it…\n5:21 AM Yes, DG is back and a search for ley returned 270 records with complete facets. Always a good test.\nNow, how about SNOWBALL and our HDL server, will it restart now?\nMark M. 5:29 AM :drum_with_drumsticks:\n[root@dgdocker1 dg-isle]# cd /hs/handle-9.2.0 [root@dgdocker1 handle-9.2.0]# ./bin/hdl-server /hs/svr_1 Handle.Net Server Software version 9.2.0 Enter the passphrase for this server\u0026#39;s authentication private key: Note: Your passphrase will be displayed as it is entered ***The fact that I have to type this phrase every time is the reason why all of this can\u0026#39;t be automated, but that\u0026#39;s the way SNOWBALL is. She can\u0026#39;t help it , after all, she was born in 1995 and that\u0026#39;s just how things worked back in the good \u0026#39;ol days, I guess*** HTTP handle Request Listener: address: 132.161.132.103 port: 8000 Starting HTTP server... UDP handle Request Listener: address: 132.161.132.103 port: 2641 TCP handle Request Listener: address: 132.161.132.103 port: 2641 Starting TCP request handlers... Starting UDP request handlers... ^Z [1]+ Stopped ./bin/hdl-server /hs/svr_1 [root@dgdocker1 handle-9.2.0]# bg [1]+ ./bin/hdl-server /hs/svr_1 \u0026amp; [root@dgdocker1 handle-9.2.0]# exit exit [islandora@dgdocker1 dg-isle]$ ps -ef | grep handle root 24821 24812 8 06:19 ? 00:00:26 /opt/java/openjdk/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djava.awt.headless=true -server -Xmx2048M -Xms512M -XX:+UseG1GC -XX:+UseStringDeduplication -XX:MaxGCPauseMillis=200 -XX:InitiatingHeapOccupancyPercent=70 -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Addresses=true -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap start root 42021 42012 36 06:19 ? 00:01:55 /opt/java/openjdk/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djava.awt.headless=true -server -Xmx4096M -Xms1024M -XX:+UseG1GC -XX:+UseStringDeduplication -XX:MaxGCPauseMillis=200 -XX:InitiatingHeapOccupancyPercent=70 -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Addresses=true -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap start root 59454 59443 9 06:20 ? 00:00:28 /opt/java/openjdk/bin/java -Djava.util.logging.config.file=/usr/local/tomcat/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djava.awt.headless=true -server -Xmx2048M -Xms512M -XX:+UseG1GC -XX:+UseStringDeduplication -XX:MaxGCPauseMillis=200 -XX:InitiatingHeapOccupancyPercent=70 -Djava.net.preferIPv4Stack=true -Djava.net.preferIPv4Addresses=true -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dcantaloupe.config=/usr/local/cantaloupe/cantaloupe.properties -Dorg.apache.tomcat.util.buf.UDecoder.ALLOW_ENCODED_SLASH=true -Djava.library.path=/usr/local/lib:/usr/local/tomcat/lib -DLD_LIBRARY_PATH=/usr/local/lib:/usr/local/tomcat/lib -Dignore.endorsed.dirs= -classpath /usr/local/tomcat/bin/bootstrap.jar:/usr/local/tomcat/bin/tomcat-juli.jar -Dcatalina.base=/usr/local/tomcat -Dcatalina.home=/usr/local/tomcat -Djava.io.tmpdir=/usr/local/tomcat/temp org.apache.catalina.startup.Bootstrap start root 92190 1 6 06:23 pts/0 00:00:05 java -Xmx200M -server -cp :/hs/handle-9.2.0/bin/../lib/admintool-9.2.0.jar:/hs/handle-9.2.0/bin/../lib/bcpkix-jdk15on-1.59.jar:/hs/handle-9.2.0/bin/../lib/bcprov-jdk15on-1.59.jar:/hs/handle-9.2.0/bin/../lib/cnriutil-2.0.jar:/hs/handle-9.2.0/bin/../lib/commons-codec-1.11.jar:/hs/handle-9.2.0/bin/../lib/gson-2.8.5.jar:/hs/handle-9.2.0/bin/../lib/handle-9.2.0.jar:/hs/handle-9.2.0/bin/../lib/javax.servlet-api-3.0.1.jar:/hs/handle-9.2.0/bin/../lib/je-7.5.11.jar:/hs/handle-9.2.0/bin/../lib/jna-5.3.1.jar:/hs/handle-9.2.0/bin/../lib/jna-platform-5.3.1.jar:/hs/handle-9.2.0/bin/../lib/jython-2.2.1.jar:/hs/handle-9.2.0/bin/../lib/oldadmintool-9.2.0.jar:/hs/handle-9.2.0/bin/../lib/oshi-core-3.13.3.jar:/hs/handle-9.2.0/bin/../lib/slf4j-api-1.7.25.jar:/hs/handle-9.2.0/bin/../lib/jetty/com.sun.el-2.2.0.v201108011116.jar:/hs/handle-9.2.0/bin/../lib/jetty/javax.annotation-1.1.0.v201108011116.jar:/hs/handle-9.2.0/bin/../lib/jetty/javax.el-2.2.0.v201108011116.jar:/hs/handle-9.2.0/bin/../lib/jetty/javax.servlet.jsp-2.2.0.v201112011158.jar:/hs/handle-9.2.0/bin/../lib/jetty/javax.servlet.jsp.jstl-1.2.0.v201105211821.jar:/hs/handle-9.2.0/bin/../lib/jetty/javax.transaction-1.1.1.v201105210645.jar:/hs/handle-9.2.0/bin/../lib/jetty/jetty-all-8.1.22.v20160922.jar:/hs/handle-9.2.0/bin/../lib/jetty/org.apache.jasper.glassfish-2.2.2.v201112011158.jar:/hs/handle-9.2.0/bin/../lib/jetty/org.apache.taglibs.standard.glassfish-1.2.0.v201112081803.jar:/hs/handle-9.2.0/bin/../lib/jetty/org.eclipse.jdt.core-3.7.1.jar:/hs/handle-9.2.0/bin/../lib/jetty/org.objectweb.asm-3.1.0.v200803061910.jar net.handle.server.Main /hs/svr_1 islando+ 92358 2346 0 06:25 pts/0 00:00:00 grep --color=auto handle 5:33 So, what does all that $hit mean? IT WORKS!\nIn SNOWBALL terms it means that at 5:32 AM I can go back to bed. Notice I said bed, not sleep. There’s no telling how long it might take me to forget all this $hit so I can relax a little.\nOh yeah, I also have to remember to work out that mount -t nfs -o username=mcfatem 132.161.252.72:/nas/LibArchive /libarchive failure sometime.\nProbably ought to capture all of this in my blog sometime too, but who has time for that?\nG’night all.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/106-snowball/","tags":["Java","SNOWBALL"],"title":"SNOWBALL"},{"categories":null,"contents":"For some time now we\u0026rsquo;ve had a problem lurking in Digital.Grinnell, when large files are opened for viewing or download one of the DG services makes a temporary copy of the file in the Apache container\u0026rsquo;s /tmp directory. Locally, and in staging I\u0026rsquo;ve debugged the code that is responsible for removing the temporary file once the operation is complete. Running locally or in staging the process does its job, the temporary files get deleted soon after creation, but this never happens in production. The result, our root disk on the production server fills up after a few days of use, and the server stops serving content. Even more sinister, the server doesn\u0026rsquo;t crash and restart \u0026ndash; a condition that would also clear the offending /tmp files \u0026ndash; it doesn\u0026rsquo;t even report a fatal error, it just refuses to serve content, which is really its only function. Very frustrating indeed!\nSo I\u0026rsquo;ve set a reminder to bring the DG stack down, gracefully, every few days, and immediately restart it. This works nicely becuase stopping and restarting the stack clears out all temporary files.\nWhat\u0026rsquo;s Different About Staging? This afternoon I was poking around in my staging server, https://dg-staging.grinnell.edu, and noticed something different in the Apache container\u0026rsquo;s /tmp directory there. The folder contains a .htaccess file, one that does NOT exist in production. Hmmm.\nThe other glaring difference is that in staging the Apache /tmp directory is owned by UID islandora, as is most everything in staging. In production, however, that directory is owned by root. Hmmmm, very interesting indeed.\nFirst Attempt, a New .htaccess File So my first move was to duplicate the .htaccess file I found in staging and add it to production\u0026rsquo;s Apache /tmp directory. After watching DG performance for a bit I could see this was having no effect, almost immediately some very large, temporary files started to accumulate. Rats.\nChanging /tmp Ownership Since the permissions on the /tmp directory looked right my next move was to change the ownership of /tmp. I did that after shelling in to the running Apache container, like so:\ndocker exec -it isle-apache-dg bash cd / ls -alh chown -R www-data:www-data tmp Did That Work? Only :clock: will tell! After about 10 minutes time there\u0026rsquo;s no evidence of any temporary files accumulating. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/105-adding-htaccess-to-apache-tmp-directory/","tags":["tmp","temporary","Apache",".htaccess"],"title":"Adding `.htaccess` to Apache Container `/tmp` Directory"},{"categories":null,"contents":"A colleague and I were recently sifting through the Digital.Grinnell logs and came across some recurring 404, \u0026ldquo;page not found\u0026rdquo;, status messages. 404\u0026rsquo;s are not uncommon in DG, but these were particularly troubling because they were requests of the form drupal/fedora/repository/grinnell:162 and drupal/fedora/repository/grinnell:86. That\u0026rsquo;s the old, and by that I mean VERY OLD, like Drupal v6 vintage from 2012 or 2013, form for an object address. All such references were to some of DG\u0026rsquo;s oldest digital objects, too.\nWe tried to figure out where such old address references might be coming from, but we struck out. I wonder if it\u0026rsquo;s even possible to back-track a request like that given today\u0026rsquo;s GDPR environment and related privacy practices?\nSo, in lieu of finding the source of these requests, I did a little research and found Redirecting specific pages to new URLs (301 redirects in Drupal) . It didn\u0026rsquo;t take long to implement an Apache redirect rule based on this document, so that\u0026rsquo;s what I did. I first implemented a new rule in the /var/www/html/.htaccess file on our staging server, https://dg-staging.grinnell.edu, and tested it there. It works nicely, so I pushed the change to production and tested again. Works well there too!\nThe 301 Redirect Rule The addition I made, per the aforementioned document are currently lines 115-124 in https://github.com/Digital-Grinnell/dg-islandora/blob/main/.htaccess. Like so:\nRewriteBase / ## The following rule lifted from https://www.drupal.org/node/38960 ## Implemented in April 2021 in order to redirect old object addresses of the form ## drupal/fedora/repository/grinnell:182, to a proper equivalent form like ## islandora/object/grinnell:182 ## # custom redirects RewriteRule ^drupal/fedora/repository/(.+)$ https://digital.grinnell.edu/islandora/object/$1 [R=301,L] # end custom redirects I used an Apache mod_rewrite Introduction document to refresh my memory regarding Apache rewrite rules and syntax.\nNote that the RewriteBase / statement was already in the .htaccess file, but it was previously commented out. I saw no harm in activating it, and the document somewhat suggests it should be turned on, so I removed the comment and added the 9 lines that follow it.\nA Definite Plus One of the things I like most about this solution is reflected in a statement lifted from 301 redirects in Drupal), specifically:\n\u0026hellip;301 redirects are considered the best way to handle redirected pages, for they inform search engines to update their databases with the new paths. This way, you should not risk your search engine pagerank or lose site visitors with 404 \u0026ldquo;not found\u0026rdquo; errors.\nNot Great, But It Works I say it\u0026rsquo;s not a \u0026ldquo;great\u0026rdquo; rule because it assumes anything passed to DG with drupal/fedora/repository/... in the address should be redirected to https://digital.grinnell.edu/islandora/object/... and that\u0026rsquo;s not explicitly true when I\u0026rsquo;m working with our staging server, but the intent is to fix a condition that should only exist in production, so it will do. I did try some more advanced substitution (see below), but it didn\u0026rsquo;t work, so I returned to a simple substitution that does work in production.\nAdvanced Rule Fails This rule, RewriteRule ^(.+)/drupal/fedora/repository/(.+)$ $1/islandora/object/$2 [R=301,L], seemed like a better rule since it doesn\u0026rsquo;t assume the host is https://digital.grinnell.edu, but it doesn\u0026rsquo;t work properly. I\u0026rsquo;m betting some more RewriteCond rules would be required to make it so.\nTest It? If you\u0026rsquo;d care to test the new rule, just take any of these URLs for a spin in your browser:\nhttps://digital.grinnell.edu/drupal/fedora/repository/grinnell:86 https://digital.grinnell.edu/drupal/fedora/repository/grinnell:182 And that\u0026rsquo;s a wrap. Until next time, happy redirecting!\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/104-301-redirect-in-digital.grinnell/","tags":["301","Redirect","Apache",".htaccess"],"title":"301 Redirect in Digital.Grinnell"},{"categories":null,"contents":"Earlier this year I installed and configured the remote-atom package to assist with editing content and code for my personal blog. Naturally, I wrote a blog post to document it.\nIn the past week I\u0026rsquo;ve added the remote-atom package, and configuration to many of my Digital.Grinnell servers. The installation and configuration was virtually identical to what I described in Adding remote-atom to my DigitalOcean Server. Thus far the package and it\u0026rsquo;s configuration have been implemented on the following workstations, both personal and professional, with the following username@server configurations:\nWorkstation: MA8660 - Grinnell College iMac administrator@static islandora@dgdocker1 islandora@dgdockerx mcfatem@dgdocker3 Note: The ~/.ssh/config file for the settings shown above looks like this:\nHost static Hostname 132.161.151.30 RemoteForward 52698 localhost:52698 User administrator Host dgdocker3 Hostname 132.161.151.50 RemoteForward 52698 localhost:52698 User mcfatem Host dgdocker1 Hostname 132.161.132.103 RemoteForward 52698 localhost:52698 User islandora Host dgdockerx Hostname 132.161.132.101 RemoteForward 52698 localhost:52698 User islandora Workstation: Mark\u0026rsquo;s Mac Mini - Personal Desktop centos@digitalOcean mcfatem@dgdocker3 administrator@static islandora@dgdocker1 Workstation: MA7053 - Grinnell College MacBook Air administrator@dgadmin Regarding Port 52698 During recent updates to DGDockerX I had network interruptions that left processes on the remote holding on to port 52698, the port that I need to make ratom work. I found this simple means of freeing up that port\u0026hellip;\n╭─islandora@dgdockerx ~ ╰─$ sudo netstat -plant | grep 52698 tcp 0 0 127.0.0.1:52698 0.0.0.0:* LISTEN 121885/sshd: island tcp6 0 0 ::1:52698 :::* LISTEN 121885/sshd: island tcp6 0 0 ::1:41724 ::1:52698 ESTABLISHED 24103/bash tcp6 0 0 ::1:52698 ::1:41724 ESTABLISHED 121885/sshd: island tcp6 0 0 ::1:52698 ::1:41382 ESTABLISHED 121885/sshd: island tcp6 0 0 ::1:41400 ::1:52698 ESTABLISHED 122118/bash tcp6 0 0 ::1:41610 ::1:52698 ESTABLISHED 23600/bash tcp6 0 0 ::1:41382 ::1:52698 ESTABLISHED 122040/bash tcp6 0 0 ::1:52698 ::1:41400 ESTABLISHED 121885/sshd: island tcp6 0 0 ::1:52698 ::1:41610 ESTABLISHED 121885/sshd: island ╭─islandora@dgdockerx ~ ╰─$ sudo kill -9 121885 And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/085-remote-atom/","tags":["Atom","remote-atom","ratom",52698],"title":"Remote Atom"},{"categories":null,"contents":"For the past couple of years I\u0026rsquo;ve been working in Digital.Grinnell to remove as much \u0026ldquo;customization\u0026rdquo; as I can. The effort is coming along, but still, there\u0026rsquo;s a long way to go. Every now and then I come across a feature that we just can\u0026rsquo;t live without, and it\u0026rsquo;s in times like those that I turn to PHPStorm for development and testing. Unfortunately, I\u0026rsquo;ve been operating without PHPStorm in ISLE because I worried that configuring the PHPStorm debugger in a Docker environment would be a time-consuming, tall task. It was tricky, but worry no more!\nThe Goal Simple: Get PHPStorm configured with ISLE and my http://dg.localdomain development instance of Digital.Grinnell, with real-time debugging. I do my development on any one of three Mac\u0026rsquo;s, so making things work in an OS X environment was also a requirement. I searched, seemingly for days, through a host of blog posts, tutorials, and documents, for any applicable guidance that might get me to my goal. There is LOTS of advice and guidance available, but in many cases it\u0026rsquo;s for a very specific environment, or written strictly from a \u0026ldquo;How I Did It\u0026rdquo; perspective, with little or no explanation of \u0026ldquo;why\u0026rdquo; certain things were done.\nThis post is another \u0026ldquo;How I Did It\u0026rdquo; account, but the resources I used include some welcome explanations.\nThe Resources There was one key document that I came to rely on: Turbocharged PHP Development with Xdebug, Docker \u0026amp; PHPStorm, a dev.to blog post by James Seconde. Thank you, James! Excellent work, and thank you for sharing!\nThe other document that\u0026rsquo;s still open in my browser is Create a local server configuration, part of JetBrains\u0026rsquo; documentation suite. This document is linked in from James\u0026rsquo; post too.\nA Summary of Changes dg-isle is my Digital.Grinnell copy of the project repository identifed as yourprojectnamehere-isle in the ISLE documentation. All of my changes were made in that repo, and they are summarized in the following git status result:\n╭─mark@Marks-Mac-Mini ~/ISLE/dg-isle ‹main*› ╰─$ git status On branch main Your branch is up to date with \u0026#39;origin/main\u0026#39;. Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git checkout -- \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: docker-compose.local.yml modified: local.env Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) xdebug-local.ini no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) Contents of Those Files docker-compose.local.yml In docker-compose.local.yml I added only the last line you see below, in the volumes: portion of the apache: container config.\napache: image: islandoracollabgroup/isle-apache:1.5.4 ...lines removed for clarity... volumes: # Customization: Bind mounting Drupal Code instead of using default Docker volumes for local development with an IDE. - ~/ISLE/dg-islandora:/var/www/html:cached ...lines removed for clarity... - ./xdebug-local.ini:/etc/php/7.1/apache2/conf.d/20-xdebug-local.ini Attention! While the line I added above does work, it\u0026rsquo;s a bit of a brute force approach and is probably not sustainable. I\u0026rsquo;ve found a much better approach is to inject my debug config like so:\napache: image: islandoracollabgroup/isle-apache:1.5.4 ...lines removed for clarity... volumes: # Customization: Bind mounting Drupal Code instead of using default Docker volumes for local development with an IDE. - ~/ISLE/dg-islandora:/var/www/html:cached ...lines removed for clarity... - ./xdebug-local.ini:/etc/php/7.1/mods-available/xdebug.ini local.env and .env In local.env I added a comment followed by three lines of configuration, all near the top of the file, like so:\n## ISLE Local ## This local.env file is used with / for the docker-compose.local.yml ## Windows user should uncomment the following line: # COMPOSE_CONVERT_WINDOWS_PATHS=1 ## Parameters for XDEBUG and PHPStorm ENABLE_XDEBUG=true PHP_MEMORY_LIMIT=\u0026#39;128M\u0026#39; PHP_IDE_CONFIG=\u0026#39;serverName=dg_local\u0026#39; ...remaining lines removed for clarity... Note that dg_local above is the same as the COMPOSE_PROJECT_NAME parameter from my .env file. The contents of .env remains unchanged:\n#### Activated ISLE environment # To use an environment other than the default Demo, please change values below # from the default Demo to one of the following: Local, Test, Staging or Production # For more information, consult https://islandora-collaboration-group.github.io/ISLE/install/install-environments/ COMPOSE_PROJECT_NAME=dg_local BASE_DOMAIN=dg.localdomain CONTAINER_SHORT_ID=ld COMPOSE_FILE=docker-compose.local.yml xdebug-local.ini This is a new file, one not previously found in the ISLE configuration. James mapped this file into his configuration like so:\n- ./xdebug-local.ini:/usr/local/etc/php/conf.d/xdebug-local.ini Don\u0026rsquo;t use that in ISLE! The correct config for ISLE is the line I added to docker-compose.local.yml which reads:\n- ./xdebug-local.ini:/etc/php/7.1/apache2/mods-available/xdebug.ini The ./ prefix in that line dictates that your xdebug-local.ini file should be in your project\u0026rsquo;s root directory, the same directory where docker-compose.local.yml resides. In my case that\u0026rsquo;s ~/ISLE/dg-isle.\nThe contents of your xdebug-local.ini file, assuming you\u0026rsquo;re using OS X, should be identical to James\u0026rsquo; copy, and mine:\nzend_extension=xdebug xdebug.remote_enable=1 xdebug.remote_autostart=1 xdebug.remote_port=9001 xdebug.remote_host=host.docker.internal How I Did It To be honest, I tried so many things, I am not absolutely certain which parts were necessary, and which were not. So, let me just explain what I did in each of the steps that James outlined.\nStep One: Installing Xdebug\nI did NOT create a new Dockerfile or new build of ISLE\u0026rsquo;s Apache container. All that was required in that regard, I believe, was the inclusion of the ENABLE_XDEBUG=true line in local.env.\nStep Two: Configuring Xdebug\nI followed James\u0026rsquo; guidance explicitly here. Please read his post!\nStep Three: Inject Config to Docker\nAgain, I followed James explicitly, but omitted the DB_ parameters since I\u0026rsquo;m only interested in debugging PHP code. As James suggests, the serverName parameter in PHP_IDE_CONFIG: is critical.\nStep Four: Set up PHPStorm\nThis is where things got a bit confusing, so I\u0026rsquo;ll dispense with my recollection and just show you my end results in a series of screen grabs. Please DO give James\u0026rsquo; post a good read though, there\u0026rsquo;s lots of helpful explanation within.\nMy Project Structure Locally, my ISLE configuration and code live in a pair of repositories named dg-isle and dg-islandora inside an ~/ISLE directory. That ~/ISLE directory now includes a .idea hidden folder, and that\u0026rsquo;s where all of my associated PHPStorm project info lives. It all looks like this:\n╭─mark@Marks-Mac-Mini ~/ISLE ╰─$ ls -alh total 16 drwxr-xr-x 6 mark staff 192B Mar 11 13:08 . drwxrwxr-x+ 137 mark staff 4.3K Mar 12 12:18 .. drwxr-xr-x 13 mark staff 416B Mar 12 12:14 .idea drwxr-x---@ 36 mark staff 1.1K Mar 11 14:18 dg-islandora drwxr-xr-x 42 mark staff 1.3K Mar 11 12:41 dg-isle First Steps in PHPStorm The first thing I remember doing in PHPStorm was creating my project. That process is probably best shown in this screen grab:\nFigure 1 \u0026middot; Creating My PHPStorm Project Structure I used the New Project button and New Project from Existing Files... pull-down option to get started. Be sure to choose the Web server is installed locally... scenario too.\nThe resulting structure can be seen in this screen grab showing the upper-left corner of my PHPStorm application window.\nFigure 2 \u0026middot; My PHPStorm Project Structure Next Steps This is where things really get foggy, so, more screen grabs showing the results.\nMy Run/Debug Configurations look like this:\nFigure 3 \u0026middot; PHPStorm Run/Debug Configurations My Preferences \u0026gt; Language \u0026amp; Frameworks \u0026gt; PHP \u0026gt; Servers configuration looks like this:\nFigure 4 \u0026middot; Preferences Please don\u0026rsquo;t ask me to recall exactly how I created those configurations. Here\u0026rsquo;s what I do remember\u0026hellip;\nLet the Magic Happen The final pieces of this puzzle just seem to \u0026ldquo;magically\u0026rdquo; fall into place once you begin your first debugging session. In my case, after configuring most of what you\u0026rsquo;ve seen above, I did this:\nOpened PHPStorm to the ISLE project.\nOpened index.php and set a breakpoint. I also elected to add a simple print statement like you see below.\nFigure 5 \u0026middot; My Breakpoint in index.php Toggled the \u0026ldquo;Start Listening\u0026rdquo; button (the button with the phone and red circle) in the included image.\nFigure 6 \u0026middot; Start Listening Opened my local ISLE instance, http://dg.localdomain, in the browser of my choice, Firefox, and clicked Refresh. The first time I did this with PHPStorm \u0026ldquo;listening\u0026rdquo;, I was prompted with some \u0026ldquo;connection\u0026rdquo; follow-up questions. That happens only once, and the questions are not too difficult to answer.\nEnjoyed debugging ISLE without a single var_dump() or print_r() statement!\nBut What About Debugging in Drush? I have a number of PHP scripts implemented in drush as part of a Drupal module I call idu, and I need to be able to debug those too. But the above configuration doesn\u0026rsquo;t work properly with drush commands. So, what\u0026rsquo;s missing?\nAfter considerable searching I found two critical changes were necessary\u0026hellip;\nExport the serverName Config Before making any drush-specific configruation changes I was seeing messages like this in PHPStorm when attempting to debug a script:\nFigure 7 \u0026middot; Can\u0026rsquo;t Find dg_local Server Turns out for cli debugging you have to explicitly \u0026ldquo;export\u0026rdquo; the PHP_IDE_CONFIG value inside the running container, like so:\nroot@7793f5d333fd:/var/www/html/sites/default# export PHP_IDE_CONFIG=\u0026#34;serverName=dg_local\u0026#34; But, hold on, that\u0026rsquo;s still not right! The dg_local server isn\u0026rsquo;t what I want for cli debugging. For that I have a PHPStorm server named apache that was automatically created for me. So the command I need to run in the container is:\nroot@7793f5d333fd:/var/www/html/sites/default# export PHP_IDE_CONFIG=\u0026#34;serverName=apache\u0026#34; Don\u0026rsquo;t Use Automatic Breakpoints! In my configuration, drush lives in my Apache container at ./opt/drush-8.x/vendor/drush/drush/drush, but the source code I mapped to PHPStorm only includes the directories and file that live in and below /var/www/html. So, if you leave automatic breakpoints turned on in PHPStorm and run drush, the debugger will stop in a location that you can\u0026rsquo;t \u0026ldquo;see\u0026rdquo;, making it nearly impossible to step through your code.\nThe fix for this is to turn OFF the last two PHPStorm preferences checkboxes you see below. Turning those off will ensure that PHPStorm does not stop execution at the first line inside the Apache containers drush command script.\nFigure 8 \u0026middot; Do NOT Set Automatic Breakpoints And that\u0026rsquo;s a wrap. Until next time, happy debugging!\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/103-debugging-isle-local-with-phpstorm/","tags":["debug","XDEBUG","ISLE","local","PHPStorm","drush"],"title":"Debugging ISLE Local with PHPStorm"},{"categories":null,"contents":"Digital.Grinnell used to support and use \u0026ldquo;entities\u0026rdquo;, metadata-only objects that are referential in nature. Entities are best described in this Entities Solution Pack documentation. Quoting from that resource\u0026hellip;\nThis module allows you to add person, place, event, and organization entities to an Islandora repository. Entities are small, metadata-based objects. A number of forms and additional features are provided in this module for those building an institutional repository with Islandora. Much of the functionality for batch ingest and autocomplete (to use entities as authority objects) centres [sic] around the MADS forms provided with the module.\nCustom thumbnails can be added via the manage tab to ingested Events, Places, and Organizations. When creating People entities, a thumbnail can be added at time of object creation. If the thumbnail is not added afterward for the Events, Places, and Organizations Entities, the default icon is the folder icon used for collections.\nObjects ingested under Events, Places, Organization and People content models are also affiliated with the generic \u0026ldquo;Entity\u0026rdquo; content model provided with the solution pack. To use the Entity content model, you must add it explicitly to the collection policy and associate an appropriate form.\nAs of today, February 17, 2021, \u0026ldquo;entities\u0026rdquo; are back-in-play in Digital.Grinnell, thanks to implementation of the Islandora Scholar Extended Profiles module. This module is the first part of LASIR to be implemented here at Grinnell.\nPrimary Function of Entities Entities are primarily used in conjunction with autocomplete operations in Islandora\u0026rsquo;s forms where they help to promote consistent use. Entities are also a form of linked data so they can also assist with discovery and presentation of related objects.\nDG\u0026rsquo;s Old Entity Structure In Digital.Grinnell\u0026rsquo;s first implementation of Islandora entities, circa 2015, the islandora:entity_collection was established under the repository root, so it\u0026rsquo;s a child/sub-collection of islandora:root. A summary snapshot of that collection shows\u0026hellip;\nFigure 1 \u0026middot; Existing Entity Collection Entity objects in this collection were of 4 types:\nentity - A generic entity type that was previously NOT used. person - An entity representing a person, typically an author. Entities of this type were created in the grinnell-person: namespace and they employ the islandora:personCModel content model. subject - An entity representing a subject or \u0026ldquo;topic\u0026rdquo; like \u0026ldquo;Chemistry\u0026rdquo;. There\u0026rsquo;s only one remaining entity of this type in DG and it uses a now-defunct namespace of grinnell-subject:. Subjects employ the islandora:subjectCModel content model. organization - An entity representing an organization, typically a Grinnell College department of study, or similar group. Entities of this type were created in the grinnell-organization: namespace and they employ the islandora:organizationCModel content model. DG\u0026rsquo;s New Entity Structure With the implementation of the Islandora Scholar Extended Profiles module come some necessary changes. The old islandora:entity_collection remains intact, as do all of the related content models, but with some enhancements, and new namespaces to accommodate a new set of objects. Note that the old objects listed in the figure above are still present, for now.\nNew entity objects are of 4 types, with the first two to be implemented immediately, and the remaining two to follow, if/when they are ever needed.\nperson - An entity representing a person, typically an author. New entities of this type will be created in the profile: namespace and they will employ the islandora:personCModel content model. organization - An entity representing an organization, typically a Grinnell College department of study, or similar group. Entities of this type will be created in the organization: namespace and will employ the islandora:organizationCModel content model. event - An entity representing an event, such as a symposium or conference. New entities of this type will be created in the event: namespace and they will employ the islandora:eventCModel content model. place - An entity representing a place, like \u0026ldquo;Burling Library\u0026rdquo;. New entities of this type will created in the place: namespace and they will employ the islandora:placeCModel content model. Adding a New Organization to DG Login to Digital.Grinnell as an admin. Visit https://digital.grinnell.edu/islandora/object/islandora%3Aentity_collection/manage/overview/ingest to add a new entity. To create a new organization select New Organization in the Select a Content Model to Ingest drop-down, and click Next. In the Select a Form field choose Department MADS form and click Next. Fill out the form making sure all required fields (denoted with a red asterisk) are filled in, then click Ingest to complete the operation. Attention! Pay special attention to the U2 Identifier field. It should be a short ID string that uniquely identifies the organization among all organizations in the repository. The value you specify here may subsequently be repeated when adding new profiles (persons) to the repository to associate the person with the organization.\nAdding a New Person/Profile to DG Login to Digital.Grinnell as an admin. Visit https://digital.grinnell.edu/islandora/object/islandora%3Aentity_collection/manage/overview/ingest to add a new entity. To create a new profile select New Person in the Select a Content Model to Ingest drop-down, and click Next. In the Select a Form field choose Person MADS form and click Next. Fill out the form making sure all required fields (denoted with a red asterisk) are filled in, then click Ingest to complete the operation. Attention! Pay special attention to the Identifier field. It should be a short ID string that uniquely identifies the person/profile among all profiles in the repository. The value you specify here may subsequently be repeated when adding new scholarly works to the repository. This identifier will associate the scholarly work with the identified person, and subsequently with their department/organization.\nBulk Ingest of New DG Persons As stated in the Entities Solution Pack documentation, new person entities can be bulk-ingested using the Islandora Entities CSV Import which is documented in DG\u0026rsquo;s Apache container at /var/www/html/sites/all/modules/islandora/islandora_solution_pack_entities/modules/islandora_entities_csv_import/README.md. Since that file can be hard to find, here\u0026rsquo;s the contents pulled from Digital.Grinnell\u0026hellip;\nIslandora Entities CSV Import Introduction This module is for adding person entities to Islandora using a .csv file.\nRequirements This module requires the following modules/libraries:\nIslandora Islandora Basic Collection Islandora Entities Installation Install as usual, see this for further information.\nConfiguration Prepare a comma-delimited CSV file using the column names below. Only columns with names in the list will be processed; all others will be ignored. Any comma within a field must be replaced with a double pipe ie - \u0026lsquo;Nursing, Department of\u0026rsquo; must be replaced with \u0026lsquo;Nursing|| Department of\u0026rsquo;.\nMultiple arguments within one column can be separated with a tilde (~). However, this may yield unexpected results (missing XML attributes, improper nesting) if used outside the following fields: FAX, PHONE, EMAIL, POSITION.\nSTATUS POSITION EMAIL BUILDING ROOM_NUMBER IDENTIFIER TERM_OF_ADDRESS GIVEN_NAME FAMILY_NAME FAX PHONE DISPLAY_NAME DEPARTMENT BUILDING CAMPUS NAME_DATE STREET CITY STATE COUNTRY POSTCODE START_DATE END_DATE ROOM_NUMBER BUILDING CAMPUS This will be transformed into the following MADS record:\n\u0026lt;mads xmlns=\u0026#34;http://www.loc.gov/mads/v2\u0026#34; xmlns:mads=\u0026#34;http://www.loc.gov/mads/v2\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xmlns:xlink=\u0026#34;http://www.w3.org/1999/xlink\u0026#34;\u0026gt; \u0026lt;authority\u0026gt; \u0026lt;name type=\u0026#34;personal\u0026#34;\u0026gt; \u0026lt;namePart type=\u0026#34;given\u0026#34;\u0026gt;[GIVEN_NAME]\u0026lt;/namePart\u0026gt; \u0026lt;namePart type=\u0026#34;family\u0026#34;\u0026gt;[FAMILY_NAME]\u0026lt;/namePart\u0026gt; \u0026lt;namePart type=\u0026#34;termsOfAddress\u0026#34;\u0026gt;[TERM_OF_ADDRESS]\u0026lt;/namePart\u0026gt; \u0026lt;namePart type=\u0026#34;date\u0026#34;\u0026gt;[NAME_DATE]\u0026lt;/namePart\u0026gt; \u0026lt;/name\u0026gt; \u0026lt;titleInfo\u0026gt; \u0026lt;title\u0026gt;[DISPLAY_NAME]\u0026lt;/title\u0026gt; \u0026lt;/titleInfo\u0026gt; \u0026lt;/authority\u0026gt; \u0026lt;affiliation\u0026gt; \u0026lt;organization\u0026gt;[DEPARTMENT]\u0026lt;/organization\u0026gt; \u0026lt;position\u0026gt;[POSITION]\u0026lt;/position\u0026gt; \u0026lt;address\u0026gt; \u0026lt;email\u0026gt;[EMAIL]\u0026lt;/email\u0026gt; \u0026lt;phone\u0026gt;[PHONE]\u0026lt;/phone\u0026gt; \u0026lt;fax\u0026gt;[FAX]\u0026lt;/fax\u0026gt; \u0026lt;street\u0026gt;[STREET]\u0026lt;/street\u0026gt; \u0026lt;city\u0026gt;[CITY]\u0026lt;/city\u0026gt; \u0026lt;state\u0026gt;[STATE]\u0026lt;/state\u0026gt; \u0026lt;country\u0026gt;[COUNTRY]\u0026lt;/country\u0026gt; \u0026lt;postcode\u0026gt;[POSTCODE]\u0026lt;/postcode\u0026gt; \u0026lt;start_date\u0026gt;[START_DATE]\u0026lt;/start_date\u0026gt; \u0026lt;end_date\u0026gt;[END_DATE]\u0026lt;/end_date\u0026gt; \u0026lt;/address\u0026gt; \u0026lt;/affiliation\u0026gt; \u0026lt;note type=\u0026#34;address\u0026#34;\u0026gt;[ROOM_NUMBER] [BUILDING] [CAMPUS]\u0026lt;/note\u0026gt; \u0026lt;identifier type=\u0026#34;u1\u0026#34;\u0026gt;[IDENTIFIER]\u0026lt;/identifier\u0026gt; \u0026lt;note type=\u0026#34;status\u0026#34;\u0026gt;[STATUS]\u0026lt;/note\u0026gt; \u0026lt;/mads\u0026gt; Documentation Further documentation for this module is available at our wiki.\nTroubleshooting/Issues Having problems or solved a problem? Check out the Islandora google groups for a solution.\nIslandora Group Islandora Dev Group Maintainers/Sponsors Current maintainers:\nRosie Le Faive Development If you would like to contribute to this module, please check out our helpful Documentation for Developers info, as well as our Developers section on the Islandora.ca site.\nLicense GPLv3\nUnfortunately, it\u0026rsquo;s not clear if such a bulk ingest will still work properly with the new Islandora Scholar Extended Profiles module in-play? Also, there\u0026rsquo;s no mention of similar capability for organizations, and it would be nice to have a bulk ingest process that works for both. So, I\u0026rsquo;m going to play with \u0026ldquo;manual\u0026rdquo; creation of a few entities, a couple organizations and a couple of profiles, and then see if I can determine what it might take to auto-populate both kinds of objects using an IMI import.\nSuggestions for Profile and Organization Identifiers It appears that creation and use of \u0026ldquo;U1\u0026rdquo; and \u0026ldquo;U2\u0026rdquo; identifiers is key to making objects (scholarly works) properly associate with profiles (authors and contributors), and profiles (people) properly associate with organizations (departments, etc.). So, I\u0026rsquo;d like to suggest that we use a person\u0026rsquo;s Grinnell College email address prefix, the portion before @grinnell.edu, as their identifier. My identifier, for example, would be mcfatem, taken from my college email address which is mcfatem@grinnell.edu.\nUnfortunately, I\u0026rsquo;m not quite sure what to suggest for our organization identifiers, and we may need to create a policy for handling persons who do not have an @grinnell.edu address.\nAnd that\u0026rsquo;s a wrap. Until next time, always remember to \u0026ldquo;Use your entities, Luke\u0026rdquo;.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/102-digital-grinnell-entities/","tags":["Entities","person","organization","event","place","profile","islandora:entityCModel","Islandora Scholar Extended Profiles","LASIR"],"title":"Everything You Ever Wanted to Know About Digital.Grinnell Entities, but were Afraid to Ask"},{"categories":null,"contents":"It\u0026rsquo;s high-time this was posted to my blog, but the canonical copy of this document can be found in smb://Storage/LIBRARY/mcfatem/DG-Book-Ingest-Workflow.md.\nValid Book Datastream Structure I want to begin here by showing what I see as a \u0026ldquo;proper\u0026rdquo; working book datastream structure in Digital.Grinnell. The image below is a screen grab of the datastreams from the Grinnell College Yearbook 1961, DG object grinnell:23749:\nFigure 1 \u0026middot; Valid Book Datastreams Creating a Valid Book Structure One of the biggest problems I have encountered with ingest of books is uploading very large multi-page PDFs. Fortunately, I\u0026rsquo;ve crafted the following procedure for working around that limitation.\nProcedure On the host workstation (DGDocker1 in the case of Digital.Grinnell) open a command terminal and\u0026hellip;\nMount the .pdf file(s) representing the book(s) into the host\u0026rsquo;s /mnt/storage folder using something like: sudo mount -t cifs -o username=mcfatem //storage.grinnell.edu/MEDIADB/DGIngest /mnt/storage Find or create an empty book.pdf file on the host using something like: touch ~/book.pdf Open a browser from your local workstation and\u0026hellip;\nLogin to https://digital.grinnell.edu as an admin (Library Staff for example). Navigate your browser to the book\u0026rsquo;s intended parent collection object in DG. Append /manage to the end of the parent object address and return. Click the link to Add an object to this Collection. Choose the Islandora Internet Archive Book Content Model content type. Enter necessary MODS metadata and submit the form. In the PDF Upload field navigate to the book.pdf file created in Step 2 and upload it. The file should now appear in the isle-apache-dg container as /tmp/book.pdf Return to the open command terminal and\u0026hellip;\nCopy the book\u0026rsquo;s actual .pdf file into the isle-apache-dg container like so: docker cp /mnt/storage/name-of-book.pdf isle-apache-dg:/tmp/book.pdf When the docker cp command is finished return to your browser and\u0026hellip;\nComplete the process by clicking Submit at the end of the form. Now sit back and watch the magic.\nIncomplete Book\u0026hellip; Orphaned pages Unfortunately, I also have some \u0026ldquo;broken\u0026rdquo; books that have a slew of ingested book pages all pointing to the wrong parent. One of my entries in Trello, for the 1962 Cyclone relates to one such case:\ngrinnell:25521 - 25862 [Pages but NO Book!] Empty book object is grinnell:23747 The problem in the case of grinnell:23747 is twofold:\nThat book object has no PDF - This condition can be corrected simply by uploading the appropriate PDF file as a new PDF datastream in the book/parent object. The pages/children of that book/parent all incorrectly reference grinnell:25520 as their book/parent - Fortunately, there\u0026rsquo;s an easy fix for this too, see below. Since grinnell:25520 never ingested properly all of the pages are effectively \u0026ldquo;orphans\u0026rdquo;. In the case of the 1962 Cyclone I used this command running inside the isle-apache-dg container:\nroot@28eb71ea69bf:/var/www/html/sites/default# drush -u 1 iduF grinnell:25521-25862 ChangeText --find=\u0026#34;grinnell:25520\u0026#34; --replace=\u0026#34;grinnell:23747\u0026#34; --dsid=RELS-EXT ","permalink":"https://static.grinnell.edu/dlad-blog/posts/097-book-ingest-in-digital-grinnell/","tags":["Book","Ingest","Workflow","Digital.Grinnell"],"title":"Book Ingest in Digital.Grinnell"},{"categories":null,"contents":"I seem to have a never-ending struggle with git submodules. Today, I need to add some new features to one of my submodules in a non-ISLE project, but I\u0026rsquo;ve elected to post this here because this blog is relatively easy to search. I also feel fortunate to have found Mastering Git Submodules from Christophe Porteneuve.\nSo, what I need to do now is update some submodule code, commit, and push that change back to its remote. I will also want to subsequentmly update a pair of projects that use the submodule so they are referencing the newest submodule code.\nIn case Christophe\u0026rsquo;s post ever disappers, here\u0026rsquo;s the key portion that I need right now\u0026hellip;\nUpdating a Submodule Inside Container Code git submodule update \u0026ndash;remote \u0026ndash;rebase \u0026ndash; path/to/module cd path/to/module Local work, testing, eventually staging git commit -am \u0026ldquo;Update to central submodule: blah blah\u0026rdquo; git push cd - git commit -am \u0026ldquo;Updated submodule X to: blah blah\u0026rdquo; Updating a Project To Use Latest Submodule Code Shortly after updating my submodule code and pushing that to GitHub, I naturally wanted to update one or two of my projects to use that latest version of the submodule. Hmmm, what\u0026rsquo;s the best way to do that? Luckily I found this reply and the guidance there worked nicely. In case the post or reply is ever lost, here\u0026rsquo;s the important parts:\nThe git submodule update command actually tells git that you simply want your submodules to each check out the commit already mentioned in the index of the superproject.\nIf you want to update your submodules to the most recent commit available from their remote, you\u0026rsquo;ll try this directly within the submodules.\nSo in summary:\n# Get the submodule initially git submodule add ssh://bla submodule_dir git submodule init # Time passes, submodule upstream is updated and you now want to update # Change to the submodule directory cd submodule_dir # Checkout desired branch git checkout master # Update git pull # Get back to your project root cd... # now the submodules are in the state you wanted, so git commit -am \u0026#34;Pulled down update to submodule_dir\u0026#34; And that\u0026rsquo;s a wrap. Hope this helps others as much as it helped me.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/100-git-submodule-tips/","tags":["Git","Submodule","Workflow","Commit"],"title":"Git Submodule Tips"},{"categories":null,"contents":"Lately my passwordless, SSH logins to all my servers have quit working, at least they seem to have stopped working from the only accessible \u0026ldquo;work\u0026rdquo; workstation that I have at the moment, MA7053. Since our enterprise malware mitigation agent, Traps, is blocking my access to my \u0026ldquo;real\u0026rdquo; workstation, MA8660, this has become more than just a nuisance.\nSo here\u0026rsquo;s what I came up with as a process to try and determine exactly where the problems are\u0026hellip;\nTo Debug SSHD Issues with Key Logins From a terminal opened in the target (CentOS 7 in this example) server:\nsudo su # stop the sshd service systemctl stop sshd.service # as root, restart the sshd service in DEBUG mode. Note that your terminal will NOT return, it\u0026#39;s spooling debug output /usr/sbin/sshd -d # attempt to connect again and look for DEBUG output in your terminal window # once resolved, ctrl-c to kill the above process, then be sure to restart sshd like so: systemctl start sshd.service To Create and Engage a New SSH Key On your local workstation open a terminal and enter the following with defaults and NO password or phrase: ssh-keygen Next, using the islandora user at dgdocker1.grinnell.edu as an example, enter the following to copy the key to the target server: ssh-copy-id -i ~/.ssh/id_rsa islandora@dgdocker1.grinnell.edu My DGAdmin Experience Today, January 26, 2021, I set out to configure a new server, namely dgadmin.grinnell.edu. After I\u0026rsquo;d done all of the above to set the server up for ssh/pubkey authentication it still would not work. I subsequently opened a help ticket and my esteemed colleague, Mike Conner, came to my rescue. Mike\u0026rsquo;s response to my ticket included this:\nIs the private key corresponding to the public key in /home/administrator/.ssh/authorized_keys loaded in your ssh-agent? You can specify which private key to use for the connection using the -i flag:\nssh -i /path/to/id_rsa administrator@dgadmin.grinnell.edu\nYou can also debug using the verbose flag in your ssh command:\nssh -i /path/to/id_rsa -vvv administrator@dgadmin.grinnell.edu\nMike hit the nail on the head, my ssh-agent must have been using a diffeent pubkey. I executed the command that Mike had suggested and it worked. Specifically that command was: ssh -i ~/.ssh/id_rsa -vvv administrator@dgadmin.grinnell.edu.\nExtras Are No Longer Necessary After the above command logged me in without a password I ran one more test. Would I always need to run my ssh commands in that form? No, I found that once I had run that command successfully the ssh-agent remembers the correct pubkey to use so subsequent logins can use just ssh administrator@dgadmin.grinnell.edu.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/073-debugging-ssh-login-issues/","tags":["ssh","sshd","ssh-keygen","ssh-copy-id"],"title":"Debugging SSH Login Issues"},{"categories":null,"contents":"In hindsight, this really should have been post number 100 in the blog, or perhaps it should have happened even earlier. In any event it\u0026rsquo;s high-time I made this move and captured the process. In June 2020 the good folks at GitHub announced that they would begin removing references to master as a small step forward in removing divisive language in tech. The change dictates that the default branch name of future repositories should be main in place of master.\nToday, I am dictating that the same should be true for ALL of my repositories, old and new alike. Besides, I find it very confusing to have some defaults using one name while others do not. To begin this transformation of old to new, I elected to start here with this blog.\nMoving master to main In support of this effort I went looking for sound guidance and found 5 steps to change GitHub default branch from master to main. In case that post ever disappears, here are the key elements:\nAll commands\n# Step 1 # create main branch locally, taking the history from master git branch -m master main # Step 2 # push the new local main branch to the remote repo (GitHub) git push -u origin main # Step 3 # switch the current HEAD to the main branch git symbolic-ref refs/remotes/origin/HEAD refs/remotes/origin/main # Step 4 # change the default branch on GitHub to main # https://docs.github.com/en/github/administering-a-repository/setting-the-default-branch # Step 5 # delete the master branch on the remote git push origin --delete master If main Already Exists In cases, like this blog, where a main branch already exists, a couple of preemptive steps must be taken. I found some useful and brief TL;DR version guidance, and here again are the key parts:\nTL;DR version\n// delete branch locally git branch -d localBranchName // delete branch remotely git push origin --delete remoteBranchName And that\u0026rsquo;s a wrap. Until next time, I hope this encourages others to take similar action.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/101-changing-master-to-main/","tags":["Git","master","main","inclusive"],"title":"Changing Master to Main"},{"categories":null,"contents":"This afternoon I finally got back to trying to implement some features of LASIR, the Liberal Arts Sprint for Institutional Repositories. The information I found initially was not what I expected to encounter, and that left me scratching my head just a bit. Fortunately, a colleague at the ICG came through with the needed information and I immediately added it to my list of key resources. You can find those resources in my OneTab at:\nhttps://www.one-tab.com/page/9OatzLo-TB6XuuizqbGrwA And that\u0026rsquo;s a\u0026hellip; break. I hope to add some details, and more links, when I return.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/099-adding-lasir-features-to-dg/","tags":["LASIR","dg.localdomain"],"title":"Adding LASIR Features to DG"},{"categories":null,"contents":"In the past few days I\u0026rsquo;ve attempted to update the \u0026ldquo;staging\u0026rdquo; copy of Digital.Grinnell at https://dg-staging.grinnell.edu and learned a valuable lesson regarding workflow around Git and submodules. Specifically, I found the following resource to be most helpful:\nhttps://intellipaat.com/community/9971/git-update-submodule-to-latest-commit-on-origin In case that post ever disappers, here\u0026rsquo;s the gist of it\u0026hellip;\nThe git submodule update command actually tells git that you simply want your submodules to each check out the commit already mentioned in the index of the superproject.\nIf you want to update your submodules to the most recent commit available from their remote, you\u0026rsquo;ll try this directly within the submodules.\nSo in summary:\n# Get the submodule initially git submodule add ssh://bla submodule_dir git submodule init # Time passes, submodule upstream is updated # and you now want to update # Change to the submodule directory cd submodule_dir # Checkout desired branch git checkout master # Update git pull # Get back to your project root cd... # now the submodules are in the state you wanted, so git commit -am \u0026quot;Pulled down update to submodule_dir\u0026quot; Or, if you\u0026rsquo;re a busy person:\ngit submodule foreach git pull origin master And that\u0026rsquo;s a\u0026hellip; break. When I return I hope to add some DG specifics.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/098-git-submodule-workflow-in-isle/","tags":["Git","Submodule","Workflow","ISLE"],"title":"Git Submodule Workflow in ISLE"},{"categories":null,"contents":" This post is an addendum to earlier post 092. It is intended to chronicle my efforts to complete migration of our production instance of Digital.Grinnell from ISLE v1.1 to ISLE v1.5.1 on Linux node DGDocker1.grinnell.edu. The remainder of this document is an annotated copy of Production ISLE Installation: Migrate Existing Islandora Site.\nAnnotations, with information specific to my experience with Digital.Grinnell migration, appear in text blocks like this one.\nProduction ISLE Installation: Migrate Existing Islandora Site Expectations: It takes an average of 2-4+ hours to read this documentation and complete this installation.\nThis Production ISLE Installation will be similar to the Local ISLE Installation: Migrate Existing Islandora Site instructions you just followed but in addition to using a copy of your currently running Production themed Drupal website, a copy of the Production Fedora repository will also be needed for you to continue migrating to ISLE with the end goal of first deploying to an ISLE Production environment and then cut over from the existing non-ISLE Production and Production servers to their new ISLE counterparts.\nIslandora Drupal site code here should be finished and ready for public consumption. Fedora data will be a mirror of your currently running non-ISLE Production Fedora repository. It is recommended that this remote site not be publicly accessible until you are ready to cutover and give public access.\nThis installation builds a Production environment for the express purpose of migrating a previously existing Islandora site onto the ISLE platform. If you need to build a brand new Production site for development and are not migrating an existing Islandora site, then please stop and use the Production ISLE Installation: New Site instead.\nAs this Production domain will require a real domain name or FQDN, you will need to ask your IT department or appropriate resource for an \u0026ldquo;A record\u0026rdquo; to be added for your domain to \u0026ldquo;point\u0026rdquo; to your Production Host Server IP address in your institution\u0026rsquo;s DNS records.\nExample: https://yourprojectnamehere.institution.edu\nOnce this has been completed, if you do not want to use Let\u0026rsquo;s Encrypt, you can also request commercial SSL certificates from your IT department for this domain as well. Please note the DNS records will need to exist prior to the creation of any SSL certificate (Commercial or Let\u0026rsquo;s Encrypt.)\nIf you already have pre-existing Production commercial SSL certificates, they can certainly be reused and copied into the ISLE project as directed. Unlike the Local and Demo setups, you will not have to edit /etc/localhosts to view your domain given that DNS is now involved. Your new domain will no longer use the .localdomain but instead something like https://yourprojectnamehere.institution.edu\nThis document also has directions on how you can save newly updated ISLE code into a git software repository as a workflow process designed to manage and upgrade the environments throughout the development process from Local to Production and finally to Production. The ISLE Installation: Environments documentation can also help with explaining the new ISLE structure, the associated files and what values ISLE end-users should use for the .env, production.env, etc.\nThis document does not have directions on how you can save previously existing Islandora Drupal code into a git repository and assumes this step has already happened. The directions below will explain how to clone Islandora Drupal code from a previously existing Islandora Drupal git repository that should already be accessible to you.\nPlease post questions to the public Islandora ISLE Google group, or subscribe to receive emails. The Glossary defines terms used in this documentation.\nAssumptions / Prerequisites This Production ISLE installation is intended for an existing Production Islandora Drupal site to be imported along with a copy of the current Production Fedora Repository for further ISLE migration testing, Drupal theme development, ingest testing, etc. on a remote ISLE host server.\nSome materials are to be \u0026ldquo;migrated\u0026rdquo; from the work you performed on your personal computer from the prior steps and processes in Local ISLE Installation: Migrate Existing Islandora Site instructions. You will be using ISLE version 1.2.0 or higher.\nYou are using Docker-compose 1.24.0 or higher.\nYou have git installed on your personal computer as well as the remote ISLE host server.\nYou have already provisioned a remote ISLE hosts server and have documented its IP address. * You may have used the ISLE Ansible script to accomplish this. * If doing this manually, please review the following to ensure the remote Production ISLE host server has all dependencies e.g. CPU, memory and disk space prior to deploying the ISLE Production environment profile for deploy * Hardware Requirements * Software Dependencies * This server should be running at the time of deploy. * Critical - This Production server has the same amount of disk space as your current Production Fedora server does in order to store a copy of the Fedora repository. Please ensure that these sizes match. Please also plan on adding additional capacity as needed for any potential ingest testing, etc.\nYou have a previously existing private Islandora Drupal git repository.\nYou have access to a private git repository in Github, Bitbucket, Gitlab, etc.\nIf you do not, please contact your IT department for git resources, or else create an account with one of the above providers. WARNING: Only use Private git repositories given the sensitive nature of the configuration files. DO NOT share these git repositories publicly. You have already have the appropriate A record entered into your institutions DNS system and can resolve the Production domain (https://yourprojectnamehere.institution.edu) using a tool like https://www.whatsmydns.net/\nRECOMMENDATION: We recommend that you use a temporary domain name e.g. https://yourprojectnamehere-newprod.institution.edu to check the new Production ISLE site while an existing non-ISLE Production site is still running and compare for differences. This will also allow end users to still access the old Production system while the work for the new ISLE system is ongoing.\nYou\u0026rsquo;ll need to add an additional A record for yourprojectnamehere-newprod.institution.edu to point to the same ISLE Production Host Server IP. Please adjust all involved domain names and configuration files accordingly. When ready to launch the new ISLE production site publicly, you can remove the -newprod from all domain references and configuration files. Update the DNS records to repoint the current non-ISLE Production server A record for yourprojectnamehere.institution.edu to the new ISLE host server IP. Remove the temporary DNS A record for yourprojectnamehere-newprod.institution.edu If using commercial SSLs, you\u0026rsquo;ll also need to copy them over to the ./config/proxy/ssl-certs directory and adjust the traefik.production.toml file accordingly with the new file names. You have reviewed the ISLE Installation: Environments for more information about suggested Production values.\nYou are familiar with using tools like scp, cp or rsync to move configurations, files and data from your local to the remote Production server.\nYou have access to your Production Islandora Drupal, Solr and Fedora data and copy from your servers to the new ISLE Production server.\nYou will schedule a content freeze for all Production Fedora ingests and additions to your Production website. This will allow you to get up to date data from Production to Production.\nIndex of Instructions This process will differ slightly from previous builds in that there is work to be done on the local to then be pushed to the Production ISLE Host server with additional followup work to be performed on the remote Production ISLE Host server.\nThe instructions that follow below will have either a On Local or a On Remote Production pre-fix to indicate where the work and focus should be. In essence, the git workflow established during the local build process will be extended for deploying on Production and for future ISLE updates and upgrades.\nSteps 1-6: On Local - Configure the ISLE Production Environment Profile for Deploy to Remote\nStep 1: Copy Production Data to Your Local Step 2: On Local - Shutdown Any Local Containers \u0026amp; Review Local Code Step 3: On Local - Create New Users and Passwords by Editing \u0026ldquo;production.env\u0026rdquo; Step 4: On Local - Review and Edit \u0026ldquo;docker-compose.production.yml\u0026rdquo; Step 4A: On Local - (Optional) Changes for \u0026ldquo;docker-compose.production.yml\u0026rdquo; Step 5: On Local Production - If Using Commercial SSLs Step 6: On Local - Commit ISLE Code to Git Repository Steps 7-18: On Remote Production - Configure the ISLE Production Environment Profile for Launch and Usage\nStep 7: On Remote Production - Git Clone the ISLE Repository to the Remote Production ISLE Host Server Step 8: On Remote Production - Create the Appropriate Local Data Paths for Apache, Fedora and Log Data Step 9: On Remote Production - Clone Your Production Islandora Code Step 10: On Remote Production - Copy Over the Production Data Directories Step 11: On Remote Production - If Using Let\u0026rsquo;s Encrypt Step 12: On Remote Production - Edit the \u0026ldquo;.env\u0026rdquo; File to Change to the Production Environment Step 13: On Remote Production - Download the ISLE Images Step 14: On Remote Production - Start Containers Step 15: On Remote Production - Import the Production MySQL Drupal Database Step 16: On Remote Production - Run ISLE Scripts Step 17: On Remote Production - Re-Index Fedora \u0026amp; Solr Step 18: On Remote Production - Review and Test the Drupal Production Site Step 1: Copy Production Data to Your Local Drupal Site Database You are repeating this step given that data may have changed on the Production site since creating your local. It is critical that Production be a mirror or close to exact copy of Production.\nPrior to attempting this step, please consider the following:\nDrupal website databases can have a multitude of names and conventions. Confer with the appropriate IT departments for your institution\u0026rsquo;s database naming conventions.\nRecommended that the production databases be exported using the .sql /or .gz file formats (e.g. \u0026ldquo;prod_drupal_site_082019.sql.gz\u0026rdquo;) for better compression and minimal storage footprint.\nIf the end user is running multi-sites, there will be additional databases to export.\nDo not export the fedora3 database\nIf possible, on the production Apache web server, run drush cc all from the command line on the production server in the /var/www/html directory PRIOR to any db export(s). Otherwise issues can occur on import due to all cache tables being larger than innodb_log_file_size allows\nExport the Production MySQL Islandora Drupal Database Export the MySQL database for the current Production Islandora Drupal site in use and copy it to your local in an easy to find place. In later steps you\u0026rsquo;ll be directed to import this file. Please be careful performing any of these potential actions below as the process impacts your Production site. If you are not comfortable or familiar with performing these actions, we recommend that you instead work with your available IT resources to do so. To complete this process, you may use a MySQL GUI client or, if you have command line access to the MySQL database server, you may run the following command, substituting your actual user and database names: Example: mysqldump -u username -p database_name \u0026gt; prod_drupal_site_082019.sql Copy this file down to your personal computer. I started this process by creating an intermediate storage directory on DGDocker1 at ~/DG-PROD/data, that\u0026rsquo;s /home/islandora/DG-PROD/data. I subsequently used the following commands to dump the production database and capture necessary artifacts from production:\nmkdir -p ~/DG-PROD/files rsync -aruvi /opt/ISLE/persistent/html/sites/default/files/. ~/DG-PROD/data/files/. --progress mkdir -p ~/DG-PROD/data/config/proxy cp -fr /opt/ISLE/config/proxy/. ~/DG-PROD/data/config/proxy docker exec -it isle-mysql-dg bash mysqldump -u isle_dg_user -p isle_dg \u0026gt; prod_drupal_site_112220.sql exit docker cp isle-mysql-dg:prod_drupal_site_112220.sql ~/DG-PROD/data/prod_drupal_site_112220.sql Step 2: On Local - Shutdown Any Local Containers \u0026amp; Review Local Code I found this step to be UNECESSARY since my dg-isle and dg-islandora repositories have already been prepared. However, see below\u0026hellip;\nEnsure that your ISLE and Islandora git repositories have all the latest commits and pushes from the development process that took place on your personal computer. If you haven\u0026rsquo;t yet finished, do not proceed until everything is completed.\nOnce finished, open a terminal (Windows: open Git Bash)\nNavigate to your Local ISLE repository Shut down any local containers e.g. docker-compose down I did take this opportunity to docker-compose down from DGDocker1 in the /opt/ISLE/ directory. Unfortunately, the command was unable to stop the isle-images-dg container, and failed to stop any others apart from Portaioner. The output was:\nERROR: for isle-images-dg UnixHTTPConnectionPool(host=\u0026#39;localhost\u0026#39;, port=None): Read timed out. (read timeout=70) ERROR: An HTTP request took too long to complete. Retry with --verbose to obtain debug information. If you encounter this issue regularly because of slow network conditions, consider setting COMPOSE_HTTP_TIMEOUT to a higher value (current value: 60). Even with COMPOSE_HTTP_TIMEOUT set to a much higher value, I was unable to stop the stack with docker-compose down. So I tried docker stop 063, where \u0026ldquo;063\u0026rdquo; was the first three digits of the isle-images-dg container. This also FAILED!\nAs a last resort I tried and failed, again, with:\n[islandora@dgdocker1 ISLE]$ docker rm -f 063312b1d155 Error response from daemon: removal of container 063312b1d155 is already in progress So, I consulted StackOverflow and found a post that suggested the following:\n[islandora@dgdocker1 ISLE]$ ps aux | grep 063312b1d155 root 5563 0.0 0.0 108744 6636 ? Sl May05 8:16 containerd-shim -namespace moby -workdir /var/lib/containerd/io.containerd.runtime.v1.linux/moby/063312b1d155e129740c040d47cb84a01556c1cc1c89821d695cb314ac7415da -address /run/containerd/containerd.sock -containerd-binary /usr/bin/containerd -runtime-root /var/run/docker/runtime-runc islando+ 108134 0.0 0.0 112716 968 pts/2 S+ 16:57 0:00 grep --color=auto 063312b1d155 And that lead me to try:\n[islandora@dgdocker1 ISLE]$ sudo kill -9 5563 [islandora@dgdocker1 ISLE]$ docker-compose down Stopping isle-apache-dg ... Stopping isle-fedora-dg ... error Stopping isle-solr-dg ... error Stopping isle-mysql-dg ... error Stopping isle-proxy-dg ... error ERROR: for isle-apache-dg UnixHTTPConnectionPool(host=\u0026#39;localhost\u0026#39;, port=None): Read timed out. (read timeout=70) ERROR: An HTTP request took too long to complete. Retry with --verbose to obtain debug information. If you encounter this issue regularly because of slow network conditions, consider setting COMPOSE_HTTP_TIMEOUT to a higher value (current value: 60). Having exhausted all reasonable options, I elected to use vCenter to reboot the DGDocker1 server. But first, for safe-keeping\u0026hellip;\nsudo rsync -aruvi ~/DG-PROD/. /mnt/storage/DG-PROD/. --progress After this I shut down the guest OS on DGDocker1 using vCenter, then restarted it, and the server came back but without running any Docker containers. So I did sudo yum update, then sudo yum upgrade, and rebooted. The server now reports that it is up-to-date and running this:\nServer: Docker Engine - Community Engine: Version: 19.03.13 API version: 1.40 (minimum version 1.12) Go version: go1.13.15 Git commit: 4484c46d9d Built: Wed Sep 16 17:02:21 2020 OS/Arch: linux/amd64 Experimental: false Step 3: On Local - Create New Users and Passwords by Editing \u0026ldquo;production.env\u0026rdquo; I found this step to be UNECESSARY since my dg-isle and dg-islandora repositories have already been prepared.\nOpen the \u0026ldquo;production.env\u0026rdquo; file in a text editor.\nFind each comment that begins with: # Replace this comment with a ... and follow the commented instructions to edit the passwords, database and user names. Review carefully as some comments request that you replace with ...26 alpha-numeric characters while others request that you create an ...easy to read but short database name. It is okay if you potentially repeat the values previously entered for your local (DRUPAL_DB) \u0026amp; (DRUPAL_DB_USER) in this Production environment but we strongly recommend not reusing all passwords for environments (e.g. (DRUPAL_DB_PASS) \u0026amp; (DRUPAL_HASH_SALT) should be unique values for each environment). In many cases the username is already pre-populated. If it doesn\u0026rsquo;t have a comment directing you to change or add a value after the =, then don\u0026rsquo;t change it. Once finished, save and close the file. Open the config/apache/settings_php/settings.production.php file.\nFind the first comment that begins with: # ISLE Configuration and follow the commented instructions to edit the database, username and password. Find the second comment that begins with: # ISLE Configuration and follow the instructions to edit the Drupal hash salt. Once finished, save and close the file. Step 4: On Local - Review and Edit \u0026ldquo;docker-compose.production.yml\u0026rdquo; I found this step to be UNECESSARY since my dg-isle and dg-islandora repositories have already been prepared.\nReview the disks and volumes on your remote Production ISLE Host server to ensure they are of an adequate capacity for your collection needs and match what has been written in the docker-compose.production.yml file.\nPlease read through the docker-compose.production.yml file as there are bind mount points that need to be configured on the host machine, to ensure data persistence. There are suggested bind mounts that the end-user can change to fit their needs or they can setup additional volumes or disks to match the suggestions.\nIn the fedora services section - /mnt/data/fedora/datastreamStore:/usr/local/fedora/data/datastreamStore - /opt/data/fedora/objectStore:/usr/local/fedora/data/objectStore In the apache services section - /opt/data/apache/html:/var/www/html Review the your docker-compose.local.yml file for custom edits made and copy them to the docker-compose.production.yml file as needed, this can include changes to Fedora GSearch Transforms, Fedora hash size and more.\nSSL Certificates Depending on your choice of SSL type (Commercial SSL files or the Let\u0026rsquo;s Encrypt service), you\u0026rsquo;ll need to uncomment only one line of the traefik services section. There are also inline instructions to this effect in the docker-compose.production.yml file.\nTo use Let's Encrypt for SSL, uncomment:\n- ./config/proxy/acme.json:/acme.json To use commercial SSLs, uncomment:\n./config/proxy/ssl-certs:/certs:ro Additionally you\u0026rsquo;ll need to add your SSL certs (.cert, .pem, .key) files to config/proxy/ssl-certs Based on the choice of SSL type made above, you\u0026rsquo;ll need to refer to the /config/proxy/traefik.production.toml file for further configuration instructions.\nStep 4A: On Local - (Optional) Changes for \u0026ldquo;docker-compose.production.yml\u0026rdquo; I found this step to be UNECESSARY since my dg-isle and dg-islandora repositories have already been prepared.\nThis section is for optional changes for the docker-compose.production.yml, end-users do not have feel like they have to make any choices here and can continue to Step 4 as needed.\nThe options include PHP settings, Java Memory Allocation, MySQL configuration and use of the TICK Stack\n(Optional) - You can change PHP settings such as file upload limits and memory usage by uncommenting the following in the apache services section.\n- ./config/apache/php_ini/php.production.ini:/etc/php/7.1/apache2/php.ini You\u0026rsquo;ll then need to make edits in the ./config/apache/php_ini/php.production.ini file. (Optional) - This line is already uncommented by default in ISLE but we\u0026rsquo;re calling it out here that you can changes to the suggested levels or values within the ./config/mysql/ISLE.cnf file if needed. When setting up for the first time, it is best practice to leave these settings in place. Over time, you can experiment with further tuning and experimentation based on your project or system needs.\n(Optional) - You can change the suggested JAVA_MAX_MEM \u0026amp; JAVA_MIN_MEM levels but do not exceed more than 50% of your system memory. When setting up for the first time, it is best practice to leave these settings in place as they are configured for a Production ISLE Host Server using 16 GB of RAM. Over time, you can experiment with further tuning and experimentation based on your project or system needs.\n(Optional) - You can opt to uncomment the TICK stack settings for monitoring but you\u0026rsquo;ll need to follow the TICK Stack instructions prior to committing changes to your ISLE git repository.\nAll TICK related code can be found at the end of all ISLE services within the docker-compose.production.yml file. Example: ## _(Optional)_: Uncomment lines below to run ISLE with the TICK monitoring system logging: driver: syslog options: tag: \u0026#34;{{.Name}}\u0026#34; Uncomment the lines found in the new TICK stack services section of the docker-compose.production.yml file for hosting of that monitoring service on the Production ISLE Host server. There are additional configurations to be made to files contained within ./config/tick but you\u0026rsquo;ll need to follow the TICK Stack instructions prior to committing changes to your ISLE git repository. Uncomment the TICK stack data volumes as well at the bottom of the file. Step 5: On Local Production - If Using Commercial SSLs I found this step to be UNECESSARY since my dg-isle and dg-islandora repositories have already been prepared.\nIf you are going to use Let\u0026rsquo;s Encrypt instead, you can skip this step and move onto the next one. There will be additional steps further in this document, to help you configure it.\nIf you have decided to use Commercial SSL certs supplied to you by your IT team or appropriate resource, please continue following this step.\nAdd your Commercial SSL certificate and key files to the ./config/proxy/ssl-certs directory\nExample: ./config/proxy/ssl-certs/yourprojectnamehere.domain.cert ./config/proxy/ssl-certs/yourprojectnamehere.domain.key Edit the ./config/proxy/traefik.production.toml and follow the in-line instructions. Replace the .pem \u0026amp; .key with the name of your Production SSL certificate and associated key. Do note the positioning of the added lines. Third character indentation.\nNote: despite the instruction examples differing on file type, (.pem or cert), either one is compatible, use what you have been given. Merely change the file type suffix accordingly.\nExample: .cert\n[entryPoints.https.tls] [[entryPoints.https.tls.certificates]] certFile = \u0026#34;/certs/yourprojectnamehere.domain.cert\u0026#34; keyFile = \u0026#34;/certs/yourprojectnamehere.domain.key\u0026#34; Example: .pem\n[entryPoints.https.tls] [[entryPoints.https.tls.certificates]] certFile = \u0026#34;/certs/yourprojectnamehere.institution.edu.pem\u0026#34; keyFile = \u0026#34;/certs/yourprojectnamehere.institution.edu.key\u0026#34; Step 6: On Local - Commit ISLE Code to Git Repository I found this step to be UNECESSARY since my dg-isle and dg-islandora repositories have already been prepared.\nOnce you have made all of the appropriate changes to your Production profile. Please note the steps below are suggestions. You might use a different git commit message. Substitute \u0026lt;changedfileshere\u0026gt; with the actual file names and paths. You may need to do this repeatedly prior to the commit message. git add \u0026lt;changedfileshere\u0026gt; git commit -m \u0026quot;Changes for Production\u0026quot; git push origin master On Remote Production - Configure the ISLE Production Environment Profile for Launch and Usage Step 7: On Remote Production - Git Clone the ISLE Repository to the Remote Production ISLE Host Server This assumes you have setup an Islandora deploy user. If not use a different non-root user for this purpose.\nYou will also need to ensure that any /home/islandora/.ssh/id_rsa.pub key has been added to your git repository admin panel to allow for cloning from your two private git repositories.\nSince the /opt directory might not let you do this at first, we suggest the following workaround which you\u0026rsquo;ll only need to do once. Future ISLE updates will not require this step.\nShell into your Production ISLE host server as the Islandora user.\nClone your ISLE project repository with the newly committed changes for Production to the Islandora user home directory.\ngit clone https://yourgitproviderhere.com/yourinstitutionhere/yourprojectnamehere-isle.git /home/islandora/ This may take a few minutes (2-4) depending on your server\u0026rsquo;s Internet connection. Move the newly cloned directory to the /opt directory as the root user\nsudo mv /home/islandora/yourprojectnamehere-isle /opt/yourprojectnamehere-isle Fix the permissions so that the islandora user has access.\nsudo chown -Rv islandora:islandora /opt/yourprojectnamehere-isle Important!\nThe /opt/ISLE directory on DGDocker1 is already configured to allow the islandora user to read, write, and execute, so I moved the existing copy of that directory, as well as the production_data directory, to /opt/.out-of-the-way and created a new /opt/ISLE directory owned by islandora:islandora. I will use /opt/ISLE as my project \u0026ldquo;root\u0026rdquo;, in place of /opt.\nSo, I did this\u0026hellip;\n[islandora@dgdocker1 opt]$ cd /opt/ISLE [islandora@dgdocker1 ISLE]$ git clone --recursive https://github.com/Digital-Grinnell/dg-isle Cloning into \u0026#39;dg-isle\u0026#39;... Username for \u0026#39;https://github.com\u0026#39;: digital@grinnell.edu Password for \u0026#39;https://digital@grinnell.edu@github.com\u0026#39;: remote: Enumerating objects: 5172, done. remote: Counting objects: 100% (5172/5172), done. remote: Compressing objects: 100% (1853/1853), done. remote: Total 5172 (delta 3277), reused 5072 (delta 3183), pack-reused 0 Receiving objects: 100% (5172/5172), 7.25 MiB | 0 bytes/s, done. Resolving deltas: 100% (3277/3277), done. Submodule \u0026#39;data/apache/html/sites/all/themes/bootstrap\u0026#39; (https://github.com/drupalprojects/bootstrap.git) registered for path \u0026#39;data/apache/html/sites/all/themes/bootstrap\u0026#39; Submodule \u0026#39;data/apache/html/sites/all/themes/digital_grinnell_bootstrap\u0026#39; (https://github.com/DigitalGrinnell/digital_grinnell_bootstrap.git) registered for path \u0026#39;data/apache/html/sites/all/themes/digital_grinnell_bootstrap\u0026#39; Submodule \u0026#39;data/apache/html/sites/default/themes/digital_grinnell_bootstrap\u0026#39; (https://github.com/DigitalGrinnell/digital_grinnell_bootstrap.git) registered for path \u0026#39;data/apache/html/sites/default/themes/digital_grinnell_bootstrap\u0026#39; Cloning into \u0026#39;data/apache/html/sites/all/themes/bootstrap\u0026#39;... remote: Enumerating objects: 12635, done. remote: Total 12635 (delta 0), reused 0 (delta 0), pack-reused 12635 Receiving objects: 100% (12635/12635), 2.83 MiB | 0 bytes/s, done. Resolving deltas: 100% (9255/9255), done. Submodule path \u0026#39;data/apache/html/sites/all/themes/bootstrap\u0026#39;: checked out \u0026#39;c050d1d6ae85ef344be85dbf0a4ed9ec68ac32ce\u0026#39; Cloning into \u0026#39;data/apache/html/sites/all/themes/digital_grinnell_bootstrap\u0026#39;... remote: Enumerating objects: 190, done. remote: Total 190 (delta 0), reused 0 (delta 0), pack-reused 190 Receiving objects: 100% (190/190), 1.08 MiB | 0 bytes/s, done. Resolving deltas: 100% (70/70), done. Submodule path \u0026#39;data/apache/html/sites/all/themes/digital_grinnell_bootstrap\u0026#39;: checked out \u0026#39;654a43649a5130fa998d549e4233f9736aa4cca6\u0026#39; Cloning into \u0026#39;data/apache/html/sites/default/themes/digital_grinnell_bootstrap\u0026#39;... remote: Enumerating objects: 190, done. remote: Total 190 (delta 0), reused 0 (delta 0), pack-reused 190 Receiving objects: 100% (190/190), 1.08 MiB | 0 bytes/s, done. Resolving deltas: 100% (70/70), done. Submodule path \u0026#39;data/apache/html/sites/default/themes/digital_grinnell_bootstrap\u0026#39;: checked out \u0026#39;654a43649a5130fa998d549e4233f9736aa4cca6\u0026#39; Step 8: On Remote Production - Create the Appropriate Local Data Paths for Apache, Fedora and Log Data Create the /opt/data directory sudo mkdir -p /opt/data Change the permissions to the Islandora user. sudo chown -Rv islandora:islandora /opt/data To adjust for my project root at /opt/ISLE, I did this instead:\nsudo mkdir -p /opt/ISLE/data sudo chown -Rv islandora:islandora /opt/ISLE/data Step 9: On Remote Production - Clone Your Production Islandora Code Please clone from your existing Production Islandora git repository.\ngit clone git@yourgitproviderhere.com/yourinstitutionhere/yourprojectnamehere-islandora.git /opt/data/apache/html\nFix the permissions so that the islandora user has access.\nsudo chown -Rv islandora:islandora /opt/yourprojectnamehere-islandora Again, to adjust for my project root, I did this:\n[islandora@dgdocker1 ~]$ git clone --recursive https://github.com/Digital-Grinnell/dg-islandora /opt/ISLE/data/apache/html Cloning into \u0026#39;/opt/ISLE/data/apache/html\u0026#39;... Username for \u0026#39;https://github.com\u0026#39;: digital@grinnell.edu Password for \u0026#39;https://digital@grinnell.edu@github.com\u0026#39;: remote: Enumerating objects: 646, done. remote: Counting objects: 100% (646/646), done. remote: Compressing objects: 100% (592/592), done. remote: Total 10452 (delta 118), reused 243 (delta 45), pack-reused 9806 Receiving objects: 100% (10452/10452), 63.00 MiB | 40.34 MiB/s, done. Resolving deltas: 100% (2212/2212), done. Submodule \u0026#39;sites/all/libraries/glip\u0026#39; (https://github.com/halstead/glip) registered for path \u0026#39;sites/all/libraries/glip\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/dg7\u0026#39; (https://github.com/DigitalGrinnell/dg7) registered for path \u0026#39;sites/all/modules/islandora/dg7\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/idu\u0026#39; (https://github.com/DigitalGrinnell/idu) registered for path \u0026#39;sites/all/modules/islandora/idu\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_binary_object\u0026#39; (https://github.com/Islandora-Labs/islandora_binary_object) registered for path \u0026#39;sites/all/modules/islandora/islandora_binary_object\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_collection_search\u0026#39; (https://github.com/discoverygarden/islandora_collection_search) registered for path \u0026#39;sites/all/modules/islandora/islandora_collection_search\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_datastream_exporter\u0026#39; (https://github.com/Islandora-Labs/islandora_datastream_exporter.git) registered for path \u0026#39;sites/all/modules/islandora/islandora_datastream_exporter\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_datastream_replace\u0026#39; (https://github.com/DigitalGrinnell/islandora_datastream_replace.git) registered for path \u0026#39;sites/all/modules/islandora/islandora_datastream_replace\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_mods_display\u0026#39; (https://github.com/DigitalGrinnell/islandora_mods_display.git) registered for path \u0026#39;sites/all/modules/islandora/islandora_mods_display\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_mods_via_twig\u0026#39; (https://github.com/DigitalGrinnell/islandora_mods_via_twig.git) registered for path \u0026#39;sites/all/modules/islandora/islandora_mods_via_twig\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_multi_importer\u0026#39; (https://github.com/DigitalGrinnell/islandora_multi_importer) registered for path \u0026#39;sites/all/modules/islandora/islandora_multi_importer\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_solr_collection_view\u0026#39; (https://github.com/Islandora-Labs/islandora_solr_collection_view.git) registered for path \u0026#39;sites/all/modules/islandora/islandora_solr_collection_view\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_solr_views\u0026#39; (https://github.com/DigitalGrinnell/islandora_solr_views) registered for path \u0026#39;sites/all/modules/islandora/islandora_solr_views\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_solution_pack_oralhistories\u0026#39; (https://github.com/Islandora-Labs/islandora_solution_pack_oralhistories.git) registered for path \u0026#39;sites/all/modules/islandora/islandora_solution_pack_oralhistories\u0026#39; Submodule \u0026#39;sites/all/themes/bootstrap\u0026#39; (https://github.com/drupalprojects/bootstrap.git) registered for path \u0026#39;sites/all/themes/bootstrap\u0026#39; Submodule \u0026#39;sites/default/themes/digital_grinnell_bootstrap\u0026#39; (https://github.com/DigitalGrinnell/digital_grinnell_bootstrap.git) registered for path \u0026#39;sites/default/themes/digital_grinnell_bootstrap\u0026#39; Cloning into \u0026#39;sites/all/libraries/glip\u0026#39;... remote: Enumerating objects: 319, done. remote: Total 319 (delta 0), reused 0 (delta 0), pack-reused 319 Receiving objects: 100% (319/319), 101.97 KiB | 0 bytes/s, done. Resolving deltas: 100% (163/163), done. Submodule path \u0026#39;sites/all/libraries/glip\u0026#39;: checked out \u0026#39;79f5472af4b9261d20f51e92f07d4cca01e83a2c\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/dg7\u0026#39;... remote: Enumerating objects: 335, done. remote: Total 335 (delta 0), reused 0 (delta 0), pack-reused 335 Receiving objects: 100% (335/335), 201.05 KiB | 0 bytes/s, done. Resolving deltas: 100% (203/203), done. Submodule path \u0026#39;sites/all/modules/islandora/dg7\u0026#39;: checked out \u0026#39;f6cca12904d24c57fcc408b93db92893a564e231\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/idu\u0026#39;... remote: Enumerating objects: 21, done. remote: Counting objects: 100% (21/21), done. remote: Compressing objects: 100% (15/15), done. remote: Total 64 (delta 10), reused 16 (delta 6), pack-reused 43 Unpacking objects: 100% (64/64), done. Submodule path \u0026#39;sites/all/modules/islandora/idu\u0026#39;: checked out \u0026#39;0d91bd6e563f2955563649fc9168c4e8e518f45c\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_binary_object\u0026#39;... remote: Enumerating objects: 353, done. remote: Total 353 (delta 0), reused 0 (delta 0), pack-reused 353 Receiving objects: 100% (353/353), 80.16 KiB | 0 bytes/s, done. Resolving deltas: 100% (187/187), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_binary_object\u0026#39;: checked out \u0026#39;53b67d57cf1ca8052910a90006ad0af183a23389\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_collection_search\u0026#39;... remote: Enumerating objects: 438, done. remote: Total 438 (delta 0), reused 0 (delta 0), pack-reused 438 Receiving objects: 100% (438/438), 83.91 KiB | 0 bytes/s, done. Resolving deltas: 100% (209/209), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_collection_search\u0026#39;: checked out \u0026#39;69545bed8d953d71344fee44de39074d88da0f90\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_datastream_exporter\u0026#39;... remote: Enumerating objects: 80, done. remote: Total 80 (delta 0), reused 0 (delta 0), pack-reused 80 Unpacking objects: 100% (80/80), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_datastream_exporter\u0026#39;: checked out \u0026#39;b1be7e77fd9f14b72dd1a78130109ce0d9a51fd3\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_datastream_replace\u0026#39;... remote: Enumerating objects: 6, done. remote: Counting objects: 100% (6/6), done. remote: Compressing objects: 100% (4/4), done. remote: Total 84 (delta 2), reused 6 (delta 2), pack-reused 78 Unpacking objects: 100% (84/84), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_datastream_replace\u0026#39;: checked out \u0026#39;0e786886d8da2ebc30a89d23654710aaa180cceb\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_mods_display\u0026#39;... remote: Enumerating objects: 128, done. remote: Counting objects: 100% (128/128), done. remote: Compressing objects: 100% (80/80), done. remote: Total 271 (delta 91), reused 85 (delta 48), pack-reused 143 Receiving objects: 100% (271/271), 282.28 KiB | 0 bytes/s, done. Resolving deltas: 100% (182/182), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_mods_display\u0026#39;: checked out \u0026#39;08249e5a0486c28c698acb7d11db24af5c6ec63c\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_mods_via_twig\u0026#39;... remote: Enumerating objects: 197, done. remote: Counting objects: 100% (197/197), done. remote: Compressing objects: 100% (137/137), done. remote: Total 197 (delta 126), reused 127 (delta 60), pack-reused 0 Receiving objects: 100% (197/197), 44.94 KiB | 0 bytes/s, done. Resolving deltas: 100% (126/126), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_mods_via_twig\u0026#39;: checked out \u0026#39;5cc26c14bd35ce1131f246e3b36657d06be0b126\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_multi_importer\u0026#39;... remote: Enumerating objects: 978, done. remote: Total 978 (delta 0), reused 0 (delta 0), pack-reused 978 Receiving objects: 100% (978/978), 643.47 KiB | 0 bytes/s, done. Resolving deltas: 100% (681/681), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_multi_importer\u0026#39;: checked out \u0026#39;78c06b719287a3c0250e902552ec05f760780b9b\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_solr_collection_view\u0026#39;... remote: Enumerating objects: 134, done. remote: Total 134 (delta 0), reused 0 (delta 0), pack-reused 134 Receiving objects: 100% (134/134), 31.88 KiB | 0 bytes/s, done. Resolving deltas: 100% (72/72), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_solr_collection_view\u0026#39;: checked out \u0026#39;c4b2f33251e3f46bb3bc4789480a60a8efe5d351\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_solr_views\u0026#39;... remote: Enumerating objects: 615, done. remote: Total 615 (delta 0), reused 0 (delta 0), pack-reused 615 Receiving objects: 100% (615/615), 174.41 KiB | 0 bytes/s, done. Resolving deltas: 100% (354/354), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_solr_views\u0026#39;: checked out \u0026#39;3c17f497a51bc7f5bf90a6bd38e02733b79dca94\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_solution_pack_oralhistories\u0026#39;... remote: Enumerating objects: 11, done. remote: Counting objects: 100% (11/11), done. remote: Compressing objects: 100% (11/11), done. remote: Total 3434 (delta 3), reused 2 (delta 0), pack-reused 3423 Receiving objects: 100% (3434/3434), 11.20 MiB | 0 bytes/s, done. Resolving deltas: 100% (1392/1392), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_solution_pack_oralhistories\u0026#39;: checked out \u0026#39;60b84295deb4c7b11fbe5c172e3f815eb78cd942\u0026#39; Cloning into \u0026#39;sites/all/themes/bootstrap\u0026#39;... remote: Enumerating objects: 12635, done. remote: Total 12635 (delta 0), reused 0 (delta 0), pack-reused 12635 Receiving objects: 100% (12635/12635), 2.83 MiB | 0 bytes/s, done. Resolving deltas: 100% (9255/9255), done. Submodule path \u0026#39;sites/all/themes/bootstrap\u0026#39;: checked out \u0026#39;c050d1d6ae85ef344be85dbf0a4ed9ec68ac32ce\u0026#39; Cloning into \u0026#39;sites/default/themes/digital_grinnell_bootstrap\u0026#39;... remote: Enumerating objects: 190, done. remote: Total 190 (delta 0), reused 0 (delta 0), pack-reused 190 Receiving objects: 100% (190/190), 1.08 MiB | 0 bytes/s, done. Resolving deltas: 100% (70/70), done. Submodule path \u0026#39;sites/default/themes/digital_grinnell_bootstrap\u0026#39;: checked out \u0026#39;22b51ecfb61c7e348e25892e988bcf82d0b1c781\u0026#39; Step 10: On Remote Production - Copy Over the Production Data Directories It is recommended that you schedule a content freeze for all Production Fedora ingests and additions to your Production website. This will allow you to get up to date data from Production to Production.\nAs you may have made some critical decisions potentially from \u0026ldquo;Step 0: Copy Production Data to Your Local\u0026rdquo; of the Local ISLE Installation: Migrate Existing Islandora Site instructions, you need to re-follow the steps to get your:\nProduction Drupal site files directory Solr schema \u0026amp; Islandora transforms If you picked Easy option: then you don\u0026rsquo;t need to do anything here for the Solr schema \u0026amp; Islandora transforms If you picked the Intermediate or Advanced options: You\u0026rsquo;ll need to copy in the customizations and files you created during the local environment into the docker-compose.production.yml. Ensure that one set of transforms and schema are used across all environments. Production Fedora datastreamStore directory You\u0026rsquo;ll need to adjust the paths below in case your setup differs on either the non-ISLE Production server or the ISLE Production server. Copy your /usr/local/fedora/data/datastreamStore data to the suggested path of /mnt/data/fedora/datastreamStore You may need to change the permissions to root:root on the Production /mnt/data/fedora/datastreamStore directory above after copying so the Fedora container can access properly. Do not do this on your existing Production system please. Production Fedora objectStore. Copy your /usr/local/fedora/data/objectStore data to the suggested path of /opt/data/fedora/objectStore You may need to change the permissions to root:root on the Production /opt/data/fedora/objectStore above after copying so the Fedora container can access properly. Do not do this on your existing Production system please. All I did for this step was:\n[islandora@dgdocker1 ~]$ mkdir -p /opt/ISLE/data/apache/html/sites/default/files [islandora@dgdocker1 ~]$ cp -fr ~/DG-PROD/data/files/. /opt/ISLE/data/apache/html/sites/default/files/. [islandora@dgdocker1 ~]$ chown -R islandora:islandora /opt/ISLE/data/apache Step 11: On Remote Production - If Using Let\u0026rsquo;s Encrypt If you are using Commercial SSLs, then please stop and move onto the next step.\nIf using Let\u0026rsquo;s Encrypt, please continue to follow this step.\nCreate an empty acme.json within the ./config/proxy/ssl-certs/ directory of your ISLE project. touch /opt/yourprojectnamehere/config/proxy/ssl-certs/acme.json chmod 600 /opt/yourprojectnamehere/config/proxy/ssl-certs/acme.json This file will be ignored by git and won\u0026rsquo;t cause any errors with checking in code despite the location Do note that you may need to open your firewall briefly to allow the SSL certs to be added to the acme.json file. This will be indicated in the following steps. Open your firewall to ports 80, 443 prior to starting up the containers to ensure SSL cert creation. My IT department will NOT permit me to open port 443, not even for an instant, so I have to handle our existing acme.json file differently. I\u0026rsquo;m using a backup copy of acme.json, like so:\n[islandora@dgdocker1 proxy]$ cp -f /home/islandora/DG-PROD/data/config/proxy/acme.json /opt/ISLE/dg-isle/config/proxy/acme.json [islandora@dgdocker1 proxy]$ chmod 400 /opt/ISLE/dg-isle/config/proxy/acme.json Step 12: On Remote Production - Edit the \u0026ldquo;.env\u0026rdquo; File to Change to the Production Environment This step is a multi-step, involved process that allows an end-user to make appropriate changes to the .env and then commit it locally to git. This local commit that never gets pushed back to the git repository is critical to allow future ISLE updates or config changes.\nCopy the sample.env to .env. By default, the Demo environment is setup. You will need to edit this file to match the correct environment. Please note that the .env is no longer tracked by git as of ISLE version 1.5. Instructions below involving git are for ISLE versions below 1.5. However the settings recommended below for the environment can still be followed as needed.\ncp sample.env .env Edit the .env, remove the local settings and then commit locally (only if using an ISLE version below 1.5)\ncd /opt/yourprojectnamehere vi / nano / pico /opt/yourprojectnamehere/.env Edit COMPOSE_PROJECT_NAME= and replace the local settings with: COMPOSE_PROJECT_NAME= (Suggested) Add an identifiable project or institutional name plus environment e.g. acme_digital_production` Edit BASE_DOMAIN= and replace the local settings with: BASE_DOMAIN= (Suggested) Add the full production domain here e.g. digital.institution.edu Edit CONTAINER_SHORT_ID= and replace the local settings with: CONTAINER_SHORT_ID= (Suggested) Make an easy to read acronym from the letters of your institution and collection names plus environment e.g. (acme digitalcollections production) is acdcp Edit COMPOSE_FILE change local to production COMPOSE_FILE=docker-compose.production.yml Save the file For users of ISLE version 1.5 and above, these git instructions below are not needed. The .env file is no longer tracked in git.\nFor users of ISLE versions 1.4.2 and below, you will need to continue to follow these instructions until you upgrade.\nEnter git status - You\u0026rsquo;ll now see the following: On branch master Your branch is up to date with \u0026#39;origin/master\u0026#39;. Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git checkout -- \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: .env You\u0026rsquo;ll need to add this file and commit it in git to be able to get future updates from ISLE as a process.\ngit add .env git commit -m \u0026quot;Added the edited .env configuration file for Production. DO NOT PUSH BACK TO UPSTREAM REPOSITORY - Jane Doe 8/2019\u0026quot; This is a suggested warning for users NOT TO push back this configuration change to the main git repository. If that were done it could conflict with other setups. You may run into the following:\n*** Please tell me who you are. Run git config --global user.email \u0026#34;you@example.com\u0026#34; git config --global user.name \u0026#34;Your Name\u0026#34; to set your account\u0026#39;s default identity. Omit --global to set the identity only in this repository. fatal: empty ident name (for \u0026lt;islandora@yourprojectnamehere.institution.edu\u0026gt;) not allowed Configure your server git client but don\u0026rsquo;t use the --global setting as that could interfere with other git repositories e.g. your Islandora Drupal code.\nExample: Within /opt/yourprojectnamehere git config user.email \u0026quot;jane@institution.edu\u0026quot; git config user.name \u0026quot;Jane Doe\u0026quot; Now re-run the commit command:\ngit commit -m \u0026#34;Added the edited .env configuration file for Production. DO NOT PUSH BACK TO UPSTREAM REPOSITORY - Jane Doe 8/2019\u0026#34; [master 7ab3fcf9] Added the edited .env configuration file for Production. DO NOT PUSH BACK TO UPSTREAM REPOSITORY - Jane Doe 8/2019 1 file changed, 4 insertions(+), 4 deletions(-) In this section I chose to simply copy sample.env to .env, and edit using nano to come up with this .env file:\n#### Activated ISLE environment # To use an environment other than the default Demo, please change values below # from the default Demo to one of the following: Local, Test, Staging or Production # For more information, consult https://islandora-collaboration-group.github.io/ISLE/install/install-environments/ COMPOSE_PROJECT_NAME=dg BASE_DOMAIN=digital.grinnell.edu CONTAINER_SHORT_ID=dg COMPOSE_FILE=docker-compose.production.yml Step 13: On Remote Production - Download the ISLE Images Download all of the latest ISLE Docker images (~6 GB of data may take 5-10 minutes). Using the same open terminal: Navigate to the root of your ISLE project cd ~/opt/yourprojectnamehere docker-compose pull In my case:\ncd /opt/ISLE/dg-isle docker-compose pull Step 14: On Remote Production - Start Containers Note: Prior to starting the launch process, it is recommended that you briefly open your firewall to allow ports 80 and 443 access to the world. You\u0026rsquo;ll only need to keep this open for 3 -5 minutes and then promptly close access once the Let\u0026rsquo;s Encrypt SSL certificates have been generated.\nUsing the same open terminal:\ndocker-compose up -d Please wait a few moments for the stack to fully come up. Approximately 3-5 minutes.\nUsing the same open terminal:\nView only the running containers: docker ps View all containers (both those running and stopped): docker ps -a All containers prefixed with isle- are expected to have a STATUS of Up (for x time). If any of these are not UP, then use ISLE Installations: Troubleshooting to solve before continuing below. \u0026lt;!- TODO: This could be confusing if (a) there are other, non-ISLE containers, or (b) the isle-varnish container is installed but intentionally not running, or (c) older exited ISLE containers that maybe should be removed. -\u0026gt; In your web browser, enter your Production site URL: https://yourprojectnamehere.institution.edu\nNote: You should not see any errors with respect to the SSL certifications, you should see a nice green lock padlock for the site security. If you see a red error or unknown SSL cert provider, you\u0026rsquo;ll need to shut the containers down and review the previous steps taken especially if using Let\u0026rsquo;s Encrypt. You may need to repeat those steps to get rid of the errors. In my case, there are still containers from the old stack that have been stopped, but are still hanging around. Those need to be deleted before this can proceed. So:\ncd /opt/ISLE/dg-isle ./destroy.sh docker-compose up -d Note the destroy.sh script contains:\n#!/bin/bash # docker stop $(docker ps -q); docker rm -v $(docker ps -qa); docker image rm $(docker image ls -q) docker system prune --force Step 15: On Remote Production - Import the Local MySQL Drupal Database Prior to attempting this step, please consider the following:\nIf the end user is running multi-sites, there will be additional databases to export.\nDo not import the fedora3 database\nImport the Local MySQL Islandora Drupal Database Copy the local_drupal_site_082019.sql created in Step 1 to the Remote Production server.\nImport the exported Local MySQL database for use in the current Production Drupal site. Refer to your production.env for the usernames and passwords used below.\nYou can use a MySQL GUI client for this process instead but the command line directions are only included below. Run docker ps to determine the MySQL container name Using the same open terminal: Shell into your currently running Production MySQL container docker exec -it your-mysql-containername bash Import the Local Islandora Drupal database. Replace the \u0026ldquo;DRUPAL_DB_USER\u0026rdquo; and \u0026ldquo;DRUPAL_DB\u0026rdquo; in the command below with the values found in your \u0026ldquo;production.env\u0026rdquo; file. mysql -u DRUPAL_DB_USER -p DRUPAL_DB \u0026lt; local_drupal_site_082019.sql Enter the appropriate password: value of DRUPAL_DB_PASS in the \u0026ldquo;production.env\u0026rdquo;) This might take a few minutes depending on the size of the file. Type exit to exit the container Ok, so my \u0026ldquo;staging\u0026rdquo; database is in much better shape than production at this point, so my process was:\n[islandora@dgdocker1 dg-isle]$ docker cp ~/DG-PROD/data/staging_drupal_site_112220.sql isle-mysql-dg:staging.sql [islandora@dgdocker1 dg-isle]$ docker exec -it isle-mysql-dg bash root@95d316d986a9:/# mysql -u admin -p digital_grinnell \u0026lt; staging.sql Enter password: root@95d316d986a9:/# exit Also, my configuration requires that I run composer update for the IMI module, so:\n[islandora@dgdocker1 dg-isle]$ docker exec -w /var/www/html/sites/all/modules/islandora/islandora_multi_importer/ isle-apache-dg composer update ...output removed for clarity... [islandora@dgdocker1 dg-isle]$ docker exec -w /var/www/html/sites/default isle-apache-dg drush cc all \u0026#39;all\u0026#39; cache was cleared. Once these steps were complete https://digital.grinnell.edu will open, but displays no objects. Next up, I need to rebuild the FEDORA resource and Solr indicies.\nStep 16: On Remote Production - Run ISLE Scripts I found this step to be UNECESSARY since my dg-isle and dg-islandora repositories have already been prepared.\nThis step will show you how to run the \u0026ldquo;migration_site_vsets.sh\u0026rdquo; script on the Apache container to change Drupal database site settings for ISLE connectivity.\nUsing the same open terminal:\nRun docker ps to determine the apache container name Copy the \u0026ldquo;migration_site_vsets.sh\u0026rdquo; to the root of the Drupal directory on your Apache container docker cp ./scripts/apache/migration_site_vsets.sh your-apache-containername:/var/www/html/migration_site_vsets.sh Change the permissions on the script to make it executable docker exec -it your-apache-containername bash -c \u0026quot;chmod +x /var/www/html/migration_site_vsets.sh\u0026quot; Run the script docker exec -it your-apache-containername bash -c \u0026quot;cd /var/www/html \u0026amp;\u0026amp; ./migration_site_vsets.sh\u0026quot; This step will show you how to shell into your currently running Production Apache container, and run the \u0026ldquo;fix-permissions.sh\u0026rdquo; script to fix the Drupal site permissions.\ndocker exec -it your-apache-containername bash sh /utility-scripts/isle_drupal_build_tools/drupal/fix-permissions.sh --drupal_path=/var/www/html --drupal_user=islandora --httpd_group=www-data This process will take 2-5 minutes You should see a lot of green [ok] messages. If the script appears to pause or prompt for y/n, DO NOT enter any values; the script will automatically answer for you. Type exit to exit the container For Microsoft Windows: You may be prompted by Windows to: - Share the C drive with Docker. Click Okay or Allow. - Enter your username and password. Do this. - Allow vpnkit.exe to communicate with the network. Click Okay or Allow (accept default selection). - If the process seems to halt, check the taskbar for background windows. Step 17: On Remote Production - Re-Index Fedora \u0026amp; Solr When migrating any non-ISLE Islandora site, it is crucial to rebuild (reindex) the following three indices from the FOXML and datastream files on disk.\nFedora\u0026rsquo;s indices:\nResource Index - The Resource Index is the Fedora module that provides the infrastructure for indexing relationships among objects and their components. SQL database - fedora3 contains information vital for the Drupal site to connect to Fedora correctly. Solr index - Solr an open source enterprise search platform works in conjunction with the Islandora Solr module to provide a way to configure the Islandora search functions, the search results display, and the display of metadata on object pages. The index serves as a list of those objects for fast searching across large collections.\nYou can use the command-line interactive utility fedora-rebuild.sh on the fedora container to rebuild all indices when the Fedora (not Tomcat) server is offline.\nDepending on the size of your repository, this entire process may take minutes (thousands of objects) or hours (millions of objects) to complete.\nReindex Fedora RI \u0026amp; Fedora SQL Database (2/3) Since this command can take minutes or hours depending on the size of your repository, As such, it is recommended starting a screen session prior to running the following commands. Learn more about screen here\nNote: The method described below is a longer way of doing this process to onboard users.\nShell into your currently running Production Fedora container\nRun docker ps to determine the Fedora container name\ndocker exec -it your-fedora-containername bash Navigate to the utility_scripts directory\ncd utility_scripts Run the rebuildFedora.sh script. This script will give you output like the example below.\n./rebuildFedora.sh OK - Stopped application at context path [/fedora] Starting the rebuild process in the background. This may take a while depending on your Fedora repository size. To watch the log and process run: tail -f $CATALINA_HOME/logs/fedora-rebuild.out Truncating old SQL tables. mysql: [Warning] Using a password on the command line interface can be insecure. Automatically tailing the log file... Press CTRL+C to stop watching at any time. This will NOT stop the rebuild process After a good period of time, again depending on the size of your Fedora collection there should be output like the example below. This indicates that the Fedora RI \u0026amp; SQL reindex process was successful. The number of objects rebuilt will vary. You can hit the CNTRL and C keys to exit out of the process, if need be. Do not exit the Fedora container yet, one more index to go; Solr. Adding object #31: islandora:sp_web_archive_collection Adding object #32: islandora:sp_web_archive Adding object #33: islandora:newspaperPageCModel Adding object #34: islandora:compound_collection Adding object #35: islandora:newspaperCModel Adding object #36: islandora:newspaperIssueCModel Adding object #37: ir:citationCollection Adding object #38: islandora:sp_basic_image_collection SUCCESS: 38 objects rebuilt. OK - Started application at context path [/fedora] My process here was:\n[islandora@dgdocker1 dg-isle]$ docker exec -w /utility_scripts isle-fedora-dg ./rebuildFedora.sh % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 202 0 202 0 0 1819 0 --:--:-- --:--:-- --:--:-- 1819 Stopping FEDORA. % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 51 0 51 0 0 18 0 --:--:-- 0:00:02 --:--:-- 18 OK - Stopped application at context path [/fedora] Starting the rebuild process in the background. This may take a while depending on your Fedora repository size. ... Reindex Solr (3/3) WARNING - This reindex process takes the longest of all three, with up to 1-30 or more hours to complete depending on the size of your Fedora collection. As such, it is recommended starting a screen session prior to running the following command. Learn more about screen here\nStill staying within the utility_scripts directory on the Fedora container or reenter the Fedora container having started a new screen session, now run the updateSolrIndex.sh script. This script will give you output like the example below. ./updateSolrIndex.sh FedoraGenericSearch (FGS) update Solr index from Fedora helper script. Starting to reindex your Fedora repository. This process runs in the background and may take some time. Checked and this operation is still running. You may disconnect and the process will continue to run. Find logs at /usr/local/tomcat/logs/fgs-update-foxml.out and /usr/local/tomcat/logs/fgs-update-foxml.err. You can watch log file \u0026#39;tail -f /usr/local/tomcat/logs/fedoragsearch.daily.log\u0026#39; as the process runs. Note: Within this output, options to tail logs and watch progress are offered. Depending on the size of your collection this process may take hours, however it is okay to exit out of the container and even log off the remote Production server. You can check back frequently by running tail -f /usr/local/tomcat/logs/fgs-update-foxml.out on the Fedora container. If you visit your Drupal site and run a Solr search, you should start to see objects and facets start to work. The number of objects will increase over time.\nAfter a good period of time, again depending on the size of your Fedora collection, when the Solr re-index process finishes, output like the example below will appear in the /usr/local/tomcat/logs/fgs-update-foxml.out log. This indicates that the Solr reindex process was completed. The number of objects rebuilt will vary. You can hit the CNTRL and C keys to exit out of the tail process, if need be. tail -f /usr/local/tomcat/logs/fgs-update-foxml.out Args 0=http://localhost:8080 1=updateIndex 2=fromFoxmlFiles \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;resultPage operation=\u0026#34;updateIndex\u0026#34; action=\u0026#34;fromFoxmlFiles\u0026#34; value=\u0026#34;\u0026#34; repositoryName=\u0026#34;FgsRepos\u0026#34; indexNames=\u0026#34;\u0026#34; resultPageXslt=\u0026#34;\u0026#34; dateTime=\u0026#34;Thu Aug 08 20:43:12 GMT 2019\u0026#34;\u0026gt; \u0026lt;updateIndex xmlns:dc=\u0026#34;http://purl.org/dc/elements/1.1/\u0026#34; xmlns:foxml=\u0026#34;info:fedora/fedora-system:def/foxml#\u0026#34; xmlns:zs=\u0026#34;http://www.loc.gov/zing/srw/\u0026#34; warnCount=\u0026#34;0\u0026#34; docCount=\u0026#34;13\u0026#34; deleteTotal=\u0026#34;0\u0026#34; updateTotal=\u0026#34;13\u0026#34; insertTotal=\u0026#34;0\u0026#34; indexName=\u0026#34;FgsIndex\u0026#34;/\u0026gt; \u0026lt;/resultPage\u0026gt; Type exit when finished to exit the container. Like the previous step, my process here was:\n[islandora@dgdocker1 dg-isle]$ docker exec -w /utility_scripts isle-fedora-dg ./updateSolrIndex.sh Followed by a few hours of testing and waiting.\nStep 18: On Remote Production - Review and Test the Drupal Production Site In your web browser, enter this URL: https://yourprojectnamehere.institution.edu\nPlease note: You should not see any errors with respect to the SSL certifications. If so, please review your previous steps especially if using Let\u0026rsquo;s Encrypt. You may need to repeat those steps to get rid of the errors. Log in to the local Islandora site with the credentials (\u0026ldquo;DRUPAL_ADMIN_USER\u0026rdquo; and \u0026ldquo;DRUPAL_ADMIN_PASS\u0026rdquo;) you created in \u0026ldquo;production.env\u0026rdquo;.\nNote: You are free to use previously Drupal admin or user accounts created during the Local site development process. You can decide to further QC and review the site as you wish or start to add digital collections and objects.\nYou could also further test using the Islandora Sample Objects as you may have done in the previous Local installation. Eureka! It works. The site is up and running, with no apparent issues, at https://digital.grinnell.edu.\nNext Steps Here again, I found these \u0026ldquo;Next Steps\u0026rdquo; to be UNECESSARY.\nOnce you are ready to deploy your finished Drupal site, you may progress to launching the Production site publicly which could involve the following steps depending on choices made earlier in this document and process:\nIf you followed the use of the temporary -newprod suggestion in the Assumptions section, remove the -newprod from all domain references and configuration files and recommit the change on the remote server in git.\nIf using commercial SSLs, you\u0026rsquo;ll also need to copy them over to the ./config/proxy/ssl-certs directory and adjust the traefik.production.toml file accordingly with the new file names. If using Let\u0026rsquo;s Encrypt, upon restart with the new settings, the acme.json file contents will change automatically. Update the DNS records to repoint the current non-ISLE Production server A record for yourprojectnamehere.institution.edu to the new ISLE host server IP. Remove the temporary DNS A record for yourprojectnamehere-newprod.institution.edu Lift any firewall restrictions and allowing full Internet access on http (port 80) and https (443).\nDo not allow any other port to be publicly accessible. Additional Resources ISLE Installation: Environments help with explaining the ISLE structure, the associated files, and what values ISLE end-users should use for the \u0026ldquo;.env\u0026rdquo;, \u0026ldquo;local.env\u0026rdquo;, etc. Local ISLE Installation: Resources contains Docker container passwords and URLs for administrator testing. ISLE Installation: Troubleshooting contains help for port conflicts, non-running Docker containers, etc. End of Production ISLE Installation: New Site And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/096-production-isle/","tags":["ISLE","migrate","production"],"title":"Production ISLE Installation: Migrate Existing Islandora Site - with Annotations"},{"categories":null,"contents":" I\u0026rsquo;ve nearly completed migration of all Grinnell College Libraries\u0026rsquo; static sites to Azure and I\u0026rsquo;m updating this post to reflect that move. Wherever possible I\u0026rsquo;ll strike-through outdated information like this, and replace it with updated info.\nThis post will instruct the reader to install necessary tools and engage the workflow I now use to develop and maintain a handful of Hugo static websites. The list of sites now includes both professional, those owned and operated by Grinnell College, as well as \u0026ldquo;personal\u0026rdquo; sites that I develop, maintain and host myself.\nProfessional Sites These include:\nRootstalk, This Blog \u0026hellip;previously at https://static.grinnell.edu/blogs/McFateM, The Static.Grinnell.edu Landing Page \u0026hellip; previously at https://static.grinnell.edu, and VAF \u0026hellip;previously at https://vaf.grinnell.edu The detailed information provided in this blog post uses Rootstalk and this blog as examples, but the concepts apply equally to all of the professional and personal sites I\u0026rsquo;ve listed.\nPersonal Sites These include:\nThe SummittDweller.com Landing Page, My Personal Blog, and The Compass Rose Band Local Development This section briefly describes all that is necessary to collaborate effectively on the development and maintenance of Rootstalk from your OS X workstation, presumably a Mac desktop or laptop.\nRequired Software The workflow which follows will require you to install, or update, the following software packages:\nSoftware How to Install git The suggested Homebrew instructions lead down an infinite rabbit-hole, so use https://sourceforge.net/projects/git-osx-installer/ instead. atom See https://flight-manual.atom.io/getting-started/sections/installing-atom/ hugo The easiest way to install on a Mac is using the Binary (Cross-platform) technique. The latest release can always be found on GitHub in gohugoio/hugo. As of the writing of this post the package of choice for a Mac was named hugo_extended_0.78.2_macOS-64bit.tar.gz. Once you have downloaded the .gz file, double-click on it to expand the archive. Then try this command to install it in your $PATH: sudo cp -f ~/Downloads/hugo_extended_0.78.2_macOS-64bit/hugo /usr/local/bin/hugo Usual Workflow It is recommended that you clone this repository to an OS X workstation where git, Atom, and Hugo are installed and running in an up-to-date versions.\nMy typical workflow for local development, after installation of the above, goes like this:\nmkdir -p ~/GitHub cd ~/GitHub git clone https://github.com/Digital-Grinnell/rootstalk --recursive cd rootstalk git checkout -b \u0026lt;new-branch-name\u0026gt; atom . hugo server atom . The atom . command opens the project in my Atom editor which provides many tools and shortcuts to speed development and maintenance. Atom Basics is well worth reading to get you up-to-speed with the powerful editor you now have at your disposal.\nNote that when Atom is installed it should include a command-line shortcut so that the atom . command will work; however, if atom . will not launch Atom it should be possible to add the necessary command-line option as directed in this StackOverflow answer.\nhugo server The hugo server command compiles and launches a local instance of the site and provides a link, usually http://localhost:1313, to that site if there are no errors. This local site will respond immediately to any changes made and saved in Atom.\nNote that if your Mac complains that hugo isn\u0026rsquo;t from an \u0026ldquo;approved\u0026rdquo; developer, you should run the following command to override the need for \u0026ldquo;approval\u0026rdquo;: sudo xattr -d com.apple.quarantine /usr/local/bin/hugo\nEditing and Saving Changes Locally Once you have both atom . and hugo server running the process of editing, testing, saving, and sharing your changes should be pretty straightforward. You will generally use the following commands and operations, in sequence, and repeated as often as necessary.\nOpen your local site by visiting https://localhost:1313, or whatever address the hugo server command returned, in the web browser of your choice. When you have identified a change that needs to be made, find the corresponding file in the left-most panel of your Atom editor window \u0026ndash; this will usually be some .md (Markdown) file beneath the content directory. Click on that file to open it in your editor window. Make changes to the file using the appropriate syntax, usually Markdown. There\u0026rsquo;s a nice basic syntax guide for Markdown at https://www.markdownguide.org/basic-syntax/. Once your changes to the file are complete, visit the File menu in Atom and select Save to save the change. As soon as the file is saved you should see your site change immediately, or almost so, in your browser window. Check that the changes are properly reflected. Repeat the above steps for as many files as needed. Sharing Your Changes with the Project Team Assuming you have made proper changes, and tested them locally, you should be ready to share your work with the project team. The first time you do this it\u0026rsquo;s likely that you will need some live assistance in order to get proper git credentials and configuration applied. Reach out to your team development leader for help. An example of the steps that you will be taking, and repeating quite often, are:\ngit status git add . git status git commit -m \u0026#34;Mark\u0026#39;s edits to post 095\u0026#34; git status git remote -v git push origin post-095-edits This git status, git add ., git commit... and git push... sequence should become VERY familiar over time. Since I am currently using this workflow to edit the document you are reading, I\u0026rsquo;m going to change my \u0026ldquo;example\u0026rdquo; project and execute these commands now, sharing the results with you below.\ngit status In this example I previously used the command git checkout -b post-095-edits to create a new branch for my work here. I subsequently edited this file, named 095-collaborating-on-hugo-site-development, and tested then saved my changes. Now, when I run git status I see this:\n╭─mark@Marks-Mac-Mini ~/GitHub/blogs-McFateM ‹post-095-edits› ╰─$ git status On branch post-095-edits Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git checkout -- \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) (commit or discard the untracked or modified content in submodules) modified: content/posts/095-collaborating-on-hugo-site-development.md modified: themes/hugo-theme-m10c (modified content) no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) This command indicates that I\u0026rsquo;m working in my post-095-edits branch, as intended. It also shows that I\u0026rsquo;ve made changes to content/posts/095-collaborating-on-hugo-site-development.md as well as to my themes/hugo-theme-m10c theme. Finally, it tells me that none of these changes have been \u0026ldquo;committed\u0026rdquo; yet. You can\u0026rsquo;t see it here, but the two lines that begin with the word modified: appear in my terminal in red; indicating that they are not quite ready to share yet.\ngit add . I\u0026rsquo;m comfortable with changes made to ALL of the files reported as \u0026ldquo;modified\u0026rdquo; above, so I want to \u0026ldquo;stage\u0026rdquo; ALL of them to be committed, and I do that using git add .. The dot at the end captures ALL of the modified files. If there were changes that I\u0026rsquo;m not comfortable with I could be more specific and repeat the path of each file to be committed like so: git add content/posts/095-collaborating-on-hugo-site-development.md.\nIf all goes as planned, the git add . command won\u0026rsquo;t return anything.\ngit status Now I can check my progress with another git status, like so:\n╭─mark@Marks-Mac-Mini ~/GitHub/blogs-McFateM ‹post-095-edits*› ╰─$ git status On branch post-095-edits Changes to be committed: (use \u0026#34;git reset HEAD \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: content/posts/095-collaborating-on-hugo-site-development.md Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git checkout -- \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) (commit or discard the untracked or modified content in submodules) modified: themes/hugo-theme-m10c (modified content) This time git status is showing me that my content/posts/095-collaborating-on-hugo-site-development.md file is \u0026ldquo;to be committed\u0026rdquo;, and it appears in my terminal in green to indicate that it\u0026rsquo;s \u0026ldquo;ready to go\u0026rdquo;. The line that reads modified: themes/hugo-theme-m10c (modified content) still appears in red and is \u0026ldquo;not staged for commit\u0026rdquo;, and that\u0026rsquo;s OK because I didn\u0026rsquo;t make any necessary changes to that path, the system did, and they don\u0026rsquo;t need to be saved.\nSo, now we are ready to commit our \u0026ldquo;staged\u0026rdquo; changes.\ngit commit -m \u0026ldquo;Mark\u0026rsquo;s edits to post 095\u0026rdquo; The git commit command should always include a short but descriptive \u0026ldquo;message\u0026rdquo; following a -m flag as you see here:\n╭─mark@Marks-Mac-Mini ~/GitHub/blogs-McFateM ‹post-095-edits*› ╰─$ git commit -m \u0026#34;Mark\u0026#39;s edits to post 095\u0026#34; [post-095-edits d5e2c50] Mark\u0026#39;s edits to post 095 1 file changed, 2 insertions(+), 1 deletion(-) The output indicates that I changed and am committing just one file, where I inserted two new blocks of text, and deleted another.\ngit remote -v The git remote... command is not absolutely necesary, but I use it just to confirm where my next git push is going to go. The command lists for me all of the \u0026ldquo;remotes\u0026rdquo;, the project repositories, that are associated with this project. In this case:\n╭─mark@Marks-Mac-Mini ~/GitHub/blogs-McFateM ‹post-095-edits› ╰─$ git remote -v origin\thttps://github.com/McFateM/blogs-McFateM.git (fetch) origin\thttps://github.com/McFateM/blogs-McFateM.git (push) This indicates that I have just one remote with an alias of origin, and any git push origin... or git fetch origin... commands will reference this blog\u0026rsquo;s project repository on GitHub at https://github.com/McFateM/blogs-McFateM.git. This is the same repository that I cloned from earlier using git clone git clone https://github.com/McFateM/blogs-McFateM.git --recursive.\ngit push origin post-095-edits This git push statement will attempt to \u0026ldquo;push\u0026rdquo; my committed changes up to the project\u0026rsquo;s origin remote, as described above. The results are:\n╭─mark@Marks-Mac-Mini ~/GitHub/blogs-McFateM ‹post-095-edits› ╰─$ git push origin post-095-edits Enumerating objects: 9, done. Counting objects: 100% (9/9), done. Delta compression using up to 6 threads Compressing objects: 100% (5/5), done. Writing objects: 100% (5/5), 635 bytes | 635.00 KiB/s, done. Total 5 (delta 4), reused 0 (delta 0) remote: Resolving deltas: 100% (4/4), completed with 4 local objects. remote: remote: Create a pull request for \u0026#39;post-095-edits\u0026#39; on GitHub by visiting: remote: https://github.com/McFateM/blogs-McFateM/pull/new/post-095-edits remote: To https://github.com/McFateM/blogs-McFateM.git * [new branch] post-095-edits -\u0026gt; post-095-edits This git push... was a success! The committed changes were pushed to my blog\u0026rsquo;s repository where a new post-095-edits branch has been created.\nWhat\u0026rsquo;s Next? As you can see in the output above, the next step in the process is for someone, presumably the project lead, that\u0026rsquo;s me, to create a pull request so that the new post-095-edits branch of the project can be tested and merged into the master branch. In all of my workflows master is the branch that is ultimately used in production. Untill a pull request is created, tested, and merged, the changes that we just committed will still not be available to the public.\nYou can learn more about pull-requests, specifically as they apply to GitHub repositories and workflows, at https://docs.github.com/en/free-pro-team@latest/github/collaborating-with-issues-and-pull-requests/about-pull-requests.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/095-collaborating-on-hugo-site-development/","tags":["Hugo","local development","git","Atom","Rootstalk"],"title":"Collaborating on Hugo Site Development"},{"categories":null,"contents":"This post is an addendum to an earlier post, Local ISLE Installation: Migrate Existing Islandora Site - with Annotations, where I exhaustively documented my workflow for building a local/development instance of ISLE to mimic the behavoir of Digital.Grinnell.\nGoal The goal of this project is to present a repeatable, minimal set of instructions for spinning up a safe, stand-alone, local/development instance of ISLE on any Mac running OS X.\nPrerequisites This document assumes the user will be spinning up https://dg.localdomain on a Mac with a suitable DG-FEDORA USB stick mounted and accessible. See my README.md public gist for complete instructions regarding creation, updates and handling of such a USB stick. Additional prerequisites for running https://dg.localdomain can be found in ISLE documentation and in my DG-FEDORA: A Portable FEDORA Repository blog post.\nCredentials Some of the scripts documented below may require login credentials including:\nYour workstation (Mac) password, and\nA username and password combination authorizing you to clone the Digital-Grinnell/dg-isle and Digital-Grinnell/dg-islandora private GitHub repositories.\nThese credentials can be found inside the Digital Grinnell Bits portion of the GC Libraries\u0026rsquo; LastPass vault. The information is listed there under the name DG Private Repositories (dg-isle + dg-islandora). Alternatively, you may obtain these credentials indirectly by emailing digital@grinnell.edu. Four RESTART Scripts The process of spinning up a https://dg.localdomain stack has been boiled down to a sequence of roughly 20 necessary commands. While it would be possible to enter these commands sequentially into your workstation\u0026rsquo;s terminal, there is a much easier path. The commands have been captured in a series of bash scripts named RESTART-1.sh, RESTART-2.sh, RESTART-3.sh, and RESTART-4.sh.\nScript Number of Commands Purpose RESTART-1.sh ~15 commands This script does all the heavy lifting. It creates and populates the necessary directory structure, clones project repositories, pull Docker images, and launches 7 containers that define a minimal ISLE stack. RESTART-2.sh 1 command repeated up to 10 times over a ten-minute span This script echoes the log file of the last command executed in RESTART-1.sh. RESTART-3.sh 4 commands This script imports a previously exported Drupal database, updates some dynamic components, and begins the process of indexing DG-FEDORA\u0026rsquo;s digital objects. RESTART-4.sh 2 commands This script wraps things up by recreating DG-FEDORA\u0026rsquo;s Solr index and flushing the Drupal caches. All four scripts can be found on your DG-FEDORA USB stick.\nTypical Use With a DG-FEDORA USB stick properly mounted on your Mac, you simply need to open a terminal window and execute the following command sequence:\ncd /Volumes/DG-FEDORA ./RESTART-1.sh ./RESTART-2.sh ./RESTART-3.sh ./RESTART-4.sh DO NOT simply copy and paste these commands into your terminal! They need to be run one-at-a-time, in sequence, with execution of each command following successful completion of the previous command.\nColors Each script will set your terminal background to black and subsequently provide output using the following color scheme:\nColor Meaning Cyan Normal output Yellow Progress or status messages Green Successful completion messages Magenta Prompts and alerts. Pay particular attention to these! Red Errors Troubleshooting This section of the document will be populated as issues are encountered. If you have a question or concern, or encounter any issues with this process please bring it to my attention by emailing digital@grinnell.edu. Your experience and feedback will help shape this section for the benefit of others! Thank you!\nAnd that\u0026rsquo;s a wrap. Until next time or until the feedback starts to roll in, whichever comes first\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/094-compact-build-of-dg.localdomain/","tags":["ISLE","local","development","dg.localdomain","concise","compact","DG-FEDORA"],"title":"Compact Build of dg.localdomain - Concise Instructions"},{"categories":null,"contents":"This post is a follow-up to Dockerized Traefik Host Using ACME DNS-01 Challenge and Staging ISLE Installation: Migrate Existing Islandora Site - with Annotations, specifically Step 11 in the later document. It introduces a Digital.Grinnell-specific implementation of the Traefik with Acme.sh.\nTesting with McFateM/docker-traefik2-acme-host I started work on this implementation with a test, by cloning https://github.com/McFateM/docker-traefik2-acme-host and proceeding as directed in the repository\u0026rsquo;s README.md document, as user islandora on node DGDockerX, like so:\nDGDockerX Host Commands cd ~ git clone https://github.com/McFateM/docker-traefik2-acme-host host --recursive cd host Working in ~/host/acme As suggested, I made a copy of the .env file from the corresponding acme directory on Grinnell\u0026rsquo;s dgdocker3.grinnell.edu server, something like this:\n╭─islandora@dgdockerx ~/host/acme ‹master*› ╰─$ rsync -aruvi islandora@dgdocker3.grinnell.edu:/home/islandora/host/acme/.env . --progress The authenticity of host \u0026#39;dgdocker3.grinnell.edu (132.161.151.***)\u0026#39; can\u0026#39;t be established. ECDSA key fingerprint is SHA256:************************************************. ECDSA key fingerprint is MD5:****************************************. Are you sure you want to continue connecting (yes/no)? yes Warning: Permanently added \u0026#39;dgdocker3.grinnell.edu,132.161.151.***\u0026#39; (ECDSA) to the list of known hosts. islandora@dgdocker3.grinnell.edu\u0026#39;s password: receiving incremental file list \u0026gt;f+++++++++ .env 896 100% 875.00kB/s 0:00:00 (xfr#1, to-chk=0/1) sent 43 bytes received 1,011 bytes 39.77 bytes/sec total size is 896 speedup is 0.85 I made the following edits/changes on the DGDocker3 host as suggested in the project repository README.md file, using nano.\nFile Edits ~/host/certs/certs.toml Changed initial tls.certificates entry to reference dgdockerx.grinnell.edu.\u0026lt;extension\u0026gt; ~/host/acme/DGDOCKERX.md Copied file DGDOCKER3.md and changed all dgdocker3.grinnell.edu references to dgdockerx.grinnell.edu ~/host/acme/.env Changed the HOST and added/activated a new DNS_LIST variable appropriate for dgdockerx.grinnell.edu Working in ~/host/traefik As suggested in the documentation, I moved into the traefik directory and had a look at the README.md file there. Following it\u0026rsquo;s advice, like so:\n╭─islandora@dgdockerx ~/host/acme ‹master*› ╰─$ cd ../traefik ╭─islandora@dgdockerx ~/host/traefik ‹master*› ╰─$ ll total 12K -rw-rw-r--. 1 islandora islandora 1.9K Sep 15 14:29 docker-compose.yml -rw-rw-r--. 1 islandora islandora 673 Sep 15 14:29 README.md -rw-rw-r--. 1 islandora islandora 442 Sep 15 14:29 traefik-tls.toml ╭─islandora@dgdockerx ~/host/traefik ‹master*› ╰─$ cat README.md --- Host: Defined as ${HOST} in .env Service: traefik URL: https://${HOST}/dashboard/ --- This document should be used to launch the `traefik` service on ANY host as part of a `docker-traefik2-acme-host` stack. \u0026gt; Note that this process should be started only AFTER the `acme` service! ## Preparation Before entering the prescribed \u0026#34;Command Sequence\u0026#34;, below, the user should take steps to copy any pertinent `.env` files from an existing deployment. Try something like this: - `rsync -aruvi administrator@static.grinnell.edu:/home/administrator/host/traefik/.env . --progress` ## Command Sequence - cd ~/host/traefik - docker-compose up -d; docker-compose logs ╭─islandora@dgdockerx /opt/containers/host/traefik ‹master*› ╰─$ rsync -aruvi administrator@static.grinnell.edu:/home/administrator/host/traefik/.env . --progress administrator@static.grinnell.edu\u0026#39;s password: receiving incremental file list \u0026gt;f+++++++++ .env 25 100% 24.41kB/s 0:00:00 (xfr#1, to-chk=0/1) sent 43 bytes received 140 bytes 11.09 bytes/sec total size is 25 speedup is 0.14 ╭─islandora@dgdockerx /opt/containers/host/traefik ‹master*› ╰─$ nano .env File Edits ~/host/traefik/.env Changed the HOST to dgdockerx.grinnell.edu Working in the Remaining Directories I did the same as above in each of the remaining directories: portainer, watchtower, and whoami. Afterward, I returned to the ~/host directory and made necessary changes to restart.sh.\nUltimately the changes I made were intended to create the following services and addresses for testing purposes only:\nService Address traefik https://dgdockerx.grinnell.edu/dashboard/ portainer https://dgdockerx.grinnell.edu/portainer/ whoami https://dgdockerx.grinnell.edu/whoami/ Test Launching the Stack With my test configured I should be able to launch the Traefik/Portainer/WhoAmI stack for testing by simply running the ~/host/destroy.sh script followed by ~/host/restart.sh. Initially when I did this I had configured restart.sh with an acme.sh command like this:\ndocker exec -it acme --issue --dns dns_azure --server https://acme-staging-v02.api.letsencrypt.org/directory -d dgdockerx.grinnell.edu -d isle-staging.grinnell.edu -d dg-staging.grinnell.edu --domain-alias _acme-challenge.leverify.info --key-file /certs/dgdockerx.grinnell.edu.key --cert-file /certs/dgdockerx.grinnell.edu.cert --standalone --force --log --debug 2 That acme command failed because it tried to create and validate a cert for three different subdomains, dgdockerx.grinnell.edu, isle-staging.grinnell.edu, and dg-staging.grinnell.edu. Validation like this requires that each target subdomain has three things in place:\nA valid A record in the Grinnell College external and/or internal DNS tables directing the subdomain to an appropriate service endpoint. A corresponding CNAME record in the college\u0026rsquo;s Azure DNS. acme uses this to generate TXT records that are subsequently used for validation. A working service endpoint capable of returning a valid response. When I initiated that first test by running ~/host/restart.sh, none of my three subdomains had the necessary CNAME records and only dgdockerx.grinnell.edu had a working service endpoint. Naturally, that test failed, and I learned from subsequent tests that any error in the running of the acme script apparently negates the entire command. So, with that in mind, I am taking steps to limit each acme command I run to a single subdomain. For example:\ndocker exec -it acme --issue --dns dns_azure --server https://acme-staging-v02.api.letsencrypt.org/directory -d dgdockerx.grinnell.edu --domain-alias _acme-challenge.leverify.info --key-file /certs/dgdockerx.grinnell.edu.key --cert-file /certs/dgdockerx.grinnell.edu.cert --standalone --force --log --debug 2 docker exec -it acme --issue --dns dns_azure --server https://acme-staging-v02.api.letsencrypt.org/directory -d isle-staging.grinnell.edu --domain-alias _acme-challenge.leverify.info --key-file /certs/isle-staging.grinnell.edu.key --cert-file /certs/isle-staging.grinnell.edu.cert --standalone --force --log --debug 2 docker exec -it acme --issue --dns dns_azure --server https://acme-staging-v02.api.letsencrypt.org/directory -d dg-staging.grinnell.edu --domain-alias _acme-challenge.leverify.info --key-file /certs/dg-staging.grinnell.edu.key --cert-file /certs/dg-staging.grinnell.edu.cert --standalone --force --log --debug 2 Another Test Launch Before attempting to engage the acme.sh validation scheme with my staging instance of ISLE, I elected to run another test with my whoami service and a new dgdockerx-landing-page service as well. I would do so using the one-subdomain-per-command approach mentioned above. This new test needs to introduce a new service and URL as well, so I elected to add a static site \u0026ldquo;landing page\u0026rdquo; to this server. The services and addresses I intend to create will include:\nService Address landing page/site https://dgdockerx.grinnell.edu/ traefik https://dgdockerx.grinnell.edu/dashboard/ portainer https://dgdockerx.grinnell.edu/portainer/ whoami (permanent) https://dgdockerx.grinnell.edu/whoami/ whoami (test) https://dg-staging.grinnell.edu/ I am exceptionally pleased to report that\u0026hellip; IT JUST WORKS. \u0026#x2757; \u0026#x1f600; \u0026#x2757; \u0026#x1f600; \u0026#x2757;\nAll of the updated files, sans any .env files needed for complete configuration, have been pushed back into the docker-traefik2-acme-host public repository. On September 28, 2020, I also took the liberty of upgrading the aforementioned project to use Portainer v2.0.0 as well as Traefik:latest (currently equates to 2.3.0) and docker-compose 3.3. All of those changes have also been pushed back and merged into master. Enjoy.\nNext: Repeat with DG-STAGING (ISLE v1.5.1) at https://dg-staging.grinnell.edu This could be a real challenge because docker-traefik2-acme-host uses Traefik v2 while ISLE still employ Traefik v1. Fortunately, the key component to making this work is the acme.sh script and service, and that should work with any version of Traefik. So let\u0026rsquo;s take some baby steps\u0026hellip;\nIntroduce acme.sh Into ISLE All that\u0026rsquo;s necessary to introduce acme.sh into ISLE is copying the current ~/host/acme directory to /opt/dg-isle/acme, changing a litte of the configuration there, and invoking the acme service to obtain our certs before we bring up the ISLE stack. Working on DGDockerX as user islandora I made the copy and edits like so:\ncp -f ~/host/destroy.sh /opt/dg-isle/destroy.sh cp -f ~/host/restart.sh /opt/dg-isle/restart.sh cp -fr ~/host/acme /opt/dg-isle/acme cd /opt/dg-isle nano /opt/dg-isle/acme/docker-compose.yml # In nano I changed all instances of the external network name from \u0026#39;proxy\u0026#39; to \u0026#39;isle-external\u0026#39;, and our host \u0026#39;../certs:\u0026#39; directory to \u0026#39;../config/proxy/ssl-certs\u0026#39; in order to match ISLE conventions. nano /opt/dg-isle/restart.sh # In nano I changed the external network name from \u0026#39;proxy\u0026#39; to \u0026#39;isle-external\u0026#39; and modified directory names for other portions from `~/host` to `/opt/dg-isle` # The next command will use the new `restart.sh` script to launch ISLE ./restart.sh And that\u0026rsquo;s a wrap for this episode. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/093-traefik-and-acme.sh-for-dg-staging/","tags":["ISLE","migrate","staging","Docker","Traefik","ACME","DNS-01","docker-traefik2-host","docker-compose"],"title":"Traefik and Acme.sh for DG-STAGING"},{"categories":null,"contents":"Last evening, just before the World Champion Kansas City Chiefs kicked off the NFL\u0026rsquo;s 2020-21 season (I hope the season is healthy all the way into 2021), I attempted to update all the Drupal contrib modules, and core, in my new local instance of ISLE as chronicled in the Next Steps chapter of ISLE Local Migration Customization. Ultimately that update process left me with a host of incorrect owner/group/permissions issues in the Drupal code, and I was able to remedy those in short order. But that left me with lots of remaining \u0026lsquo;mixed content\u0026rsquo;, or \u0026lsquo;mixed mode\u0026rsquo;, errors.\nWhat\u0026rsquo;s a \u0026lsquo;Mixed Content\u0026rsquo; Error? ISLE, and virtually everything I do on the web these days is run in https, that is to say that all my web sites and apps are accessible via https://subdomain.domain style references, where the s in https stands for \u0026ldquo;secure\u0026rdquo;. It is vitally important! So a \u0026ldquo;mixed content\u0026rdquo; error is one that generally is exposed in a message like this:\nBlocked loading mixed active content \u0026#34;http://dg.localdomain/sites/default/files/css/css_lQaZfjVpwP_oGNqdtWCSpJT1EMqXdMiU84ekLLxQnc4.css\u0026#34; Note that there\u0026rsquo;s NO s at the end of http in that message. Essentially, this means that the offending CSS, in this particular case, is coded to use an http:// reference to a resource, but our process and browser are expecting \u0026ldquo;secure\u0026rdquo; communications and use of https throughout. That condition is what I have come to call a \u0026ldquo;mixed mode\u0026rdquo; error, also commonly referred to as a \u0026ldquo;mixed content\u0026rdquo; condition.\nNot My First Rodeo I have encountered and corrected \u0026ldquo;mixed content\u0026rdquo; errors like this before, but only in some of my Hugo web sites, until now. The rather clumsy corrections I made in the Hugo instances didn\u0026rsquo;t seem likely to work with ISLE and Drupal, so I was at a loss and getting frustrated that such a seemingly simple thing had brought my new ISLE instance to its knees.\nMy Heroes: Noah Smith, Born-Digital, and the ICG Fortunately, when I get frustrated by such things I commonly turn to my esteemed colleagues in the Islandora Collaboration Group (ICG). In this instance I posted a question to the group\u0026rsquo;s Slack workspace and the #isle-support channel. My dear friend Noah Smith, from Born-Digital (BD), saved the day with a very quick, concise, and elegant solution!\nIt seems my update to Drupal core included changes to my main .htaccess file, and apparently those changes removed one critical statement. The wisdom that Noah shared with me was this:\nthis should be in there to force HTTPS\nSetEnvIf X-Forwarded-Proto https HTTPS=on\nI added that line to the end of /var/www/html/.htaccess in my Apache container and revisited https://dg.localdomain\u0026hellip; and it worked perfectly! Kudos and eternal gratitude to Noah and all my heroes at BD and the ICG!\nBorn Digital\u0026rsquo;s Script Noah related to me that at Born Digital they routinely run the following script.\ngrep -ri \u0026#39;SetEnvIf X-Forwarded-Proto https HTTPS=on\u0026#39; web/.htaccess || echo \\\u0026#34;SetEnvIf X-Forwarded-Proto https HTTPS=on\\\u0026#34; | tee -a web/.htaccess And that\u0026rsquo;s a \u0026#x1f603; wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/091-fixing-mixed-mode-errors/","tags":["mixed content","mixed mode","error","JavaScript",".htaccess"],"title":"Fixing 'Mixed Content' Errors"},{"categories":null,"contents":" Attention: Take note that annotations stop where my process departed from this script in Step 16. Enter at your own risk beyond the annotation in Step 16!\nThis post is an addendum to earlier posts 087 and 090. It is intended to chronicle my efforts to migrate to a staging instance of Digital.Grinnell on Linux node DGDockerX.grinnell.edu. The remainder of this document is an annotated copy of Staging ISLE Installation: Migrate Existing Islandora Site.\nAnnotations, with information specific to my experience with Digital.Grinnell migration, appear in text blocks like this one.\nStaging ISLE Installation: Migrate Existing Islandora Site Expectations: It takes an average of 2-4+ hours to read this documentation and complete this installation.\nThis Staging ISLE Installation will be similar to the Local ISLE Installation: Migrate Existing Islandora Site instructions you just followed but in addition to using a copy of your currently running Production themed Drupal website, a copy of the Production Fedora repository will also be needed for you to continue migrating to ISLE with the end goal of first deploying to an ISLE Production environment and then cut over from the existing non-ISLE Production and Staging servers to their new ISLE counterparts.\nIslandora Drupal site code here should be considered almost finished but hosted here for last touches and team review privately prior to pushing to public Production. Fedora data will be a mirror of your currently running Production Fedora repository. It is recommended that this remote site not be publicly accessible.\nThis installation builds a Staging environment for the express purpose of migrating a previously existing Islandora site onto the ISLE platform. If you need to build a brand new Staging site for development and are not migrating an existing Islandora site, then please stop and use the Local ISLE Installation: New Site instructions first and then the Staging ISLE Installation: New Site instead.\nAs this Staging domain will require a real domain name or FQDN, we recommend the following:\nIf you do not have a Staging server:\nWork with your IT department or appropriate resource for an \u0026ldquo;A record\u0026rdquo; to be added for your domain to \u0026ldquo;point\u0026rdquo; to your new Staging Host Server IP address in your institution\u0026rsquo;s DNS records. We recommend that this sub-domain use -staging to differentiate it from the Production site Example:https://yourprojectnamehere-staging.institution.edu If you have a current non-ISLE Staging server(s):\nYou shutdown any current non-ISLE Staging servers and only use the ISLE server from now on. Work with your IT department or appropriate resource for the existing \u0026ldquo;A record\u0026rdquo; for the current non-ISLE Staging domain to now \u0026ldquo;point\u0026rdquo; to your new Staging Host Server IP address in your institution\u0026rsquo;s DNS records. This is critical to be performed PRIOR to any further work below. Once this has been completed, if you do not want to use Let\u0026rsquo;s Encrypt, you can also request commercial SSL certificates from your IT department for this domain as well. Please note the DNS records will need to exist prior to the creation of any SSL certificate (Commercial or Let\u0026rsquo;s Encrypt.)\nIf you already have pre-existing Staging commercial SSL certificates, they can certainly be reused and copied into the ISLE project as directed. Unlike the Local and Demo setups, you will not have to edit /etc/localhosts to view your domain given that DNS is now involved. Your new domain will no longer use the .localdomain but instead something like https://yourprojectnamehere-staging.institution.edu\nThis document also has directions on how you can save newly updated ISLE code into a git software repository as a workflow process designed to manage and upgrade the environments throughout the development process from Local to Staging to Production. The ISLE Installation: Environments documentation can also help with explaining the new ISLE structure, the associated files and what values ISLE end-users should use for the .env, staging.env, etc.\nThis document does not have directions on how you can save previously existing Islandora Drupal code into a git repository and assumes this step has already happened. The directions below will explain how to clone Islandora Drupal code from a previously existing Islandora Drupal git repository that should already be accessible to you.\nPlease post questions to the public Islandora ISLE Google group, or subscribe to receive emails. The Glossary defines terms used in this documentation.\nAssumptions / Prerequisites This Staging ISLE installation is intended for an existing Production Islandora Drupal site to be imported along with a copy of the current Production Fedora Repository for further ISLE migration testing, Drupal theme development, ingest testing, etc. on a remote ISLE host server.\nSome materials are to be \u0026ldquo;migrated\u0026rdquo; from the work you performed on your personal computer from the prior steps and processes in Local ISLE Installation: Migrate Existing Islandora Site instructions. You will be using ISLE version 1.2.0 or higher.\nYou are using Docker-compose 1.24.0 or higher.\nYou have git installed on your personal computer as well as the remote ISLE host server.\nYou have already provisioned a remote ISLE hosts server and have documented its IP address.\nYou may have used the ISLE Ansible script to accomplish this. If doing this manually, please review the following to ensure the remote Staging ISLE host server has all dependencies e.g. CPU, memory and disk space prior to deploying the ISLE Staging environment profile for deploy Hardware Requirements Software Dependencies This server should be running at the time of deploy. Critical - This Staging server has the same amount of disk space as your current Production Fedora server does in order to store a copy of the Fedora repository. Please ensure that these sizes match. Please also plan on adding additional capacity as needed for any potential ingest testing, etc. You have a previously existing private Islandora Drupal git repository.\nYou have access to a private git repository in Github, Bitbucket, Gitlab, etc.\nIf you do not, please contact your IT department for git resources, or else create an account with one of the above providers. WARNING: Only use Private git repositories given the sensitive nature of the configuration files. DO NOT share these git repositories publicly. You have already have the appropriate A record entered into your institutions DNS system and can resolve the Staging domain (https://yourprojectnamehere-staging.institution.edu) using a tool like https://www.whatsmydns.net/\nYou have reviewed the ISLE Installation: Environments for more information about suggested Staging values.\nYou are familiar with using tools like scp, cp or rsync to move configurations, files and data from your local to the remote Staging server.\nYou have access to your Production Islandora Drupal, Solr and Fedora data and copy from your servers to the new ISLE Staging server.\nYou will schedule a content freeze for all Production Fedora ingests and additions to your Production website. This will allow you to get up to date data from Production to Staging.\nIndex of Instructions This process will differ slightly from previous builds in that there is work to be done on the local to then be pushed to the Staging ISLE Host server with additional followup work to be performed on the remote Staging ISLE Host server.\nThe instructions that follow below will have either a On Local or a On Remote Staging pre-fix to indicate where the work and focus should be. In essence, the git workflow established during the local build process will be extended for deploying on Staging and for future ISLE updates and upgrades.\nSteps 1-6: On Local - Configure the ISLE Staging Environment Profile for Deploy to Remote\nStep 1: Copy Production Data to Your Local Step 2: On Local - Shutdown Any Local Containers \u0026amp; Review Local Code Step 3: On Local - Create New Users and Passwords by Editing \u0026ldquo;staging.env\u0026rdquo; Step 4: On Local - Review and Edit \u0026ldquo;docker-compose.staging.yml\u0026rdquo; Step 4A: On Local - (Optional) Changes for \u0026ldquo;docker-compose.staging.yml\u0026rdquo; Step 5: On Local Staging - If Using Commercial SSLs Step 6: On Local - Commit ISLE Code to Git Repository Steps 7-18: On Remote Staging - Configure the ISLE Staging Environment Profile for Launch and Usage\nStep 7: On Remote Staging - Git Clone the ISLE Repository to the Remote Staging ISLE Host Server Step 8: On Remote Staging - Create the Appropriate Local Data Paths for Apache, Fedora and Log Data Step 9: On Remote Staging - Clone Your Production Islandora Code Step 10: On Remote Staging - Copy Over the Production Data Directories Step 11: On Remote Staging - If Using Let\u0026rsquo;s Encrypt Step 12: On Remote Staging - Edit the \u0026ldquo;.env\u0026rdquo; File to Change to the Staging Environment Step 13: On Remote Staging - Download the ISLE Images Step 14: On Remote Staging - Start Containers Step 15: On Remote Staging - Import the Production MySQL Drupal Database Step 16: On Remote Staging - Run ISLE Scripts Step 17: On Remote Staging - Re-Index Fedora \u0026amp; Solr Step 18: On Remote Staging - Review and Test the Drupal Staging Site Step 1: Copy Production Data to Your Local I\u0026rsquo;ve made no changes to my production data since it was last copied to my local workstation, therefore I skipped this step.\nDrupal Site Database You are repeating this step given that data may have changed on the Production site since creating your local. It is critical that Staging be a mirror or close to exact copy of Production.\nPrior to attempting this step, please consider the following:\nDrupal website databases can have a multitude of names and conventions. Confer with the appropriate IT departments for your institution\u0026rsquo;s database naming conventions.\nRecommended that the production databases be exported using the .sql /or .gz file formats (e.g. \u0026ldquo;prod_drupal_site_082019.sql.gz\u0026rdquo;) for better compression and minimal storage footprint.\nIf the end user is running multi-sites, there will be additional databases to export.\nDo not export the fedora3 database\nIf possible, on the production Apache web server, run drush cc all from the command line on the production server in the /var/www/html directory PRIOR to any db export(s). Otherwise issues can occur on import due to all cache tables being larger than innodb_log_file_size allows\nExport the Production MySQL Islandora Drupal Database Export the MySQL database for the current Production Islandora Drupal site in use and copy it to your local in an easy to find place. In later steps you\u0026rsquo;ll be directed to import this file. Please be careful performing any of these potential actions below as the process impacts your Production site. If you are not comfortable or familiar with performing these actions, we recommend that you instead work with your available IT resources to do so. To complete this process, you may use a MySQL GUI client or, if you have command line access to the MySQL database server, you may run the following command, substituting your actual user and database names: Example: mysqldump -u username -p database_name \u0026gt; prod_drupal_site_082019.sql Copy this file down to your personal computer. Step 2: On Local - Shutdown Any Local Containers \u0026amp; Review Local Code I conducted this work from my MacBook where there are no local containers to shut down or review.\nEnsure that your ISLE and Islandora git repositories have all the latest commits and pushes from the development process that took place on your personal computer. If you haven\u0026rsquo;t yet finished, do not proceed until everything is completed.\nOnce finished, open a terminal (Windows: open Git Bash)\nNavigate to your Local ISLE repository Shut down any local containers e.g. docker-compose down Step 3: On Local - Create New Users and Passwords by Editing \u0026ldquo;staging.env\u0026rdquo; Open the \u0026ldquo;staging.env\u0026rdquo; file in a text editor. Find each comment that begins with: # Replace this comment with a ... and follow the commented instructions to edit the passwords, database and user names. Review carefully as some comments request that you replace with ...26 alpha-numeric characters while others request that you create an ...easy to read but short database name. It is okay if you potentially repeat the values previously entered for your local (DRUPAL_DB) \u0026amp; (DRUPAL_DB_USER) in this Staging environment but we strongly recommend not reusing all passwords for environments e.g. (DRUPAL_DB_PASS) \u0026amp; (DRUPAL_HASH_SALT) should be unique values for each environment. In many cases the username is already pre-populated. If it doesn\u0026rsquo;t have a comment directing you to change or add a value after the =, then don\u0026rsquo;t change it. Once finished, save and close the file. I checked and found no significant differences between my local environment and staging, so I copied the local to staging and changed only the Drupal hash salt value, and the first two comment lines.\nOpen the config/apache/settings_php/settings.staging.php file. Find the first comment that begins with: # ISLE Configuration and follow the commented instructions to edit the database, username and password. Find the second comment that begins with: # ISLE Configuration and follow the instructions to edit the Drupal hash salt. Once finished, save and close the file. Once again, I checked and found no significant differences between my local environment and staging, so I copied the local to staging and changed only the Drupal hash salt value, making it match the changes noted above.\nStep 4: On Local - Review and Edit \u0026ldquo;docker-compose.staging.yml\u0026rdquo; Review the disks and volumes on your remote Staging ISLE Host server to ensure they are of an adequate capacity for your collection needs and match what has been written in the docker-compose.staging.yml file.\nPlease read through the docker-compose.staging.yml file as there are bind mount points that need to be configured on the host machine, to ensure data persistence. There are suggested bind mounts that the end-user can change to fit their needs or they can setup additional volumes or disks to match the suggestions.\nIn the fedora services section - /mnt/data/fedora/datastreamStore:/usr/local/fedora/data/datastreamStore - /opt/data/fedora/objectStore:/usr/local/fedora/data/objectStore In the apache services section - /opt/data/apache/html:/var/www/html Review the your docker-compose.local.yml file for custom edits made and copy them to the docker-compose.staging.yml file as needed, this can include changes to Fedora GSearch Transforms, Fedora hash size and more.\nAs was the case with my local environment, I\u0026rsquo;ve chosen to append a separate docker-compose file, this time named docker-compose.DG-STAGING.yml. This new file configures things so that my FEDORA repository is mounted at /mnt/data/DG-FEDORA, rather than on a USB stick named DG-FEDORA.\nSSL Certificates Depending on your choice of SSL type (Commercial SSL files or the Let\u0026rsquo;s Encrypt service), you\u0026rsquo;ll need to uncomment only one line of the traefik services section. There are also inline instructions to this effect in the docker-compose.staging.yml file.\nTo use Let's Encrypt for SSL, uncomment:\n- ./config/proxy/acme.json:/acme.json To use commercial SSLs, uncomment:\n./config/proxy/ssl-certs:/certs:ro Additionally you\u0026rsquo;ll need to add your SSL certs (.cert, .pem, .key) files to config/proxy/ssl-certs Based on the choice of SSL type made above, you\u0026rsquo;ll need to refer to the /config/proxy/traefik.staging.toml file for further configuration instructions.\nI chose to use Let's Encrypt for SSL and configured files accordingly.\nStep 4A: On Local - (Optional) Changes for \u0026ldquo;docker-compose.staging.yml\u0026rdquo; I chose to make none of the optional changes at this time.\nThis section is for optional changes for the docker-compose.staging.yml, end-users do not have feel like they have to make any choices here and can continue to Step 4 as needed.\nThe options include PHP settings, Java Memory Allocation, MySQL configuration and use of the TICK Stack\n(Optional) - You can change PHP settings such as file upload limits and memory usage by uncommenting the following in the apache services section.\n- ./config/apache/php_ini/php.staging.ini:/etc/php/7.1/apache2/php.ini You\u0026rsquo;ll then need to make edits in the ./config/apache/php_ini/php.staging.ini file. (Optional) - This line is already uncommented by default in ISLE but we\u0026rsquo;re calling it out here that you can changes to the suggested levels or values within the ./config/mysql/ISLE.cnf file if needed. When setting up for the first time, it is best practice to leave these settings in place. Over time, you can experiment with further tuning and experimentation based on your project or system needs.\n(Optional) - You can change the suggested JAVA_MAX_MEM \u0026amp; JAVA_MIN_MEM levels but do not exceed more than 50% of your system memory. When setting up for the first time, it is best practice to leave these settings in place as they are configured for a Staging ISLE Host Server using 16 GB of RAM. Over time, you can experiment with further tuning and experimentation based on your project or system needs.\n(Optional) - You can opt to uncomment the TICK stack settings for monitoring but you\u0026rsquo;ll need to follow the TICK Stack instructions prior to committing changes to your ISLE git repository.\nAll TICK related code can be found at the end of all ISLE services within the docker-compose.staging.yml file. Example: ## _(Optional)_: Uncomment lines below to run ISLE with the TICK monitoring system logging: driver: syslog options: tag: \u0026#34;{{.Name}}\u0026#34; Uncomment the lines found in the new TICK stack services section of the docker-compose.staging.yml file for hosting of that monitoring service on the Staging ISLE Host server. There are additional configurations to be made to files contained within ./config/tick but you\u0026rsquo;ll need to follow the TICK Stack instructions prior to committing changes to your ISLE git repository. Uncomment the TICK stack data volumes as well at the bottom of the file. Step 5: On Local Staging - If Using Commercial SSLs I chose to use Let's Encrypt for SSL and configured files accordingly.\nIf you are going to use Let\u0026rsquo;s Encrypt instead, you can skip this step and move onto the next one. There will be additional steps further in this document, to help you configure it.\nIf you have decided to use Commercial SSL certs supplied to you by your IT team or appropriate resource, please continue following this step.\nAdd your Commercial SSL certificate and key files to the ./config/proxy/ssl-certs directory\nExample: ./config/proxy/ssl-certs/yourprojectnamehere-staging.domain.cert ./config/proxy/ssl-certs/yourprojectnamehere-staging.domain.key Edit the ./config/proxy/traefik.staging.toml and follow the in-line instructions. Replace the .pem \u0026amp; .key with the name of your Staging SSL certificate and associated key. Do note the positioning of the added lines. Third character indentation.\nNote: despite the instruction examples differing on file type, (.pem or cert), either one is compatible, use what you have been given. Merely change the file type suffix accordingly.\nExample: .cert\n[entryPoints.https.tls] [[entryPoints.https.tls.certificates]] certFile = \u0026#34;/certs/yourprojectnamehere-staging.domain.cert\u0026#34; keyFile = \u0026#34;/certs/yourprojectnamehere-staging.domain.key\u0026#34; Example: .pem\n[entryPoints.https.tls] [[entryPoints.https.tls.certificates]] certFile = \u0026#34;/certs/sitename-staging.institution.edu.pem\u0026#34; keyFile = \u0026#34;/certs/sitename-staging.institution.edu.key\u0026#34; Step 6: On Local - Commit ISLE Code to Git Repository Once you have made all of the appropriate changes to your Staging profile. Please note the steps below are suggestions. You might use a different git commit message. Substitute \u0026lt;changedfileshere\u0026gt; with the actual file names and paths. You may need to do this repeatedly prior to the commit message. git add \u0026lt;changedfileshere\u0026gt; git commit -m \u0026quot;Changes for Staging\u0026quot; git push origin master I chose to save my work in a new branch so:\n╭─markmcfate@MAC02NX13MG5RP ~/GitHub/dg-isle ‹ruby-2.3.0› ‹completed-install-local-migrate*› ╰─$ git checkout -b ready-for-staging D\tantibot M\tconfig/apache/settings_php/settings.staging.php M\tconfig/proxy/traefik.staging.toml M\tdocker-compose.staging.yml D\tislandora_mods_via_twig M\tstaging.env Switched to a new branch \u0026#39;ready-for-staging\u0026#39; Then:\n╭─markmcfate@MAC02NX13MG5RP ~/GitHub/dg-isle ‹ruby-2.3.0› ‹ready-for-staging*› ╰─$ git status On branch ready-for-staging Changes not staged for commit: (use \u0026#34;git add/rm \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git checkout -- \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) deleted: antibot modified: config/apache/settings_php/settings.staging.php modified: config/proxy/traefik.staging.toml modified: docker-compose.staging.yml deleted: islandora_mods_via_twig modified: staging.env Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) docker-compose.DG-STAGING.yml staging.original.env no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) ╭─markmcfate@MAC02NX13MG5RP ~/GitHub/dg-isle ‹ruby-2.3.0› ‹ready-for-staging*› ╰─$ git add . ╭─markmcfate@MAC02NX13MG5RP ~/GitHub/dg-isle ‹ruby-2.3.0› ‹ready-for-staging*› ╰─$ git commit -m \u0026#34;Ready for staging, maybe\u0026#34; [ready-for-staging 870bcd6] Ready for staging, maybe 8 files changed, 423 insertions(+), 60 deletions(-) delete mode 100644 antibot create mode 100644 docker-compose.DG-STAGING.yml delete mode 100644 islandora_mods_via_twig create mode 100644 staging.original.env ╭─markmcfate@MAC02NX13MG5RP ~/GitHub/dg-isle ‹ruby-2.3.0› ‹ready-for-staging› ╰─$ git push origin ready-for-staging Counting objects: 12, done. Delta compression using up to 4 threads. Compressing objects: 100% (12/12), done. Writing objects: 100% (12/12), 6.00 KiB | 3.00 MiB/s, done. Total 12 (delta 9), reused 0 (delta 0) remote: Resolving deltas: 100% (9/9), completed with 8 local objects. remote: remote: Create a pull request for \u0026#39;ready-for-staging\u0026#39; on GitHub by visiting: remote: https://github.com/Digital-Grinnell/dg-isle/pull/new/ready-for-staging remote: To https://github.com/Digital-Grinnell/dg-isle * [new branch] ready-for-staging -\u0026gt; ready-for-staging ╭─markmcfate@MAC02NX13MG5RP ~/GitHub/dg-isle ‹ruby-2.3.0› ‹ready-for-staging› ╰─$ On Remote Staging - Configure the ISLE Staging Environment Profile for Launch and Usage Step 7: On Remote Staging - Git Clone the ISLE Repository to the Remote Staging ISLE Host Server This assumes you have setup an Islandora deploy user. If not use a different non-root user for this purpose.\nYou will also need to ensure that any /home/islandora/.ssh/id_rsa.pub key has been added to your git repository admin panel to allow for cloning from your two private git repositories.\nSince the /opt directory might not let you do this at first, we suggest the following workaround which you\u0026rsquo;ll only need to do once. Future ISLE updates will not require this step.\nShell into your Staging ISLE host server as the Islandora user.\nClone your ISLE project repository with the newly committed changes for Staging to the Islandora user home directory.\ngit clone https://yourgitproviderhere.com/yourinstitutionhere/yourprojectnamehere-isle.git /home/islandora/ This may take a few minutes (2-4) depending on your server\u0026rsquo;s Internet connection. Move the newly cloned directory to the /opt directory as the root user\nsudo mv /home/islandora/yourprojectnamehere-isle /opt/yourprojectnamehere-isle Fix the permissions so that the islandora user has access.\nsudo chown -Rv islandora:islandora /opt/yourprojectnamehere-isle My chosen command sequence and results were:\n╭─islandora@dgdockerx ~ ╰─$ git clone https://github.com/Digital-Grinnell/dg-isle Cloning into \u0026#39;dg-isle\u0026#39;... Username for \u0026#39;https://github.com\u0026#39;: digital@grinnell.edu Password for \u0026#39;https://digital@grinnell.edu@github.com\u0026#39;: remote: Enumerating objects: 5103, done. remote: Counting objects: 100% (5103/5103), done. remote: Compressing objects: 100% (1799/1799), done. remote: Total 5103 (delta 3235), reused 5036 (delta 3168), pack-reused 0 Receiving objects: 100% (5103/5103), 7.23 MiB | 0 bytes/s, done. Resolving deltas: 100% (3235/3235), done. ╭─islandora@dgdockerx ~ ╰─$ sudo mv -f ./dg-isle /opt/dg-isle ╭─islandora@dgdockerx ~ ╰─$ cd /opt/dg-isle ╭─islandora@dgdockerx /opt/dg-isle ‹master› ╰─$ git checkout ready-for-staging Branch ready-for-staging set up to track remote branch ready-for-staging from origin. Switched to a new branch \u0026#39;ready-for-staging\u0026#39; ╭─islandora@dgdockerx /opt/dg-isle ‹ready-for-staging› ╰─$ sudo chown -Rv islandora:islandora /opt/dg-isle ownership of ‘/opt/dg-isle/vagrant/CentOS/Vagrantfile’ retained as islandora:islandora ownership of ‘/opt/dg-isle/vagrant/CentOS’ retained as islandora:islandora ownership of ‘/opt/dg-isle/vagrant/Ubuntu/Vagrantfile’ retained as islandora:islandora ... Step 8: On Remote Staging - Create the Appropriate Local Data Paths for Apache, Fedora and Log Data Create the /opt/data directory sudo mkdir -p /opt/data Change the permissions to the Islandora user. sudo chown -Rv islandora:islandora /opt/data My chosen command sequence and results were:\n╭─islandora@dgdockerx ~ ╰─$ sudo mkdir -p /opt/data ╭─islandora@dgdockerx /opt/data ╰─$ sudo chown -Rv islandora:islandora /opt/data ownership of ‘/opt/data’ retained as islandora:islandora Step 9: On Remote Staging - Clone Your Production Islandora Code Please clone from your existing Production Islandora git repository.\ngit clone git@yourgitproviderhere.com/yourinstitutionhere/yourprojectnamehere-islandora.git /opt/data/apache/html\nFix the permissions so that the islandora user has access.\nsudo chown -Rv islandora:islandora /opt/data/apache/html My chosen command sequence and results were:\n╭─islandora@dgdockerx ~ ╰─$ git clone https://github.com/Digital-Grinnell/dg-islandora --recursive Cloning into \u0026#39;dg-islandora\u0026#39;... Username for \u0026#39;https://github.com\u0026#39;: digital@grinnell.edu Password for \u0026#39;https://digital@grinnell.edu@github.com\u0026#39;: remote: Enumerating objects: 598, done. remote: Counting objects: 100% (598/598), done. remote: Compressing objects: 100% (557/557), done. remote: Total 10404 (delta 102), reused 200 (delta 33), pack-reused 9806 Receiving objects: 100% (10404/10404), 62.94 MiB | 37.71 MiB/s, done. Resolving deltas: 100% (2196/2196), done. Submodule \u0026#39;sites/all/modules/islandora/dg7\u0026#39; (https://github.com/DigitalGrinnell/dg7) registered for path \u0026#39;sites/all/modules/islandora/dg7\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/idu\u0026#39; (https://github.com/DigitalGrinnell/idu) registered for path \u0026#39;sites/all/modules/islandora/idu\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_binary_object\u0026#39; (https://github.com/Islandora-Labs/islandora_binary_object) registered for path \u0026#39;sites/all/modules/islandora/islandora_binary_object\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_collection_search\u0026#39; (https://github.com/discoverygarden/islandora_collection_search) registered for path \u0026#39;sites/all/modules/islandora/islandora_collection_search\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_datastream_exporter\u0026#39; (https://github.com/Islandora-Labs/islandora_datastream_exporter.git) registered for path \u0026#39;sites/all/modules/islandora/islandora_datastream_exporter\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_datastream_replace\u0026#39; (https://github.com/DigitalGrinnell/islandora_datastream_replace.git) registered for path \u0026#39;sites/all/modules/islandora/islandora_datastream_replace\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_mods_display\u0026#39; (https://github.com/DigitalGrinnell/islandora_mods_display.git) registered for path \u0026#39;sites/all/modules/islandora/islandora_mods_display\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_multi_importer\u0026#39; (https://github.com/DigitalGrinnell/islandora_multi_importer) registered for path \u0026#39;sites/all/modules/islandora/islandora_multi_importer\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_solr_collection_view\u0026#39; (https://github.com/Islandora-Labs/islandora_solr_collection_view.git) registered for path \u0026#39;sites/all/modules/islandora/islandora_solr_collection_view\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_solr_views\u0026#39; (https://github.com/DigitalGrinnell/islandora_solr_views) registered for path \u0026#39;sites/all/modules/islandora/islandora_solr_views\u0026#39; Submodule \u0026#39;sites/all/modules/islandora/islandora_solution_pack_oralhistories\u0026#39; (https://github.com/Islandora-Labs/islandora_solution_pack_oralhistories.git) registered for path \u0026#39;sites/all/modules/islandora/islandora_solution_pack_oralhistories\u0026#39; Submodule \u0026#39;sites/all/themes/bootstrap\u0026#39; (https://github.com/drupalprojects/bootstrap.git) registered for path \u0026#39;sites/all/themes/bootstrap\u0026#39; Submodule \u0026#39;sites/default/themes/digital_grinnell_bootstrap\u0026#39; (https://github.com/DigitalGrinnell/digital_grinnell_bootstrap.git) registered for path \u0026#39;sites/default/themes/digital_grinnell_bootstrap\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/dg7\u0026#39;... remote: Enumerating objects: 335, done. remote: Total 335 (delta 0), reused 0 (delta 0), pack-reused 335 Receiving objects: 100% (335/335), 201.05 KiB | 0 bytes/s, done. Resolving deltas: 100% (203/203), done. Submodule path \u0026#39;sites/all/modules/islandora/dg7\u0026#39;: checked out \u0026#39;f6cca12904d24c57fcc408b93db92893a564e231\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/idu\u0026#39;... remote: Enumerating objects: 21, done. remote: Counting objects: 100% (21/21), done. remote: Compressing objects: 100% (15/15), done. remote: Total 64 (delta 10), reused 16 (delta 6), pack-reused 43 Unpacking objects: 100% (64/64), done. Submodule path \u0026#39;sites/all/modules/islandora/idu\u0026#39;: checked out \u0026#39;0d91bd6e563f2955563649fc9168c4e8e518f45c\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_binary_object\u0026#39;... remote: Enumerating objects: 353, done. remote: Total 353 (delta 0), reused 0 (delta 0), pack-reused 353 Receiving objects: 100% (353/353), 80.16 KiB | 0 bytes/s, done. Resolving deltas: 100% (187/187), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_binary_object\u0026#39;: checked out \u0026#39;53b67d57cf1ca8052910a90006ad0af183a23389\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_collection_search\u0026#39;... remote: Enumerating objects: 438, done. remote: Total 438 (delta 0), reused 0 (delta 0), pack-reused 438 Receiving objects: 100% (438/438), 83.91 KiB | 0 bytes/s, done. Resolving deltas: 100% (209/209), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_collection_search\u0026#39;: checked out \u0026#39;69545bed8d953d71344fee44de39074d88da0f90\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_datastream_exporter\u0026#39;... remote: Enumerating objects: 80, done. remote: Total 80 (delta 0), reused 0 (delta 0), pack-reused 80 Unpacking objects: 100% (80/80), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_datastream_exporter\u0026#39;: checked out \u0026#39;b1be7e77fd9f14b72dd1a78130109ce0d9a51fd3\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_datastream_replace\u0026#39;... remote: Enumerating objects: 6, done. remote: Counting objects: 100% (6/6), done. remote: Compressing objects: 100% (4/4), done. remote: Total 84 (delta 2), reused 6 (delta 2), pack-reused 78 Unpacking objects: 100% (84/84), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_datastream_replace\u0026#39;: checked out \u0026#39;0e786886d8da2ebc30a89d23654710aaa180cceb\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_mods_display\u0026#39;... remote: Enumerating objects: 128, done. remote: Counting objects: 100% (128/128), done. remote: Compressing objects: 100% (80/80), done. remote: Total 271 (delta 91), reused 85 (delta 48), pack-reused 143 Receiving objects: 100% (271/271), 282.28 KiB | 0 bytes/s, done. Resolving deltas: 100% (182/182), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_mods_display\u0026#39;: checked out \u0026#39;08249e5a0486c28c698acb7d11db24af5c6ec63c\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_multi_importer\u0026#39;... remote: Enumerating objects: 978, done. remote: Total 978 (delta 0), reused 0 (delta 0), pack-reused 978 Receiving objects: 100% (978/978), 643.47 KiB | 0 bytes/s, done. Resolving deltas: 100% (681/681), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_multi_importer\u0026#39;: checked out \u0026#39;78c06b719287a3c0250e902552ec05f760780b9b\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_solr_collection_view\u0026#39;... remote: Enumerating objects: 134, done. remote: Total 134 (delta 0), reused 0 (delta 0), pack-reused 134 Receiving objects: 100% (134/134), 31.88 KiB | 0 bytes/s, done. Resolving deltas: 100% (72/72), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_solr_collection_view\u0026#39;: checked out \u0026#39;c4b2f33251e3f46bb3bc4789480a60a8efe5d351\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_solr_views\u0026#39;... remote: Enumerating objects: 615, done. remote: Total 615 (delta 0), reused 0 (delta 0), pack-reused 615 Receiving objects: 100% (615/615), 174.41 KiB | 0 bytes/s, done. Resolving deltas: 100% (354/354), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_solr_views\u0026#39;: checked out \u0026#39;3c17f497a51bc7f5bf90a6bd38e02733b79dca94\u0026#39; Cloning into \u0026#39;sites/all/modules/islandora/islandora_solution_pack_oralhistories\u0026#39;... remote: Enumerating objects: 11, done. remote: Counting objects: 100% (11/11), done. remote: Compressing objects: 100% (11/11), done. remote: Total 3434 (delta 3), reused 2 (delta 0), pack-reused 3423 Receiving objects: 100% (3434/3434), 11.20 MiB | 0 bytes/s, done. Resolving deltas: 100% (1392/1392), done. Submodule path \u0026#39;sites/all/modules/islandora/islandora_solution_pack_oralhistories\u0026#39;: checked out \u0026#39;60b84295deb4c7b11fbe5c172e3f815eb78cd942\u0026#39; Cloning into \u0026#39;sites/all/themes/bootstrap\u0026#39;... remote: Enumerating objects: 12635, done. remote: Total 12635 (delta 0), reused 0 (delta 0), pack-reused 12635 Receiving objects: 100% (12635/12635), 2.83 MiB | 0 bytes/s, done. Resolving deltas: 100% (9255/9255), done. Submodule path \u0026#39;sites/all/themes/bootstrap\u0026#39;: checked out \u0026#39;c050d1d6ae85ef344be85dbf0a4ed9ec68ac32ce\u0026#39; Cloning into \u0026#39;sites/default/themes/digital_grinnell_bootstrap\u0026#39;... remote: Enumerating objects: 190, done. remote: Total 190 (delta 0), reused 0 (delta 0), pack-reused 190 Receiving objects: 100% (190/190), 1.08 MiB | 0 bytes/s, done. Resolving deltas: 100% (70/70), done. Submodule path \u0026#39;sites/default/themes/digital_grinnell_bootstrap\u0026#39;: checked out \u0026#39;22b51ecfb61c7e348e25892e988bcf82d0b1c781\u0026#39; ╭─islandora@dgdockerx ~ ╰─$ cd dg-islandora ╭─islandora@dgdockerx ~/dg-islandora ‹master› ╰─$ git checkout completed-install-local-migrate Branch completed-install-local-migrate set up to track remote branch completed-install-local-migrate from origin. Switched to a new branch \u0026#39;completed-install-local-migrate\u0026#39; ╭─islandora@dgdockerx ~/dg-islandora ‹completed-install-local-migrate› ╰─$ sudo mkdir -p /opt/data/apache/html ╭─islandora@dgdockerx ~/dg-islandora ‹completed-install-local-migrate› ╰─$ sudo chown -Rv islandora:islandora /opt/data/apache/html changed ownership of ‘/opt/data/apache/html’ from root:root to islandora:islandora ╭─islandora@dgdockerx ~/dg-islandora ‹completed-install-local-migrate› ╰─$ cp -fr . /opt/data/apache/html/ ╭─islandora@dgdockerx ~/dg-islandora ‹completed-install-local-migrate› ╰─$ cd /opt/data/apache/html ╭─islandora@dgdockerx /opt/data/apache/html ‹completed-install-local-migrate› ╰─$ ls -alh total 332K drwxr-xr-x. 10 islandora islandora 4.0K Sep 14 23:37 . drwxr-xr-x. 3 root root 4.0K Sep 14 23:30 .. -rw-rw-r--. 1 islandora islandora 6.5K Sep 14 23:37 authorize.php -rw-rw-r--. 1 islandora islandora 113K Sep 14 23:37 CHANGELOG.txt -rw-rw-r--. 1 islandora islandora 1.5K Sep 14 23:37 COPYRIGHT.txt -rw-rw-r--. 1 islandora islandora 720 Sep 14 23:37 cron.php -rw-rw-r--. 1 islandora islandora 2.2K Sep 14 23:37 Digital-Grinnell-Migration-Mitigation-Script.sh -rw-rw-r--. 1 islandora islandora 317 Sep 14 23:37 .editorconfig drwxrwxr-x. 9 islandora islandora 4.0K Sep 14 23:39 .git -rw-rw-r--. 1 islandora islandora 174 Sep 14 23:37 .gitignore -rw-rw-r--. 1 islandora islandora 2.6K Sep 14 23:37 .gitmodules -rw-rw-r--. 1 islandora islandora 6.1K Sep 14 23:37 .htaccess drwxrwxr-x. 4 islandora islandora 4.0K Sep 14 23:37 includes -rw-rw-r--. 1 islandora islandora 529 Sep 14 23:37 index.php -rw-rw-r--. 1 islandora islandora 1.7K Sep 14 23:37 INSTALL.mysql.txt -rw-rw-r--. 1 islandora islandora 1.9K Sep 14 23:37 INSTALL.pgsql.txt -rw-rw-r--. 1 islandora islandora 703 Sep 14 23:37 install.php -rw-rw-r--. 1 islandora islandora 1.2K Sep 14 23:37 install_solution_packs.sh -rw-rw-r--. 1 islandora islandora 1.3K Sep 14 23:37 INSTALL.sqlite.txt -rw-rw-r--. 1 islandora islandora 18K Sep 14 23:37 INSTALL.txt -rw-rw-r--. 1 islandora islandora 18K Sep 14 23:37 LICENSE.txt -rw-rw-r--. 1 islandora islandora 8.3K Sep 14 23:37 MAINTAINERS.txt -rw-rw-r--. 1 islandora islandora 3.5K Sep 14 23:37 migration_site_vsets.sh drwxrwxr-x. 6 islandora islandora 4.0K Sep 14 23:37 misc drwxrwxr-x. 42 islandora islandora 4.0K Sep 14 23:37 modules drwxrwxr-x. 5 islandora islandora 4.0K Sep 14 23:37 profiles -rw-rw-r--. 1 islandora islandora 600 Sep 14 23:37 README.md -rw-rw-r--. 1 islandora islandora 5.3K Sep 14 23:37 README.txt -rw-rw-r--. 1 islandora islandora 2.2K Sep 14 23:37 robots.txt drwxrwxr-x. 2 islandora islandora 4.0K Sep 14 23:37 scripts drwxrwxr-x. 4 islandora islandora 4.0K Sep 14 23:37 sites drwxrwxr-x. 7 islandora islandora 4.0K Sep 14 23:37 themes -rw-rw-r--. 1 islandora islandora 20K Sep 14 23:37 update.php -rw-rw-r--. 1 islandora islandora 9.9K Sep 14 23:37 UPGRADE.txt -rw-rw-r--. 1 islandora islandora 2.2K Sep 14 23:37 web.config Step 10: On Remote Staging - Copy Over the Production Data Directories It is recommended that you schedule a content freeze for all Production Fedora ingests and additions to your Production website. This will allow you to get up to date data from Production to Staging.\nAs you may have made some critical decisions potentially from \u0026ldquo;Step 0: Copy Production Data to Your Local\u0026rdquo; of the Local ISLE Installation: Migrate Existing Islandora Site instructions, you need to re-follow the steps to get your:\nProduction Drupal site files directory Solr schema \u0026amp; Islandora transforms If you picked Easy option: then you don\u0026rsquo;t need to do anything here for the Solr schema \u0026amp; Islandora transforms If you picked the Intermediate or Advanced options: You\u0026rsquo;ll need to copy in the customizations and files you created during the local environment into the docker-compose.staging.yml. Ensure that one set of transforms and schema are used across all environments. Production Fedora datastreamStore directory You\u0026rsquo;ll need to adjust the paths below in case your setup differs on either the non-ISLE Production server or the ISLE Staging server. Copy your /usr/local/fedora/data/datastreamStore data to the suggested path of /mnt/data/fedora/datastreamStore You may need to change the permissions to root:root on the Staging /mnt/data/fedora/datastreamStore directory above after copying so the Fedora container can access properly. Do not do this on your existing Production system please. Production Fedora objectStore. Copy your /usr/local/fedora/data/objectStore data to the suggested path of /opt/data/fedora/objectStore You may need to change the permissions to root:root on the Staging /opt/data/fedora/objectStore above after copying so the Fedora container can access properly. Do not do this on your existing Production system please. Since there have been NO changes to my production instance of ISLE since the last backup, and since my configuration is using the aforementioned DG-STAGING repository, I have safely skipped this entire step.\nStep 11: On Remote Staging - If Using Let\u0026rsquo;s Encrypt If you are using Commercial SSLs, then please stop and move onto the next step.\nIf using Let\u0026rsquo;s Encrypt, please continue to follow this step.\nCreate an empty acme.json within the ./config/proxy/ directory of your ISLE project. touch /opt/yourprojectnamehere/config/proxy/acme.json chmod 600 /opt/yourprojectnamehere/config/proxy/acme.json This file will be ignored by git and won\u0026rsquo;t cause any errors with checking in code despite the location Do note that you may need to open your firewall briefly to allow the SSL certs to be added to the acme.json file. This will be indicated in the following steps. Open your firewall to ports 80, 443 prior to starting up the containers to ensure SSL cert creation. OK, this is where the proverbial $hit hits the fan, so to speak. Grinnell College ITS will NOT allow both of those last two bullets to happen at the same time. So, I\u0026rsquo;m going to try and implement the strategy documented in Traefik and Acme.sh Instead of DNS-01 now. Wish me luck\u0026hellip;\nThis effort is turning out to be such a departure from the process documented here that I\u0026rsquo;m inclined to capture it\u0026rsquo;s history, unfolding as I type, in a new blog post.\nStep 12: On Remote Staging - Edit the \u0026ldquo;.env\u0026rdquo; File to Change to the Staging Environment This step is a multi-step, involved process that allows an end-user to make appropriate changes to the .env and then commit it locally to git. This local commit that never gets pushed back to the git repository is critical to allow future ISLE updates or config changes.\nCopy the sample.env to .env. By default, the Demo environment is setup. You will need to edit this file to match the correct environment. Please note that the .env is no longer tracked by git as of ISLE version 1.5. Instructions below involving git are for ISLE versions below 1.5. However the settings recommended below for the environment can still be followed as needed.\ncp sample.env .env Edit the .env, remove the local settings and then commit locally (only if using an ISLE version below 1.5)\ncd /opt/yourprojectnamehere vi / nano / pico /opt/yourprojectnamehere/.env Edit COMPOSE_PROJECT_NAME= and replace the local settings with: COMPOSE_PROJECT_NAME= (Suggested) Add an identifiable project or institutional name plus environment e.g. acme_digital_staging` Edit BASE_DOMAIN= and replace the local settings with: BASE_DOMAIN= (Suggested) Add the full staging domain here e.g. digital-staging.institution.edu Edit CONTAINER_SHORT_ID= and replace the local settings with: CONTAINER_SHORT_ID= (Suggested) Make an easy to read acronym from the letters of your institution and collection names plus environment e.g. (acme digitalcollections staging) is acdcs Edit COMPOSE_FILE change local to staging COMPOSE_FILE=docker-compose.staging.yml Save the file For users of ISLE version 1.5 and above, these git instructions below are not needed. The .env file is no longer tracked in git.\nI performed the operations prescribed above and made necessary edits choosing a COMPOSE_PROJECT_NAME of dgs and BASE_DOMAIN of dg-staging.grinnell.edu. Since I\u0026rsquo;m working with ISLE v1.5.1, I skipped the rest of this section as advised.\nFor users of ISLE versions 1.4.2 and below, you will need to continue to follow these instructions until you upgrade. Enter git status - You\u0026rsquo;ll now see the following:\nEnter git status - You\u0026rsquo;ll now see the following:\nOn branch master Your branch is up to date with \u0026#39;origin/master\u0026#39;. Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git checkout -- \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: .env You\u0026rsquo;ll need to add this file and commit it in git to be able to get future updates from ISLE as a process.\ngit add .env git commit -m \u0026quot;Added the edited .env configuration file for Staging. DO NOT PUSH BACK TO UPSTREAM REPOSITORY - Jane Doe 8/2019\u0026quot; This is a suggested warning for users NOT TO push back this configuration change to the main git repository. If that were done it could conflict with other setups. You may run into the following:\n*** Please tell me who you are. Run git config --global user.email \u0026#34;you@example.com\u0026#34; git config --global user.name \u0026#34;Your Name\u0026#34; to set your account\u0026#39;s default identity. Omit --global to set the identity only in this repository. fatal: empty ident name (for \u0026lt;islandora@yourprojectnamehere-staging.institution.edu\u0026gt;) not allowed Configure your server git client but don\u0026rsquo;t use the --global setting as that could interfere with other git repositories e.g. your Islandora Drupal code.\nExample: Within /opt/yourprojectnamehere git config user.email \u0026quot;jane@institution.edu\u0026quot; git config user.name \u0026quot;Jane Doe\u0026quot; Now re-run the commit command:\ngit commit -m \u0026#34;Added the edited .env configuration file for Staging. DO NOT PUSH BACK TO UPSTREAM REPOSITORY - Jane Doe 8/2019\u0026#34; [master 7ab3fcf9] Added the edited .env configuration file for Staging. DO NOT PUSH BACK TO UPSTREAM REPOSITORY - Jane Doe 8/2019 1 file changed, 4 insertions(+), 4 deletions(-) Step 13: On Remote Staging - Download the ISLE Images Download all of the latest ISLE Docker images (~6 GB of data may take 5-10 minutes). Using the same open terminal: Navigate to the root of your ISLE project cd ~/opt/yourprojectnamehere Start Docker systemctl start docker docker-compose pull I completed this step as documented.\nStep 14: On Remote Staging - Start Containers Note: Prior to starting the launch process, it is recommended that you briefly open your firewall to allow ports 80 and 443 access to the world. You\u0026rsquo;ll only need to keep this open for 3 -5 minutes and then promptly close access once the Let\u0026rsquo;s Encrypt SSL certificates have been generated.\nSince I am using a different SSL certificate validation process I did NOT follow the advice shared in the note above. See blog post 093.\nUsing the same open terminal:\ndocker-compose up -d Please wait a few moments for the stack to fully come up. Approximately 3-5 minutes.\nUsing the same open terminal:\nView only the running containers: docker ps View all containers (both those running and stopped): docker ps -a All containers prefixed with isle- are expected to have a STATUS of Up (for x time). If any of these are not UP, then use ISLE Installations: Troubleshooting to solve before continuing below. In your web browser, enter your Staging site URL: https://yourprojectnamehere.institution.edu\nNote: You should not see any errors with respect to the SSL certifications, you should see a nice green lock padlock for the site security. If you see a red error or unknown SSL cert provider, you\u0026rsquo;ll need to shut the containers down and review the previous steps taken especially if using Let\u0026rsquo;s Encrypt. You may need to repeat those steps to get rid of the errors. Since my process uses Let's Encrypt with a different SSL validation than documented, I executed a modified copy of my restart.sh script BEFORE spinning up my ISLE stack.\nThat modified copy of restart.sh looked like this:\n#!/bin/bash # docker network create isle-external cd /opt/dg-isle/acme docker-compose up -d; docker-compose logs docker exec -it acme --issue --dns dns_azure --server https://acme-staging-v02.api.letsencrypt.org/directory -d dgdockerx.grinnell.edu --domain-alias _acme-challenge.leverify.info --key-file /certs/dgdockex.grinnell.edu.key --cert-file /certs/dgdockerx.grinnell.edu.cert --standalone --force docker exec -it acme --issue --dns dns_azure --server https://acme-staging-v02.api.letsencrypt.org/directory -d dg-staging.grinnell.edu --domain-alias _acme-challenge.leverify.info --key-file /certs/dg-staging.grinnell.edu.key --cert-file /certs/dg-staging.grinnell.edu.cert --standalone --force #docker exec -it acme --issue --dns dns_azure -d dgdockerx.grinnell.edu --domain-alias _acme-challenge.leverify.info --key-file /certs/dgdockerx.grinnell.edu.key --cert-file /certs/dgdockerx.grinnell.edu.cert --standalone --force #docker exec -it acme --issue --dns dns_azure -d dg-staging.grinnell.edu --domain-alias _acme-challenge.leverify.info --key-file /certs/dg-staging.grinnell.edu.key --cert-file /certs/dg-staging.grinnell.edu.cert --standalone --force # cd .. docker-compose up -d; docker-compose logs After running ./restart.sh from the /opt/dg-isle directory on DGDockerX, I had 8 healthy, running containers including one instance of neilpang/ache.sh:latest. A visit to my target address of https://dg-staging.grinnell.edu yielded a \u0026ldquo;security exception\u0026rdquo; warning and a white-screen-of-death (WSOD). That's to be expected since my acme.sh validation was run against the Let's Encrypt \u0026ldquo;staging\u0026rdquo; server, not their \u0026ldquo;production\u0026rdquo; server. The WSOD could also be expected because the ISLE configuration steps documented later in this post had not yet been executed.\nNext step\u0026hellip; run the ./destory.sh script on DGDockerX to remove what we just built, edit the restart.sh script to engage the Let's Encrypt \u0026ldquo;production\u0026rdquo; server, and launch the script again. Then, test https://dg-staging.grinnell.edu just to see if the generated certificate is valid.\nMy hope of quickly completing that \u0026ldquo;next\u0026rdquo; step was dashed by an encounter with Let's Encrypt \u0026ldquo;production\u0026rdquo; server rate limits. Those limits are documented nicely at https://letsencrypt.org/docs/rate-limits/ but I hit the Duplicate Certificate limit nonetheless. \u0026#x1f626; So it\u0026rsquo;s back to using the Let's Encrypt \u0026ldquo;staging\u0026rdquo; server and pushing forward as much as possible.\nStep 15: On Remote Staging - Import the Local MySQL Drupal Database Prior to attempting this step, please consider the following:\nIf the end user is running multi-sites, there will be additional databases to export.\nDo not import the fedora3 database\nImport the Local MySQL Islandora Drupal Database Copy the local_drupal_site_082019.sql created in Step 1 to the Remote Staging server: Run docker ps to determine the mysql container name docker cp /pathto/prod_drupal_site_082019.sql.gz your-mysql-containername:/prod_drupal_site_082019.sql.gz Example: docker cp /c/db_backups/prod_drupal_site_082019.sql.gz isle-mysql-ld:/prod_drupal_site_082019.sql.gz This might take a few minutes depending on the size of the file. I started this portion of the process from my work iMac, while using a VPN connection, by copying a backup of my production database to DGDockerX like so:\n╭─markmcfate@MAD25W812UJ1G9 ~ ‹ruby-2.3.0› ╰─$ rsync -aruvi /Users/markmcfate/Desktop/migration-copy/prod_drupal_site_083120.sql islandora@dgdockerx.grinnell.edu:/opt/. --progress building file list ... 1 file to consider \u0026lt;f+++++++ prod_drupal_site_083120.sql 849614936 100% 565.47kB/s 0:24:27 (xfer#1, to-check=0/1) sent 849718797 bytes received 42 bytes 577843.48 bytes/sec total size is 849614936 speedup is 1.00 Then I copied the SQL file to the correct container and imported it using mysql, like so:\n╭─islandora@dgdockerx /opt ╰─$ docker cp /opt/prod_drupal_site_083120.sql isle-mysql-dgs:/prod_drupal_site_083120.sql ╭─islandora@dgdockerx /opt ╰─$ docker exec -it isle-mysql-dgs bash root@e1e6d5be6de8:/\\# mysql -u admin -p digital_grinnell \u0026lt; prod_drupal_site_083120.sql Enter password: *************************** Next, looking back at my annotations from Step 9 of Local ISLE Installation: Migrate Existing Islandora Site - with Annotations I elected to bring my ./sites/default/files directory over from production, like so:\n╭─markmcfate@MAD25W812UJ1G9 ~ ‹ruby-2.3.0› ╰─$ rsync -aruvi /Users/markmcfate/Desktop/migration-copy/var islandora@dgdockerx.grinnell.edu:/opt/. --progress Import the exported Local MySQL database for use in the current Staging Drupal site. Refer to your staging.env for the usernames and passwords used below. You can use a MySQL GUI client for this process instead but the command line directions are only included below. Run docker ps to determine the MySQL container name Using the same open terminal: Shell into your currently running Staging MySQL container docker exec -it your-mysql-containername bash Import the Local Islandora Drupal database. Replace the \u0026ldquo;DRUPAL_DB_USER\u0026rdquo; and \u0026ldquo;DRUPAL_DB\u0026rdquo; in the command below with the values found in your \u0026ldquo;staging.env\u0026rdquo; file. mysql -u DRUPAL_DB_USER -p DRUPAL_DB \u0026lt; local_drupal_site_082019.sql Enter the appropriate password: value of DRUPAL_DB_PASS in the \u0026ldquo;staging.env\u0026rdquo;) This might take a few minutes depending on the size of the file. Type exit to exit the container Step 16: On Remote Staging - Run ISLE Scripts This step was NOT necessary in my case since all of the modifications made here were already executed in my local instance of ISLE, and have been captured in my dg-isle and dg-islandora repositories.\nIn lieu of this and subsequent steps, I executed a number of terminal command to work through the remainder of this process. An abridged transcript of those commands and results can be found in this public gist.\nmigration_site_vsets.sh: updates Drupal database settings\nThis step will show you how to run the \u0026ldquo;migration_site_vsets.sh\u0026rdquo; script on the Apache container to change Drupal database site settings for ISLE connectivity.\nUsing the same open terminal:\nRun docker ps to determine the apache container name Copy the \u0026ldquo;migration_site_vsets.sh\u0026rdquo; to the root of the Drupal directory on your Apache container docker cp ./scripts/apache/migration_site_vsets.sh your-apache-containername:/var/www/html/migration_site_vsets.sh Change the permissions on the script to make it executable docker exec -it your-apache-containername bash -c \u0026quot;chmod +x /var/www/html/migration_site_vsets.sh\u0026quot; Run the script docker exec -it your-apache-containername bash -c \u0026quot;cd /var/www/html \u0026amp;\u0026amp; ./migration_site_vsets.sh\u0026quot; fix-permissions.sh: adjusts directory and file permissions in your Drupal site\nThis step will show you how to shell into your currently running Staging Apache container, and run the \u0026ldquo;fix-permissions.sh\u0026rdquo; script to fix the Drupal site permissions.\ndocker exec -it your-apache-containername bash sh /utility-scripts/isle_drupal_build_tools/drupal/fix-permissions.sh --drupal_path=/var/www/html --drupal_user=islandora --httpd_group=www-data This process will take 2-5 minutes You should see a lot of green [ok] messages. If the script appears to pause or prompt for y/n, DO NOT enter any values; the script will automatically answer for you. Type exit to exit the container For Microsoft Windows: You may be prompted by Windows to: - Share the C drive with Docker. Click Okay or Allow. - Enter your username and password. Do this. - Allow vpnkit.exe to communicate with the network. Click Okay or Allow (accept default selection). - If the process seems to halt, check the taskbar for background windows. install_solution_packs.sh: installs Islandora solution packs\nSince you\u0026rsquo;ve imported an existing Drupal database, you must now reinstall the Islandora solution packs so the Fedora repository will be ready to ingest objects.\nCopy the \u0026ldquo;install_solution_packs.sh\u0026rdquo; to the root of the Drupal directory on your Apache container\ndocker cp scripts/apache/install_solution_packs.sh your-apache-containername:/var/www/html/install_solution_packs.sh Change the permissions on the script to make it executable\nFor Mac/Ubuntu/CentOS/etc: docker exec -it your-apache-containername bash -c \u0026quot;chmod +x /var/www/html/install_solution_packs.sh\u0026quot; For Microsoft Windows: docker exec -it your-apache-containername bash -c \u0026quot;chmod +x /var/www/html/install_solution_packs.sh\u0026quot; Run the script\nFor Mac/Ubuntu/CentOS/etc: docker exec -it your-apache-containername bash -c \u0026quot;cd /var/www/html \u0026amp;\u0026amp; ./install_solution_packs.sh\u0026quot; For Microsoft Windows: docker exec -it your-apache-containername bash -c \u0026quot;cd /var/www/html \u0026amp;\u0026amp; ./install_solution_packs.sh\u0026quot; The above process will take a few minutes depending on the speed of your local and Internet connection.\nYou should see a lot of green [ok] messages. If the script appears to pause or prompt for \u0026ldquo;y/n\u0026rdquo;, DO NOT enter any values; the script will automatically answer for you. Proceed only after this message appears: \u0026ldquo;Done. \u0026lsquo;all\u0026rsquo; cache was cleared.\u0026rdquo;\nStep 17: On Remote Staging - Re-Index Fedora \u0026amp; Solr When migrating any non-ISLE Islandora site, it is crucial to rebuild (reindex) the following three indices from the FOXML and datastream files on disk.\nFedora\u0026rsquo;s indices:\nResource Index - The Resource Index is the Fedora module that provides the infrastructure for indexing relationships among objects and their components. SQL database - fedora3 contains information vital for the Drupal site to connect to Fedora correctly. Solr index - Solr an open source enterprise search platform works in conjunction with the Islandora Solr module to provide a way to configure the Islandora search functions, the search results display, and the display of metadata on object pages. The index serves as a list of those objects for fast searching across large collections.\nYou can use the command-line interactive utility fedora-rebuild.sh on the fedora container to rebuild all indices when the Fedora (not Tomcat) server is offline.\nDepending on the size of your repository, this entire process may take minutes (thousands of objects) or hours (millions of objects) to complete.\nReindex Fedora RI \u0026amp; Fedora SQL Database (2/3) Since this command can take minutes or hours depending on the size of your repository, As such, it is recommended starting a screen session prior to running the following commands. Learn more about screen here\nNote: The method described below is a longer way of doing this process to onboard users.\nShell into your currently running Staging Fedora container\nRun docker ps to determine the Fedora container name docker exec -it your-fedora-containername bash Navigate to the utility_scripts directory\ncd utility_scripts Run the rebuildFedora.sh script. This script will give you output like the example below.\n./rebuildFedora.sh OK - Stopped application at context path [/fedora] Starting the rebuild process in the background. This may take a while depending on your Fedora repository size. To watch the log and process run: tail -f $CATALINA_HOME/logs/fedora-rebuild.out Truncating old SQL tables. mysql: [Warning] Using a password on the command line interface can be insecure. Automatically tailing the log file... Press CTRL+C to stop watching at any time. This will NOT stop the rebuild process After a good period of time, again depending on the size of your Fedora collection there should be output like the example below. This indicates that the Fedora RI \u0026amp; SQL reindex process was successful. The number of objects rebuilt will vary. You can hit the CNTRL and C keys to exit out of the process, if need be. Do not exit the Fedora container yet, one more index to go; Solr. Adding object #31: islandora:sp_web_archive_collection Adding object #32: islandora:sp_web_archive Adding object #33: islandora:newspaperPageCModel Adding object #34: islandora:compound_collection Adding object #35: islandora:newspaperCModel Adding object #36: islandora:newspaperIssueCModel Adding object #37: ir:citationCollection Adding object #38: islandora:sp_basic_image_collection SUCCESS: 38 objects rebuilt. OK - Started application at context path [/fedora] Reindex Solr (3/3) WARNING - This reindex process takes the longest of all three, with up to 1-30 or more hours to complete depending on the size of your Fedora collection. As such, it is recommended starting a screen session prior to running the following command. Learn more about screen here\nStill staying within the utility_scripts directory on the Fedora container or reenter the Fedora container having started a new screen session, now run the updateSolrIndex.sh script. This script will give you output like the example below. ./updateSolrIndex.sh FedoraGenericSearch (FGS) update Solr index from Fedora helper script. Starting to reindex your Fedora repository. This process runs in the background and may take some time. Checked and this operation is still running. You may disconnect and the process will continue to run. Find logs at /usr/local/tomcat/logs/fgs-update-foxml.out and /usr/local/tomcat/logs/fgs-update-foxml.err. You can watch log file \u0026#39;tail -f /usr/local/tomcat/logs/fedoragsearch.daily.log\u0026#39; as the process runs. Note: Within this output, options to tail logs and watch progress are offered. Depending on the size of your collection this process may take hours, however it is okay to exit out of the container and even log off the remote Staging server. You can check back frequently by running tail -f /usr/local/tomcat/logs/fgs-update-foxml.out on the Fedora container. If you visit your Drupal site and run a Solr search, you should start to see objects and facets start to work. The number of objects will increase over time.\nAfter a good period of time, again depending on the size of your Fedora collection, when the Solr re-index process finishes, output like the example below will appear in the /usr/local/tomcat/logs/fgs-update-foxml.out log. This indicates that the Solr reindex process was completed. The number of objects rebuilt will vary. You can hit the CNTRL and C keys to exit out of the tail process, if need be. tail -f /usr/local/tomcat/logs/fgs-update-foxml.out Args 0=http://localhost:8080 1=updateIndex 2=fromFoxmlFiles \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;resultPage operation=\u0026#34;updateIndex\u0026#34; action=\u0026#34;fromFoxmlFiles\u0026#34; value=\u0026#34;\u0026#34; repositoryName=\u0026#34;FgsRepos\u0026#34; indexNames=\u0026#34;\u0026#34; resultPageXslt=\u0026#34;\u0026#34; dateTime=\u0026#34;Thu Aug 08 20:43:12 GMT 2019\u0026#34;\u0026gt; \u0026lt;updateIndex xmlns:dc=\u0026#34;http://purl.org/dc/elements/1.1/\u0026#34; xmlns:foxml=\u0026#34;info:fedora/fedora-system:def/foxml#\u0026#34; xmlns:zs=\u0026#34;http://www.loc.gov/zing/srw/\u0026#34; warnCount=\u0026#34;0\u0026#34; docCount=\u0026#34;13\u0026#34; deleteTotal=\u0026#34;0\u0026#34; updateTotal=\u0026#34;13\u0026#34; insertTotal=\u0026#34;0\u0026#34; indexName=\u0026#34;FgsIndex\u0026#34;/\u0026gt; \u0026lt;/resultPage\u0026gt; Type exit when finished to exit the container. Step 18: On Remote Staging - Review and Test the Drupal Staging Site In your web browser, enter this URL: https://yourprojectnamehere.institution.edu\nPlease note: You should not see any errors with respect to the SSL certifications. If so, please review your previous steps especially if using Let\u0026rsquo;s Encrypt. You may need to repeat those steps to get rid of the errors. Log in to the local Islandora site with the credentials (\u0026ldquo;DRUPAL_ADMIN_USER\u0026rdquo; and \u0026ldquo;DRUPAL_ADMIN_PASS\u0026rdquo;) you created in \u0026ldquo;staging.env\u0026rdquo;.\nNote: You are free to use previously Drupal admin or user accounts created during the Local site development process. You can decide to further QC and review the site as you wish or start to add digital collections and objects.\nYou could also further test using the Islandora Sample Objects as you may have done in the previous Local installation. Next Steps Once you are ready to deploy your finished Drupal site, you may progress to:\nProduction ISLE Installation: Migrate Existing Islandora Site Additional Resources ISLE Installation: Environments help with explaining the ISLE structure, the associated files, and what values ISLE end-users should use for the \u0026ldquo;.env\u0026rdquo;, \u0026ldquo;local.env\u0026rdquo;, etc. Local ISLE Installation: Resources contains Docker container passwords and URLs for administrator testing. ISLE Installation: Troubleshooting contains help for port conflicts, non-running Docker containers, etc. End of Staging ISLE Installation: New Site And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/092-staging-isle/","tags":["ISLE","migrate","staging"],"title":"Staging ISLE Installation: Migrate Existing Islandora Site - with Annotations"},{"categories":null,"contents":"This post is an addendum to earlier post 087. It is intended to chronicle my customization efforts, necessary steps that follow the aforementioned document\u0026rsquo;s Step 11, to migrate to a local development instance of Digital.Grinnell on my work-issued iMac, MA8660, currently identified as MAD25W812UJ1G9. Please refer to Steps 0 - 11 in post 087 for background info.\nNote that it should NOT be necessary to repeat steps taken in this document. Pertinent changes made herein were saved into a new completed-install-local-migrate branch of my dg-islandora project repository, effectively capturing all progress made within.\nGoal The goal of this project is once again to spin up a local Islandora stack using the ISLE project following the guidance of the project\u0026rsquo;s install-local-migrate document. My process will be slightly different than documented since I\u0026rsquo;ve already created a pair of private dg-isle and dg-islandora repositories.\nNote that there are no formal \u0026ldquo;annotations\u0026rdquo; in this document because everything here is an addendum/annotation to the original install-local-migrate document.\nOutcomes of Step 11 As part of Step 11 I visited https://dg.localdomain on my iMac desktop and found that the site came up looking and behaving just as it should, but with two warnings. The complete list of warnings was:\nUser warning: The following module is missing from the file system: antibot. For information about how to fix this, see the documentation page. in _drupal_trigger_error_with_delayed_logging() (line 1156 of /var/www/html/includes/bootstrap.inc). User warning: The following module is missing from the file system: islandora_mods_via_twig. For information about how to fix this, see the documentation page. in _drupal_trigger_error_with_delayed_logging() (line 1156 of /var/www/html/includes/bootstrap.inc). Dealing with Warnings Resolving these warnings would basically involve adding the missing modules to my dg-islandora repository. Remember, in our docker-compose.local.yml file we map the following:\nvolumes: # Customization: Bind mounting Drupal Code instead of using default Docker volumes for local development with an IDE. - ../dg-islandora:/var/www/html:cached So the task before me was to add these things to dg-islandora, and save the changes into the dg-islandora repo for use later on. Looking at my production instance of ISLE on DGDocker1, I found that the only missing \u0026ldquo;contrib\u0026rdquo; module was antibot which currently resides in /opt/ISLE/persistent/html/sites/all/modules/contrib/antibot. The other missing module was part of the \u0026ldquo;islandora\u0026rdquo; branch of the module tree, residing in subdirectories of /opt/ISLE/persistent/html/sites/all/modules/islandora/islandora_mods_via_twig.\nantibot The antibot module currently resides in /opt/ISLE/persistent/html/sites/all/modules/contrib/antibot on DGDocker1. I was curious what would happen if I opened a terminal into my local Apache container and used drush pm-download... to try installing this module. My command sequence on my desktop workstation was this:\nWorkstation Commands docker exec -w /var/www/html/sites/default isle-apache-ld drush pm-download antibot docker exec -w /var/www/html/sites/default isle-apache-ld drush cc all The results were very promising as the first command returned this:\nProject antibot (7.x-1.2) downloaded to [success] /var/www/html/sites/all/modules/contrib/antibot. The 2nd drush... command subsequently returned only one warning since the antibot warning was gone! A peek inside my dg-islandora instance on the workstation was also promising. There I found this:\n╭─markmcfate@MAD25W812UJ1G9 ~/GitHub/dg-islandora/sites/all/modules/contrib ‹ruby-2.3.0› ‹master*› ╰─$ ll total 0 drwxr-x---@ 21 markmcfate staff 672B Sep 1 15:31 admin_menu drwxr-x---@ 9 markmcfate staff 288B Sep 1 15:31 admin_theme drwxr-x---@ 12 markmcfate staff 384B Sep 1 15:31 announcements drwxr-xr-x 11 markmcfate staff 352B Jun 10 2018 antibot drwxr-x---@ 12 markmcfate staff 384B Sep 1 15:31 backup_migrate ... So it appears that antibot was properly downloaded to the correct owner/group, and all that\u0026rsquo;s missing is a proper set of its permissions, although I suspect it will function properly just as it is. A peek inside the new antibot directory showed this:\n╭─markmcfate@MAD25W812UJ1G9 ~/GitHub/dg-islandora/sites/all/modules/contrib/antibot ‹ruby-2.3.0› ‹master*› ╰─$ ll total 96 -rw-r--r-- 1 markmcfate staff 18K Nov 16 2016 LICENSE.txt -rw-r--r-- 1 markmcfate staff 91B Jun 6 2018 README.txt -rw-r--r-- 1 markmcfate staff 1.2K Jun 6 2018 antibot.admin.inc -rw-r--r-- 1 markmcfate staff 286B Jun 10 2018 antibot.info -rw-r--r-- 1 markmcfate staff 224B Jun 6 2018 antibot.install -rw-r--r-- 1 markmcfate staff 5.2K Jun 6 2018 antibot.module -rw-r--r-- 1 markmcfate staff 765B Jun 6 2018 antibot.pages.inc drwxr-xr-x 3 markmcfate staff 96B Jun 6 2018 js drwxr-xr-x 3 markmcfate staff 96B Jun 6 2018 templates A check of the equivalent directories and files on DGDocker1 revealed much the same as you can see in the abridged output below.\n[islandora@dgdocker1 contrib]$ ll total 268 drwxr-x---. 2 islandora 33 4096 Nov 19 2014 addanother drwxr-x---. 3 islandora 33 4096 Dec 6 2010 admin_theme drwxr-x---. 5 islandora 33 4096 Apr 14 2018 advanced_help drwxr-x---. 2 islandora 33 4096 Jan 20 2015 announcements drwxr-xr-x. 4 islandora 33 4096 Jun 10 2018 antibot drwxr-x---. 4 islandora 33 4096 Dec 15 2018 backup_migrate ... [islandora@dgdocker1 contrib]$ cd antibot [islandora@dgdocker1 antibot]$ ll total 48 -rw-r--r--. 1 islandora 33 1196 Jun 7 2018 antibot.admin.inc -rw-r--r--. 1 islandora 33 286 Jun 10 2018 antibot.info -rw-r--r--. 1 islandora 33 224 Jun 7 2018 antibot.install -rw-r--r--. 1 islandora 33 5370 Jun 7 2018 antibot.module -rw-r--r--. 1 islandora 33 765 Jun 7 2018 antibot.pages.inc drwxr-xr-x. 2 islandora 33 23 Jun 7 2018 js -rw-r--r--. 1 islandora 33 18092 Nov 16 2016 LICENSE.txt -rw-r--r--. 1 islandora 33 91 Jun 7 2018 README.txt drwxr-xr-x. 2 islandora 33 34 Jun 7 2018 templates I subsequently logged in to both my production and local instances of ISLE and visited the corresponding antibot configuration pages at https://digital.grinnell.edu/admin/config/system/antibot and https://dg.localdomain/islandora/object/islandora%3Aroot#overlay=admin/config/system/antibot. The nearly identical pages indicate that my antibot configruation, presumably part of my imported production Drupal database, is intact and exactly as it should be. This was good news indeed!\nislandora_mods_via_twig On DGDocker1 I found a .git subdirectory in the /opt/ISLE/persistent/html/sites/all/modules/islandora/islandora_mods_via_twig directory indicating that this module should be provisioned using git. The module has a git remote -v response of origin https://github.com/DigitalGrinnell/islandora_mods_via_twig.git so I believe it would be prudent to add it as another git submodule. I did that like so:\nWorkstation Commands cd ~/GitHub/dg-islandora git submodule add https://github.com/DigitalGrinnell/islandora_mods_via_twig.git sites/all/modules/islandora/islandora_mods_via_twig docker exec -w /var/www/html/sites/default isle-apache-ld drush cc all A quick visit to https://dg.localdomain on my local workstation shows the site is working and with no visible errors or warnings! Woot!\nTwo Probable Issues So, it works\u0026hellip;maybe. However, I am concerned with a couple of things so I decided to take a peek inside my new local ISLE instance to check. I did indeed find a couple of problems.\nPermissions and Ownership of \u0026ldquo;antibot\u0026rdquo; and \u0026ldquo;islandora_mods_via_twig\u0026rdquo; Inside the Apache container I checked the permissions of my /var/www/html/sites/all/modules/islandora directory and found this, abridged for clarity:\ndrwxr-x--- 10 islandora www-data 320 Sep 9 22:21 islandora_mods_display/ drwxr-xr-x 8 www-data www-data 256 Sep 10 03:08 islandora_mods_via_twig/ drwxr-x--- 20 islandora www-data 640 Sep 10 02:10 islandora_multi_importer/ Likewise, inside /var/www/html/sites/all/modules/contrib I found this, also abridged for clarity:\ndrwxr-x--- 12 islandora www-data 384 Sep 9 22:20 announcements/ drwxr-xr-x 11 www-data www-data 352 Jun 11 2018 antibot/ drwxr-x--- 12 islandora www-data 384 Sep 9 22:20 backup_migrate/ And inside both of those subdirectories I found structures with owner/group and permissions like this:\nroot@5686019d1a3a:/var/www/html/sites/all/modules/islandora/islandora_mods_via_twig# ll total 88 drwxr-xr-x 8 root root 256 Sep 10 03:08 ./ drwxr-x--- 63 islandora www-data 2016 Sep 10 03:08 ../ -rw-r--r-- 1 root root 88 Sep 10 03:08 .git -rwxr-xr-x 1 root root 16683 Sep 10 03:08 islandora_mods_via_twig.drush.inc* -rw-r--r-- 1 root root 265 Sep 10 03:08 islandora_mods_via_twig.info -rw-r--r-- 1 www-data www-data 67 Sep 10 03:08 islandora_mods_via_twig.module -rw-r--r-- 1 root root 35064 Sep 10 03:08 LICENSE.txt -rw-r--r-- 1 root root 17925 Sep 10 03:08 README.md Not good from an owner/group perspective, but looking OK in terms of permissions? So, the necessary changes I need to make here are ones that were previously performed as part of the migration_site_vsets.sh script back in Step 10: Run Islandora Drupal Site Scripts. Specifically, inside the Apache container I need to rerun this command from that script:\nisle-apache-ld Container Commands /bin/bash /utility-scripts/isle_drupal_build_tools/drupal/fix-permissions.sh --drupal_path=/var/www/html --drupal_user=islandora --httpd_group=www-data A quick check of the aforementioned directories and files inside the Apache container shows that owner/group and permissions are now as-they-should-be. Yay!\nWhere Have All the Files Gone? Ok, so I\u0026rsquo;m showing my age with that subtitle, I know. But it\u0026rsquo;s a valid question, perhaps best summed up in this Slack post of mine:\nAt https://github.com/Islandora-Collaboration-Group/ISLE/blob/master/docs/install/install-local-migrate.md#drupal-site-files-and-code I made a copy of my production /var/www/html/sites/default/files directory anticipating that \u0026ldquo;You\u0026rsquo;ll move this directory in later steps.\u0026rdquo; I must have missed something, because I can\u0026rsquo;t find anyplace in the document where I moved those files into my new local instance of ISLE.\nThe resolution of this issue is now covered in the annotation at the end of Step 9: Import the Production MySQL Drupal Database.\nNext Steps After completion of everything mentioned in this document, I returned to Step 12: Ingest Sample Objects but ended my work there differently than suggested. The differences are all covered in one final annotation there.\nThe install-local-migrate.md document subsequently suggests moving on to Staging ISLE Installation: Migrate Existing Islandora Site and I believe I will do just that, probably with production of another annotated document to chronicle my specific experience.\nBut before I go\u0026hellip; I elected to take two more steps here. The first is\u0026hellip;\nUpdating my dg-islandora repository with all of the changes made thus far. Note that I did indeed update dg-islandora, but not the master branch. In my workstation\u0026rsquo;s ~/GitHub/dg-islandora directory I did this in order to save all changes made thus far into a new completed-install-local-migrate branch for safe-keeping. ╭─markmcfate@MAD25W812UJ1G9 ~/GitHub/dg-islandora ‹ruby-2.3.0› ‹master*› ╰─$ git status On branch master Your branch is up to date with \u0026#39;origin/master\u0026#39;. Changes to be committed: (use \u0026#34;git reset HEAD \u0026lt;file\u0026gt;...\u0026#34; to unstage) modified: .gitmodules new file: html/sites/all/modules/islandora/islandora_mods_via_twig new file: sites/all/modules/islandora/islandora_mods_via_twig Changes not staged for commit: (use \u0026#34;git add/rm \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git checkout -- \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) (commit or discard the untracked or modified content in submodules) modified: .gitignore modified: .gitmodules deleted: html/sites/all/modules/islandora/islandora_mods_via_twig modified: install_solution_packs.sh modified: migration_site_vsets.sh modified: sites/all/modules/islandora/dg7 (modified content) modified: sites/all/modules/islandora/islandora_mods_display (modified content) modified: sites/all/modules/islandora/islandora_mods_via_twig (modified content) modified: sites/all/modules/islandora/islandora_multi_importer (modified content) modified: sites/all/modules/islandora/islandora_scholar/modules/citeproc/composer.lock modified: sites/all/modules/islandora/islandora_solution_pack_oralhistories (modified content) Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) sites/all/modules/contrib/antibot/ ╭─markmcfate@MAD25W812UJ1G9 ~/GitHub/dg-islandora ‹ruby-2.3.0› ‹master*› ╰─$ git checkout -b completed-install-local-migrate M\t.gitignore M\t.gitmodules M\tinstall_solution_packs.sh M\tmigration_site_vsets.sh M\tsites/all/modules/islandora/dg7 M\tsites/all/modules/islandora/islandora_mods_display A\tsites/all/modules/islandora/islandora_mods_via_twig M\tsites/all/modules/islandora/islandora_multi_importer M\tsites/all/modules/islandora/islandora_scholar/modules/citeproc/composer.lock M\tsites/all/modules/islandora/islandora_solution_pack_oralhistories Switched to a new branch \u0026#39;completed-install-local-migrate\u0026#39; I considered trying to update Drupal and its contrib modules from the web interface at https://dg.localdomain/#overlay=admin/modules/update but doing so requires FTP acccess which my local instance does not have. So, I considered using drush up instead, and it appears to be working properly. So, in my workstation I got this:\n╭─markmcfate@MAD25W812UJ1G9 ~/GitHub/dg-isle ‹ruby-2.3.0› ‹master*› ╰─$ docker exec -it isle-apache-ld bash root@e34ab55f94aa:/# cd /var/www/html/sites/default/ root@e34ab55f94aa:/var/www/html/sites/default# drush up Update information last refreshed: Thu, 2020-09-10 15:57 Name Installed Version Proposed version Message Drupal 7.67 7.72 SECURITY UPDATE available Views Bulk Operations (views_bulk_operations) 7.x-3.5 7.x-3.6 Update available Backup and Migrate (backup_migrate) 7.x-3.6 7.x-3.9 Update available Colorbox (colorbox) 7.x-2.13 7.x-2.15 Update available Git Deploy (git_deploy) 7.x-1.x-dev 7.x-1.3 Update available Maillog / Mail Developer (maillog) 7.x-1.0-alpha1 7.x-1.0-rc1 Update available Views (views) 7.x-3.23 7.x-3.24 Update available Webform (webform) 7.x-4.20 7.x-4.23 SECURITY UPDATE available NOTE: A security update for the Drupal core is available. Drupal core will be updated after all of the non-core projects are updated. Security and code updates will be made to the following projects: Views Bulk Operations (VBO) [views_bulk_operations-7.x-3.6], Backup and Migrate [backup_migrate-7.x-3.9], Colorbox [colorbox-7.x-2.15], Git Deploy [git_deploy-7.x-1.3], Maillog / Mail Developer [maillog-7.x-1.0-rc1], Views (for Drupal 7) [views-7.x-3.24], Webform [webform-7.x-4.23] Note: A backup of your project will be stored to backups directory if it is not managed by a supported version control system. Note: If you have made any modifications to any file that belongs to one of these projects, you will have to migrate those modifications after updating. Do you really want to continue with the update process? (y/n): y Project views_bulk_operations was updated successfully. Installed version is now 7.x-3.6. Backups were saved into the directory [ok] /root/drush-backups/digital_grinnell/20200910205644/modules/views_bulk_operations. Project backup_migrate was updated successfully. Installed version is now 7.x-3.9. Backups were saved into the directory /root/drush-backups/digital_grinnell/20200910205644/modules/backup_migrate. [ok] Project colorbox was updated successfully. Installed version is now 7.x-2.15. Backups were saved into the directory /root/drush-backups/digital_grinnell/20200910205644/modules/colorbox. [ok] Project git_deploy was updated successfully. Installed version is now 7.x-1.3. Backups were saved into the directory /root/drush-backups/digital_grinnell/20200910205644/modules/git_deploy. [ok] Project maillog was updated successfully. Installed version is now 7.x-1.0-rc1. Backups were saved into the directory /root/drush-backups/digital_grinnell/20200910205644/modules/maillog. [ok] Project views was updated successfully. Installed version is now 7.x-3.24. Backups were saved into the directory /root/drush-backups/digital_grinnell/20200910205644/modules/views. [ok] Project webform was updated successfully. Installed version is now 7.x-4.23. Backups were saved into the directory /root/drush-backups/digital_grinnell/20200910205644/modules/webform. [ok] Code updates will be made to drupal core. WARNING: Updating core will discard any modifications made to Drupal core files, most noteworthy among these are .htaccess and robots.txt. If you have made any modifications to these files, please back them up before updating so that you can re-create your modifications in the updated version of the file. Note: Updating core can potentially break your site. It is NOT recommended to update production sites without prior testing. Do you really want to continue? (y/n): y Project drupal was updated successfully. Installed version is now 7.72. Backups were saved into the directory /root/drush-backups/digital_grinnell/20200910205644/drupal. [ok] require_once(/lib/glip.php): failed to open stream: No such file or directory git_deploy.module:46 [warning] PHP Fatal error: require_once(): Failed opening required \u0026#39;/lib/glip.php\u0026#39; (include_path=\u0026#39;.:/usr/share/php\u0026#39;) in /var/www/html/sites/all/modules/contrib/git_deploy/git_deploy.module on line 46 Drush command terminated abnormally due to an unrecoverable error. [error] Error: require_once(): Failed opening required \u0026#39;/lib/glip.php\u0026#39; (include_path=\u0026#39;.:/usr/share/php\u0026#39;) in /var/www/html/sites/all/modules/contrib/git_deploy/git_deploy.module, line 46 The external command could not be executed due to an application error. So, it looks like modules and core were updated successfully? I\u0026rsquo;m just not sure what the impact of the warning and PHP Fatal error might be. \u0026#x1f626;\nA little investigation suggests that we are missing the glip library, so following the guidance in the git_deploy module\u0026rsquo;s README.txt I did this from my workstation:\n╭─markmcfate@MAD25W812UJ1G9 ~/GitHub/dg-islandora/sites/all/libraries ‹ruby-2.3.0› ‹completed-install-local-migrate*› ╰─$ git clone git://github.com/halstead/glip.git cd glip git checkout 1.1 Cloning into \u0026#39;glip\u0026#39;... remote: Enumerating objects: 319, done. remote: Total 319 (delta 0), reused 0 (delta 0), pack-reused 319 Receiving objects: 100% (319/319), 101.97 KiB | 593.00 KiB/s, done. Resolving deltas: 100% (163/163), done. Note: checking out \u0026#39;1.1\u0026#39;. You are in \u0026#39;detached HEAD\u0026#39; state. You can look around, make experimental changes and commit them, and you can discard any commits you make in this state without impacting any branches by performing another checkout. If you want to create a new branch to retain commits you create, you may do so (now or later) by using -b with the checkout command again. Example: git checkout -b \u0026lt;new-branch-name\u0026gt; HEAD is now at 79f5472... Implement support for alternates object stores. Then, taking another shot at drush up from my Apache container:\n╭─markmcfate@MAD25W812UJ1G9 ~/GitHub/dg-isle ‹ruby-2.3.0› ‹master*› ╰─$ docker exec -it isle-apache-ld bash root@e34ab55f94aa:/# cd /var/www/html/sites/default/ root@e34ab55f94aa:/var/www/html/sites/default# drush up Error while trying to find the common path for enabled extensions of project dg-islandora. Extensions are: [error] announcements, islandora_altmetrics, islandora_badges, islandora_example_simple_text, islandora_image_annotation, islandora_oadoi, islandora_scopus, islandora_sync, islandora_sync_field_collection, islandora_sync_relation, islandora_webform_ingest, islandora_wos, webform_bonus, webform_digest. Update information last refreshed: Thu, 2020-09-10 15:57 Name Installed Version Proposed version Message Bootstrap (bootstrap) 7.x-3.21 7.x-3.26 SECURITY UPDATE available Security updates will be made to the following projects: Bootstrap [bootstrap-7.x-3.26] Note: A backup of your project will be stored to backups directory if it is not managed by a supported version control system. Note: If you have made any modifications to any file that belongs to one of these projects, you will have to migrate those modifications after updating. Do you really want to continue with the update process? (y/n): y Project bootstrap was updated successfully. Installed version is now 7.x-3.26. Backups were saved into the directory /root/drush-backups/digital_grinnell/20200910213813/themes/bootstrap. [ok] System 7083 Add \u0026#39;jquery-html-prefilter-3.5.0-backport.js\u0026#39; to the \u0026#39;jquery\u0026#39; library. System 7084 Rebuild JavaScript aggregates to include \u0026#39;ajax.js\u0026#39; fix for Chrome 83. Backup_migrate 7308 Update profiles table filter field to accommodate larger serialized strings. Backup_migrate 7309 NodeSquirrel support has been removed. Backup_migrate 7310 Disable e-mail destinations. Backup_migrate 7311 Adjust the default performance settings. Git_deploy 7000 Save current location of glip library. Maillog 7100 Rename the \u0026#39;idmaillog\u0026#39; field to just \u0026#39;id\u0026#39;. Maillog 7101 Clear the menu cache so the new paths will be picked up. Maillog 7102 Clear the Views cache so the updated admin page will get the new paths. Islandora 7002 Implements hook_update_N(). Removing old variable around changes to ingestDatastream signature. These changes are complete and deprecation warnings are removed. Islandora_oai 7103 Add support for QDC. Islandora_scholar 7103 Maintain existing RGB colorspace profile configuration. Islandora_scholar 7104 Disable Islandora Google Scholar if it is currently enabled, since logic now lives in Islandora Scholar Citeproc 7101 Set citeproc version if you are updating. That way the library used doesn\u0026#39;t switch under your feet, when updating to the latest version. Islandora_entitie 7100 Add the default description field to the Scholar solr metadata profile. s Islandora_entitie 7200 Print and log a message about potential lost Position data. @see s https:jira.duraspace.orgbrowseISLANDORA-2137 Islandora_pdf 7001 Maintain existing RGB colorspace profile configuration. Islandora_pdf 7100 Set and maintain new dUseCIEColor switch variable. Islandora_video 7101 Implements hook_update_N(). Deletes the unused legacy variable islandora_video_retain_original. Islandora_webform 7001 Add webform_workflow state permissions schema to webform table. Islandora_webform 7002 Add draft_access field to webform table. Xml_form_builder 7105 Make title_field not required. Repeating because hook_schema wasn\u0026#39;t updated. Xml_form_builder 7106 Update fields to have not NULL. Had a syntax error in our schema. Xml_form_builder 7107 Allow null in tranform field. Previous update was causing issues when the transform was allowed to be null. Xml_form_elements 7001 Rename the datepicker element as it collides with another element. Do you wish to run all pending updates? (y/n): y Performed update: backup_migrate_update_7308 [ok] Performed update: backup_migrate_update_7309 [ok] Performed update: maillog_update_7100 [ok] Performed update: xml_form_builder_update_7105 [ok] Performed update: system_update_7083 [ok] Set colorspace configuration to RBG to maintain existing profile. [ok] Performed update: islandora_pdf_update_7001 [ok] No destinations were affected by this change. [ok] Performed update: backup_migrate_update_7310 [ok] Performed update: xml_form_builder_update_7106 [ok] Performed update: maillog_update_7101 [ok] Performed update: islandora_webform_update_7001 [ok] Set colorspace configuration to RBG to maintain existing profile. [ok] Performed update: islandora_scholar_update_7103 [ok] Performed update: islandora_entities_update_7100 [ok] Performed update: xml_form_builder_update_7107 [ok] Performed update: islandora_webform_update_7002 [ok] Performed update: islandora_video_update_7101 [ok] Performed update: islandora_pdf_update_7100 [ok] Illegal offset type in isset or empty module.inc:280 [warning] The Islandora Google Scholar submodule has been removed, and its functionality has been moved into the main [ok] Islandora Scholar module. Performed update: islandora_scholar_update_7104 [ok] The original default form used two-word labels in the Position element, but removed spaces in the MADS XML. This [ok] has been fixed as of 7.x-1.13. When editing existing objects with the new form, older values under Position will not be read and will be discarded. Please consult the \u0026lt;a href=\u0026#34;https://jira.duraspace.org/browse/ISLANDORA-2137\u0026#34;\u0026gt;ticket\u0026lt;/a\u0026gt; for further information. Performed update: islandora_entities_update_7200 [ok] Performed update: citeproc_update_7101 [ok] Performed update: system_update_7084 [ok] Performed update: islandora_oai_update_7103 [ok] Performed update: islandora_update_7002 [ok] Performed update: maillog_update_7102 [ok] Performed update: git_deploy_update_7000 [ok] Performed update: backup_migrate_update_7311 [ok] Performed update: xml_form_elements_update_7001 [ok] Parsing DG One Form to Rule Them All for datepicker elements. [ok] NodeSquirrel stopped being usable as a backup destination on October 1st, 2019 and ceased operations entirely on [status] November 1st, 2019. Because of this, the NodeSquirrel functionality has been disabled. Please switch to an alternate destination if necessary. Please note that the NodeSquirrel configuration itself has not been removed. Skipped DG One Form to Rule Them All (201) as no occurrences were found. [status] \u0026#39;all\u0026#39; cache was cleared. [success] Finished performing updates. [ok] In spite of the one error, this is looking much better. However, a quick visit to the local site returns a page with LOTS of Javascript errors and so the image tiles that appear on our home page are no longer constrained to any reasonable size. \u0026#x1f626; Most errors are of the form:\nBlocked loading mixed active content \u0026#34;http://dg.localdomain/sites/default/files/css/css_lQaZfjVpwP_oGNqdtWCSpJT1EMqXdMiU84ekLLxQnc4.css\u0026#34; This was not my first rodeo, so I immediately took a look at file permissions and ownership in the Apache container\u0026rsquo;s /var/www/html directory, and it\u0026rsquo;s a mess. Time to run the fix-permissions.sh script again\u0026hellip;\nisle-apache-ld Container Commands /bin/bash /utility-scripts/isle_drupal_build_tools/drupal/fix-permissions.sh --drupal_path=/var/www/html --drupal_user=islandora --httpd_group=www-data And that helped a great deal, but didn\u0026rsquo;t quite bring the site back exactly as it should have been. Upon detailed examination of the aforementioned error, I recognized an old enemy, a \u0026ldquo;mixed mode\u0026rdquo; browser warning. But I could not recall how I\u0026rsquo;ve fixed this in the past, so I turned to the ICG\u0026rsquo;s #isle-support Slack channel where my hero, Noah Smith, from Born-Digital immediately came to my rescue, again!\nThe Mixed Mode Fix Noah suggested that they routinely run this script to fix \u0026ldquo;mixed mode\u0026rdquo; errors.\ngrep -ri \u0026#39;SetEnvIf X-Forwarded-Proto https HTTPS=on\u0026#39; web/.htaccess || echo \\\u0026#34;SetEnvIf X-Forwarded-Proto https HTTPS=on\\\u0026#34; | tee -a web/.htaccess Next Steps NOT Yet Taken It\u0026rsquo;s worth noting here that other things I considered doing next, but did not, include:\nUpdating all of the Drupal contrib and Islandora modules \u0026ndash; this should have gotten done for the contrib modules and Drupal core, but I\u0026rsquo;m not sure how to best approach this for all of the Islandora modules, so I chose not to do this yet. So, I\u0026rsquo;m planning to wait and complete this step from my staging instance of ISLE. Install and configure LASIR \u0026ndash; this will also be delayed until after my migration instance is up-to-date and in production. Configure and engage \u0026ldquo;new\u0026rdquo; ISLE features like \u0026ldquo;Automated Testing\u0026rdquo; \u0026ndash; this will also be delayed until after my migration instance is up-to-date and in production. And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/090-isle-local-migration-customization/","tags":["ISLE","migrate","local","development"],"title":"Local ISLE Installation: Migrate Existing Islandora Site - One-Time Customizations"},{"categories":null,"contents":"Ever since moving to a new college-controlled VPN client, Cisco AnyConnect, for VPN access on August 1, 2020, I\u0026rsquo;ve had to repeatedly enter both my username and system password every time I launch a VPN connection. The annoying prompt/dialog looked like this:\nFigure 1 \u0026middot; Keychain Access Prompt Fortunately, I found this post and answer explaining how to enable Keychain Access without having to repeat the User Name twice each time. The suggested process is this:\nLaunch /Applications/Utilities/Keychain Access\n• Select \u0026ldquo;System\u0026rdquo; from the Keychains menu in the upper left\n• Select \u0026ldquo;Certificates\u0026rdquo; from the Category menu in the lower left\n• Find the entry that corelates to your computer\u0026rsquo;s name in the list on the right, and click on the disclosure triangle.\n• Secondary click on the \u0026ldquo;Private Key\u0026rdquo; entry that appears and select \u0026ldquo;Get Info\u0026rdquo; from the contextual menu that appears.\n• Select the Access Control tab.\n• You can then either add AnyConnect to the the list at the bottom of the screen (more secure, but you will need to repeat this process anytime the version of AnyConnect changes), or toggle the radio button to \u0026ldquo;Allow all applications to access this item\u0026rdquo;.\nNote that I chose to add AnyConnect to my list of allowed apps, so presumably I\u0026rsquo;ll need to repeat this change every time that a new version of Cisco AnyConnect is installed.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/089-stop-macosx-from-requiring-user-name/","tags":["Cisco","AnyConnect","VPN","Keychain"],"title":"Stop Mac OS X from Requiring a Repeat of User Name"},{"categories":null,"contents":"This post is an addendum to earlier posts 021 and 058, with simiar titles. It is intended to chronicle my efforts to migrate to a local development instance of Digital.Grinnell on my work-issued iMac, MA8660, currently identified as MAD25W812UJ1G9.\nGoal The goal of this project is once again to spin up a local Islandora stack using the ISLE project following the guidance of the project\u0026rsquo;s install-local-migrate document. My process will be slightly different than documented since I\u0026rsquo;ve already created a pair of private dg-isle and dg-islandora repositories. This workflow will also take steps to introduce elements like the Digital Grinnell theme and custom modules like DG7. Once these pieces are in-place and working, I\u0026rsquo;ll begin adding other critical components as well as a robust set of data gleaned from https://digital.grinnell.edu/.\nChanges Since Post 058 There have been three significant changes since I last attempted this effort:\nISLE v1.5.1 is now the latest release, I now have proper HTTPS access to, and availability of, DGDockerX.Grinnell.edu to use for staging ISLE, and All of my GitHub-hosted projects have moved from a local directory of ~/Projects to ~/GitHub. Using This Document There are just a couple of notes regarding this document that I\u0026rsquo;d like to pass along to make it more useful.\nGists - You will find a few places in this post where I generated a gist to take the place of lengthy command output. Instead of a long stream of text you\u0026rsquo;ll find a simple link to a gist.\nWorkstation Commands - There are lots of places in this document where I\u0026rsquo;ve captured a series of command lines along with output from those commands in block text. Generally speaking, after each such block you will find a Workstation Commands table that can be used to conveniently copy and paste the necessary commands directly into your workstation. The tables look something like this:\nWorkstation Commands cd ~/GitHub git clone https://github.com/DigitalGrinnell/ISLE cd ISLE git checkout -b ld Apache Container Commands - Similar to Workstation Commands, a tabulated list of commands may appear with a heading of Apache Container Commands. *Commands in such tables can be copied and pasted into your command line terminal, but ONLY after you have opened a shell into the Apache container. The asterisk (*) at the end of the table heading is there to remind you of this! See the next section of this document for additional details. These tables looks something like this: Apache Container Commands* cd /var/www/html/sites/all/modules/contrib drush dl backup_migrate drush -y en backup_migrate Opening a Shell in the Apache Container This is something I find myself doing quite often during ISLE configuration, so here\u0026rsquo;s a reminder of how I generally do this\u0026hellip;\nWorkstation Commands docker exec -it isle-apache-ld bash Cleaning Up I typically use the following command stream to clean up any Docker cruft before I begin anew. Note: Uncomment the third line ONLY if you want to delete images and download new ones. If you do, be patient, it could take several minutes depending on connection speed.\nWorkstation Commands docker stop $(docker ps -q) docker rm -v $(docker ps -qa) # docker image rm $(docker image ls -q) --force docker system prune --force Local ISLE Installation: Migrate Existing Islandora Site What follows is my annotated copy of Local ISLE Installation: Migrate Existing Islandora Site. Annotations in this document use the annotation shortcode and appear in a highlighted box like the one you are reading from now.\nExpectations: It takes an average of 2-4+ hours to read this documentation and complete this installation.\nThis Local ISLE Installation builds a local environment for the express purpose of migrating a previously existing Islandora site onto the ISLE platform. If you need to build a brand new local development site, please stop and use the Local ISLE Installation: New Site instead.\nThis Local ISLE Installation will use a copy of a currently running Production Islandora Drupal website and an empty Fedora repository for end users to test migration to ISLE and do site development and design with the end goal of deploying to ISLE Staging and Production environments for public use. The final goal would be to cut over from the existing non-ISLE Production and Staging servers to their new ISLE counterparts.\nThis Local ISLE Installation will allow you to locally view this site in your browser with the domain of your choice (Example: \u0026ldquo;https://yourprojectnamehere.localdomain\u0026rdquo;), instead of being constrained to the Demo URL (\u0026ldquo;https://isle.localdomain\u0026rdquo;).\nThis document has directions on how you can save newly created ISLE code into a git software repository as a workflow process designed to manage and upgrade the environments throughout the development process from Local to Staging to Production. The ISLE Installation: Environments documentation offers an overview of the ISLE structure, the associated files, and what values ISLE end users should use for the \u0026ldquo;.env\u0026rdquo;, \u0026ldquo;local.env\u0026rdquo;, etc.\nThis document does not have directions on how you can save previously existing Islandora Drupal code into a git repository and assumes this step has already happened. The directions below will explain how to clone Islandora Drupal code from a previously existing Islandora Drupal git repository that should already be accessible to you.\nPlease post questions to the public Islandora ISLE Google group, or subscribe to receive emails. The Glossary defines terms used in this documentation.\nAssumptions / Prerequisites This Local ISLE installation expects that an existing Production Islandora Drupal site will be imported on a personal computer for further ISLE migration testing, Drupal theme development, ingest testing, etc.\nYou will be using ISLE version 1.2.0 or higher.\nYou are using Docker-compose 1.24.0 or higher.\nYou have git installed on your personal computer.\nYou have a previously existing private Islandora Drupal git repository.\nYou have access to a private git repository in Github, Bitbucket, Gitlab, etc.\nIf you do not, please contact your IT department for git resources, or else create an account with one of the above providers. WARNING: Only use Private git repositories given the sensitive nature of the configuration files. DO NOT share these git repositories publicly. For Microsoft Windows:\nYou have installed Git for Windows and will use its provided \u0026ldquo;Git Bash\u0026rdquo; as your command line interface; this behaves similarly to LINUX and UNIX environments. Git for Windows also installs \u0026ldquo;openssl.exe\u0026rdquo; which will be needed to generate self-signed SSL certs. (Note: PowerShell is not recommended as it is unable to run UNIX commands or execute bash scripts without a moderate degree of customization.) Set your text editor to use UNIX style line endings for files. (Text files created on DOS/Windows machines have different line endings than files created on Unix/Linux. DOS uses carriage return and line feed (\u0026quot;\\r\\n\u0026quot;) as a line ending, which Unix uses just line feed (\u0026quot;\\n\u0026quot;).) Index of Instructions Step 0: Copy Production Data to Your Personal Computer Step 1: Choose a Project Name Step 1.5: Edit \u0026ldquo;/etc/hosts\u0026rdquo; File Step 2: Setup Git Project Repositories Step 3: Git Clone the Production Islandora Drupal Site Code Step 4: Edit the \u0026ldquo;.env\u0026rdquo; File to Change to the Local Environment Step 5: Create New Users and Passwords by Editing \u0026ldquo;local.env\u0026rdquo; File Step 6: Create New Self-Signed Certs for Your Project Step 7: Download the ISLE Images Step 8: Launch Process Step 9: Import the Production MySQL Drupal Database Step 10: Run Islandora Drupal Site Scripts Step 11: Test the Site Step 12: Ingest Sample Objects Step 0: Copy Production Data to Your Personal Computer Be sure to run a backup of any current non-ISLE systems prior to copying or exporting any files.\nDrupal Site Files and Code Copy the /var/www/html/sites/default/files directory from your Production Apache server to an appropriate storage area on your personal computer. You\u0026rsquo;ll move this directory in later steps. On the production instance of Digital.Grinnell the Apache container\u0026rsquo;s Drupal webroot at /var/www/html maps to the host, DGDocker1 with a static IP address of 132.161.132.103, at /opt/ISLE/persistent/html. So, as directed above, I put a copy of the /var/www/html/sites/default/files directory into ~/Desktop/migration-copy from my iMac like so:\nWorkstation Commands rsync -aruvi islandora@132.161.132.103:/opt/ISLE/persistent/html/sites/default/files/. ~/Desktop/migration-copy/var/www/html/sites/default/files/ --progress Locate and note the previously existing private Islandora Drupal git repository. https://github.com/Digital-Grinnell/dg-islandora You\u0026rsquo;ll be cloning this into place once the ISLE project has been cloned in later steps. Drupal Site Database Prior to attempting this step, please consider the following:\nDrupal website databases can have a multitude of names and conventions. Confer with the appropriate IT departments for your institution\u0026rsquo;s database naming conventions.\nRecommended that the production databases be exported using the \u0026ldquo;.sql\u0026rdquo; /or \u0026ldquo;.gz\u0026rdquo; file formats (e.g. \u0026ldquo;prod_drupal_site_082019.sql.gz\u0026rdquo;) for better compression and minimal storage footprint.\nIf the end user is running multi-sites, there will be additional databases to export.\nDo not export the \u0026ldquo;fedora3\u0026rdquo; database or any system tables (such as \u0026ldquo;information_schema\u0026rdquo;, \u0026ldquo;performance_schema\u0026rdquo;, \u0026ldquo;mysql\u0026rdquo;)\nIf possible, on the production Apache web server, run drush cc all from the command line on the production server in the /var/www/html directory PRIOR to any db export(s). Otherwise issues can occur on import due to all cache tables being larger than \u0026ldquo;innodb_log_file_size\u0026rdquo; allows\nExport the Production MySQL Islandora Drupal Database Export the MySQL database for the current Production Islandora Drupal site in use and copy it to your personal computer (local) in an easy to find place. In later steps you\u0026rsquo;ll be directed to import this file. Please be careful performing any of these potential actions below as the process impacts your Production site.\nIf you are not comfortable or familiar with performing these actions, we recommend that you instead work with your available IT resources to do so.\nTo complete this process, you may use a MySQL GUI client or, if you have command line access to the MySQL database server, you may run the following command, substituting your actual user and database names: Example: mysqldump -u username -p database_name \u0026gt; prod_drupal_site_082019.sql Copy this file down to your personal computer. Even though the production instance of Digital.Grinnell has built-in facilities for performing this step, I chose to use an easily-repeated command line from an SSH session open on DGDocker1, like so:\nDGDocker1 Commands docker exec -w /var/www/html/sites/default isle-apache-dg drush cc all docker exec -it isle-mysql-dg bash Then, working inside the isle-mysql-dg container:\nisle-mysql-dg Container Commands mysqldump -u isle_dg_user -p isle_dg \u0026gt; prod_drupal_site_083120.sql exit Back in the DGDocker1 terminal:\nDGDocker1 Commands docker cp isle-mysql-dg:/prod_drupal_site_083120.sql ~/. exit Next, from my desktop terminal this command saved a copy of the exported database in /Users/markmcfate/Desktop/migration-copy/prod_drupal_site_083120.sql:\nWorkstation Commands rsync -aruvi islandora@132.161.132.103:/home/islandora/prod_drupal_site_083120.sql ~/Desktop/migration-copy/prod_drupal_site_083120.sql Fedora Hash Size (Conditional) Are you migrating an existing Islandora site that has greater than one million objects? No If true, then please carefully read about the Fedora Hash Size (Conditional).\nSolr Schema and Islandora Transforms This data can be challenging depending on the level of customizations to contend with and as such, ISLE maintainers recommends following one of the three (3), \u0026ldquo;Easy\u0026rdquo;, \u0026ldquo;Intermediate\u0026rdquo;, and \u0026ldquo;Advanced\u0026rdquo; strategies outlined below.\nStrategy 1: Easy - Run \u0026ldquo;Stock\u0026rdquo; ISLE Don\u0026rsquo;t copy any existing production Solr schemas, GSearch .xslt files, etc., and opt instead to use ISLE\u0026rsquo;s default versions. Import some objects from your existing Fedora repository and see if they display properly in searches as you like.\nStrategy 2: Intermediate - Bind Mount in Existing Transforms and Schemas Bind mount in existing transforms and schemas to override ISLE settings with your current Production version.\nWARNING This approach assumes you are running Solr 4.10.x.; only attempt if you are running that version on Production.\nCopy these current Production files and directory to your personal computer in an appropriate location.\nSolr schema.xml GSearch foxmltoSolr.xslt file GSearch islandora_transforms Keep the files you create during this process; you will need them again for Step 2a (below)! Note: You may need to further review paths in the files mentioned above, and edit them to match ISLE system paths. i.e. If foxmltoSolr.xslt and any transforms within islandora_transforms include xsl:include statements, make sure they match the paths noted in Step 2a (i.e. /usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/islandora_transforms).\nStrategy 3: Advanced - Diff and Merge Current Production Customization Edits Into ISLE Configs I chose this approach, Strategy 3, just as I have in the past.\nCopy these current production files and directory to your personal computer in an appropriate location.\nSolr schema.xml GSearch foxmltoSolr.xslt file GSearch islandora_transforms Run the Demo ISLE briefly to pull files for modification and correct ISLE system paths.\nSince I already have \u0026ldquo;customized\u0026rdquo; copies of the files in question (see post 058 I executed this step in two parts so that I can identify any changes in the files since the last time I did this. Specifically, I loaded and pulled the files and directory listed below from both ISLE v1.5.1 and ISLE v1.3.0, the version that I used in my previous attempt. In each case, before spinning up the Demo ISLE instance I did git checkout ISLE-1.3.0 and git checkout master, the later was to create a v1.5.1 instance.\nYou can find these paths by running the Demo and copying these files out to an appropriate location.\ndocker cp isle-solr-ld:/usr/local/solr/collection1/conf/schema.xml schema.xml docker cp isle-fedora-ld:/usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/foxmlToSolr.xslt foxmlToSolr.xslt docker cp isle-fedora-ld:/usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/islandora_transforms islandora_transforms Using the commands above, I pulled content from ISLE v1.5.1 and saved it in ~/Desktop/isle-demo-v1.5.1. The same content from ISLE v1.3.0 is saved in ~/Desktop/isle-demo-v1.3.0 for comparison.\nUsing a \u0026ldquo;diff\u0026rdquo; tool (software that allows one to compare and find the differences between two files), compare:\nyour Production Solr schema.xml file to the ISLE demo schema.xml file. your Production GSearch foxmltoSolr.xslt file to the ISLE demo foxmltoSolr.xslt file. your Production GSearch islandora_transforms directory to the ISLE demo islandora_transforms directory. I subsequently used diff -bur ~/Desktop/isle-demo-v1.3.0 ~/Desktop/isle-demo-v1.5.1 to recursively report any differences between the two versions of files and directories. There were NONE.\nLook for edits and comments that indicate specific customization and make note of the differences.\nMerge in the customizations into the ISLE versions. Keep the files you create during this merge process; you will need them again for Step 2a (below)! Step 1: Choose a Project Name Please choose a project name (concatenated, with no spaces) that describes your institution or your collection platform. You will substitute in your preferred project name whenever the documentation refers to \u0026ldquo;yourprojectnamehere\u0026rdquo;. (Be creative. Some real-life examples include: arminda, dhinitiative, digital, digitalcollections, digitallibrary, unbound, etc.)\nAs in past cases, I chose the very simple name dg.\nStep 1.5: Edit \u0026ldquo;/etc/hosts\u0026rdquo; File Enable the Local ISLE Installation to be viewed locally on a personal computer browser using \u0026ldquo;yourprojectnamehere\u0026rdquo; (e.g. \u0026ldquo;https://yourprojectnamehere.localdomain\u0026rdquo;).\nPlease use these instructions to Edit the \u0026ldquo;/etc/hosts\u0026rdquo; File. Checked my /etc/hosts file and the necessary edits are in place. To protect that file from future updates to Docker, I changed the file\u0026rsquo;s permissions to 444, read-only!\nStep 2: Setup Git Project Repositories The prescribed private repositories, dg-isle and dg-islandora, were created long ago in blog posts 051 and 059. Adjust accordingly!\nYou will create two new, empty, private git repositories (if they do not already exist) within your git repository hosting service (e.g Github, Bitbucket, Gitlab). Below, we suggest a naming convention that will clearly distinguish your ISLE code from your Islandora code. It\u0026rsquo;s very important to understand that these are two separate code repositories, and not to confuse them.\nLogin to your git repository hosting service. Create a new private repository for ISLE. We suggest you name it: yourprojectnamehere-isle (This git repository will hold your ISLE code and your environment-specific customizations. Storing this in a private repository and following the workflow below will save you a lot of time and confusion.) Create a new private repository for Islandora Drupal. We suggest you name it: yourprojectnamehere-islandora (This git repository will hold your Islandora Drupal code and your site specific customizations. Storing this in a private repository and following the workflow below will save you a lot of time and confusion.) Note: This documentation will walk you through using git on the command line.\nYou will open a terminal and use the command line to clone your newly created (and empty) yourprojectnamehere-isle repository from your git hosting service to create a local directory/copy on your personal computer:\nOpen a terminal (Windows: open Git Bash)\nUse the \u0026ldquo;cd\u0026rdquo; command to navigate to a directory where you want to locate your new yourprojectnamehere-isle directory. (We recommend using the default user home directory. You may choose a different location, but it must not be a protected folder such as system or root directory.)\nExample (Mac): cd ~ Example (Windows): cd /c/Users/somebody/ Clone your new yourprojectnamehere-isle repository to your personal computer:\nExample: git clone https://yourgitproviderhere.com/yourinstitutionhere/yourprojectnamehere-isle.git Note: It is OKAY if you see this warning message: \u0026ldquo;Warning: You appear to have cloned an empty repository.\u0026rdquo;\nNavigate to the new directory created by the above clone operation:\nExample: cd yourprojectnamehere-isle Add the ICG ISLE git repository as a git upstream:\ngit remote add icg-upstream https://github.com/Islandora-Collaboration-Group/ISLE.git View your connections to remote git repositories:\ngit remote -v You should now see the following:\nicg-upstream\thttps://github.com/Islandora-Collaboration-Group/ISLE.git (fetch) icg-upstream\thttps://github.com/Islandora-Collaboration-Group/ISLE.git (push) origin\thttps://yourgitproviderhere.com/yourinstitutionhere/yourprojectnamehere-isle.git (fetch) origin\thttps://yourgitproviderhere.com/yourinstitutionhere/yourprojectnamehere-isle.git (push) Run a \u0026ldquo;git fetch\u0026rdquo; from the icg-upstream repository:\ngit fetch icg-upstream Pull down the ICG ISLE \u0026ldquo;master\u0026rdquo; branch into your yourprojectnamehere-isle local \u0026ldquo;master\u0026rdquo; branch:\ngit pull icg-upstream master View the ISLE code you now have in this directory:\nls -lha Push this code to your git hosting provider:\ngit push -u origin master This will take 2-5 minutes depending on your internet speed. You now have the current ISLE project code checked into git as a foundation to make changes on specific to your local and project needs. You\u0026rsquo;ll use this git \u0026ldquo;icg-upstream\u0026rdquo; process in the future to pull updates and new releases from the main ISLE project.\nStep 2a: Add Customizations from \u0026ldquo;Step 0\u0026rdquo; to the Git Workflow This step is intended for users who followed either the \u0026ldquo;Intermediate\u0026rdquo; or \u0026ldquo;Advanced\u0026rdquo; migration options in \u0026ldquo;Step 0\u0026rdquo; above. If you choose the Easy migration option you may safely skip Step 2a.\nNavigate to your local yourprojectnamehere-isle directory:\ncd /path/to/yourprojectnamehere-isle Create new directories under \u0026ldquo;./config\u0026rdquo; to hold the Solr and GSearch files you retrieved in \u0026ldquo;Step 0\u0026rdquo;. Do the following::\nmkdir -p ./config/solr mkdir -p ./config/fedora/gsearch Copy your \u0026ldquo;schema.xml\u0026rdquo; file from \u0026ldquo;Step 0\u0026rdquo; into the new \u0026ldquo;./config/solr/\u0026rdquo; directory.\nCopy your \u0026ldquo;foxmltoSolr.xslt\u0026rdquo; file and \u0026ldquo;islandora_transforms\u0026rdquo; directory from \u0026ldquo;Step 0\u0026rdquo; into the \u0026ldquo;config/fedora/gsearch/\u0026rdquo; directory.\nAdd a new line in the Solr volumes section of your \u0026ldquo;docker-compose.local.yml\u0026rdquo;\n- config/solr/schema.xml:/usr/local/solr/collection1/conf/schema.xml` Add new lines in the Fedora volumes section of your \u0026ldquo;docker-compose.local.yml\u0026rdquo; - ./config/fedora/gsearch/islandora_transforms:/usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/islandora_transforms - ./config/fedora/gsearch/foxmlToSolr.xslt:/usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/foxmlToSolr.xslt Continue the local setup as directed below and ultimately import some objects from your existing Fedora repository and see if they display properly in searches as you like.\nAll prescribed \u0026ldquo;customizations\u0026rdquo; were previously incorporated into my dg-isle repository. Since no changes to the base files, see annotations above in Step 0 - Strategy 3, were detected, I will assume that my previous customizations remain valid.\nStep 3: Git Clone the Production Islandora Drupal Site Code This step assumes you have an existing Islandora Drupal site, like yourprojectnamehere-islandora, checked into a git repository. (If not, then you\u0026rsquo;ll need to check your Drupal site into a git repository following the same commands from Local ISLE Installation: New Site documentation. Your git repository should be initialized at the Drupal root.)\nNote: If below you see a \u0026ldquo;fatal: Could not read from remote repository.\u0026rdquo; error, then please read Fatal: Could not read from remote repository.\nUsing the same open terminal:\nCreate a location outside of your /path/to/yourprojectnamehere-isle directory where your Islandora Drupal site code will be stored. While you may create this location anywhere, we suggest that you put it at the same level as your existing yourprojectnamehere-isle directory. From your /path/to/yourprojectnamehere-isle directory, go up one level: cd /path/to/yourprojectnamehere-isle cd .. git clone https://yourgitproviderhere.com/yourinstitutionhere/yourprojectnamehere-islandora.git Example: The above process created a folder named \u0026ldquo;yourprojectnamehere-islandora\u0026rdquo; Attention: Digital.Grinnell\u0026rsquo;s dg-islandora repository contains numerous theme and module git submodules! It is therefore absolutely imperative that the aforementioned git clone... command be run with a --recursive option, like so:\nWorkstation Commands cd ~/GitHub/ git clone https://github.com/Digital-Grinnell/dg-islandora.git --recursive Now update the \u0026ldquo;docker-compose.local.yml\u0026rdquo; in the yourprojectnamehere-isle directory to create a bind-mount to the Islandora Drupal site code: Search for \u0026ldquo;apache:\u0026rdquo; Find the sub-section called \u0026ldquo;volumes:\u0026rdquo; Find this line: \u0026ldquo;- ./data/apache/html:/var/www/html:cached\u0026rdquo; Edit the above line to be like this: - ../yourprojectnamehere-islandora:/var/www/html:cached Your Production Islandora site code is now configured to be used in this local setup. Step 4: Edit the \u0026ldquo;.env\u0026rdquo; File to Change to the Local Environment Navigate to your local yourprojectnamehere-isle directory.\nCopy the sample.env to .env. By default, the Demo environment is setup. You will need to edit this file to match the correct environment. Please note that the .env is no longer tracked by git as of ISLE version 1.5. Instructions below involving git are for ISLE versions below 1.5. However the settings recommended below for the environment can still be followed as needed.\ncp sample.env .env Open the \u0026ldquo;.env\u0026rdquo; file in a text editor.\nChange only the following lines in the \u0026ldquo;.env\u0026rdquo; file so that the resulting values look like the following: Please note: the following below is an example not actual values you should use. Use one word to describe your project and follow the conventions below accordingly\nCOMPOSE_PROJECT_NAME=yourprojectnamehere_local BASE_DOMAIN=yourprojectnamehere.localdomain CONTAINER_SHORT_ID=ld leave default setting of ld as is. Do not change. COMPOSE_FILE=docker-compose.local.yml Save and close the file.\nAdditionally, depending on your decision from \u0026ldquo;Step 0\u0026rdquo;, you may need to make additional edits to docker-compose.local.yml and move files into place as directed from the (Intermediate) and (Advanced) sections.\nNote: We highly recommend that you also review the contents of the docker-compose.local.yml file as the Apache service volume section uses bind mounts for the intended Drupal Code instead of using default Docker volumes. This allows users to perform Local Islandora Drupal site development with an IDE. This line is a suggested path and users are free to change values to the left of the : to match their Apache data folder of choice. However we recommend starting out with the default setting below. Default: - ./data/apache/html:/var/www/html:cached\nI checked for significant changes to sample.env (for .env) and docker-compose.local.yml between past versions and v1.5.1, and found NONE. So, I believe my previous changes to these files should still be valid.\nStep 5: Create New Users and Passwords by Editing \u0026ldquo;local.env\u0026rdquo; File You can reuse some of the older Production settings in the \u0026ldquo;local.env\u0026rdquo; if you like (e.g. the database name \u0026ldquo;DRUPAL_DB\u0026rdquo;, database user \u0026ldquo;DRUPAL_DB_USER\u0026rdquo; even the Drupal database user password \u0026ldquo;DRUPAL_DB_PASS\u0026rdquo; if that makes it easier). It is important to avoid repeating passwords in the ISLE Staging and Production environments.\nOpen the \u0026ldquo;local.env\u0026rdquo; file in a text editor.\nFind each comment that begins with: # Replace this comment with a ... and follow the commented instructions to edit the passwords, database and user names.\nReview carefully as some comments request that you replace with ...26 alpha-numeric characters while others request that you create an ...easy to read but short database name.\nIn many cases the username is already pre-populated. If it doesn\u0026rsquo;t have a comment directing you to change or add a value after the =, then don\u0026rsquo;t change it.\nFor Microsoft Windows:\nFind the following line: # COMPOSE_CONVERT_WINDOWS_PATHS=1 In the above line, delete the first two characters (# ) so as to uncomment the line. It should now look like this: COMPOSE_CONVERT_WINDOWS_PATHS=1 Once finished, save and close the file.\nOpen the \u0026ldquo;config/apache/settings_php/settings.local.php\u0026rdquo; file in a text editor.\nFind the first comment that begins with: # ISLE Configuration and follow the commented instructions to edit the database, username and password. Find the second comment that begins with: # ISLE Configuration and follow the instructions to edit the Drupal hash salt. Once finished, save and close the file. As in previous sections, I checked for significant changes to local.env and config/apache/settings_php/settings.local.php between past versions and v1.5.1, and found NONE. So, I believe my previous changes to these files should still be valid.\nStep 6: Create New Self-Signed Certs for Your Project Open the appropriate file in a text editor:\nFor Mac/Ubuntu/CentOS/etc: \u0026ldquo;./scripts/proxy/ssl-certs/local.sh\u0026rdquo; For Microsoft Windows: \u0026ldquo;./scripts/proxy/ssl-certs/local-windows-only.sh\u0026rdquo; Follow the in-line instructions to add your project\u0026rsquo;s name to the appropriate areas.\nOnce finished, save and close the file. Using the same open terminal:, navigate to \u0026ldquo;/path/to/yourprojectnamehere-isle/scripts/proxy/ssl-certs/\u0026rdquo;\ncd ./scripts/proxy/ssl-certs/ Change the permissions on the script to make it executable\nFor Mac/Ubuntu/CentOS/etc: chmod +x local.sh For Microsoft Windows: chmod +x local-windows-only.sh Run the following command to generate new self-signed SSL keys using your \u0026ldquo;yourprojectnamehere.localdomain\u0026rdquo; domain. This now secures the local site.\nFor Mac/Ubuntu/CentOS/etc: ./local.sh For Microsoft Windows: ./local-windows-only.sh The generated keys can now be found in: cd ../../../config/proxy/ssl-certs Add the SSL .pem and .key file names generated from running the above script to the \u0026ldquo;./config/proxy/traefik.local.toml\u0026rdquo; file.\ncd .. Open traefik.local.toml in a text editor. Example: certFile = \u0026quot;/certs/yourprojectnamehere.localdomain.pem\u0026quot; keyFile = \u0026quot;/certs/yourprojectnamehere.localdomain.key\u0026quot; As in previous sections, I checked for significant changes to the files involved in this section between past versions and v1.5.1, and found NONE. So, I believe my previous changes to these files should still be valid.\nStep 7: Download the ISLE Images Download all of the latest ISLE Docker images (~6 GB of data may take 5-10 minutes): Using the same open terminal: Navigate to the root of your local yourprojectnamehere-isle directory: cd ~/path/to/yourprojectnamehere-isle docker-compose pull I followed the instructions in this section to-the-letter. Specifically\u0026hellip;\nWorkstation Commands cd ~/GitHub/dg-isle docker-compose pull Step 8: Launch Process Using the same open terminal:\ndocker-compose up -d Please wait a few moments for the stack to fully come up. Approximately 3-5 minutes.\nUsing the same open terminal:\nView only the running containers: docker ps View all containers (both those running and stopped): docker ps -a All containers prefixed with \u0026ldquo;isle-\u0026rdquo; are expected to have a \u0026ldquo;STATUS\u0026rdquo; of \u0026ldquo;Up\u0026rdquo; (for x time). If any of these are not \u0026ldquo;UP\u0026rdquo;, then use Non-Running Docker Containers to solve before continuing below. I followed the instructions in this section to-the-letter. Specifically\u0026hellip;\nWorkstation Commands cd ~/GitHub/dg-isle docker-compose up -d Step 9: Import the Production MySQL Drupal Database Method A: Use a MySQL client with a GUI\nConfigure the client with the following: Host = 127.0.0.1 Port: 3306 or a different port if you changed it Username: root Password: YOUR_MYSQL_ROOT_PASSWORD in the \u0026ldquo;local.env\u0026rdquo;) Select the Drupal database \u0026ldquo;DRUPAL_DB\u0026rdquo; in the \u0026ldquo;local.env\u0026rdquo;) Click File \u0026gt; Import (or equivalent) Select your exported Production Islandora Drupal database file (e.g. \u0026ldquo;prod_drupal_site_082019.sql.gz\u0026rdquo;) The import process will take 1 -3 minutes depending on the size. Method B: Use the command line\nCopy the Production Islandora Drupal database file (e.g. \u0026ldquo;prod_drupal_site_082019.sql.gz\u0026rdquo;) to your ISLE MySQL container\nRun docker ps to determine the mysql container name docker cp /pathto/prod_drupal_site_082019.sql.gz your-mysql-containername:/prod_drupal_site_082019.sql.gz Example: docker cp /c/db_backups/prod_drupal_site_082019.sql.gz isle-mysql-ld:/prod_drupal_site_082019.sql.gz This might take a few minutes depending on the size of the file. Shell into the mysql container by copying and pasting the appropriate command:\nFor Mac/Ubuntu/CentOS/etc: docker exec -it your-mysql-containername bash For Microsoft Windows: winpty docker exec -it your-mysql-containername bash Import the Production Islandora Drupal database. Replace the \u0026ldquo;DRUPAL_DB_USER\u0026rdquo; and \u0026ldquo;DRUPAL_DB\u0026rdquo; in the command below with the values found in your \u0026ldquo;local.env\u0026rdquo; file.\nmysql -u DRUPAL_DB_USER -p DRUPAL_DB \u0026lt; prod_drupal_site_082019.sql.gz This might take a few minutes depending on the size of the file.\nType exit to exit the container\nI chose Method B and used the following commands to complete this step:\nWorkstation Commands docker cp /Users/markmcfate/Desktop/migration-copy/prod_drupal_site_083120.sql isle-mysql-ld:/prod_drupal_site_083120.sql docker exec -it isle-mysql-ld bash isle-mysql-ld Container Commands mysql -u admin -p digital_grinnell \u0026lt; prod_drupal_site_083120.sql Once I completed this entire process, as documented, I was left wondering what to do with the backup copy of my production /var/www/html/sites/default/files directory that I created back in Step 0? After posting a question to the isle-support Slack channel about this, I agreed that the handling of that backup directory should be covered here, and I opened an issue #388 in GitHub to deal with the documentation. The command sequence I used is documented here.\nWorkstation Commands docker cp /Users/markmcfate/Desktop/migration-copy/var/www/html/sites/default/files/. isle-apache-ld:/var/www/html/sites/default/files/ Since I introduced and executed this annotation after completing the entire process, I found it necessary to repeat a portion of the migration_site_vsets.sh script introduced later in the document. I did this working in the Apache container, like so:\nisle-apache-ld Container Commands /bin/bash /utility-scripts/isle_drupal_build_tools/drupal/fix-permissions.sh --drupal_path=/var/www/html --drupal_user=islandora --httpd_group=www-data cd /var/www/html/sites/default drush cc all Step 10: Run Islandora Drupal Site Scripts migration_site_vsets.sh: updates Drupal database settings\nThis step will show you how to run the \u0026ldquo;migration_site_vsets.sh\u0026rdquo; script on the Apache container to change Drupal database site settings for ISLE connectivity.\nUsing the same open terminal:\nRun docker ps to determine the apache container name Copy the \u0026ldquo;migration_site_vsets.sh\u0026rdquo; to the root of the Drupal directory on your Apache container docker cp scripts/apache/migration_site_vsets.sh your-apache-containername:/var/www/html/migration_site_vsets.sh Change the permissions on the script to make it executable For Mac/Ubuntu/CentOS/etc: docker exec -it your-apache-containername bash -c \u0026quot;chmod +x /var/www/html/migration_site_vsets.sh\u0026quot; For Microsoft Windows: winpty docker exec -it your-apache-containername bash -c \u0026quot;chmod +x /var/www/html/migration_site_vsets.sh\u0026quot; Run the script For Mac/Ubuntu/CentOS/etc: docker exec -it your-apache-containername bash -c \u0026quot;cd /var/www/html \u0026amp;\u0026amp; ./migration_site_vsets.sh\u0026quot; For Microsoft Windows: winpty docker exec -it your-apache-containername bash -c \u0026quot;cd /var/www/html \u0026amp;\u0026amp; ./migration_site_vsets.sh\u0026quot; Before running the prescribed scripts for this step, I executed the following commands to run a script intended to eliminate some unnecessary warnings:\nisle-apache-ld Container Commands cd /var/www/html source Digital-Grinnell-Migration-Mitigation-Script.sh Final output of the Digital-Grinnell-Migration-Mitigation-Script.sh script was:\nThe following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;antibot\u0026lt;/em\u0026gt;. For information about [warning] how to fix this, see \u0026lt;a href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;islandora_mods_via_twig\u0026lt;/em\u0026gt;. For [warning] information about how to fix this, see \u0026lt;a href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 \u0026#39;all\u0026#39; cache was cleared. Attention: If the output of the script above yields warnings about missing module dg7, and other islandora_... modules, then you probably forgot to clone the dg-islandora project using the --recursive option! If that is the case you should return to [Step 3](#step-3-git-clone-the-production- islandora-drupal-site-code) and take note of the important annotation there.\nNext, I ran the following commands to complete this portion of Step 10:\nWorkstation Commands docker cp scripts/apache/migration_site_vsets.sh isle-apache-ld:/var/www/html/migration_site_vsets.sh docker exec -w /var/www/html isle-apache-ld chmod +x migration_site_vsets.sh time docker exec -w /var/www/html isle-apache-ld ./migration_site_vsets.sh The abridged output of the script is included below.\nDrush vset of ISLE specific configurations The following module is missing from the file system: \u0026lt;em [warning] class=\u0026#34;placeholder\u0026#34;\u0026gt;antibot\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em [warning] class=\u0026#34;placeholder\u0026#34;\u0026gt;islandora_mods_via_twig\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The two warnings shown above were repeated many times in the original output, but all subsequent instances have been removed from the abridged version shown below. No other changes were made to this output.\nislandora_base_url was set to \u0026#34;fedora:8080/fedora\u0026#34;. [success] islandora_solr_url was set to \u0026#34;solr:8080/solr\u0026#34;. [success] imagemagick_convert was set to \u0026#34;/usr/local/bin/convert\u0026#34;. [success] image_toolkit was set to \u0026#34;imagemagick\u0026#34;. [success] islandora_ocr_tesseract was set to \u0026#34;/usr/bin/tesseract\u0026#34;. [success] islandora_batch_java was set to \u0026#34;/usr/bin/java\u0026#34;. [success] islandora_lame_url was set to \u0026#34;/usr/bin/lame\u0026#34;. [success] islandora_paged_content_gs was set to \u0026#34;/usr/bin/gs\u0026#34;. [success] islandora_video_ffmpeg_path was set to \u0026#34;/usr/bin/ffmpeg\u0026#34;. [success] islandora_video_ffmpeg2theora_path was set to [success] \u0026#34;/usr/bin/ffmpeg2theora\u0026#34;. islandora_use_kakadu was set to FALSE. [success] islandora_kakadu_url was set to \u0026#34;/usr/local/bin/kdu_compress\u0026#34;. [success] islandora_pdf_path_to_pdftotext was set to \u0026#34;/usr/bin/pdftotext\u0026#34;. [success] islandora_fits_executable_path was set to \u0026#34;/usr/local/bin/fits\u0026#34;. [success] islandora_openseadragon_iiif_identifier was set to [success] \u0026#34;[islandora_openseadragon:pid]~[islandora_openseadragon:dsid]~[islandora_openseadragon:token]\u0026#34;. islandora_openseadragon_iiif_token_header was set to 0. [success] islandora_openseadragon_iiif_url was set to \u0026#34;iiif/2\u0026#34;. [success] islandora_openseadragon_tilesource was set to \u0026#34;iiif\u0026#34;. [success] islandora_internet_archive_bookreader_iiif_identifier was set to [success] \u0026#34;[islandora_iareader:pid]~[islandora_iareader:dsid]~[islandora_iareader:token]\u0026#34;. islandora_internet_archive_bookreader_iiif_token_header was set to 0. [success] islandora_internet_archive_bookreader_iiif_url was set to \u0026#34;iiif/2\u0026#34;. [success] islandora_internet_archive_bookreader_pagesource was set to \u0026#34;iiif\u0026#34;. [success] \u0026#34;anonymous user\u0026#34; already has the permission \u0026#34;view fedora repository [ok] objects\u0026#34; Running fix-permissions script Changing ownership of all contents of /var/www/html: user =\u0026gt; islandora group =\u0026gt; www-data Changing permissions of all directories inside /var/www/html to rwxr-x---... Changing permissions of all files inside /var/www/html to rw-r-----... Changing permissions of files directories in /var/www/html/sites to rwxrwx---... Changing permissions of all files inside all files directories in /var/www/html/sites to rw-rw----... Changing permissions of all directories inside all files directories in /var/www/html/sites to rwxrwx---... find: \u0026#39;./*/files\u0026#39;: No such file or directory find: \u0026#39;./*/files\u0026#39;: No such file or directory Done setting proper permissions on files and directories Configuring cron job to run every 3 hours Running Drupal Cron first time and clearing Drupal Caches. WD smtp: phpmailerException: SMTP Error: Could not connect to SMTP [error] host. in PHPMailer-\u0026gt;SmtpConnect() (line 810 of /var/www/html/sites/all/modules/contrib/smtp/smtp.phpmailer.inc). WD smtp: Error sending e-mail from digital@grinnell.edu to [error] digital@grinnell.edu : SMTP Error: Could not connect to SMTP host. WD mail: Error sending e-mail (from digital@grinnell.edu to [error] digital@grinnell.edu). Cron run successful. [success] Unable to send e-mail. Contact the site administrator if the problem [error] persists. Drush script finished! ...exiting docker exec -w /var/www/html isle-apache-ld ./migration_site_vsets.sh 0.05s user 0.04s system 0% cpu 7:59.50 total Note: In the above output the two lines that read find: './*/files': No such file or directory were present because our production backup copy of /var/www/html/sites/defaults/files had not been restored. Completion of that missing portion of Step 9 should resolve/remove those warnings.\ninstall_solution_packs.sh: installs Islandora solution packs\nSince you\u0026rsquo;ve imported an existing Drupal database, you must now reinstall the Islandora solution packs so the Fedora repository will be ready to ingest objects.\nCopy the \u0026ldquo;install_solution_packs.sh\u0026rdquo; to the root of the Drupal directory on your Apache container docker cp scripts/apache/install_solution_packs.sh your-apache-containername:/var/www/html/install_solution_packs.sh Change the permissions on the script to make it executable For Mac/Ubuntu/CentOS/etc: docker exec -it your-apache-containername bash -c \u0026quot;chmod +x /var/www/html/install_solution_packs.sh\u0026quot; For Microsoft Windows: winpty docker exec -it your-apache-containername bash -c \u0026quot;chmod +x /var/www/html/install_solution_packs.sh\u0026quot; Run the script For Mac/Ubuntu/CentOS/etc: docker exec -it your-apache-containername bash -c \u0026quot;cd /var/www/html \u0026amp;\u0026amp; ./install_solution_packs.sh\u0026quot; For Microsoft Windows: winpty docker exec -it your-apache-containername bash -c \u0026quot;cd /var/www/html \u0026amp;\u0026amp; ./install_solution_packs.sh\u0026quot; The above process will take a few minutes depending on the speed of your local and Internet connection. You should see a lot of green [ok] messages. If the script appears to pause or prompt for \u0026ldquo;y/n\u0026rdquo;, DO NOT enter any values; the script will automatically answer for you. For Microsoft Windows: You may be prompted by Windows to: - Share the C drive with Docker. Click Okay or Allow. - Enter your username and password. Do this. - Allow vpnkit.exe to communicate with the network. Click Okay or Allow (accept default selection). - If the process seems to halt, check the taskbar for background windows. Proceed only after this message appears: \u0026ldquo;Done. \u0026lsquo;all\u0026rsquo; cache was cleared.\u0026rdquo; I ran the following commands to complete this portion of Step 10:\nWorkstation Commands docker cp scripts/apache/install_solution_packs.sh isle-apache-ld:/var/www/html/install_solution_packs.sh docker exec -w /var/www/html isle-apache-ld chmod +x install_solution_packs.sh time docker exec -w /var/www/html isle-apache-ld ./install_solution_packs.sh The two aforementioned warnings were repeated several times, but have once again been removed from the abridged output for clarity. The abridged output is:\nInstalling all Islandora modules Resinstalling the Islandora module with Solution Packs islandora: Successfully reinstalled. Top-level Collection. [status] Replaced islandora:sp-audioCModel - Islandora Audio Content Model Replaced islandora:collectionCModel - Islandora Collection Content Model Replaced islandora:sp_basic_image - Islandora Basic Image Content Model Replaced islandora:pageCModel - Islandora Page Content Model Replaced islandora:bookCModel - Islandora Internet Archive Book Content Model Replaced islandora:compoundCModel - Islandora Compound Object Content Model Replaced islandora:sp_large_image_cmodel - Islandora Large Image Content Model Replaced islandora:sp_pdf - Islandora PDF Content Model Replaced islandora:sp_videoCModel - Islandora Video Content Model Drush script finished! ...exiting docker exec -w /var/www/html isle-apache-ld ./install_solution_packs.sh 0.03s user 0.02s system 0% cpu 4:31.81 total Step 11: Test the Site In your web browser, enter this URL: https://yourprojectnamehere.localdomain I visited https://dg.localdomain on my iMac desktop as instructed. The site comes up with the appropriate theme apparently enabled and working. The two aforementioned warnings are present in a different format than before (this is a web/html response rather than command-line output), but no other warnings or errors are present!\nNote: You may see an SSL error warning that the site is unsafe. It is safe, it simply uses \u0026ldquo;self-signed\u0026rdquo; SSL certs. Ignore the error and proceed to the site.\nLog in to the local Islandora site with the credentials (\u0026ldquo;DRUPAL_ADMIN_USER\u0026rdquo; and \u0026ldquo;DRUPAL_ADMIN_PASS\u0026rdquo;) you created in \u0026ldquo;local.env\u0026rdquo;.\nYou can also attempt to use login credentials that the Production server would have stored in its database. In order to attempt to login I explicitly visited https://dg.localdomain/user/login. Unfortunately, neither the old nor new credentials worked, so I followed the additional steps below to add the new user to the Apache container.\nIf the newly created Drupal login doesn\u0026rsquo;t work then, you\u0026rsquo;ll need to Shell into the Apache container:\nFor Mac/Ubuntu/CentOS/etc: docker exec -it your-apache-containername bash For Microsoft Windows: winpty docker exec -it your-apache-containername bash Navigate to this directory\ncd /var/www/html Create the user found in \u0026ldquo;DRUPAL_ADMIN_USER\u0026rdquo; and set its password to the value of \u0026ldquo;DRUPAL_ADMIN_PASS\u0026rdquo; as you previously created in \u0026ldquo;local.env\u0026rdquo;.\nIn the example below swap-out \u0026ldquo;DRUPAL_ADMIN_USER\u0026rdquo; and \u0026ldquo;DRUPAL_ADMIN_PASS\u0026rdquo; with those found in \u0026ldquo;local.env\u0026rdquo;.\ndrush user-create DRUPAL_ADMIN_USER --mail=\u0026quot;youremailaddresshere\u0026quot; --password=\u0026quot;DRUPAL_ADMIN_PASS\u0026quot;;\ndrush user-add-role \u0026quot;administrator\u0026quot; DRUPAL_ADMIN_USER\nType exit to exit the container\nOn my workstation I executed the following docker exec... commands\nWorkstation Commands docker exec -w /var/www/html isle-apache-ld drush user-create admin --mail=\u0026quot;mark.mcfate@icloud.com\u0026quot; --password=\u0026quot;**obfuscated**\u0026quot; docker exec -w /var/www/html isle-apache-ld drush user-add-role \u0026quot;Administrator\u0026quot; admin Note the changes above:\nIn my case, the prefered email address digital@grinnell.edu is already assigned to a user, and There is no \u0026ldquo;administrator\u0026rdquo; role, only \u0026ldquo;Administrator\u0026rdquo;, with a capital \u0026ldquo;A\u0026rdquo;. With corresponding changes the commands appear to have worked with output as follows:\nUser ID : 1119378 User name : admin User mail : mark.mcfate@icloud.com User roles : authenticated user User status : active Added role Administrator role to admin Attempt to login again I did as instructed and this time the admin login was accepted. The output included both of the aforementioned warnings, but no additional issues were present.\nPlease refer to post 090 for details of the customizations I implemented to complete the process of migrating to a new local instance of ISLE, one that looks and behaves exactly like Digital.Grinnell.\nStep 12: Ingest Sample Objects Since my new local instance of ISLE includes the use of my DG-FEDORA portable (USB stick) repository, there was no need for me to execute the steps outlined in this section. Instead, I followed the update guidance provided in DG-FEDORA: A Portable FEDORA Repository.\nAfter following the aforementioned blog post my repository containing more than 120 objects was visible, as expected.\nDigital.Grinnell\u0026rsquo;s home page \u0026ldquo;view\u0026rdquo; is somewhat complex and requires some attention before it will function properly. Guidance to update/configure the dg7 Collection View is provided in Updating DG\u0026rsquo;s Collection Views and I followed that guidance to ensure that our new local ISLE instance was behaving properly.\nFollowing this critical update I\u0026rsquo;m pleased to report that https://dg.localdomain appears to be working perfectly!\nIt is recommended that end users migrating their sites opt to either import sample objects from their non-ISLE Production Fedora servers or use the following below:\nThe Islandora Collaboration Group provides a set of Islandora Sample Objects with corresponding metadata for testing Islandora\u0026rsquo;s ingest process. These sample objects are organized by solution pack and are zipped for faster bulk ingestion.\nTo download these sample objects, clone them to your computer\u0026rsquo;s desktop: git clone https://github.com/Islandora-Collaboration-Group/islandora-sample-objects.git Follow these ingestion instructions How to Add an Item to a Digital Collection\n(Note: Getting Started with Islandora contains explanations about content models, collections, and datastreams.)\nAfter ingesting content, you may need to add an Islandora Simple Search block to the Drupal structure. (The default search box will only search Drupal content, not Islandora content.) This might already exist in your current Drupal Production site as a feature.\nSelect from the menu: Structure \u0026gt; Blocks \u0026gt; Islandora Simple Search\nSelect: Sidebar Second\nClick: Save Blocks at bottom of page\nYou may now search for ingested objects that have been indexed by SOLR\nAfter ingesting either the ICG sample objects or a selection of your pre-existing Fedora Production objects, continue to QC the migrated site, ensuring that objects display properly, the theme and design continue to work properly, there are no errors in the Drupal watchdog and everything matches the functionality of the previous non-ISLE Production Islandora Drupal site.\nNext Steps Once you are ready to deploy your finished Drupal site, you may progress to:\nStaging ISLE Installation: Migrate Existing Islandora Site Additional Resources ISLE Installation: Environments helps explain the ISLE workflow structure, the associated files, and what values ISLE end users should use for the \u0026ldquo;.env\u0026rdquo;, \u0026ldquo;local.env\u0026rdquo;, etc. Local ISLE Installation: Resources contains Docker container passwords and URLs for administrator testing. ISLE Installation: Troubleshooting contains help for port conflicts, non-running Docker containers, etc. End of Local ISLE Installation: Migrate Existing Islandora Site ","permalink":"https://static.grinnell.edu/dlad-blog/posts/087-rebuilding-isle-ld-again/","tags":["ISLE","migrate","local","development"],"title":"Local ISLE Installation: Migrate Existing Islandora Site - with Annotations"},{"categories":null,"contents":" Update: September 4, 2020\nAs of this morning my Synergy solution stopped working and I could not easily get it back in-sync between machines. This has happened before, more than once, and it\u0026rsquo;s largely the reason I abandoned Synergy in favor of Sharemouse. But Sharemouse was plagued with similar issues, and it absolutely didn\u0026rsquo;t work well with VPN active.\nThe other factor today\u0026hellip; now that my GC iMac has a new \u0026ldquo;assigned identity\u0026rdquo;, I think the old system tag has to be removed from my config because Synergy keeps expecting to find that hostname.\nSo, I\u0026rsquo;ve disabled Synergy in favor of Sharemouse, again, with intent to let it run for a few days just to see if it behaves any better with my KM switch. I also added Sharemouse as a keyword in this post. If things work out, I\u0026rsquo;ll update the rest of this post next week to formally reflect this change.\nI think I have finally found (created?) a multi-computer desktop environment that is nearly perfect; it combines Synergy with a 4-port USB KM switch that I purchased a few months ago after my other solutions failed, again. The many miserable failures that came before this solution tell a bleak story, but perhaps my telling it here will save others some grief.\nThe Goal Just to be clear, the purpose of my journey has always been to engage multiple computers with one or more monitors from a SINGLE keyboard and mouse. I absolutely loathe having to use more than one keyboard/mouse combination to support multiple computers. I\u0026rsquo;ve also found that it\u0026rsquo;s really nice if those computers can share things like:\nThe clipboard - So that I can copy and paste files and material from one comptuer to another. USB Peripherals - Things like printers, scanners, external hard drives, and my label printer. My Desktop Environment The desktop environment that I have in my home-office features a personal Mac Mini connected to a 39\u0026quot; Vizio LCD screen, plus a college-owned 21\u0026quot; iMac with an extra 21\u0026quot; LCD dedicated monitor. I also have two MacBook Air portable computers, one personal and the other owned by the college, that I connect into the mix when needed, or when I need computing power on-the-road. Elsewhere in my home I have a collection that includes two NAS (network attached storage) computers running Linux, a Mac Mini with a large monitor or my daughter, another iMac for my wife, and three more MacBook Air computers distributed between my wife and daughter. Mackenzie, my daughter, also has the only remining Windows machine in the house, a Lenovo laptop.\nAll of this, plus a handful of iPhones, iPads, and Apple Watches exist on my LAN along with a pair of networked laser printers. Suffice it to say, there are wires, and wireless signals, EVERYWHERE.\nWhat Used to Work What used to work? Well, for one, Apple\u0026rsquo;s Universal Clipboard used to work, but it doesn\u0026rsquo;t anymore. 🙍 It\u0026rsquo;s a wonderful feature, easy to configure and use, except when the controling agency/department won\u0026rsquo;t let it. In my case the controlling agency/department is a \u0026ldquo;Windows shop\u0026rdquo;, so I\u0026rsquo;m not surprised that their \u0026ldquo;solutions\u0026rdquo; don\u0026rsquo;t mesh with Apple\u0026rsquo;s.\nAside from Universal Clipboard, the litany of things I\u0026rsquo;ve tried in the past is frankly too long to remember in detail, so in no particular order, they include:\nNumerous KVM switches - These all involved lots of wires and a single monitor, which was nice, but no longer practical. I need LOTS of monitor real estate these days because my vision isn\u0026rsquo;t what it used to be. 😢 A StarTech USB cable - This thing combined a \u0026ldquo;smart\u0026rdquo; USB cable, between two computers, with some software to provide a really slick experience that had all the features I needed. Unfortunately, it was flaky and frequently misbehaved. When OS X transitioned to 64-bit only the thing was no longer supported and became uterly useless. The aforementioned 4-port USB KM switch - Another really nice solution, but it lacked the software that the StarTech cable had, so it wasn\u0026rsquo;t as flaky, but also has no clipboard sharing and only allows sharing of limited peripherals, one\u0026rsquo;s that don\u0026rsquo;t have to be continuously connected. Also, it requires me to click a built-in hardware switch to move my KM focus from one computer to another. Synergy - This is a wonderful piece of software (no hardware involved) that provides ALL of the capability that I need, and it\u0026rsquo;s inexpensive, so I purchased a license in January 2019. Unfortunately, like the StarTech cable, it is also a little flaky and somewhat difficult to configure. Sharemouse - This is another software-only solution that\u0026rsquo;s a little easier to configure than Synergy, and seems to be a bit more stable. But still, there are significant limitations that I\u0026rsquo;ll talk about in general below. Why Those Things No Longer Work The real key to my computing environment is the college-owned iMac, it is the most powerful machine I have and the only one that has all the tools I need to do my job. Since it is a college-owned machine it must comply with college IT policy, and for the most part, that\u0026rsquo;s why my previous soltuions have all failed. I understand why the policy is what it is, but there are available workarounds and strategies that I beleive could be successfully used, but nobody has vetted them nor are they willing to spend time doing so. I get that too.\nThe most frustrating issue for me involves the college VPN which requires use of the Cisco AnyConnect Secure Mobility Client configured to the college\u0026rsquo;s specifications and under their control. I connect via VPN quite often and for long periods of time so this is especially painful. When engaged, the Cisco client takes complete control of the iMac\u0026rsquo;s networking such that NONE of my network-dependent resources work. That effectively renders me unable to share ANY resources including my keyboard and mouse, printers, scanners, and external memory devices. Even with my \u0026ldquo;almost ideal\u0026rdquo; solution, this issue with loss of connectivity whenever VPN is active remains a big problem, but one that I can now workaround even when things aren\u0026rsquo;t working optimally.\nThe Deal Breaker The software solutions I\u0026rsquo;ve used, Synergy and Sharemouse, are by far the best options; they just work, except when they don\u0026rsquo;t. 😦 When using either solution the VPN issue has always been present, but there are also times when the software just gets wonky. I\u0026rsquo;ve never been able to pinpoint all the conditions that cause things to go awry, but believe me, it happens all the time, every day, sometimes more than once per day. Even worse, when the software fails I\u0026rsquo;m generally stuck using whatever machine had my keyboard/mouse focus at the time, and the other machine is effectively unreachable. In the past I had to physically connect a \u0026ldquo;spare\u0026rdquo; keyboard and mouse to the \u0026ldquo;zombie\u0026rdquo; machine to get control back, then I\u0026rsquo;d have to restart both machines and reconfigure the whole mess again. Rinse and repeat, daily, or sometimes every few hours.\nWhat Works\u0026hellip;the Good Part For many months now I abandoned all of the software solutions in favor of a more reliable, but less capable and convenient, hardware soltuion. Until this past week I had never even thought about bringing any of the software soltuions, specifically Synergy, back to use in-concert with the hardware. I was pleasantly surprised at how well the combination works.\nThe hardware portion of the scheme, my 4-port USB KM switch, seems to be very reliable, albeit not very convenient. The software component, Synergy, provides me with the ability to not only share they keyboard and mouse, but it does so seemlessly. I simply move the mouse from one screen to the other (the software is configured to know where the screens are in relation to each other) and the focus changes instantly. Even better, anything I copy on one screen or machine, is instantly available to paste on the other, so I no longer have to email or IM text or files between adjacent machines. All of this works with no apparent ill-effects from using the two schemes in-tandem. It just works! Woot woot!\nAnd the very best feature, if the software does gets wonky (it hasn\u0026rsquo;t thus far, execpt in a couple of tests where I forced it to fail) I don\u0026rsquo;t have to connect a spare keyboard/mouse combination to get control back, I simply press my KM switch and move the existing keyboard/mouse focus to the \u0026ldquo;other\u0026rdquo; machine. Best of all, having made this switch it looks like I can take any action necessary to get the software back in-sync and working properly.\nThis is as close to desktop computing nirvana as I can get, and I\u0026rsquo;m happy to have it. 😄\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/088-synergy-with-a-km-switch/","tags":["Synergy","Sharemouse","KM switch","Universal Clipboard"],"title":"Synergy with a KM (Keyboard/Mouse) Switch: A Near-Perfect Combination"},{"categories":null,"contents":" Attention: On 21-May-2020 this optional, but recommended, sixth step was added to the workflow that is documented in Exporting, Editing, \u0026amp; Replacing MODS Datastreams and Exporting, Editing, \u0026amp; Replacing MODS Datastreams: Technical Details. This addtional workflow step comes in the form of a new Drush command: islandora_mods_post_processing, an addition to my previous work in islandora_mods_via_twig.\nPurpose Many of the objects in Digital.Grinnell are \u0026ldquo;shared\u0026rdquo; between two or more collections. For example, grinnell:10361 can be found in both the \u0026ldquo;Social Justice\u0026rdquo; and \u0026ldquo;Student Scholarship\u0026rdquo; collections.\nThis step in the workflow is designed to account for all of an object\u0026rsquo;s \u0026ldquo;duplicate\u0026rdquo; MODS record exports, no matter which collection(s) they appear in. The intent is to make the \u0026ldquo;duplicates\u0026rdquo; easy to recognize so that editors don\u0026rsquo;t spend time modifying the same record more than once.\nRecap: The Original 5-Step Workflow This document is a follow-up and additon, with technical details, to Exporting, Editing, \u0026amp; Replacing MODS Datastreams: Technical Details, post 070, in my blog. As such, it is NOT necessary for metadata editors working on the 2020 Grinnell College Libraries review of Digital Grinnell MODS metadata to implement this step, but this document may help them better understand the process as a whole.\nAttention: This document uses a shorthand ./ in place of the frequently referenced //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/ directory. For example, ./social-justice is equivalent to the Social Justice collection sub-directory at //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/social-justice.\nBriefly, the initial five steps in this workflow are:\nExport of all grinnell:* MODS datastreams using drush islandora_datastream_export. This step, last performed on April 14, 2020, was responsible for creating all of the grinnell_\u0026lt;PID\u0026gt;_MODS.xml exports found in ./\u0026lt;collection-PID\u0026gt;.\nExecute my Map-MODS-to-MASTER Python 3 script on iMac MA8660 to create a mods.tsv file for each collection, along with associated grinnell_\u0026lt;PID\u0026gt;_MODS.log and grinnell_\u0026lt;PID\u0026gt;_MODS.remainder files for each object. The resultant ./\u0026lt;collection-PID\u0026gt;/mods.tsv files are tab-seperated-value (.tsv) files, and they are key to this process.\nEdit the MODS .tsv files. Refer Exporting, Editing, \u0026amp; Replacing MODS Datastreams for details and guidance.\nUse drush islandora_mods_via_twig in each ready-for-update collection to generate new .xml MODS datastream files. For a specified collection, this command will find and read the ./\u0026lt;collection-PID\u0026gt;/mods-imvt.tsv and create one ./\u0026lt;collection-PID\u0026gt;/ready-for-datastream-replace/grinnell_\u0026lt;PID\u0026gt;_MODS.xml file for each object.\nExecute the drush islandora_datastream_replace command once for each collection. This command will process each ./\u0026lt;collection-PID\u0026gt;/ready-for-datastream-replace/grinnell_\u0026lt;PID\u0026gt;_MODS.xml file and replace the corresponding object\u0026rsquo;s MODS datastream with the contents of the .xml file. The digital_grinnell branch version of the islandora_datastream_replace command also performs an implicit update of the object\u0026rsquo;s \u0026ldquo;Title\u0026rdquo;, a transform of the new MODS to DC (Dublin Core), and a re-indexing of the new metadata in Solr.\nStep 6 - Islandora MODS post-processing This is an optional, but recommended, step at the end of the workflow, and it is intended for use by a system admin, presumably the same person who executed steps 4 and 5. The process calls for running a new Drush command inside the Apache container on the Digital.Grinnell host.\nTo process a collection after completion of steps 4 and 5, all that\u0026rsquo;s required is running drush islandora_mods_post_processing. Running that command with the --help option produces:\n[islandora@dgdocker1 ~]$ docker exec -it isle-apache-dg bash root@122092fe8182:/# cd /var/www/html/sites/default/ root@122092fe8182:/var/www/html/sites/default# drush -u 1 islandora_mods_post_processing --help Find a collection\u0026#39;s ./ready-for-datastream-replace/*.used files and comment out found PIDs from all *.tsv files. Examples: drush -u 1 islandora_mods_post_processing social-justice Process ../social-justice/ready-for-database-replace/*.used files. Arguments: collection The name of the collection to be examined for *.used files. Defaults to \u0026#34;social-justice\u0026#34;. Aliases: impp So, my command sequence to run islandora_mods_post_processing for the \u0026ldquo;Social Justice\u0026rdquo; collection, as an example, was:\n[islandora@dgdocker1 ~]$ docker exec -it isle-apache-dg bash root@122092fe8182:/# cd /var/www/html/sites/default/ root@122092fe8182:/var/www/html/sites/default# drush -u 1 islandora_mods_post_processing social-justice Process Details The islandora_mods_post_processing command does the following:\nBuilds a list of all the object PIDs that have successfully passed through steps 1 through 5, by identifying all of the *.used files in the //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/\u0026lt;target-collection\u0026gt; directory. Note that .used files are generated as part of Step 5 in this workflow.\nBuilds a list of all the *.tsv files that exist in all //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/\u0026lt;target-collection\u0026gt; sub-directories.\nFor each object PID identified in step 1 (above), the process searches for a match at the start of each row (the first column) in each *.tsv file identifed in step 2 (above). When a match is found that PID is replaced by a hashtag-prefixed string, a \u0026ldquo;comment\u0026rdquo;. The replacement/comment string includes the hashtag, the object PID with a double colon (::), a timestamp, and the name of the collection that was processed. It will look something like this:\n``` # grinnell::102 - reviewed and modified at Thu, 21 May 20 23:19:59 -0500 as part of social-justice ``` Repeat As Needed Note that this process is designed to be repeated as often as required. Since the process modifies the PIDs that are searched for, there should be no chance of duplication since subsequent executions won\u0026rsquo;t find the same PIDs that were found previously.\nTesting I performed some preliminary tests of islandora_mods_post_processing using the \u0026ldquo;Social Justice\u0026rdquo; collection, and once those all passed I elected to try post-processing the \u0026ldquo;College Buildings\u0026rdquo; collection. Five objects in that collection passed through workflow steps 1 through 5 earlier this month, and I\u0026rsquo;m preparing to process many more \u0026ldquo;College Buildings\u0026rdquo; objects later today.\nPost-Processing the \u0026ldquo;College Buildings\u0026rdquo; Collection The input and output from post-processing the \u0026ldquo;College Buildings\u0026rdquo; collection, run from within the isle-apache-dg container, is captured here.\nroot@122092fe8182:/var/www/html/sites/default# drush -u 1 islandora_mods_post_processing college-buildings Welcome to drush islandora_mods_post_processing! The collections contain 39 .tsv files to check and modify. Now processing the collection in: college-buildings. Now processing PID: \u0026#39;grinnell:1000\u0026#39;, pattern: \u0026#39;/^grinnell:1000 *\t/m\u0026#39; Checking /mnt/metadata-review/A_reingest/AA_RC_mods-imvt-test - Copy.tsv for grinnell:1000... Checking /mnt/metadata-review/alumni-oral-histories/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/ancient-coins/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/archives-suppressed/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/campus-collections/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/college-buildings/mods-imvt-original.tsv for grinnell:1000... Found grinnell:1000 in /mnt/metadata-review/college-buildings/mods-imvt-original.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::1000 - reviewed and modified at Fri, 22 May 20 20:12:01 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods-imvt-original.tsv. Checking /mnt/metadata-review/college-buildings/mods-imvt.tsv for grinnell:1000... Found grinnell:1000 in /mnt/metadata-review/college-buildings/mods-imvt.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::1000 - reviewed and modified at Fri, 22 May 20 20:12:01 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods-imvt.tsv. Checking /mnt/metadata-review/college-buildings/mods.tsv for grinnell:1000... Found grinnell:1000 in /mnt/metadata-review/college-buildings/mods.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::1000 - reviewed and modified at Fri, 22 May 20 20:12:01 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods.tsv. Checking /mnt/metadata-review/college-handbooks/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/college-history/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/college-life/mods.tsv for grinnell:1000... Found grinnell:1000 in /mnt/metadata-review/college-life/mods.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::1000 - reviewed and modified at Fri, 22 May 20 20:12:01 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-life/mods.tsv. Checking /mnt/metadata-review/curricular-materials/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/digital-stories-social-justice/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/faculty-scholarship/mods-imvt.tsv for grinnell:1000... Checking /mnt/metadata-review/faculty-scholarship/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/faulconer/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/geology-suppressed/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/geology/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/grinnell-in-china/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/gwcc/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/jimmy-ley/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/kleinschmidt/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/pending-review/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/phpp-community/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/phpp-dcl/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/phpp-ghm/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/phpp-oral-history/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/phpp/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/physics-museum/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/postcards/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/recent-art-acquisitions/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/scholarship/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/social-gospel/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/social-justice/mods-generated-2020-Apr-18.tsv for grinnell:1000... Checking /mnt/metadata-review/social-justice/mods-imvt-test-original.tsv for grinnell:1000... Checking /mnt/metadata-review/social-justice/mods-imvt.tsv for grinnell:1000... Checking /mnt/metadata-review/soviet-graphic-art/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/special-collections/mods.tsv for grinnell:1000... Checking /mnt/metadata-review/student-scholarship/mods.tsv for grinnell:1000... Now processing PID: \u0026#39;grinnell:1030\u0026#39;, pattern: \u0026#39;/^grinnell:1030 *\t/m\u0026#39; Checking /mnt/metadata-review/A_reingest/AA_RC_mods-imvt-test - Copy.tsv for grinnell:1030... Checking /mnt/metadata-review/alumni-oral-histories/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/ancient-coins/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/archives-suppressed/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/campus-collections/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/college-buildings/mods-imvt-original.tsv for grinnell:1030... Found grinnell:1030 in /mnt/metadata-review/college-buildings/mods-imvt-original.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::1030 - reviewed and modified at Fri, 22 May 20 20:12:03 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods-imvt-original.tsv. Checking /mnt/metadata-review/college-buildings/mods-imvt.tsv for grinnell:1030... Found grinnell:1030 in /mnt/metadata-review/college-buildings/mods-imvt.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::1030 - reviewed and modified at Fri, 22 May 20 20:12:03 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods-imvt.tsv. Checking /mnt/metadata-review/college-buildings/mods.tsv for grinnell:1030... Found grinnell:1030 in /mnt/metadata-review/college-buildings/mods.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::1030 - reviewed and modified at Fri, 22 May 20 20:12:03 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods.tsv. Checking /mnt/metadata-review/college-handbooks/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/college-history/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/college-life/mods.tsv for grinnell:1030... Found grinnell:1030 in /mnt/metadata-review/college-life/mods.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::1030 - reviewed and modified at Fri, 22 May 20 20:12:03 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-life/mods.tsv. Checking /mnt/metadata-review/curricular-materials/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/digital-stories-social-justice/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/faculty-scholarship/mods-imvt.tsv for grinnell:1030... Checking /mnt/metadata-review/faculty-scholarship/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/faulconer/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/geology-suppressed/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/geology/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/grinnell-in-china/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/gwcc/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/jimmy-ley/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/kleinschmidt/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/pending-review/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/phpp-community/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/phpp-dcl/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/phpp-ghm/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/phpp-oral-history/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/phpp/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/physics-museum/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/postcards/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/recent-art-acquisitions/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/scholarship/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/social-gospel/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/social-justice/mods-generated-2020-Apr-18.tsv for grinnell:1030... Checking /mnt/metadata-review/social-justice/mods-imvt-test-original.tsv for grinnell:1030... Checking /mnt/metadata-review/social-justice/mods-imvt.tsv for grinnell:1030... Checking /mnt/metadata-review/soviet-graphic-art/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/special-collections/mods.tsv for grinnell:1030... Checking /mnt/metadata-review/student-scholarship/mods.tsv for grinnell:1030... Now processing PID: \u0026#39;grinnell:10399\u0026#39;, pattern: \u0026#39;/^grinnell:10399 *\t/m\u0026#39; Checking /mnt/metadata-review/A_reingest/AA_RC_mods-imvt-test - Copy.tsv for grinnell:10399... Checking /mnt/metadata-review/alumni-oral-histories/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/ancient-coins/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/archives-suppressed/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/campus-collections/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/college-buildings/mods-imvt-original.tsv for grinnell:10399... Found grinnell:10399 in /mnt/metadata-review/college-buildings/mods-imvt-original.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::10399 - reviewed and modified at Fri, 22 May 20 20:12:04 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods-imvt-original.tsv. Checking /mnt/metadata-review/college-buildings/mods-imvt.tsv for grinnell:10399... Found grinnell:10399 in /mnt/metadata-review/college-buildings/mods-imvt.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::10399 - reviewed and modified at Fri, 22 May 20 20:12:04 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods-imvt.tsv. Checking /mnt/metadata-review/college-buildings/mods.tsv for grinnell:10399... Found grinnell:10399 in /mnt/metadata-review/college-buildings/mods.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::10399 - reviewed and modified at Fri, 22 May 20 20:12:04 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods.tsv. Checking /mnt/metadata-review/college-handbooks/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/college-history/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/college-life/mods.tsv for grinnell:10399... Found grinnell:10399 in /mnt/metadata-review/college-life/mods.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::10399 - reviewed and modified at Fri, 22 May 20 20:12:04 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-life/mods.tsv. Checking /mnt/metadata-review/curricular-materials/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/digital-stories-social-justice/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/faculty-scholarship/mods-imvt.tsv for grinnell:10399... Checking /mnt/metadata-review/faculty-scholarship/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/faulconer/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/geology-suppressed/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/geology/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/grinnell-in-china/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/gwcc/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/jimmy-ley/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/kleinschmidt/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/pending-review/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/phpp-community/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/phpp-dcl/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/phpp-ghm/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/phpp-oral-history/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/phpp/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/physics-museum/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/postcards/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/recent-art-acquisitions/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/scholarship/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/social-gospel/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/social-justice/mods-generated-2020-Apr-18.tsv for grinnell:10399... Checking /mnt/metadata-review/social-justice/mods-imvt-test-original.tsv for grinnell:10399... Checking /mnt/metadata-review/social-justice/mods-imvt.tsv for grinnell:10399... Checking /mnt/metadata-review/soviet-graphic-art/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/special-collections/mods.tsv for grinnell:10399... Checking /mnt/metadata-review/student-scholarship/mods.tsv for grinnell:10399... Now processing PID: \u0026#39;grinnell:10400\u0026#39;, pattern: \u0026#39;/^grinnell:10400 *\t/m\u0026#39; Checking /mnt/metadata-review/A_reingest/AA_RC_mods-imvt-test - Copy.tsv for grinnell:10400... Checking /mnt/metadata-review/alumni-oral-histories/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/ancient-coins/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/archives-suppressed/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/campus-collections/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/college-buildings/mods-imvt-original.tsv for grinnell:10400... Found grinnell:10400 in /mnt/metadata-review/college-buildings/mods-imvt-original.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::10400 - reviewed and modified at Fri, 22 May 20 20:12:06 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods-imvt-original.tsv. Checking /mnt/metadata-review/college-buildings/mods-imvt.tsv for grinnell:10400... Found grinnell:10400 in /mnt/metadata-review/college-buildings/mods-imvt.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::10400 - reviewed and modified at Fri, 22 May 20 20:12:06 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods-imvt.tsv. Checking /mnt/metadata-review/college-buildings/mods.tsv for grinnell:10400... Found grinnell:10400 in /mnt/metadata-review/college-buildings/mods.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::10400 - reviewed and modified at Fri, 22 May 20 20:12:06 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods.tsv. Checking /mnt/metadata-review/college-handbooks/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/college-history/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/college-life/mods.tsv for grinnell:10400... Found grinnell:10400 in /mnt/metadata-review/college-life/mods.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::10400 - reviewed and modified at Fri, 22 May 20 20:12:06 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-life/mods.tsv. Checking /mnt/metadata-review/curricular-materials/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/digital-stories-social-justice/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/faculty-scholarship/mods-imvt.tsv for grinnell:10400... Checking /mnt/metadata-review/faculty-scholarship/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/faulconer/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/geology-suppressed/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/geology/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/grinnell-in-china/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/gwcc/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/jimmy-ley/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/kleinschmidt/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/pending-review/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/phpp-community/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/phpp-dcl/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/phpp-ghm/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/phpp-oral-history/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/phpp/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/physics-museum/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/postcards/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/recent-art-acquisitions/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/scholarship/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/social-gospel/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/social-justice/mods-generated-2020-Apr-18.tsv for grinnell:10400... Checking /mnt/metadata-review/social-justice/mods-imvt-test-original.tsv for grinnell:10400... Checking /mnt/metadata-review/social-justice/mods-imvt.tsv for grinnell:10400... Checking /mnt/metadata-review/soviet-graphic-art/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/special-collections/mods.tsv for grinnell:10400... Checking /mnt/metadata-review/student-scholarship/mods.tsv for grinnell:10400... Now processing PID: \u0026#39;grinnell:10401\u0026#39;, pattern: \u0026#39;/^grinnell:10401 *\t/m\u0026#39; Checking /mnt/metadata-review/A_reingest/AA_RC_mods-imvt-test - Copy.tsv for grinnell:10401... Checking /mnt/metadata-review/alumni-oral-histories/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/ancient-coins/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/archives-suppressed/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/campus-collections/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/college-buildings/mods-imvt-original.tsv for grinnell:10401... Found grinnell:10401 in /mnt/metadata-review/college-buildings/mods-imvt-original.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::10401 - reviewed and modified at Fri, 22 May 20 20:12:07 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods-imvt-original.tsv. Checking /mnt/metadata-review/college-buildings/mods-imvt.tsv for grinnell:10401... Found grinnell:10401 in /mnt/metadata-review/college-buildings/mods-imvt.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::10401 - reviewed and modified at Fri, 22 May 20 20:12:07 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods-imvt.tsv. Checking /mnt/metadata-review/college-buildings/mods.tsv for grinnell:10401... Found grinnell:10401 in /mnt/metadata-review/college-buildings/mods.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::10401 - reviewed and modified at Fri, 22 May 20 20:12:07 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-buildings/mods.tsv. Checking /mnt/metadata-review/college-handbooks/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/college-history/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/college-life/mods.tsv for grinnell:10401... Found grinnell:10401 in /mnt/metadata-review/college-life/mods.tsv. Executed 1 replacement(s). New fields say: \u0026#39;# grinnell::10401 - reviewed and modified at Fri, 22 May 20 20:12:07 -0500 as part of college-buildings \u0026#39; Replacement saved to /mnt/metadata-review/college-life/mods.tsv. Checking /mnt/metadata-review/curricular-materials/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/digital-stories-social-justice/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/faculty-scholarship/mods-imvt.tsv for grinnell:10401... Checking /mnt/metadata-review/faculty-scholarship/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/faulconer/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/geology-suppressed/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/geology/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/grinnell-in-china/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/gwcc/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/jimmy-ley/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/kleinschmidt/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/pending-review/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/phpp-community/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/phpp-dcl/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/phpp-ghm/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/phpp-oral-history/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/phpp/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/physics-museum/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/postcards/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/recent-art-acquisitions/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/scholarship/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/social-gospel/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/social-justice/mods-generated-2020-Apr-18.tsv for grinnell:10401... Checking /mnt/metadata-review/social-justice/mods-imvt-test-original.tsv for grinnell:10401... Checking /mnt/metadata-review/social-justice/mods-imvt.tsv for grinnell:10401... Checking /mnt/metadata-review/soviet-graphic-art/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/special-collections/mods.tsv for grinnell:10401... Checking /mnt/metadata-review/student-scholarship/mods.tsv for grinnell:10401... drush islandora_mods_post_processing is finished! A quick check of /mnt/metadata-review/college-life/mods.tsv, one of the changed .tsv files, shows that all is well. A repeat of the same command produced no additional changes, as expected.\nAnd that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/075-islandora-mods-post-processing/","tags":["islandora_mods_via_twig","islandora_mods_post_processing"],"title":"Islandora MODS Post-Processing"},{"categories":null,"contents":"A few months ago I migrated a site, The Compass Rose Band, from Drupal to Hugo for my Uncle. Since then I have been maintaining the site, adding and removing dates, and updating the site about every other week. With the passage of time events on the site \u0026ldquo;automatically\u0026rdquo; move from \u0026ldquo;upcoming\u0026rdquo; to \u0026ldquo;past\u0026rdquo;, but since it is a \u0026ldquo;static\u0026rdquo; site, that only happens when I recompile and rebuild the site. So I needed to automate builds.\nI tried a few different ideas I\u0026rsquo;d heard about, including crontab, but on my Mac desktop I ended up settling on an Automator workflow tied to a Calendar event.\nCreate a Calendar Event in Automator First, I launched Automator, selected New Document, then Calendar Alarm. Next, I selected the Calendar path under Library, then I dragged New Calendar Event into the workspace on the right half of the window. The Automator startup screen and my new Calendar Alarm config looked like this:\nFigure 1 \u0026middot; Automator startup Figure 2 \u0026middot; Calendar alarm config Take note that in the image above I named my event \u0026ldquo;CRB-Update\u0026rdquo;, added it to my existing Automator calendar, and set it to happen one time with a duration of zero minutes. It turns out that this settign didn\u0026rsquo;t matter as you\u0026rsquo;ll see later on. Later, I also learned that it\u0026rsquo;s easy to change the frequency and time of the event once it is confirmed to be working properly.\nNext, under Library I selected Utilities, then I selected Run Shell Script and dragged that into the workspace on the right side of the window, dropping it below my \u0026ldquo;CRB-Update\u0026rdquo; event. The new config looked like this with /bin/bash selected in the Shell dropdown:\nFigure 3 \u0026middot; Run shell script added The commands that I need to run in this bash shell are described in the table below.\nCommand Purpose cd ~/GitHub/compass-rose-band Change the working directory to the local home for my CRB site/project. git stash Make sure any uncommitted changes I\u0026rsquo;ve made in this project are stashed away. git pull Make a \u0026ldquo;clean\u0026rdquo; pull of the project from its origin/master branch in GitHub. ./push-update.sh Execute the push-update.sh script to compile and push the project to production. When written as a one-liner this becomes:\ncd ~/GitHub/compass-rose-band; git stash; git pull; ./push-update.sh Figure 4 \u0026middot; Shell with commands push-update.sh Changes Once I got the Run Shell Script option configured I thought things would work nicely, but the original version of ./push-update.sh used a series of docker login commands without any Docker credentials; it required user input to run properly. I couldn\u0026rsquo;t just add the docker login info to the file and expose the login credentials, so, after some Googling I decided to create a .txt file in my home directory containing only the Docker password, and then passed that in to the docker login command using cat as documented in Provide a password using STDIN. Note that in the aforementioned .txt file the login password appears on a single line with NO newline at the end!\nI also updated push-update.sh so it contained only one login instead of the three docker login commands that were there previously. That script now looks begins like this:\n#!/bin/bash # current=`git symbolic-ref --short -q HEAD` # git checkout ${current} ## Login to docker, one time only! cat ~/summittdweller-docker-login.txt | /usr/local/bin/docker login -u summittdweller --password-stdin # Compile the site before copying to the new image. Round 1 = compassroseband.net /usr/local/bin/hugo --ignoreCache --ignoreVendor --minify --debug --verbose --baseURL=https://compassroseband.net echo \u0026#34;Hugo\u0026#39;s compassroseband.net compilation is complete.\u0026#34; echo \u0026#34;Starting docker image build...\u0026#34; /usr/local/bin/docker image build -f push-update-Dockerfile --no-cache -t compassrose . echo \u0026#34;docker image build is complete.\u0026#34; /usr/local/bin/docker tag compassrose summittdweller/compassrose1:latest /usr/local/bin/docker push summittdweller/compassrose1:latest ... One Final Glitch Unfortunately, this also failed because Docker was still configured to use a \u0026ldquo;credstore\u0026rdquo; that is not what I wanted. A little more searching the web lead me to this Github Issue post and it was just what I needed. After making the specified change my Automator seems to works nicely.\nSaving the Full Configuration Just one thing left to do in order to wrap things up\u0026hellip; save my work. To do this I clicked File and Save in the Automator main menu, and saved the configuration with a name of \u0026ldquo;CRB-Update.app\u0026rdquo; with a type of Calendar Alarm. The complete configuration looked like this:\nFigure 5 \u0026middot; Complete CRB-Update.app Configuration Saving the configruation as \u0026ldquo;CRB-Update.app\u0026rdquo; automatically creates a new immediate Calendar event, and this is a nice feature because it effectively runs a live test of the config. In my case it created an event like so:\nFigure 6 \u0026middot; New Calendar Event Thankfully, the \u0026ldquo;test\u0026rdquo; that this \u0026ldquo;immedaite\u0026rdquo; event initiated worked perfectly! So, the only thing left was to duplicate the \u0026ldquo;CRB-Update\u0026rdquo; event and give it a reasonable start time and recurrence. My end result looks like this in Calendar:\nFigure 7 \u0026middot; Final Calendar Events It Works! Throughout the process I used the Run option (in the top right of the Automator window) to test out the workflow. Now that it works I can relax and watch it do its thing! \u0026#x1f605;\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip; Thank you Mackenzie!\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/086-scheduled-updates-for-a-hugo-static-site/","tags":["docker","Automator","Hugo","calendar"],"title":"Scheduled Updates for a Hugo Static Site"},{"categories":null,"contents":"Once upon a time this blog had an address of https://static.grinnell.edu/blogs/McFateM and it used a theme called M10c. Well, what was once is once again. If you read my New Blog Style and Features post you can learn a little more of this blog\u0026rsquo;s \u0026ldquo;interim\u0026rdquo; history.\nBy the way, if you\u0026rsquo;re reading this post at https://dlad.summittdweller.com, then you\u0026rsquo;re missing out. If you are looking at that old blog you probably see something more like this:\nPlease jump to https://static.grinnell.edu/blogs/McFateM and catch up as quick as you can! I think you\u0026rsquo;ll be happier in the end, I know I will.\nWhy Move Back? Zzo was (still is) a great theme, and over time you should see some elements of it creeping back into this blog. But Zzo switched (in December 2019) from using Lunr.js for search, to using Fuse.js. Fuse.js is also great, but it\u0026rsquo;s \u0026ldquo;fuzzy\u0026rdquo;, and I found that what I really needed is a more robust and flexible \u0026ldquo;exact\u0026rdquo; search capability.\nYou see, I knew there were several instances of phrases like docker exec -it isle-apache-dg bash in my blog posts from the past, but I didn\u0026rsquo;t know which posts to look in, and Fuse.js was no help. So I switched back to my \u0026ldquo;old\u0026rdquo; blog style, the one you\u0026rsquo;re reading now (I hope), and found that good ol\u0026rsquo; reliable M10c with Lunr.js works nicely. It\u0026rsquo;s not as flashy as Zzo or Fuse.js, but it works!\nWhat\u0026rsquo;s to Come? Like I said above, over time you should see elements of Zzo and my \u0026ldquo;interim\u0026rdquo; blog, https://dlad.summittdweller.com, creeping back into this blog. As that happens I will do my best to document those changes here, or at least compile an index of new blog posts here, to complete this tale.\nAnd it\u0026rsquo;s time to do some real work\u0026hellip; I\u0026rsquo;ll be back to share more here soon. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/083-back-in-scarlet-and-black/","tags":["Zzo","M10c","Lunr.js","Fuse.js"],"title":"Back in (Scarlet and) Black"},{"categories":null,"contents":"So, you want to add a new collection or sub-collection (they are really the same thing) to Digital.Grinnell, eh? It\u0026rsquo;s easy, but there are some things to consider before I show you how.\nCollection Guidelines There are no \u0026ldquo;formal\u0026rdquo; guidelines for the creation of a collection or sub-collection in Digital.Grinnell, but I can think of a couple \u0026ldquo;informal\u0026rdquo; things to consider.\nAre there or will there ever be enough objects, old and/or new, with something in common to warrant a new collection? How many is \u0026ldquo;enough\u0026rdquo;? I dunno, but anything fewer than a dozen individual objects is borderline at best, at least in my book.\nA corollary to this \u0026ldquo;rule\u0026rdquo;: If a collection has sub-collections, there should be at least two of them, and three is a better minimum. Why? Because we don\u0026rsquo;t want to create a vertical \u0026ldquo;tower\u0026rdquo; of collections stacked one atop another. For more on this see the next rule, below. Is the new collection ever likely to have its own \u0026ldquo;sub-collections\u0026rdquo;?\nAs a rule, we don\u0026rsquo;t like having \u0026ldquo;hybrid\u0026rdquo; or \u0026ldquo;mixed\u0026rdquo; collections because they complicate presentation and sorting. So the rule-of-thumb in Digital.Grinnell is that a collection can have EITHER many individual objects OR three or more sub-collections, but NEVER a mix of individual objects and sub-collections. As of this date, 25-June-2020, our Grinnell College Museum of Art collection is in violation of this \u0026ldquo;rule\u0026rdquo; because it contains numerous individual objects AND two sub-collections. Have a look at the collection using the link provided and you\u0026rsquo;ll probably see why this is not a good idea. Every collection needs a \u0026ldquo;parent\u0026rdquo; unless it is going to be a \u0026ldquo;top-level\u0026rdquo; collection, and those require SPECIAL consideration.\nBottom line, before you set about to create a new collection, presumably a sub-collection, we have to identify the existing \u0026ldquo;parent\u0026rdquo; collection that it will reside in. What Do I Need Before I Create a New Collection? Glad you asked. For starters, you\u0026rsquo;ll need to have considered the questions posed above. Having answered those questions, if you elect to proceed, your collection will need the following:\nA \u0026ldquo;parent\u0026rdquo; or \u0026ldquo;home\u0026rdquo; collection to reside in. We need to know the PID of that \u0026ldquo;parent\u0026rdquo; collection. See the last bullet point above. A \u0026ldquo;title\u0026rdquo;, something short and to-the-point, usually expressed in title case. A non-numeric PID (Persistent IDentifier). PIDs in Digital.Grinnell have the form: namespace:identifier. Numeric identifiers are reserved for individual objects, but collections have non-numeric identifiers like: grinnell:faculty-scholarship or grinnell:jimmy-ley. The namespace is almost always \u0026ldquo;grinnell\u0026rdquo;, and the identifier should be short but descriptive, with no spaces, please. An \u0026ldquo;abstract\u0026rdquo; or descriptive short paragraph describing the collection. Have a look at existing collections for some good examples. A thumbnail image. This should be a .jpg or .png image to represent the collection. Any size is acceptable, but something on the order of 200-400 pixels is probably best. Larger images will be automatically reduced but they tend to waste space and bandwidth. The image shown below is a fine example. Ok, How Do I Add My New Collection? That\u0026rsquo;s really easy\u0026hellip; send an email to digital@grinnell.edu and ask, politely, please. Honestly, that\u0026rsquo;s what you need to do.\nSomeday maybe digital@grinnell.edu will take the time to document that part of the process too. \u0026#x1f609;\nAdmin Reminders So, there are a couple of things that the Digital.Grinnell admins need to remember when adding a collection to the mix:\nEdit the following source code files in the dg7 custom module and add the new collection PID as needed to support custom sorting and display. dg7.module dg7.views.inc Note: Editing this file is probably NOT necessary. Perform the steps documented in Updating DG\u0026rsquo;s Collection Views to update DG\u0026rsquo;s \u0026ldquo;collection view\u0026rdquo; and make the new collection display properly. Visit https://digital.grinnell.edu/admin/islandora/tools/collection_search and determine if the new collection should be added as a \u0026ldquo;Collection Search\u0026rdquo; target! And it\u0026rsquo;s time to do some real work\u0026hellip; I\u0026rsquo;ll be back to share more here, someday. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/084-how-to-add-a-collection-to-digital-grinnell/","tags":["collection","sub-collection","PID","thumbnail","title","abstract","Digital.Grinnell","search"],"title":"How to Add a Collection to Digital.Grinnell"},{"categories":null,"contents":"The term WARC, an abbreviation of Web ARChive, always reminds me of things like hobbits, elves, dark lords, and orcs, of course. But this post has nothing to do with those things so I need to clear my head and press on.\nA WARC is essentially a file format used to capture the content and organization of a web site. Recently, I was asked to add a pair of WARCs to Digital.Grinnell. Doing so proved to be quite an adventure, but I am pleased to report that we now have these two objects to show for it:\nThe Global Mongol Century, and World Music Instruments. WARC Ingest - Failures and Success What follows is a table of the steps, both failed and successful, taken to ingest the two new Digital.Grinnell objects, along with notes about each step in the process.\nIngest Step Outcome Notes 1. .warc File Creation Success Rebecca Ciota used a wget command to create a .warc archive file and a .cdx \u0026ldquo;index\u0026rdquo; of each site from existing, \u0026ldquo;live\u0026rdquo; web content. That command took this form: wget --warc-file=\u0026lt;FILENAME\u0026gt; --recursive --level=5 --warc-cdx --page-requisites --html-extension --convert-links --execute robots=off --directory-prefix=. -x /solr-search --wait=10 --random-wait \u0026lt;WEBSITE-URL\u0026gt; 2. .gz Compression Success Rebecca compressed each .warc and .cdx file pair into a compressed .gz archive to package the contents for subsequent processing. 3. MODS Metadata Prep Success Rebecca added two rows of control data and MODS metadata to Google Sheet https://docs.google.com/spreadsheets/d/1X3rs7UhIdS6SumTwFUvRR0F6-OnGEIF5xnGzcLrFqNY to prep for IMI ingest. 4. .gz Files Added to //Storage Success The two .gz files generated in a previous step were copied to //Storage for ingest. 5. Attempted IMI Ingest Failed The aforementioned IMI ingest process was invoked with Google Sheet https://docs.google.com/spreadsheets/d/1X3rs7UhIdS6SumTwFUvRR0F6-OnGEIF5xnGzcLrFqNY in Digital.Grinnell. The process ran for a very long time, in excess of 30 minutes, before it failed with no error messages or indication of root cause. 6. Attempted \u0026ldquo;Forms\u0026rdquo; Ingest of WMI Object Failed I navigated to the management page in our Pending Review collection and engaged the \u0026ldquo;Add an object to this Collection\u0026rdquo; link. I selected the \u0026ldquo;Islandora Web ARChive Content Model\u0026rdquo; and \u0026ldquo;Web ARChive MODS Form\u0026rdquo; for ingest. The form was not available so the ingest failed. 7. WARC Module Updated Success Steps were taken to update the key module, islandora_solution_pack_web_archive on Digital Grinnell\u0026rsquo;s node DGDocker1. The update ran without error. 8. Repeated Step 6 Failed Still in the Pending Review Manage page I repeated the previous step (6) but even after the update no \u0026ldquo;Web ARChive MODS Form\u0026rdquo; was available. 9. Repeated Step 8 with the Correct Form Failed Still in the Pending Review Manage page I repeated the previous steps (6 and 8) choosing the \u0026ldquo;DG ONE Form to Rule them All\u0026rdquo;. The form opened and accepted input, but clicking \u0026ldquo;Next\u0026rdquo; presented the sub-form shown below, indicating that a .gz file could not be ingested. 10. Unzipped the .gz Files Success The WMI\u0026rsquo;s .gz file was nearly 2 GB in size, so I was unable to unzip it using the archive tool on my iMac. I was able to use gunzip to process the files on CentOS node DGDocker1. The result was a .warc file and .cdx file pair for each object (a total of 4 files). 11. Repeated Step 9 Failed Still in the Pending Review Manage page I repeated the previous step (9) and the sub-form. I terminated the upload process after about 2 hours (included my lunch break). The process could not be resumed from that point. 12. Repeated Step 11 with the \u0026ldquo;Global Mongol Century\u0026rdquo; File Limited Success Still in the Pending Review Manage page I repeated the previous step (11) but with the .warc file representing \u0026ldquo;The Global Mongol Century\u0026rdquo; archive. This .warc file was only 24.2 MB in size and the ingest worked (taking less than 5 minutes) producing grinnell:27856. No thumbnail image or other image derivatives were created, presumably because I did not choose the Upload a screenshot? option. 13. Investigated Using drush Failed The updated islandora_solution_pack_web_archive module includes at least one drush command, and a non-web ingest of such large files would be preferred. However, investigation determined that the provided drush command does not provide an alternate means of ingest. 14. Repeat Step 12 Success I repeated step 12 still using the relatively small \u0026ldquo;Global Mongol Century\u0026rdquo; .warc, but gave the object a title of \u0026ldquo;World Music Instruments\u0026rdquo;. grinnell:27858 was created, again with no thumbnail image or other image derivatives. 15. Attempted to Replace the OBJ Datastream Failed Navigating to https://digital.grinnell.edu/islandora/object/grinnell%3A27858/manage/datastreams, I selected the replace link associated with the OBJ datastream in an attempt to replace the small .warc object with the proper WMI .warc. This process failed to finish after nearly an hour of processing, presumably because the WMI .warc is simply too large causing the web process to time-out before completion. 16. Replaced the OBJ Datastream Using FEDORA Success I navigated to Digital.Grinnell\u0026rsquo;s FEDORA admin page at http://dgdocker1.grinnell.edu:8081/fedora/admin/, logged in as an admin, opened the grinnell:27858 object, and used the FEDORA admin interface there to replace its OBJ datastream. I allowed the upload portion of the process to \u0026ldquo;spin\u0026rdquo; for more than an hour, but when I stopped it and saved changes, I found a new OBJ that is 1.97 GB in size. That new OBJ appears to be viable. 17. Generated SCREENSHOT Images Success Rebecca used the \u0026ldquo;Snip Tool\u0026rdquo; in Windows to collect screenshot images of each \u0026ldquo;live\u0026rdquo; website home page. These were uploaded to //Storage for subsequent ingest. 18. Added Images via Manage/Datastreams Menu Success I navigated to each WARC object\u0026rsquo;s manage/datastreams page and used the Add a datastream links to create new SCREENSHOT datastreams using the home page .jpg images. 19. Added Empty MODS Datastreams Success While still working in each object\u0026rsquo;s manage/datastreams page I used the Add a datastream links again to create new, empty MODS datastreams. This step was necessary because Web ARChive content models do not normally include any MODS record by default. 20. Exported Google Sheet to TSV Success To prep for updating each object\u0026rsquo;s MODS record, I exported the \u0026ldquo;MASTER\u0026rdquo; tab of our Google Sheet to a .tsv, tab-seperated-values, file and saved the export in //Storage/Library/AllStaff/DG-Metadata-Review-2020-r1/WARC/mods-imvt.tsv. 21. Invoked Islandora-MODS-via-Twig Workflow Success I engaged the drush imvt, islandora_mods_via_twig, command and subsequent workflow, including islandora_datastream_replace, to replace the empty MODS records (see step 19) with correct data. See Exporting, Editing, \u0026amp; Replacing MODS Datastreams: Technical Details for complete details. 22. Initiate Derivative Regeneration Success Since the two WARC objects were not ingested in a \u0026ldquo;traditional\u0026rdquo; manner, it was necessary to regenerate all derivative datastreams to complete the object. I did so, in the case of grinnell:27858, for example, by visiting the object\u0026rsquo;s manage/properties page and clicking Regenerate all derivatives. Summary Completing steps 1-4, 7, 10, 12, 14, and 16-22, resulted in the two \u0026ldquo;complete\u0026rdquo; WARC objects we now have in:\nThe Global Mongol Century, and World Music Instruments. Move to Faculty Scholarship All of the above steps were performed while the two objects were part of the Pending-Review collection in Digital.Grinnell. Once the objects were reviewed and determined to be correct, steps were taken using each object\u0026rsquo;s manage page to Migrate this Object to another collection, choosing to move them both to Faculty Scholarship.\nSetting Object Permissions Since both of the WARC objects are for archival only, it was determined that both objects should be accessible only to system administrators. To enforce that restriction I visited each object\u0026rsquo;s manage/xacml page to set appropriate restrictions on object management and object viewing.\nAdding PDF Datastreams The islandora_solution_pack_web_archive module and the Web ARChive content model provide an option to include a PDF in the ingest process. We did not initially generate any PDFs for the two WARCs that were ingested, but we have since taken steps to add PDF datastreams in order to experience what that option has to offer. The process we employed is briefly documented below.\nPDF Creation As mentioned earlier, a wget command was used to create the .warc files that we ingested, but wget does not appear to offer a viable option to create a PDF file. Fortunately, Rebecca\u0026rsquo;s research turned up this Adobe Acrobat Pro trick: https://lenashore.com/2019/06/how-to-make-a-pdf-of-an-entire-website/.\nRebecca reports that this process works, but can take a very long time. She apparently had to specify a limited number of \u0026ldquo;levels\u0026rdquo; to enable creation of a reasonable PDF in the case of the WMI web site.\nPDF Datastream Addition Each object\u0026rsquo;s PDF file was uploaded and ingested to join its corresponding object using steps similar to 18 and 22 above. Each object\u0026rsquo;s manage/datastreams page was engaged and the Add a datastream link used to create new PDF datastreams using the .pdf files created earlier.\nAgain, since the two WARC objects were not ingested in a \u0026ldquo;traditional\u0026rdquo; manner, I thought it necessary to regenerate all derivative datastreams to complete each object. I did so, in the case of grinnell:27858, for example, by visiting the object\u0026rsquo;s manage/properties page and clicking Regenerate all derivatives. The addition of the PDF datastream did not appear to create any additional derivatives, but each object came away with an empty WARC_CSV and WARC_FILTERED datastreams that I manually removed.\nThe addition of the PDF datastreams did produce new PDF-download links like the one shown here:\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/082-here-there-be-warcs/","tags":["WARC","wget","web archive","islandora_solution_pack_web_archive","islandora_datastream_replace","PDF"],"title":"Here There Be WARCs"},{"categories":null,"contents":"Today I elected to run a vulnerability scan against our new instance of the docker-traefik2-acme-host configuration that\u0026rsquo;s running on static.grinnell.edu. The command I used, and the results, are posted below.\n╭─islandora@dgdockerx ~ ╰─$ /home/islandora/testssl.sh/testssl.sh static.grinnell.edu ########################################################### testssl.sh 3.0rc4 from https://testssl.sh/dev/ This program is free software. Distribution and modification under GPLv2 permitted. USAGE w/o ANY WARRANTY. USE IT AT YOUR OWN RISK! Please file bugs @ https://testssl.sh/bugs/ ########################################################### Using \u0026#34;OpenSSL 1.0.2-chacha (1.0.2k-dev)\u0026#34; [~183 ciphers] on dgdockerx:/home/islandora/testssl.sh/bin/openssl.Linux.x86_64 (built: \u0026#34;Jan 18 17:12:17 2019\u0026#34;, platform: \u0026#34;linux-x86_64\u0026#34;) Start 2020-06-11 17:33:25 --\u0026gt;\u0026gt; 132.161.151.30:443 (static.grinnell.edu) \u0026lt;\u0026lt;-- rDNS (132.161.151.30): vaf.grinnell.edu. rootstalk.grinnell.edu. static.grinnell.edu. rootstalk-static.grinnell.edu. Service detected: HTTP Testing protocols via sockets except NPN+ALPN SSLv2 not offered (OK) SSLv3 not offered (OK) TLS 1 offered TLS 1.1 offered TLS 1.2 offered (OK) TLS 1.3 offered (OK): final NPN/SPDY not offered ALPN/HTTP2 h2, http/1.1 (offered) Testing cipher categories NULL ciphers (no encryption) not offered (OK) Anonymous NULL Ciphers (no authentication) not offered (OK) Export ciphers (w/o ADH+NULL) not offered (OK) LOW: 64 Bit + DES, RC[2,4] (w/o export) not offered (OK) Triple DES Ciphers / IDEA offered (NOT ok) Average: SEED + 128+256 Bit CBC ciphers offered Strong encryption (AEAD ciphers) offered (OK) Testing robust (perfect) forward secrecy, (P)FS -- omitting Null Authentication/Encryption, 3DES, RC4 PFS is offered (OK) TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA ECDHE-RSA-CHACHA20-POLY1305 TLS_AES_128_GCM_SHA256 ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA Elliptic curves offered: prime256v1 secp384r1 secp521r1 X25519 Testing server preferences Has server cipher order? yes (OK) -- only for \u0026lt; TLS 1.3 Negotiated protocol TLSv1.3 Negotiated cipher TLS_AES_256_GCM_SHA384, 253 bit ECDH (X25519) Cipher order TLSv1: ECDHE-RSA-AES256-SHA AES256-SHA ECDHE-RSA-AES128-SHA AES128-SHA ECDHE-RSA-DES-CBC3-SHA DES-CBC3-SHA TLSv1.1: ECDHE-RSA-AES256-SHA AES256-SHA ECDHE-RSA-AES128-SHA AES128-SHA ECDHE-RSA-DES-CBC3-SHA DES-CBC3-SHA TLSv1.2: ECDHE-RSA-AES256-GCM-SHA384 ECDHE-RSA-AES256-SHA ECDHE-RSA-CHACHA20-POLY1305 AES256-GCM-SHA384 AES256-SHA ECDHE-RSA-AES128-GCM-SHA256 ECDHE-RSA-AES128-SHA AES128-GCM-SHA256 AES128-SHA ECDHE-RSA-DES-CBC3-SHA DES-CBC3-SHA TLSv1.3: TLS_AES_256_GCM_SHA384 TLS_CHACHA20_POLY1305_SHA256 TLS_AES_128_GCM_SHA256 Testing server defaults (Server Hello) TLS extensions (standard) \u0026#34;session ticket/#35\u0026#34; \u0026#34;renegotiation info/#65281\u0026#34; \u0026#34;EC point formats/#11\u0026#34; \u0026#34;supported versions/#43\u0026#34; \u0026#34;key share/#51\u0026#34; \u0026#34;application layer protocol negotiation/#16\u0026#34; Session Ticket RFC 5077 hint no -- no lifetime advertised SSL Session ID support yes Session Resumption Tickets no, ID: no TLS clock skew Random values, no fingerprinting possible Signature Algorithm SHA256 with RSA Server key size RSA 2048 bits Server key usage Digital Signature, Key Encipherment Server extended key usage TLS Web Server Authentication, TLS Web Client Authentication Serial / Fingerprints 035B28222F66FCE82AB86834D316C249EDB3 / SHA1 385FFF215983EFA46B2CAFCA6768356348475307 SHA256 F2A91B6B1BE83479550EE679AEA78B9551CA00D1D5BC0DA4B80D1FA1B4E3ED25 Common Name (CN) static.grinnell.edu (CN in response to request w/o SNI: TRAEFIK DEFAULT CERT) subjectAltName (SAN) static.grinnell.edu Issuer Let\u0026#39;s Encrypt Authority X3 (Let\u0026#39;s Encrypt from US) Trust (hostname) Ok via SAN and CN (SNI mandatory) Chain of trust NOT ok (chain incomplete) EV cert (experimental) no \u0026#34;eTLS\u0026#34; (visibility info) not present Certificate Validity (UTC) 88 \u0026gt;= 30 days (2020-06-10 11:51 --\u0026gt; 2020-09-08 11:51) # of certificates provided 1 Certificate Revocation List -- OCSP URI http://ocsp.int-x3.letsencrypt.org OCSP stapling not offered OCSP must staple extension -- DNS CAA RR (experimental) not offered Certificate Transparency yes (certificate extension) Testing HTTP header response @ \u0026#34;/\u0026#34; HTTP Status Code 200 OK HTTP clock skew 0 sec from localtime Strict Transport Security not offered Public Key Pinning -- Server banner nginx/1.17.10 Application banner -- Cookie(s) (none issued at \u0026#34;/\u0026#34;) Security headers -- Reverse Proxy banner -- Testing vulnerabilities Heartbleed (CVE-2014-0160) not vulnerable (OK), no heartbeat extension CCS (CVE-2014-0224) not vulnerable (OK) Ticketbleed (CVE-2016-9244), experiment. not vulnerable (OK), reply empty ROBOT not vulnerable (OK) Secure Renegotiation (CVE-2009-3555) not vulnerable (OK) Secure Client-Initiated Renegotiation not vulnerable (OK) CRIME, TLS (CVE-2012-4929) not vulnerable (OK) BREACH (CVE-2013-3587) no HTTP compression (OK) - only supplied \u0026#34;/\u0026#34; tested POODLE, SSL (CVE-2014-3566) not vulnerable (OK) TLS_FALLBACK_SCSV (RFC 7507) Downgrade attack prevention supported (OK) SWEET32 (CVE-2016-2183, CVE-2016-6329) VULNERABLE, uses 64 bit block ciphers FREAK (CVE-2015-0204) not vulnerable (OK) DROWN (CVE-2016-0800, CVE-2016-0703) not vulnerable on this host and port (OK) make sure you don\u0026#39;t use this certificate elsewhere with SSLv2 enabled services https://censys.io/ipv4?q=F2A91B6B1BE83479550EE679AEA78B9551CA00D1D5BC0DA4B80D1FA1B4E3ED25 could help you to find out LOGJAM (CVE-2015-4000), experimental not vulnerable (OK): no DH EXPORT ciphers, no DH key detected with \u0026lt;= TLS 1.2 BEAST (CVE-2011-3389) TLS1: ECDHE-RSA-AES256-SHA AES256-SHA ECDHE-RSA-AES128-SHA AES128-SHA ECDHE-RSA-DES-CBC3-SHA DES-CBC3-SHA VULNERABLE -- but also supports higher protocols TLSv1.1 TLSv1.2 (likely mitigated) LUCKY13 (CVE-2013-0169), experimental potentially VULNERABLE, uses cipher block chaining (CBC) ciphers with TLS. Check patches RC4 (CVE-2013-2566, CVE-2015-2808) no RC4 ciphers detected (OK) Testing 370 ciphers via OpenSSL plus sockets against the server, ordered by encryption strength Hexcode Cipher Suite Name (OpenSSL) KeyExch. Encryption Bits Cipher Suite Name (IANA/RFC) ----------------------------------------------------------------------------------------------------------------------------- x1302 TLS_AES_256_GCM_SHA384 ECDH 253 AESGCM 256 TLS_AES_256_GCM_SHA384 x1303 TLS_CHACHA20_POLY1305_SHA256 ECDH 253 ChaCha20 256 TLS_CHACHA20_POLY1305_SHA256 xc030 ECDHE-RSA-AES256-GCM-SHA384 ECDH 521 AESGCM 256 TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 xc014 ECDHE-RSA-AES256-SHA ECDH 521 AES 256 TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA xcca8 ECDHE-RSA-CHACHA20-POLY1305 ECDH 521 ChaCha20 256 TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256 x9d AES256-GCM-SHA384 RSA AESGCM 256 TLS_RSA_WITH_AES_256_GCM_SHA384 x35 AES256-SHA RSA AES 256 TLS_RSA_WITH_AES_256_CBC_SHA x1301 TLS_AES_128_GCM_SHA256 ECDH 253 AESGCM 128 TLS_AES_128_GCM_SHA256 xc02f ECDHE-RSA-AES128-GCM-SHA256 ECDH 521 AESGCM 128 TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 xc013 ECDHE-RSA-AES128-SHA ECDH 521 AES 128 TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA x9c AES128-GCM-SHA256 RSA AESGCM 128 TLS_RSA_WITH_AES_128_GCM_SHA256 x2f AES128-SHA RSA AES 128 TLS_RSA_WITH_AES_128_CBC_SHA xc012 ECDHE-RSA-DES-CBC3-SHA ECDH 521 3DES 168 TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA x0a DES-CBC3-SHA RSA 3DES 168 TLS_RSA_WITH_3DES_EDE_CBC_SHA Running client simulations (HTTP) via sockets Android 4.2.2 TLSv1.0 ECDHE-RSA-AES256-SHA, 521 bit ECDH (P-521) Android 4.4.2 TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, 521 bit ECDH (P-521) Android 5.0.0 TLSv1.2 ECDHE-RSA-AES256-SHA, 521 bit ECDH (P-521) Android 6.0 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256, 256 bit ECDH (P-256) Android 7.0 TLSv1.2 ECDHE-RSA-CHACHA20-POLY1305, 253 bit ECDH (X25519) Chrome 65 Win 7 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256, 253 bit ECDH (X25519) Chrome 70 Win 10 TLSv1.3 TLS_AES_128_GCM_SHA256, 253 bit ECDH (X25519) Firefox 59 Win 7 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256, 253 bit ECDH (X25519) Firefox 62 Win 7 TLSv1.2 ECDHE-RSA-AES128-GCM-SHA256, 253 bit ECDH (X25519) IE 6 XP No connection IE 7 Vista TLSv1.0 AES128-SHA, No FS IE 8 Win 7 TLSv1.0 AES128-SHA, No FS IE 8 XP TLSv1.0 DES-CBC3-SHA, No FS IE 11 Win 7 TLSv1.2 ECDHE-RSA-AES256-SHA, 256 bit ECDH (P-256) IE 11 Win 8.1 TLSv1.2 ECDHE-RSA-AES256-SHA, 256 bit ECDH (P-256) IE 11 Win Phone 8.1 TLSv1.2 AES128-SHA, No FS IE 11 Win 10 TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, 256 bit ECDH (P-256) Edge 13 Win 10 TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, 256 bit ECDH (P-256) Edge 13 Win Phone 10 TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, 256 bit ECDH (P-256) Edge 15 Win 10 TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, 253 bit ECDH (X25519) Safari 9 iOS 9 TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, 256 bit ECDH (P-256) Safari 9 OS X 10.11 TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, 256 bit ECDH (P-256) Safari 10 OS X 10.12 TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, 256 bit ECDH (P-256) Apple ATS 9 iOS 9 TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, 256 bit ECDH (P-256) Tor 17.0.9 Win 7 TLSv1.0 ECDHE-RSA-AES256-SHA, 256 bit ECDH (P-256) Java 6u45 No connection Java 7u25 TLSv1.0 ECDHE-RSA-AES128-SHA, 256 bit ECDH (P-256) Java 8u161 TLSv1.2 ECDHE-RSA-AES256-SHA, 256 bit ECDH (P-256) Java 9.0.4 TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, 256 bit ECDH (P-256) OpenSSL 1.0.1l TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, 521 bit ECDH (P-521) OpenSSL 1.0.2e TLSv1.2 ECDHE-RSA-AES256-GCM-SHA384, 256 bit ECDH (P-256) Done 2020-06-11 17:34:48 [ 85s] --\u0026gt;\u0026gt; 132.161.151.30:443 (static.grinnell.edu) \u0026lt;\u0026lt;-- Controling TLS Ciphers In previous server configs that used Traefik v1.x I was able to eliminate some potential vulnerabilities by removing old, weak ciphers. In that old environment I used a few command: options in the docker-compose.yml file along with a declaration of stronger ciphers in a traefik.toml file. That config change is documented in Removing Traefik Weak Ciphers.\nIn our new configuration, which uses Traefik v2.x, TLS cipher control is quite different. I used this post as guidance and subsequently ended up with these modified files:\ndocker-compose.yml\nversion: \u0026#34;3.3\u0026#34; services: traefik: image: traefik:2.2.1 container_name: traefik hostname: traefik restart: unless-stopped security_opt: - no-new-privileges:true command: --log.level=DEBUG --api.insecure=true --providers.docker=true --providers.docker.exposedbydefault=false --providers.file.directory=/certs --providers.file.watch=true --providers.file.filename=/traefik-tls.toml --entrypoints.web.address=:80 --entrypoints.websecure.address=:443 networks: - proxy labels: traefik.enable: true # next 4 lines...universal http --\u0026gt; https redirect per https://community.containo.us/t/a-global-http-https-redirection/864/3 traefik.http.routers.http-catchall.rule: hostregexp(`{host:[a-z-.]+}`) traefik.http.routers.http-catchall.entrypoints: web traefik.http.routers.http-catchall.middlewares: redirect-to-https traefik.http.middlewares.redirect-to-https.redirectscheme.scheme: websecure # now the Traefik-specific dashboard stuff traefik.http.middlewares.traefik-auth.basicauth.users: admin:$$2y$$05$$pJEzHJBzfoYYS7/hGAedcOP8XdsqNXE7j.LHFBVjueASOqOvvjGOy traefik.http.routers.traefik-secure.entrypoints: websecure traefik.http.routers.traefik-secure.rule: Host(`${HOST}`) \u0026amp;\u0026amp; (PathPrefix(`/api`) || PathPrefix(`/dashboard`)) traefik.http.routers.traefik-secure.middlewares: traefik-auth traefik.http.routers.traefik-secure.tls: true traefik.http.routers.traefik-secure.service: api@internal traefik.http.routers.traefik-secure.tls.options: default ports: - 80:80 - 443:443 - 8080:8080 volumes: - /etc/localtime:/etc/localtime:ro - /var/run/docker.sock:/var/run/docker.sock:ro - ../certs:/certs networks: proxy: external: true Note that in the above file I also changed the labels: syntax and removed a number of cumbersome quotation marks while doing so.\ntraefik-tls.toml\n[tls.options] [tls.options.default] minVersion = \u0026#34;VersionTLS13\u0026#34; [tls.options.tls12] minVersion = \u0026#34;VersionTLS12\u0026#34; cipherSuites = [ \u0026#34;TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\u0026#34;, \u0026#34;TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\u0026#34;, \u0026#34;TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\u0026#34;, \u0026#34;TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\u0026#34;, \u0026#34;TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\u0026#34;, \u0026#34;TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\u0026#34; ] No Effect Unfortunately, as you can see in the testssl.sh output at the top of this post, two potential vulnerabilities remain. Still, this new syntax should enable me to take control of TLS configuration if it does become necessary.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/081-new-host-config-vulnerability-scan/","tags":["Traefik","docker-traefik2-acme-host","TLS","docker-compose","OpenSSL","testssl.sh","vulnerability"],"title":"New Host Config: Vulnerability Scan"},{"categories":null,"contents":"This post is celebrating the completion (really, is anything ever complete?) of a new server/host/stack deployment project: docker-traefik2-acme-host. In order to make this post really easy to read, I\u0026rsquo;m going to wrap it up in one bullet\u0026hellip;\nREADME.md Troubleshooting Should you ever encounter an error like the one below, be sure to ask your IT provider if they created the proper CNAME record when creating your DNS entry.\n[Tue Jun 30 20:53:50 UTC 2020] ohscribe.grinnell.edu:Verify error:DNS problem: NXDOMAIN looking up TXT for _acme-challenge.ohscribe.grinnell.edu - check that a DNS record exists for this domain That turned out to be the source of this error for me when I tried to obtain a cert for ohscribe.grinnell.edu on node DGDocker3.\nAnd that\u0026rsquo;s a wrap. I am so pleased that this works (and it\u0026rsquo;s not even Friday!) \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/080-host-config-docker-traefik2-acme-host/","tags":["Docker","Traefik","ACME","DNS-01","docker-traefik2-acme-host","docker-compose","CNAME","NXDOMAIN"],"title":"Host Config: docker-traefik2-acme-host"},{"categories":null,"contents":"MA6879 is the tag ID of the Mac Mini student-workstation in my Grinnell College office in Burling Library. During the COVID-19 pandemic it\u0026rsquo;s the only Mac that I have access to on campus. But the Macs here in my home office are not allowed to run Screens 4, the software that I formerly used to make remote connections between Macs. Fortunately, since the IP address of MA6879 is \u0026ldquo;static\u0026rdquo; and known (by me!), I can use built-in tools to make my screen-share and remote-control connections.\nThere are at least two ways to make a connection.\nUsing VNC To make a connection I can use vnc as described in the second half of https://helpdesk.owu.edu/networking/other-networking-information/mac2mac/. Have a look.\nPertinent Text In case the link is ever lost, here\u0026rsquo;s what it says, verbatim\u0026hellip;\nOn the machine you\u0026rsquo;ll be remotely connecting from:\nMake sure you\u0026rsquo;re in Finder (it says \u0026ldquo;Finder\u0026rdquo; next to the apple icon at top left. You can click on the desktop to go to finder or Command+Tab to get to Finder. Select Connect to Server from the Go menu. In the Server Address field type in the IP address of your remote computer preceded by \u0026ldquo;vnc:\u0026rdquo; as it appeared in Screen Sharing above. Click the Connect button and it will open the remote desktop in a new window. You will have control over the keyboard and mouse as if you were sitting down at that computer. When you\u0026rsquo;re done, simply close that window. To access files and folders on the remote computer select Go to Folder from the Go menu. Type in the IP address of the computer you wish to connect to preceded with \u0026ldquo;afp:\u0026rdquo; as it appeared in File Sharing above. Click the Go button and the folders you have access to on the remote computer will open in a new Finder window. When you\u0026rsquo;re done, simply close that window. Using \u0026ldquo;Screen Sharing\u0026rdquo; Tim, my esteemed Grinnell College colleague, also reminds me that it should be possible to open the Screen Sharing.app application using Spotlight Search (Command + Spacebar). That app opens a simple dialog like this example:\nYou simply enter the network alias or IP address of the Mac you want to control, and click Connect.\nAnd that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/078-remote-connection-to-ma6879/","tags":["MA6879","Mac Mini","Screens 4","remote","vnc"],"title":"Remote Connection to MA6879"},{"categories":null,"contents":"This post is a follow-up to Dockerized Traefik Host Using ACME DNS-01 Challenge. It introduces an alternative to the failed process that was proposed in that earlier post.\nNote that the following config-specific elements have been replaced below:\n6 occurances of ?.grinnell.edu now say example-1.grinnell.edu, and 2 occurances of ?.info now say example-2.info. New Proposal On June 1 my colleage, Matt, suggested the following\u0026hellip;\nAs much as I would like to resolve the DNS-01 challenge using Traefik alone, I don\u0026rsquo;t believe it will support what we\u0026rsquo;re trying to do here. I\u0026rsquo;m a bit disappointed by that as Nginx makes this process very easy, and my reading through the Traefik documentation and my own tests lead me to believe that CNAME following is not currently supported in Traefik, and is basically impossible. Until the they allow for the verification domain to be specified as a provider option (in this case, specifying example-2.info as the domain for the Azure DNS provider), using the built-in ACME functionality in Traefik won\u0026rsquo;t work, no matter which DNS provider is in use.\nHowever, I believe I have a solution. Acme.sh is another tool that is commonly used to generate certificates using Let\u0026rsquo;s Encrypt and the ACME protocol, and it does support domain aliasing. There is a containerized version of this, and I was able to build a docker-compose file that launches Traefik, a simple Whoami app, and the acme.sh container for creating certificates using the DNS-01 challenge. It\u0026rsquo;s not fully automated in that you have to run a docker exec command after the first run, but I think automating that part of it should be possible. The acme.sh container will renew certificates every 60 days as long as the acme.sh container is running. Traefik is configured to watch the certificate directory for changes and will reload when the certificate is renewed.\nIt\u0026rsquo;s a fairly simple set up and I hope it can work for your use case.\nversion: \u0026#34;3.3\u0026#34; services: acme: image: neilpang/acme.sh:latest volumes: - ./certs:/certs environment: # Azure - AZUREDNS_SUBSCRIPTIONID= - AZUREDNS_TENANTID= - AZUREDNS_APPID= - AZUREDNS_CLIENTSECRET= command: daemon container_name: acme traefik: image: \u0026#34;traefik:v2.2.1\u0026#34; container_name: \u0026#34;traefik\u0026#34; command: # --log.level=DEBUG --api.insecure=true --providers.docker=true --providers.docker.exposedbydefault=false --providers.file.directory=/certs/ --providers.file.watch=true --entrypoints.web.address=:80 --entrypoints.websecure.address=:443 ports: - 80:80 - 443:443 - 8080:8080 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - ./letsencrypt:/letsencrypt - ./certs/:/certs/ whoami: image: containous/whoami container_name: simple-service labels: traefik.enable: \u0026#34;true\u0026#34; traefik.http.routers.whoami.rule: \u0026#34;Host(`example-1.grinnell.edu`)\u0026#34; traefik.http.routers.whoami.entrypoints: \u0026#34;websecure\u0026#34; traefik.http.routers.whoami.tls: \u0026#34;true\u0026#34; In my \u0026lsquo;certs\u0026rsquo; directory, I have a certs.toml file with this:\n[[tls.certificates]] certFile = \u0026#34;/certs/example-1.grinnell.edu.cert\u0026#34; keyFile = \u0026#34;/certs/example-1.grinnell.edu.key\u0026#34; Just add an additional [[tls.certificates]] section for every additional certificate you will want generated.\nNetcentos is the name of my test machine, so just change that where you see it.\nYou will also want to move your Azure environment variables from the Traefik section to the Acme.sh container section (without quotes).\nAfter running docker-compose up -d, you will need to run the acme.sh command within the container once, changing the hostname for your own. If you want an additional hostname include another -d flag and then the FQDN.\ndocker exec -it acme --issue --dns dns_azure -d example-1.grinnell.edu --domain-alias _acme-challenge.example-2.info --key-file /certs/example-1.grinnell.edu.key --cert-file /certs/example-1.grinnell.edu.cert --standalone Here\u0026rsquo;s the documentation that I followed to get here, but feel free to send me questions if you have any.\nhttps://containo.us/blog/traefik-2-tls-101-23b4fbee81f1/ https://github.com/acmesh-official/acme.sh/wiki/DNS-alias-mode https://hub.docker.com/r/neilpang/acme.sh McFateM/docker-traefik2-acme-host I started work on this proposal by cloning https://github.com/DigitalGrinnell/docker-traefik2-host, reinitializing it, and pushing back an unmodified new copy to https://github.com/McFateM/docker-traefik2-acme-host.\nFirst Experience Yesterday I created cloned the above repository to the /opt/containers directory on my development and testing host, dgdocker3. In the docker-traefik2-acme-host directory there I created a new traefik subdirectory to work from.\nAfter introducing as few changes as possible, I navigated into the aforementioned traefik subdirectory and did this, with the following log results:\ndocker-compose up -d traefik | time=\u0026#34;2020-06-03T21:03:31Z\u0026#34; level=info msg=\u0026#34;Configuration loaded from flags.\u0026#34; traefik | time=\u0026#34;2020-06-03T21:03:31Z\u0026#34; level=error msg=\u0026#34;Unable to append certificate /certs/dgdocker3.grinnell.edu.cert to store: unable to generate TLS certificate : tls: failed to find any PEM data in certificate input\u0026#34; tlsStoreName=default The error here caused me to panic for a bit, but realizing that the process wasn\u0026rsquo;t complete I eventually moved on to the necessary next step.\nInvoking Acme.sh In spite of the errors reported by docker-compose up -d, the command did indeed create three healthy containers: traefik, acme, and simple-service. That last container just provides a WhoAmI app that I\u0026rsquo;ve configured to respond to https://dgdocker3.grinnell.edu. It works, but the cert is reported to be invalid/insecure.\nMatt\u0026rsquo;s instructions do say that after docker-compose up -d there is one remaining command required to configure acme.sh to provision and keep watch on the certs to be created. This command is to be run only one time in order to create a cert. So I did this with the following obfuscated results:\n[root@dgdocker3 traefik]# docker exec -it acme --issue --dns dns_azure -d dgdocker3.grinnell.edu --domain-alias _acme-challenge.obfuscated.info --key-file /certs/dgdocker3.grinnell.edu --cert-file /cernts/dgdocker3.grinnell.edu.cert --standalone [Wed Jun 3 22:13:45 UTC 2020] Create account key ok. [Wed Jun 3 22:13:45 UTC 2020] Registering account [Wed Jun 3 22:13:46 UTC 2020] Registered [Wed Jun 3 22:13:46 UTC 2020] ACCOUNT_THUMBPRINT=\u0026#39;EOC57YZgSg-D6S1dAREUNxGArecjpDRizh_Tyo85kN8\u0026#39; [Wed Jun 3 22:13:46 UTC 2020] Creating domain key [Wed Jun 3 22:13:46 UTC 2020] The domain key is here: /acme.sh/dgdocker3.grinnell.edu/dgdocker3.grinnell.edu.key [Wed Jun 3 22:13:46 UTC 2020] Single domain=\u0026#39;dgdocker3.grinnell.edu\u0026#39; [Wed Jun 3 22:13:46 UTC 2020] Getting domain auth token for each domain [Wed Jun 3 22:13:47 UTC 2020] Getting webroot for domain=\u0026#39;dgdocker3.grinnell.edu\u0026#39; [Wed Jun 3 22:13:47 UTC 2020] Adding txt value: eMxRFkWK...mWAX4 for domain: _acme-challenge.obfuscated.info [Wed Jun 3 22:13:49 UTC 2020] validation value added [Wed Jun 3 22:13:49 UTC 2020] The txt record is added: Success. [Wed Jun 3 22:13:49 UTC 2020] Let\u0026#39;s check each dns records now. Sleep 20 seconds first. [Wed Jun 3 22:14:10 UTC 2020] Checking dgdocker3.grinnell.edu for _acme-challenge.obfuscated.info [Wed Jun 3 22:14:10 UTC 2020] Domain dgdocker3.grinnell.edu \u0026#39;_acme-challenge.obfuscated.info\u0026#39; success. [Wed Jun 3 22:14:10 UTC 2020] All success, let\u0026#39;s return [Wed Jun 3 22:14:10 UTC 2020] Verifying: dgdocker3.grinnell.edu [Wed Jun 3 22:14:12 UTC 2020] Success [Wed Jun 3 22:14:12 UTC 2020] Removing DNS records. [Wed Jun 3 22:14:12 UTC 2020] Removing txt: eMxRFkWK...WAX4 for domain: _acme-challenge.obfuscated.info [Wed Jun 3 22:14:14 UTC 2020] validation record removed [Wed Jun 3 22:14:14 UTC 2020] Removed: Success [Wed Jun 3 22:14:14 UTC 2020] Verify finished, start to sign. [Wed Jun 3 22:14:14 UTC 2020] Lets finalize the order, Le_OrderFinalize: https://acme-v02.api.letsencrypt.org/acme/finalize/87890647/3622372972 [Wed Jun 3 22:14:15 UTC 2020] Download cert, Le_LinkCert: https://acme-v02.api.letsencrypt.org/acme/cert/04591be468a971eb756bf08bc626c5905321 [Wed Jun 3 22:14:15 UTC 2020] Cert success. -----BEGIN CERTIFICATE----- MIIFZDCCBE...waHNHdLOQrtg== -----END CERTIFICATE----- [Wed Jun 3 22:14:15 UTC 2020] Your cert is in /acme.sh/dgdocker3.grinnell.edu/dgdocker3.grinnell.edu.cer [Wed Jun 3 22:14:15 UTC 2020] Your cert key is in /acme.sh/dgdocker3.grinnell.edu/dgdocker3.grinnell.edu.key [Wed Jun 3 22:14:15 UTC 2020] The intermediate CA cert is in /acme.sh/dgdocker3.grinnell.edu/ca.cer [Wed Jun 3 22:14:15 UTC 2020] And the full chain certs is there: /acme.sh/dgdocker3.grinnell.edu/fullchain.cer [Wed Jun 3 22:14:15 UTC 2020] Installing cert to:/cernts/dgdocker3.grinnell.edu.cert /root/.acme.sh/acme.sh: line 5246: can\u0026#39;t create /cernts/dgdocker3.grinnell.edu.cert: nonexistent directory [root@dgdocker3 traefik]# docker exec -it acme --issue --dns dns_azure -d dgdocker3.grinnell.edu --domain-alias _acme-challenge.obfuscated.info --key-file /certs/dgdocker3.grinnell.edu --cert-file /certs/dgdocker3.grinnell.edu.cert --standalone [Wed Jun 3 22:16:31 UTC 2020] Domains not changed. [Wed Jun 3 22:16:31 UTC 2020] Skip, Next renewal time is: Sun Aug 2 22:14:15 UTC 2020 [Wed Jun 3 22:16:31 UTC 2020] Add \u0026#39;--force\u0026#39; to force to renew. [root@dgdocker3 traefik]# docker exec -it acme --issue --dns dns_azure -d dgdocker3.grinnell.edu --domain-alias _acme-challenge.obfuscated.info --key-file /certs/dgdocker3.grinnell.edu --cert-file /certs/dgdocker3.grinnell.edu.cert --standalone --force [Wed Jun 3 22:16:42 UTC 2020] Single domain=\u0026#39;dgdocker3.grinnell.edu\u0026#39; [Wed Jun 3 22:16:42 UTC 2020] Getting domain auth token for each domain [Wed Jun 3 22:16:43 UTC 2020] Getting webroot for domain=\u0026#39;dgdocker3.grinnell.edu\u0026#39; [Wed Jun 3 22:16:43 UTC 2020] dgdocker3.grinnell.edu is already verified, skip dns-01. [Wed Jun 3 22:16:43 UTC 2020] Verify finished, start to sign. [Wed Jun 3 22:16:43 UTC 2020] Lets finalize the order, Le_OrderFinalize: https://acme-v02.api.letsencrypt.org/acme/finalize/87890647/3622403221 [Wed Jun 3 22:16:44 UTC 2020] Download cert, Le_LinkCert: https://acme-v02.api.letsencrypt.org/acme/cert/0342ac4b3f8619949070cce67d48b998ea8c [Wed Jun 3 22:16:44 UTC 2020] Cert success. -----BEGIN CERTIFICATE----- MIIFZjCCBE6...uZDIZq1GMQ7xM -----END CERTIFICATE----- [Wed Jun 3 22:16:44 UTC 2020] Your cert is in /acme.sh/dgdocker3.grinnell.edu/dgdocker3.grinnell.edu.cer [Wed Jun 3 22:16:44 UTC 2020] Your cert key is in /acme.sh/dgdocker3.grinnell.edu/dgdocker3.grinnell.edu.key [Wed Jun 3 22:16:44 UTC 2020] The intermediate CA cert is in /acme.sh/dgdocker3.grinnell.edu/ca.cer [Wed Jun 3 22:16:44 UTC 2020] And the full chain certs is there: /acme.sh/dgdocker3.grinnell.edu/fullchain.cer [Wed Jun 3 22:16:44 UTC 2020] Installing cert to:/certs/dgdocker3.grinnell.edu.cert [Wed Jun 3 22:16:44 UTC 2020] Installing key to:/certs/dgdocker3.grinnell.edu [root@dgdocker3 traefik]# Oops If you look closely above you\u0026rsquo;ll see that I ran the command twice because of a typo the first time around. As suggested in the output, I corrected the syntax and ran the command a second time with an appended --force option. That seemed to work since it looks like the second command ran without incident. However, when I visit https://dgdocker3.grinnell.edu the site comes up, but still with an invalid/insecure cert.\nThe cert appears to have been generated correctly, and against the Let\u0026rsquo;s Encrypt production server, so why is it invalid?\nA Hunch I\u0026rsquo;ve received confirmation that certs are generated only when this critical command, docker exec -it acme --issue --dns dns_azure -d dgdocker3.grinnell.edu --domain-alias _acme-challenge.obfuscated.info --key-file /certs/dgdocker3.grinnell.edu --cert-file /certs/dgdocker3.grinnell.edu.cert --standalone --force, is run. Also, the order in which the containers are created isn\u0026rsquo;t critical, so in the future it probably makes better sense to create the acme container first, and run it before traefik and the others are created.\nNow I need to confirm that the generated certs are visible to the traefik container, and that they have proper permissions. Looking at that now\u0026hellip;\nI set Traefik\u0026rsquo;s debugging level to DEBUG, and removed all of the trailing slashes from /certs references in docker-compose.yml. When I did a new docker-compose up -d this is what I got, with some obfuscation:\n╭─mcfatem@dgdocker3 /opt/containers/docker-traefik2-acme-host/traefik ‹master*› ╰─$ docker-compose up -d Recreating traefik ... simple-service is up-to-date Recreating traefik ... done ╭─mcfatem@dgdocker3 /opt/containers/docker-traefik2-acme-host/traefik ‹master*› ╰─$ docker-compose logs Attaching to traefik, simple-service, acme simple-service | Starting up on port 80 traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=info msg=\u0026#34;Configuration loaded from flags.\u0026#34; traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=info msg=\u0026#34;Traefik version 2.2.1 built on 2020-04-29T18:02:09Z\u0026#34; traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=debug msg=\u0026#34;Static configuration loaded {\\\u0026#34;global\\\u0026#34;:{\\\u0026#34;checkNewVersion\\\u0026#34;:true},\\\u0026#34;serversTransport\\\u0026#34;:{\\\u0026#34;maxIdleConnsPerHost\\\u0026#34;:200},\\\u0026#34;entryPoints\\\u0026#34;:{\\\u0026#34;traefik\\\u0026#34;:{\\\u0026#34;address\\\u0026#34;:\\\u0026#34;:8080\\\u0026#34;,\\\u0026#34;transport\\\u0026#34;:{\\\u0026#34;lifeCycle\\\u0026#34;:{\\\u0026#34;graceTimeOut\\\u0026#34;:10000000000},\\\u0026#34;respondingTimeouts\\\u0026#34;:{\\\u0026#34;idleTimeout\\\u0026#34;:180000000000}},\\\u0026#34;forwardedHeaders\\\u0026#34;:{},\\\u0026#34;http\\\u0026#34;:{}},\\\u0026#34;web\\\u0026#34;:{\\\u0026#34;address\\\u0026#34;:\\\u0026#34;:80\\\u0026#34;,\\\u0026#34;transport\\\u0026#34;:{\\\u0026#34;lifeCycle\\\u0026#34;:{\\\u0026#34;graceTimeOut\\\u0026#34;:10000000000},\\\u0026#34;respondingTimeouts\\\u0026#34;:{\\\u0026#34;idleTimeout\\\u0026#34;:180000000000}},\\\u0026#34;forwardedHeaders\\\u0026#34;:{},\\\u0026#34;http\\\u0026#34;:{}},\\\u0026#34;websecure\\\u0026#34;:{\\\u0026#34;address\\\u0026#34;:\\\u0026#34;:443\\\u0026#34;,\\\u0026#34;transport\\\u0026#34;:{\\\u0026#34;lifeCycle\\\u0026#34;:{\\\u0026#34;graceTimeOut\\\u0026#34;:10000000000},\\\u0026#34;respondingTimeouts\\\u0026#34;:{\\\u0026#34;idleTimeout\\\u0026#34;:180000000000}},\\\u0026#34;forwardedHeaders\\\u0026#34;:{},\\\u0026#34;http\\\u0026#34;:{}}},\\\u0026#34;providers\\\u0026#34;:{\\\u0026#34;providersThrottleDuration\\\u0026#34;:2000000000,\\\u0026#34;docker\\\u0026#34;:{\\\u0026#34;watch\\\u0026#34;:true,\\\u0026#34;endpoint\\\u0026#34;:\\\u0026#34;unix:///var/run/docker.sock\\\u0026#34;,\\\u0026#34;defaultRule\\\u0026#34;:\\\u0026#34;Host(`{{ normalize .Name }}`)\\\u0026#34;,\\\u0026#34;swarmModeRefreshSeconds\\\u0026#34;:15000000000},\\\u0026#34;file\\\u0026#34;:{\\\u0026#34;directory\\\u0026#34;:\\\u0026#34;/certs\\\u0026#34;,\\\u0026#34;watch\\\u0026#34;:true}},\\\u0026#34;api\\\u0026#34;:{\\\u0026#34;insecure\\\u0026#34;:true,\\\u0026#34;dashboard\\\u0026#34;:true},\\\u0026#34;log\\\u0026#34;:{\\\u0026#34;level\\\u0026#34;:\\\u0026#34;DEBUG\\\u0026#34;,\\\u0026#34;format\\\u0026#34;:\\\u0026#34;common\\\u0026#34;}}\u0026#34; traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=info msg=\u0026#34;\\nStats collection is disabled.\\nHelp us improve Traefik by turning this feature on :)\\nMore details on: https://docs.traefik.io/contributing/data-collection/\\n\u0026#34; traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=info msg=\u0026#34;Starting provider aggregator.ProviderAggregator {}\u0026#34; traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=debug msg=\u0026#34;Start TCP Server\u0026#34; entryPointName=web traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=debug msg=\u0026#34;Start TCP Server\u0026#34; entryPointName=websecure traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=debug msg=\u0026#34;Start TCP Server\u0026#34; entryPointName=traefik traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=info msg=\u0026#34;Starting provider *file.Provider {\\\u0026#34;directory\\\u0026#34;:\\\u0026#34;/certs\\\u0026#34;,\\\u0026#34;watch\\\u0026#34;:true}\u0026#34; traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=info msg=\u0026#34;Starting provider *traefik.Provider {}\u0026#34; traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=debug msg=\u0026#34;Configuration received from provider file: {\\\u0026#34;http\\\u0026#34;:{},\\\u0026#34;tcp\\\u0026#34;:{},\\\u0026#34;udp\\\u0026#34;:{},\\\u0026#34;tls\\\u0026#34;:{}}\u0026#34; providerName=file traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=debug msg=\u0026#34;Configuration received from provider internal: {\\\u0026#34;http\\\u0026#34;:{\\\u0026#34;routers\\\u0026#34;:{\\\u0026#34;api\\\u0026#34;:{\\\u0026#34;entryPoints\\\u0026#34;:[\\\u0026#34;traefik\\\u0026#34;],\\\u0026#34;service\\\u0026#34;:\\\u0026#34;api@internal\\\u0026#34;,\\\u0026#34;rule\\\u0026#34;:\\\u0026#34;PathPrefix(`/api`)\\\u0026#34;,\\\u0026#34;priority\\\u0026#34;:2147483646},\\\u0026#34;dashboard\\\u0026#34;:{\\\u0026#34;entryPoints\\\u0026#34;:[\\\u0026#34;traefik\\\u0026#34;],\\\u0026#34;middlewares\\\u0026#34;:[\\\u0026#34;dashboard_redirect@internal\\\u0026#34;,\\\u0026#34;dashboard_stripprefix@internal\\\u0026#34;],\\\u0026#34;service\\\u0026#34;:\\\u0026#34;dashboard@internal\\\u0026#34;,\\\u0026#34;rule\\\u0026#34;:\\\u0026#34;PathPrefix(`/`)\\\u0026#34;,\\\u0026#34;priority\\\u0026#34;:2147483645}},\\\u0026#34;services\\\u0026#34;:{\\\u0026#34;api\\\u0026#34;:{},\\\u0026#34;dashboard\\\u0026#34;:{},\\\u0026#34;noop\\\u0026#34;:{}},\\\u0026#34;middlewares\\\u0026#34;:{\\\u0026#34;dashboard_redirect\\\u0026#34;:{\\\u0026#34;redirectRegex\\\u0026#34;:{\\\u0026#34;regex\\\u0026#34;:\\\u0026#34;^(http:\\\\\\\\/\\\\\\\\/[^:\\\\\\\\/]+(:\\\\\\\\d+)?)\\\\\\\\/$\\\u0026#34;,\\\u0026#34;replacement\\\u0026#34;:\\\u0026#34;${1}/dashboard/\\\u0026#34;,\\\u0026#34;permanent\\\u0026#34;:true}},\\\u0026#34;dashboard_stripprefix\\\u0026#34;:{\\\u0026#34;stripPrefix\\\u0026#34;:{\\\u0026#34;prefixes\\\u0026#34;:[\\\u0026#34;/dashboard/\\\u0026#34;,\\\u0026#34;/dashboard\\\u0026#34;]}}}},\\\u0026#34;tcp\\\u0026#34;:{},\\\u0026#34;tls\\\u0026#34;:{}}\u0026#34; providerName=internal traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=debug msg=\u0026#34;No store is defined to add the certificate MIIFZjCCBE6gAwIBAgISA0KsSz+GGZSQcMzmfUi5mOqMMA0GCS, it will be added to the default store.\u0026#34; traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=error msg=\u0026#34;Unable to append certificate MIIFZjCCBE6gAwIBAgISA0KsSz+GGZSQcMzmfUi5mOqMMA0GCS to store: unable to generate TLS certificate : tls: failed to find any PEM data in key input\u0026#34; tlsStoreName=default traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=debug msg=\u0026#34;No default certificate, generating one\u0026#34; traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=info msg=\u0026#34;Starting provider *docker.Provider {\\\u0026#34;watch\\\u0026#34;:true,\\\u0026#34;endpoint\\\u0026#34;:\\\u0026#34;unix:///var/run/docker.sock\\\u0026#34;,\\\u0026#34;defaultRule\\\u0026#34;:\\\u0026#34;Host(`{{ normalize .Name }}`)\\\u0026#34;,\\\u0026#34;swarmModeRefreshSeconds\\\u0026#34;:15000000000}\u0026#34; traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=debug msg=\u0026#34;Provider connection established with docker 19.03.8 (API 1.40)\u0026#34; providerName=docker traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=debug msg=\u0026#34;Filtering disabled container\u0026#34; container=traefik-traefik-afe2703051b474ea3e259e1fcc8699a925968ddca1132fb67265479c4e93c85b providerName=docker traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=debug msg=\u0026#34;Filtering disabled container\u0026#34; providerName=docker container=acme-traefik-cda17bc48f61207b7c92c9d1760f7d428eccc4b413dd440bb97bdd84ee052df3 traefik | time=\u0026#34;2020-06-04T16:36:15Z\u0026#34; level=debug msg=\u0026#34;Configuration received from provider docker: {\\\u0026#34;http\\\u0026#34;:{\\\u0026#34;routers\\\u0026#34;:{\\\u0026#34;whoami\\\u0026#34;:{\\\u0026#34;entryPoints\\\u0026#34;:[\\\u0026#34;websecure\\\u0026#34;],\\\u0026#34;service\\\u0026#34;:\\\u0026#34;whoami-traefik\\\u0026#34;,\\\u0026#34;rule\\\u0026#34;:\\\u0026#34;Host(`dgdocker3.grinnell.edu`)\\\u0026#34;,\\\u0026#34;tls\\\u0026#34;:{}}},\\\u0026#34;services\\\u0026#34;:{\\\u0026#34;whoami-traefik\\\u0026#34;:{\\\u0026#34;loadBalancer\\\u0026#34;:{\\\u0026#34;servers\\\u0026#34;:[{\\\u0026#34;url\\\u0026#34;:\\\u0026#34;http://192.168.160.3:80\\\u0026#34;}],\\\u0026#34;passHostHeader\\\u0026#34;:true}}}},\\\u0026#34;tcp\\\u0026#34;:{},\\\u0026#34;udp\\\u0026#34;:{}}\u0026#34; providerName=docker traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=error msg=\u0026#34;Unable to append certificate MIIFZjCCBE6gAwIBAgISA0KsSz+GGZSQcMzmfUi5mOqMMA0GCS to store: unable to generate TLS certificate : tls: failed to find any PEM data in key input\u0026#34; tlsStoreName=default traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Added outgoing tracing middleware api@internal\u0026#34; middlewareType=TracingForwarder entryPointName=traefik routerName=api@internal middlewareName=tracing traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Added outgoing tracing middleware dashboard@internal\u0026#34; entryPointName=traefik routerName=dashboard@internal middlewareName=tracing middlewareType=TracingForwarder traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Creating middleware\u0026#34; entryPointName=traefik routerName=dashboard@internal middlewareType=StripPrefix middlewareName=dashboard_stripprefix@internal traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Adding tracing to middleware\u0026#34; routerName=dashboard@internal middlewareName=dashboard_stripprefix@internal entryPointName=traefik traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Creating middleware\u0026#34; entryPointName=traefik routerName=dashboard@internal middlewareName=dashboard_redirect@internal middlewareType=RedirectRegex traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Setting up redirection from ^(http:\\\\/\\\\/[^:\\\\/]+(:\\\\d+)?)\\\\/$ to ${1}/dashboard/\u0026#34; middlewareType=RedirectRegex entryPointName=traefik routerName=dashboard@internal middlewareName=dashboard_redirect@internal traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Adding tracing to middleware\u0026#34; entryPointName=traefik routerName=dashboard@internal middlewareName=dashboard_redirect@internal traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Creating middleware\u0026#34; entryPointName=traefik middlewareName=traefik-internal-recovery middlewareType=Recovery traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;No default certificate, generating one\u0026#34; traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=error msg=\u0026#34;Unable to append certificate MIIFZjCCBE6gAwIBAgISA0KsSz+GGZSQcMzmfUi5mOqMMA0GCS to store: unable to generate TLS certificate : tls: failed to find any PEM data in key input\u0026#34; tlsStoreName=default traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Added outgoing tracing middleware dashboard@internal\u0026#34; middlewareName=tracing middlewareType=TracingForwarder routerName=dashboard@internal entryPointName=traefik traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Creating middleware\u0026#34; middlewareType=StripPrefix entryPointName=traefik routerName=dashboard@internal middlewareName=dashboard_stripprefix@internal traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Adding tracing to middleware\u0026#34; routerName=dashboard@internal middlewareName=dashboard_stripprefix@internal entryPointName=traefik traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Creating middleware\u0026#34; middlewareName=dashboard_redirect@internal middlewareType=RedirectRegex entryPointName=traefik routerName=dashboard@internal traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Setting up redirection from ^(http:\\\\/\\\\/[^:\\\\/]+(:\\\\d+)?)\\\\/$ to ${1}/dashboard/\u0026#34; entryPointName=traefik routerName=dashboard@internal middlewareName=dashboard_redirect@internal middlewareType=RedirectRegex traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Adding tracing to middleware\u0026#34; middlewareName=dashboard_redirect@internal entryPointName=traefik routerName=dashboard@internal traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Added outgoing tracing middleware api@internal\u0026#34; middlewareName=tracing middlewareType=TracingForwarder entryPointName=traefik routerName=api@internal traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Creating middleware\u0026#34; entryPointName=traefik middlewareName=traefik-internal-recovery middlewareType=Recovery traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Creating middleware\u0026#34; entryPointName=websecure routerName=whoami@docker serviceName=whoami-traefik middlewareName=pipelining middlewareType=Pipelining traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Creating load-balancer\u0026#34; serviceName=whoami-traefik entryPointName=websecure routerName=whoami@docker traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Creating server 0 http://192.168.160.3:80\u0026#34; routerName=whoami@docker serviceName=whoami-traefik entryPointName=websecure serverName=0 traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Added outgoing tracing middleware whoami-traefik\u0026#34; entryPointName=websecure routerName=whoami@docker middlewareName=tracing middlewareType=TracingForwarder traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;Creating middleware\u0026#34; entryPointName=websecure middlewareName=traefik-internal-recovery middlewareType=Recovery traefik | time=\u0026#34;2020-06-04T16:36:16Z\u0026#34; level=debug msg=\u0026#34;No default certificate, generating one\u0026#34; traefik | time=\u0026#34;2020-06-04T16:36:26Z\u0026#34; level=debug msg=\u0026#34;Serving default certificate for request: \\\u0026#34;dgdocker3.grinnell.edu\\\u0026#34;\u0026#34; traefik | time=\u0026#34;2020-06-04T16:36:26Z\u0026#34; level=debug msg=\u0026#34;http: TLS handshake error from 132.161.249.186:55292: remote error: tls: bad certificate\u0026#34; | Aha! I poured over the output above and to be honest, it wasn\u0026rsquo;t all that helpful, but along the way I decided to double check something\u0026hellip;\nThe unfortunate thing with this process is that we have to run a lengthy docker exec... command. Sure, you only run it once, but it\u0026rsquo;s pretty long and therefore subject to human error. I knew that I got it wrong twice, but in reality, there were three iterations of at least one error/omission.\nThe command I typed (can\u0026rsquo;t even copy/paste since I have to be on VPN to run the command, and VPN effectively \u0026ldquo;blinds\u0026rdquo; my machine rendering the clipboard almost useless), four times now, looked like this the first time:\ndocker exec -it acme --issue --dns dns_azure -d dgdocker3.grinnell.edu --domain-alias _acme-challenge.{obfuscated}.info --key-file /cernts/dgdocker3.grinnell.edu --cert-file /certs/dgdocker3.grinnell.edu.cert --standalone There\u0026rsquo;s an obvious error there, an extra \u0026ldquo;n\u0026rdquo; in the --key-file path. It read /cernts/, but should have been /certs/. Fortunately that was pretty easy to catch and the error messages in the log helped me identify it. However, in that same portion of the command there\u0026rsquo;s another error, actually an omission, that\u0026rsquo;s more sinister. The clause --key-file /cernts/dgdocker3.grinnell.edu should actually be --key-file /certs/dgdocker3.grinnell.edu.key. They \u0026ldquo;key\u0026rdquo; difference there is the .key extension at the very end, it was omitted the first THREE times I ran this command!\nOne More Time\u0026hellip;with Feeling So, the correct docker exec... command at this point is:\ndocker exec -it acme --issue --dns dns_azure -d dgdocker3.grinnell.edu --domain-alias _acme-challenge.{obfuscated}.info --key-file /certs/dgdocker3.grinnell.edu.key --cert-file /certs/dgdocker3.grinnell.edu.cert --standalone --force The --force option is required here to \u0026ldquo;force\u0026rdquo; Acme to renew, or create, new certs since some already exist from previous iterations of the command. This time we must need to renew/create them with the correct path and file names!\nI\u0026rsquo;ve put a \u0026ldquo;copy\u0026rdquo; of that command, with obfuscated elements restored as needed, into the .env file that carries some necessary credentials. Now to give it a go\u0026hellip;\nThe End Result I am exceptionally pleased to report that\u0026hellip; IT JUST WORKS. \u0026#x2757; \u0026#x1f600; \u0026#x2757; \u0026#x1f600; \u0026#x2757;\nNext steps\u0026hellip;I\u0026rsquo;m going to split the process into a more granluar form with additional containers/services, and repeat this test. If that works I\u0026rsquo;ll take additional steps to automate everything to reduce the potential for future human-error, within reason.\nLook for all of that in a follow-up blog post.\nAnd that\u0026rsquo;s a wrap. I am so pleased that this works (and it\u0026rsquo;s not even Friday!) \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/079-traefik-and-acme.sh-instead-of-dns-01/","tags":["Docker","Traefik","ACME","DNS-01","docker-traefik2-host","docker-compose"],"title":"Traefik and Acme.sh Instead of DNS-01"},{"categories":null,"contents":"Digital.Grinnell features a fair number of transcribed digital oral histories. Most are interviews conducted with Grinnelleans as they return to campus annualy for alumni Reunion or Grinnell\u0026rsquo;s Multicultural Reunion.\nWhat follows is largely sharable \u0026ldquo;public\u0026rdquo; content lifted from a \u0026ldquo;private\u0026rdquo; GitHub repository at https://github.com/McFateM/OHScribe, the README.md and WORKFLOW.md files in particular.\nOHScribe! https://ohscribe.grinnell.edu Note: This link was disabled in the spring of 2020, but is working again as of July 1, 2020, on host DGDocker3.\nThis is a Python 3 and Flask web application designed to transform oral history transcripts, presumably created using InqScribe, into XML suitable for ingest into the Islandora Oral Histories Solution Pack to populate a TRANSCRIPT datastream and its derivatives. Islandora Oral Histories and the aforementioned solution pack are commonly referred to as IOH throughout this document.\nOHScribe! is intended to be used as part of a broader IOH transcription workflow developed at Grinnell College. The workflow documented in the section titled The Digital.Grinnell Oral History Workflow may be of significant interest to individuals tasked with transcribing IOH audio recordings. The aforementioned workflow document now includes a link to an 11.5 minute long training video.\nFormatted IOH Example The aforementioned workflow, application, and accompanying CSS (provided below) are intended to deliver oral histories that look something like this:\nNote that the names of speakers appear in different colors in the video window captions, and speaker names appear in bold in the indexed transcript below the video. Each speaker and corresponding text appears on a new line, and captions are superimposed over a thumbnail image of the speaker(s).\nInqScribe IOH Transcription Workflow A detailed description of the workflow intended for use with OHScribe! is provided in our WORKFLOW.md document.\nWorkflow Output The workflow documented in the aforementioned WORKFLOW.md should produce an XML formatted transcript which resembles the structure of the following example.\n\u0026lt;transcript\u0026gt; \u0026lt;prologue/\u0026gt; \u0026lt;scene id=\u0026#34;1\u0026#34; in=\u0026#34;00:00:00.21\u0026#34; out=\u0026#34;00:00:12.07\u0026#34;\u0026gt;\u0026lt;speaker\u0026gt; Heather Riggs \u0026lt;/speaker\u0026gt; Heather | Okay, so.. Yeah, just before we start, if you could each go around and say your name, your class year, and where you live now, just for the microphone.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;2\u0026#34; in=\u0026#34;00:00:12.08\u0026#34; out=\u0026#34;00:00:18.02\u0026#34;\u0026gt;\u0026lt;speaker\u0026gt; Margo Gray \u0026lt;/speaker\u0026gt; Margo | Cool. I’m Margo Gray of the class of 2005, and, what else am I saying?\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;3\u0026#34; in=\u0026#34;00:00:18.03\u0026#34; out=\u0026#34;00:00:19.07\u0026#34;\u0026gt;Heather | Your home. \u0026lt;speaker\u0026gt; Maggie Montanaro \u0026lt;/speaker\u0026gt; Maggie | Where you live.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;4\u0026#34; in=\u0026#34;00:00:19.08\u0026#34; out=\u0026#34;00:00:21.14\u0026#34;\u0026gt;Margo | I live in Chicago, Illinois.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;5\u0026#34; in=\u0026#34;00:00:21.15\u0026#34; out=\u0026#34;00:00:26.07\u0026#34;\u0026gt;\u0026lt;speaker\u0026gt; Jenny Noyce \u0026lt;/speaker\u0026gt; Jenny | My name is Jenny Noyce, the class of 2005 and I live in Oakland, California.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;6\u0026#34; in=\u0026#34;00:00:26.08\u0026#34; out=\u0026#34;00:00:32.11\u0026#34;\u0026gt;Maggie | I’m Maggie Montanaro, also class of 2005, and I live in Avignon, France.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;7\u0026#34; in=\u0026#34;00:00:32.12\u0026#34; out=\u0026#34;00:00:39.17\u0026#34;\u0026gt;Heather | Wow. So, what are your strongest memories of Grinnell?\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;8\u0026#34; in=\u0026#34;00:00:39.18\u0026#34; out=\u0026#34;00:00:45.05\u0026#34;\u0026gt;Maggie | Harris Parties.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;9\u0026#34; in=\u0026#34;00:00:45.06\u0026#34; out=\u0026#34;00:00:53.00\u0026#34;\u0026gt;Jenny | Mud sliding in the rain. Maggie | Yeah, mud wrestling. Mud sliding on Mac Field. Lots and lots of work.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;10\u0026#34; in=\u0026#34;00:00:53.01\u0026#34; out=\u0026#34;00:01:30.27\u0026#34;\u0026gt;Margo | I guess I remember people, like, I still am in touch with a lot of people from Grinnell and yeah. So I don’t, I mean I don’t have like these really specific memories of like meeting people, but just mostly, like this whole sort of like pool of memories of times when I was hanging out with people or working with people or, yeah. Building the sort of, you don’t think of it when you’re there, it’s not like, \u0026#34;Ah, I’m building connections to last me!\u0026#34; You’re just like, \u0026#34;I’m hanging out with my friends.\u0026#34; But those sort of things tend to last. Maggie | Lots of good hanging out. Margo | Yes.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;11\u0026#34; in=\u0026#34;00:01:30.28\u0026#34; out=\u0026#34;00:02:01.00\u0026#34;\u0026gt;Heather | What kind of Harris parties did you have? Like themed... Maggie | All the, I assume they still have them, the hall ones like the Haines Underwear Ball, the Mary B. James, Disco... what else? Lots of just themed… Jenny | They started a fetish party. Maggie | Really? They still have fetish? Jenny | I never went to that one. Maybe I was too close-minded. Maggie | Yeah.\u0026lt;/scene\u0026gt; \u0026lt;scene id=\u0026#34;12\u0026#34; in=\u0026#34;00:02:01.01\u0026#34; out=\u0026#34;00:02:18.14\u0026#34;\u0026gt;Heather | What are your first memories of Grinnell? Key parts of this exported transcript example are described in the sections below.\n\u0026lt;scene\u0026gt; Tags \u0026lt;scene\u0026gt; tags enclose data indicating who is speaking along with transcribed text from the recording. \u0026lt;scene\u0026gt; tags also include attributes indicating the position or scene\u0026rsquo;s id, in and out times provided in hours-minutes-seconds, or hh:mm:ss.s notation, and one or more optional \u0026lt;speaker\u0026gt; tags.\n\u0026lt;scene id=\u0026#34;1\u0026#34; in=\u0026#34;00:00:00.21\u0026#34; out=\u0026#34;00:00:12.07\u0026#34;\u0026gt; \u0026lt;speaker\u0026gt; Heather Riggs \u0026lt;/speaker\u0026gt; Heather | Okay, so.. Yeah, just before we start, if you could each go around and say your name, your class year, and where you live now, just for the microphone. \u0026lt;/scene\u0026gt; The example \u0026lt;scene\u0026gt; tag shown above carries an id of \u0026lsquo;1\u0026rsquo;, it\u0026rsquo;s the first scene in the transcript, with in and out times provided in hours-minutes-seconds, or hh:mm:ss.s notation. The in and out attributes indicate when the transcribed text is heard in the audio stream. This scene also encloses a \u0026lt;speaker\u0026gt; tag which, in this case, identifies the speaker \u0026ldquo;Heather\u0026rdquo;, as an individual with a full name of \u0026ldquo;Heather Riggs\u0026rdquo;.\nThe text of this scene, a single sentence, opens with a required speaker ID, the name of the speaker, Heather, followed by a pipe (|) delimiter.\nThis demonstrates an important rule\u0026hellip; Each line of transcribed text MUST begin with a speaker ID, a given/first name, or single word identifying the speaker, followed by a pipe delimiter.\n\u0026lt;scene id=\u0026#34;11\u0026#34; in=\u0026#34;00:01:30.28\u0026#34; out=\u0026#34;00:02:01.00\u0026#34;\u0026gt; Heather | What kind of Harris parties did you have? Like themed... Maggie | All the, I assume they still have them, the hall ones like the Haines Underwear Ball, the Mary B. James, Disco... what else? Lots of just themed… Jenny | They started a fetish party. Maggie | Really? They still have fetish? Jenny | I never went to that one. Maybe I was too close-minded. Maggie | Yeah. \u0026lt;/scene\u0026gt; The example \u0026lt;scene\u0026gt; tag shown above, with an id of \u0026lsquo;11\u0026rsquo;, includes in and out time attributes, and seven lines of transcribed text. Each line begins with a speaker ID, in this case the given/first name of the speaker followed by a pipe (|) delimiter. Note that this scene has NO \u0026lt;speaker\u0026gt; tags because the speakers: Heather, Maggie, and Jenny; all have corresponding \u0026lt;speaker\u0026gt; tags inside previous scenes.\n\u0026lt;speaker\u0026gt; Tags \u0026lt;speaker\u0026gt; tags are used to identify each speaker in a transcript by providing their full name, and associating the given/first name portion of their full name with subsequent speaker ID prefixes.\nAt least one \u0026lt;speaker\u0026gt; tag must appear for each speaker ID used in the transcript, and a speaker\u0026rsquo;s tag must appear BEFORE any/all corresponding speaker IDs.\nA \u0026lt;speaker\u0026gt; must occur only within an enclosing \u0026lt;scene\u0026gt; tag.\nSpeaker ID As previously mentioned, a speaker ID identifies a speaker by their given/first name or some single-word identifying term, like \u0026ldquo;Interviewer\u0026quot;or \u0026ldquo;Interviewee\u0026rdquo;. Each speaker ID consists of a given/first name, or other single-word identifier, followed by a pipe (|) delimiter.\nSpaces around pipe delimiters are recommended, but not required.\nXML for Ingest into IOH The Islandora Oral Histories (IOH) solution pack expects a TRANSCRIPT datastream of \u0026lt;cues\u0026gt; formatted like the following for successful ingest.\n\u0026lt;cues\u0026gt; \u0026lt;cue\u0026gt; \u0026lt;speaker\u0026gt;Heather Riggs\u0026lt;/speaker\u0026gt; \u0026lt;start\u0026gt;0.21\u0026lt;/start\u0026gt; \u0026lt;end\u0026gt;12.07\u0026lt;/end\u0026gt; \u0026lt;transcript\u0026gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_1\u0026#39;\u0026amp;gt;Heather: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; Okay, so.. Yeah, just before we start, if you could each go around and say your name, your class year, and where you live now, just for the microphone.\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026lt;/transcript\u0026gt; \u0026lt;/cue\u0026gt; \u0026lt;cue\u0026gt; \u0026lt;speaker\u0026gt;Margo Gray\u0026lt;/speaker\u0026gt; \u0026lt;start\u0026gt;12.08\u0026lt;/start\u0026gt; \u0026lt;end\u0026gt;18.02\u0026lt;/end\u0026gt; \u0026lt;transcript\u0026gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_2\u0026#39;\u0026amp;gt;Margo: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; Cool. I\u0026amp;#8217;m Margo Gray of the class of 2005, and, what else am I saying?\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026lt;/transcript\u0026gt; \u0026lt;/cue\u0026gt; \u0026lt;cue\u0026gt; \u0026lt;speaker\u0026gt;Heather Riggs \u0026amp;amp; Maggie Montanaro\u0026lt;/speaker\u0026gt; \u0026lt;start\u0026gt;18.03\u0026lt;/start\u0026gt; \u0026lt;end\u0026gt;19.07\u0026lt;/end\u0026gt; \u0026lt;transcript\u0026gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_1\u0026#39;\u0026amp;gt;Heather: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; Your home.\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_3\u0026#39;\u0026amp;gt;Maggie: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; Where you live.\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026lt;/transcript\u0026gt; \u0026lt;/cue\u0026gt; \u0026lt;cue\u0026gt; \u0026lt;speaker\u0026gt;Margo Gray\u0026lt;/speaker\u0026gt; \u0026lt;start\u0026gt;19.08\u0026lt;/start\u0026gt; \u0026lt;end\u0026gt;21.14\u0026lt;/end\u0026gt; \u0026lt;transcript\u0026gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_2\u0026#39;\u0026amp;gt;Margo: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; I live in Chicago, Illinois.\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026lt;/transcript\u0026gt; \u0026lt;/cue\u0026gt; \u0026lt;cue\u0026gt; \u0026lt;speaker\u0026gt;Jenny Noyce\u0026lt;/speaker\u0026gt; \u0026lt;start\u0026gt;21.15\u0026lt;/start\u0026gt; \u0026lt;end\u0026gt;26.07\u0026lt;/end\u0026gt; \u0026lt;transcript\u0026gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_4\u0026#39;\u0026amp;gt;Jenny: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; My name is Jenny Noyce, the class of 2005 and I live in Oakland, California.\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026lt;/transcript\u0026gt; \u0026lt;/cue\u0026gt; \u0026lt;cue\u0026gt; \u0026lt;speaker\u0026gt;Maggie Montanaro\u0026lt;/speaker\u0026gt; \u0026lt;start\u0026gt;26.08\u0026lt;/start\u0026gt; \u0026lt;end\u0026gt;32.11\u0026lt;/end\u0026gt; \u0026lt;transcript\u0026gt;\u0026amp;lt;span class=\u0026#39;oh_speaker_3\u0026#39;\u0026amp;gt;Maggie: \u0026amp;lt;span class=\u0026#39;oh_speaker_text\u0026#39;\u0026amp;gt; I\u0026amp;#8217;m Maggie Montanaro, also class of 2005, and I live in Avignon, France.\u0026amp;lt;/span\u0026amp;gt;\u0026amp;lt;/span\u0026amp;gt;\u0026lt;/transcript\u0026gt; \u0026lt;/cue\u0026gt; In this format each \u0026lt;cue\u0026gt; tag within the enclosing \u0026lt;cues\u0026gt; tag represents one \u0026lt;scene\u0026gt; from the previously documented workflow output. Like a \u0026lt;scene\u0026gt;, each \u0026lt;cue\u0026gt; identifies a speaker, a start and end time, and one or more lines of transcribed text.\nOHScribe! can be used to transform output from the documented workflow into this format for ingest.\nRunning OHScribe! OHScribe! is accessible at https://ohscribe.grinnell.edu and should run in any web browser. It permits a user to upload an XML file (presumably this is output from the aforementioned workflow), and if successful, it provides an output file in IOH-compatible XML format as a download. Since upload and download of content is provided the site will present the user with a required login screen like this:\nInterested users of OHScribe! should [request credentials via email to digital@grinnell.edu](mailto:digital@grinnell.edu?subject=OHScribe Credentials).\nUploading XML Once authorized, OHScribe! presents the user with a file upload form like so:\nSelecting the Browse... button will open a file-selection window on the local host. The user should select a single .XML transcript file for upload and click the Upload key to send it to the OHScribe! server.\nA successful file upload produces Main/Control screen like this:\nNote the message/status portion of the window just above the Main / Control Screen title. The message here in green print indicates a successful upload.\nTypical/Recommended Use Once an XML file has been successfully uploaded for processing, the user is presented with a six individual or single actions, or a seventh option to Do All of the Above. Users should ALWAYS choose Do All of the Above unless there are special circumstances and they have been instructed otherwise.\nTypical use of OHScribe! follows these steps.\nIn the Upload an XML File screen click the Browse button and navigate to a transcript XML file prepared in and exported from InqScribe. This action opens the selected XML file for processing with the path to the file reflected in the box at the top of the GUI. In the Main / Control Screen window click the Do All of the Above button. This inokes the six actions documented below. If successful it will transform the XML exported from InqScribe into the XML form required for IOH ingest, giving the user an option to download the transformed file. Click the Download button. This action converts \\\u0026lt;start\u0026gt; and \\\u0026lt;end\u0026gt; values from hours:minutes:seconds notation to the decimal seconds notation required for IOH. The changes are saved directly in the selected **IOH-** file. This file should be suitable for ingest into IOH. Single Actions OHScribe! divides the transformation of a transcript into six ordered, individual steps represented by the following single Action choices:\nClean-Up the XML - This action checks that the uploaded file has a .xml extension and subsequently parses the XML to verify its validity.\nTransform XML to IOH - This action transforms the valid XML creating a \u0026lt;cues\u0026gt; and enclosed \u0026lt;cue\u0026gt; tags from \u0026lt;scene\u0026gt; tags.\nConvert hh:mm:ss to Seconds - This action converts all of the in and out time codes from hours-minutes-seconds (hh:mm:ss.s) notation to necessary \u0026lt;start\u0026gt; and \u0026lt;end\u0026gt; tags expressed as the number of seconds measured from the beginning of the transcribed audio or video stream.\nFormat Speaker Tags - This action transforms all of the speaker tags and speaker IDs from the original transcription into IOH-formatted speaker tags.\nAnalyze Cue Times - This final action checks each of the \u0026lt;cue\u0026gt; tag contents against a fixed target of 10 lines. Cues with more than 10 lines of captioning may overwhelm or even overflow the underlying image or video content.\nNote that when a single action is performed the user must take steps to download intermediate results and upload those back to OHScribe! in order to perform the next step. Since this can become rather tedious, the option to Do All of the Above is recommended.\nA single action can be performed by clicking the radio button corresponding to the desired action, and clicking the Do Single Action button near the bottom of the Main / Control Screen as shown below.\nAction Results Results of a Do All of the Above action typically include status output in a box at the top of the window, a Message box explaining the outcome, and a Download your Output! button with instructions. The window typically looks something like this:\nSingle actions produce slightly different results which may also include Details of the output, and Guidance for follow-up actions as shown below.\nErrors Processing errors are generally presented with red text appearing in the status box at the top of the window, like so:\nUnexpected or unresolvable errors encountered in OHScribe! should be [reported to the author via email to digital@grinnell.edu](mailto:digital@grinnell.edu?subject=OHScribe Error) and/or added to the Issue queue at https://github.com/DigitalGrinnell/OHScribe/issues.\nCSS Required for Speaker Formatting To take advantage of the script\u0026rsquo;s \u0026ldquo;speaker formatting\u0026rdquo; capabilities you must add the following CSS, or something very similar, to the theme of the site where Islandora Oral Histories are displayed. This CSS produces coloring and formatting like that shown in the example above.\n/* Color, display and font additions for Oral Histories */ div.tier.active span { font-weight: bold; } div.tier.active span, div.tier.active span span.oh_speaker_text { color: black !important; } div.tier.active span span.oh_speaker_text { font-weight: normal !important; } span.oh_speaker_text { color: #ffff00 !important; /* yellow */ } span.oh_speaker_1 { display: block; color: #00ffff; /* aqua */ } span.oh_speaker_2 { display: block; color: #80ff00; /* bright green */ } span.oh_speaker_3 { display: block; color: #ff0000; /* bright red */ } span.oh_speaker_4 { display: block; color: #ff00ff; /* fuchsia */ } span.oh_speaker_5 { display: block; color: #ffbf00; /* orange */ } The Digital.Grinnell Oral History Workflow InqScribe IOH Transcription Workflow Grinnell College employs the transcription workflow described here when preparing oral histories for ingest into Digital Grinnell. This workflow includes a commercially available software tool called InqScribe and at Grinnell transcribers also frequently use a VEC USB Footpedal to help control playback of audio to be transcribed.\nTraining Video An 11.5 minute long is available to reinforce the concepts presented below.\nThe video moves very quickly, compressing a 2-hour transcription session down into 11.5 minutes. You may find it necessary to slow the playback down, or rewind and repeat portions of the video, using the controls available in your browser.\nWorkflow Description A typical transcription session generally involves the following steps\u0026hellip;\nOHScribe creates a new cue every time it encounters a timecode, so every timecode should be followed immediately by a newline, speaker name and pipe character. For areas of the recording that are dense with speaker changes, no timecode is needed to transition to the next speaker, i.e. the transcriber can represent a change in speaker by entering a newline, the speaker name and pipe character to start the next speaker\u0026rsquo;s dialogue. This will result in a cue that has mutliple speakers.\nInqScribe Snippets and Triggers (Shortcuts) InqScribe allows a transcriber to define and use Snippets, short bits of frequently-repeated text, with associated triggers or keyboard Shortcuts that make it easy to quickly add key elements to a transcript. The following are samples of Snippets and their corresponding Triggers/Shortcuts used in conjunction with our workflow.\nThe above image is an example of a Snippet we refer to as a \u0026lsquo;Speaker Timecode\u0026rsquo;. When triggered, this snippet will insert:\nA timecode, the ${TIME} variable portion of the snippet, and A name identifying the speaker providing the transcribed text that will follow. In this example the speaker\u0026rsquo;s name is \u0026lsquo;Darrell\u0026rsquo; and that name is followed by a REQUIRED space and a pipe character, the vertical bar, in the portion that reads Darrell | . There is a space after the pipe character so as to allow the thranscriber to simply press the triger and then immediately start to type the dialogue into InqScribe. OHScribe does not currently correctly parse a pipe character from other text unless it is surrounded by spaces on either side.\nIf a speaker has a double first name, the name will need to be hyphentated because OHScribe will only identify one word/name, separated from others by spaces, as the trigger name for the speaker. The first name in the \u0026lt;speaker\u0026gt; FirstName LastName \u0026lt;/speaker\u0026gt; tag MUST match the name that follows after the timecode.\nNote also that in this example our Speaker Timecode snippet is named Darrell Fisher and it is assigned to trigger KP1 which has a corresponding keyboard shortcut. The name of the timecode snippet is not important, and can be left vague/general, as with the Interviewer example.\nAny additional speakers can be represented in the same way by selecting \u0026lsquo;Add\u0026rsquo; and then filling in the correct information similarly to the example above.\nEach time a new speaker is introduced, there must be \u0026lt;speaker\u0026gt; FirstName LastName \u0026lt;/speaker\u0026gt; line added between the timecode and the FirstName | . Each speaker should only have one instance of speaker tags in the InqScribe file.\nThe second image, immediately above, is an example of a Snippet we refer to as a \u0026lsquo;Raw Timecode\u0026rsquo;. When triggered, this snippet will insert:\nA timecode, the value of the ${TIME} variable referenced in the snippet, and nothing else. Note that a Raw Timecode has no associated speaker name as it\u0026rsquo;s intended to be used when the speaker name is unknown, or when there isn\u0026rsquo;t time during transcription to pause for identification of the next speaker.\nThis example Raw Timecode snippet is named {$TIME} and it is assigned to the Enter trigger which generally corresponds to the Enter or Return key on the keyboard.\nExport to XML Once the transcription and timecodes are in place, save the InqScribe file and export it to an XML file by selecting File -\u0026gt; Export -\u0026gt; XML.\nAnd that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/077-oral-history-transcription-workflow/","tags":["Digital.Grinnell","Oral History","OHScribe","InqScribe","workflow"],"title":"Oral History Transcription Workflow"},{"categories":null,"contents":"Digital.Grinnell has seen a lot of customization over the years, and quite a bit of it relates to how an object\u0026rsquo;s metadata, especially its MODS metadata, is displayed. My aim for the past couple of years has been to remove as much customization as possible, but I\u0026rsquo;ve found it difficult to remove features of our MODS display because those features seem to be rather popular. This is particulary true of our compound object display.\nCustom Compound/Child MODS Display At present, May 28, 2020, the MODS display for a typical compound/child object in Digital.Grinnell looks something like the example image from grinnell:10365 included here:\nThe display feature demonstrated above is supposed to \u0026ldquo;hide\u0026rdquo; any field in the child object, the first 8 lines of metadata that appear above the \u0026ldquo;Group Record\u0026rdquo;, which are identical to the same field in the parent/compound object, the lines below the \u0026ldquo;Group Record\u0026rdquo; heading. This rule applies to all fields except \u0026ldquo;Title\u0026rdquo; and \u0026ldquo;Supporting Hoste\u0026rdquo;, but if you look closely at the image above you\u0026rsquo;ll see there are some duplicates displayed, specifically:\nRelated Item: Digital Grinnell Language: English Access Condition: Copyright to this work\u0026hellip; Details The process of hiding \u0026ldquo;duplicate\u0026rdquo; metadata from a child/parent object pair is relatively simple, but it involves some XSL transformations (XSLT), so it\u0026rsquo;s inherently messy. The heavy-lifting all takes place in Digital.Grinnell\u0026rsquo;s islandora_mods_display module.\nXSL Transform The aforementioned transform happens in a \u0026ldquo;hook\u0026rdquo; function called islandora_mods_display_preprocess_islandora_mods_display_display that can be found in theme.inc. The .xsl file used in the transform is mods_display.xsl. The \u0026ldquo;merging\u0026rdquo; of child/parent metadata happens inside the aforementioned function.\nHidden Elements Another function, islandora_mods_display_remove_redundant_rows, from theme.inc hides redundant rows by applying a CSS class named hidden to the \u0026lt;tr\u0026gt; elements of the child object MODS that have identical counterparts in the parent\u0026rsquo;s MODS metadata.\nFixing the Displayed Duplicates Found it, and fixed it too! The islandora_mods_display_remove_redundant_rows code in theme.inc added the hidden class to \u0026lt;tr\u0026gt; elements like so:\n$elements[$i] = str_replace(\u0026#39;\u0026lt;tr\u0026gt;\u0026#39;, \u0026#39;\u0026lt;tr class=\u0026#34;hidden\u0026#34;\u0026gt;\u0026#39;, $elements[$i]); The problem with that statement is that some table rows now have xmlns:xlink attributes due to the recent addition of live links in some metadata fields. As a result, those fields that now support live links could not be \u0026ldquo;hidden\u0026rdquo;. The same code now reads like so:\n$elements[$i] = str_replace(\u0026#39;\u0026lt;tr\u0026#39;, \u0026#39;\u0026lt;tr class=\u0026#34;hidden\u0026#34;\u0026#39;, $elements[$i]); Note that the closing carret has been removed from both the search string and the replacement. It works! Yay\u0026#x2757; As a result, the same object metadata shown above now looks like this:\nAnd that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/076-digital.grinnell-custom-mods-display/","tags":["Digital.Grinnell","MODS","metadata","custom","display"],"title":"Digital.Grinnell Custom MODS Display"},{"categories":null,"contents":"This post is a simplified and focused follow-up to Dockerized Traefik Host Using ACME DNS-01 Challenge.\nSimplify Today, 19-May-2020, I\u0026rsquo;m going to take a shot at simplifying my testing on dgdocker3.grinnell.edu by removing unnecessary things and consolidating as much as possible to reduce clutter in the logs and get right to the point. I\u0026rsquo;m also going to have a look to see if there are additional logs that can tell give me more detail. Everything used here, and everything that takes place here, will be found in a new directory, /opt/containers/test on DGDocker3.\nKey Files The key files involved in these tests are presented in subsections here.\n./test/docker-compose.yml version: \u0026#39;3\u0026#39; services: traefik: image: traefik:2.2.1 container_name: traefik hostname: traefik restart: unless-stopped security_opt: - no-new-privileges:true networks: - proxy ports: - 80:80 - 443:443 volumes: - /etc/localtime:/etc/localtime:ro - /var/run/docker.sock:/var/run/docker.sock:ro - ./data/traefik.yml:/traefik.yml:ro - ./data/acme.json:/acme.json labels: - \u0026#34;traefik.enable=true\u0026#34; # next 4 lines...universal http --\u0026gt; https redirect per https://community.containo.us/t/a-global-http-https-redirection/864/3 - \u0026#34;traefik.http.routers.http-catchall.rule=hostregexp(`{host:[a-z-.]+}`)\u0026#34; - \u0026#34;traefik.http.routers.http-catchall.entrypoints=http\u0026#34; - \u0026#34;traefik.http.routers.http-catchall.middlewares=redirect-to-https\u0026#34; - \u0026#34;traefik.http.middlewares.redirect-to-https.redirectscheme.scheme=https\u0026#34; # now the Traefik-specific dashboard stuff - \u0026#34;traefik.http.middlewares.traefik-auth.basicauth.users=admin:$$2y$$05$$pJEzHJBzfoYYS7/hGAedcOP8XdsqNXE7j.LHFBVjueASOqOvvjGOy\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.rule=Host(`${HOST}`) \u0026amp;\u0026amp; (PathPrefix(`/api`) || PathPrefix(`/dashboard`))\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.middlewares=traefik-auth\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.tls=true\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.tls.certresolver=http\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.service=api@internal\u0026#34; landing: image: mcfatem/dgdocker3-landing:latest container_name: landing-page restart: unless-stopped security_opt: - no-new-privileges:true networks: - proxy volumes: - /etc/localtime:/etc/localtime:ro - /var/run/docker.sock:/var/run/docker.sock:ro - ./data:/data labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=proxy\u0026#34; - \u0026#34;traefik.http.routers.landing-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.landing-secure.rule=Host(`${HOST}`)\u0026#34; - \u0026#34;traefik.http.routers.landing-secure.tls=true\u0026#34; - \u0026#34;traefik.http.routers.landing-secure.tls.certresolver=http\u0026#34; - \u0026#34;traefik.http.routers.landing-secure.service=landing-test\u0026#34; - \u0026#34;com.centurylinklabs.watchtower.enable=true\u0026#34; networks: proxy: external: true ./test/destroy.sh #!/bin/bash # rm -f /opt/containers/test/data/acme.json # docker stop $(docker ps -q); docker rm -v $(docker ps -qa); # docker image rm $(docker image ls -q) docker system prune --force # touch /opt/containers/test/data/acme.json chmod 600 /opt/containers/test/data/acme.json ./test/restart.sh #!/bin/bash # docker network create proxy cd /opt/containers/test docker-compose up -d; docker-compose logs # echo \u0026#34;Dumping traefik.log...\u0026#34; docker exec -it traefik cat /var/log/traefik.log ./test/data/traefik.yml (obfuscated) api: dashboard: true entryPoints: http: address: \u0026#34;:80\u0026#34; https: address: \u0026#34;:443\u0026#34; providers: docker: endpoint: \u0026#34;unix:///var/run/docker.sock\u0026#34; exposedByDefault: false ### for HTTP-01 challenge #certificatesResolvers: # http: # acme: # # - Uncomment caServer line below to run on the staging let\u0026#39;s encrypt server. Leave comment to go to prod. # #caServer: https://acme-staging-v02.api.letsencrypt.org/directory # email: digital@grinnell.edu # storage: acme.json # httpChallenge: # entryPoint: http ## for DNS-01 challenge certificatesResolvers: http: acme: # - Uncomment caServer line below to run on the staging Let\u0026#39;s Encrypt server. Leave comment to go to prod. caServer: https://acme-staging-v02.api.letsencrypt.org/directory email: digital@grinnell.edu storage: acme.json dnsChallenge: provider: azure delayBeforeCheck: 0 environment: AZURE_CLIENT_ID: \u0026#34;c537...d99\u0026#34; AZURE_CLIENT_SECRET: \u0026#34;6a2...154\u0026#34; AZURE_SUBSCRIPTION_ID: \u0026#34;a55...c4f\u0026#34; AZURE_TENANT_ID: \u0026#34;524...807\u0026#34; AZURE_DNS_ZONE: \u0026#34;lev...nfo\u0026#34; AZURE_RESOURCE_GROUP: \u0026#34;Net...ces\u0026#34; # #AZURE_CLIENT_ID: \u0026#34;${AZURE_CLIENT_ID}\u0026#34; #AZURE_CLIENT_SECRET: \u0026#34;${AZURE_CLIENT_SECRET}\u0026#34; #AZURE_SUBSCRIPTION_ID: \u0026#34;${AZURE_SUBSCRIPTION_ID}\u0026#34; #AZURE_TENANT_ID: \u0026#34;${AZURE_TENANT_ID}\u0026#34; #AZURE_DNS_ZONE: \u0026#34;${AZURE_DNS_ZONE}\u0026#34; #AZURE_RESOURCE_GROUP: \u0026#34;${AZURE_RESOURCE_GROUP}\u0026#34; log: level: DEBUG filePath: \u0026#34;/var/log/traefik.log\u0026#34; ./test/data/config.yml http: middlewares: https-redirect: redirectScheme: scheme: https Initial Test - Failed My initial test was a simplified repeat of Test 7. The result was much like I documented in that previous test, with no certs, and no indication of problems other than the mysterious \u0026ldquo;TLS handshake error\u0026rdquo; that I reported before. So, time to make some changes and see what happens.\nChanging the Name of the CertResolver In all my previous tests there are lots of instances of \u0026ldquo;http\u0026rdquo;, and most notably, it\u0026rsquo;s the name given to the certresolver regardless if this is an HTTP-01 or DNS-01 challenge. Since my simplified tests all focus on DNS-01 I\u0026rsquo;m changing that certresolver name to \u0026ldquo;dns\u0026rdquo;.\nThis change was made in ./test/docker-compose.yml lines 22 and 43, and line 30 of ./test/data/traefik.yml. No other files or lines were modified.\nTest S1 The \u0026ldquo;S\u0026rdquo; in \u0026ldquo;S1\u0026rdquo; distinguishes this as a \u0026ldquo;Simplified\u0026rdquo; test. To run this test I executed the following, as root, on dgdocker3.grinnell.edu:\ncd /opt/containers/test ./destroy.sh ./restart.sh grep Certificates data/acme.json The outcome is the same as before, both the Traefik dashboard (https://dgdocker3.grinnell.edu/dashboard/) and landing page (https://dgdocker3.grinnell.edu/) are working, but without valid certs so browser security exceptions are required. The output from this test can be found in this gist.\nAnd that\u0026rsquo;s a good place to break, but I\u0026rsquo;ll be baaaaak. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/074-simplified-testing-traefik-2-with-acme-dns-01/","tags":["Docker","Traefik","ACME","DNS-01","docker-traefik2-host","docker-compose"],"title":"Simplified Testing of Traefik 2 with ACME DNS-01 Challenge"},{"categories":null,"contents":"My memory isn\u0026rsquo;t what it used to be, but I have this blog. \u0026#x1f604; And on the handful of CentOS and Ubuntu servers that I maintain, I have my server-info script, my replacement for motd. \u0026#x1f601;\nRequirements: mdv and figlet The server-info script/command relies on a pair of utilities, namely mdv and figlet.\nThe server-info Script You\u0026rsquo;ll find the script itself in this gist. Sample installation of the script is documented below, and the content template can be found in this gist.\nSample Installation on DGDocker3, a CentOS 7 Host DGdocker3.Grinnell.edu is a CentOS 7 / Docker server that I use for development and testing at Grinnell College. This is what I did on DGDocker3 to install my server-info command:\nsudo yum update sudo yum install epel-release sudo yum install figlet sudo yum install python-pip sudo pip install --upgrade pip sudo pip install mdv Then, to \u0026ldquo;install\u0026rdquo; the initial script and content template\u0026hellip;\nsudo su cd /usr/local/bin curl https://gist.githubusercontent.com/McFateM/8a81e74be780697cf8ab6e63a707052f/raw \u0026gt; server-info chmod 755 server-info cd /etc curl https://gist.githubusercontent.com/McFateM/98a3247a388b826a16c7f985e1d0351c/raw \u0026gt; server-info.md Sample Installation in Ubuntu mcfate.family is a domain I currently use for testing, and right now (May 2020) it\u0026rsquo;s pointed at an Ubuntu 18.04 LTS droplet at DigitalOcean. This is what I did on that host, as root, to install my server-info command:\napt-get update apt-get install figlet apt install python-pip pip install mdv Then, to \u0026ldquo;install\u0026rdquo; the initial script and content template, again as root\u0026hellip;\ncd /usr/local/bin curl https://gist.githubusercontent.com/McFateM/8a81e74be780697cf8ab6e63a707052f/raw \u0026gt; server-info chmod 755 server-info cd /etc curl https://gist.githubusercontent.com/McFateM/98a3247a388b826a16c7f985e1d0351c/raw \u0026gt; server-info.md Use So, how\u0026rsquo;s it used? Simple, just enter server-info at any command prompt on the host.\nUpdating the Maintenance Information That\u0026rsquo;s easy too, just use sudo nano /etc/server-info.md and apply your changes/additions, then save the file.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip; \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/072-enhancing-motd-with-server-info/","tags":["motd","server-info","mdv","figlet"],"title":"Enhancing 'motd' with 'server-info'"},{"categories":null,"contents":"This post builds on My dockerized-server Config and attempts to change what was a problematic ACME HTTP-01 or httpChallenge in Traefik and Let\u0026rsquo;s Encrypt to an ACME DNS-01 or dnsChallenge. The problem with the old HTTP-01 or httpChallenge is that it requires the creation of a valid and widely accessible \u0026ldquo;A\u0026rdquo; record in our DNS before the creation of a cert; the record has to be in place so that the Let\u0026rsquo;s Encrypt CA-server can find it to confirm that the request is valid. However, doing this puts the cart-before-the-horse, so-to-speak, since we like to have a valid cert in place before we add a new DNS record.\nJust like my old dockerized-server configuration, this project revolves around a workflow that will setup a \u0026ldquo;Dockerized\u0026rdquo; server complete with Traefik, Portainer, and Who Am I. Like its predecessor, it should be relatively easy to add additional services or application stacks to any server that is initially configured using this package. For \u0026ldquo;static\u0026rdquo; servers have a look at my docker-bootstrap Workflow for an example.\nAll of my associated research and testing for this issue can be found in a OneTab at https://www.one-tab.com/page/9E_29YLjSGa9iAeckxMbIQ To overcome the HTTP-01 challenge issue mentioned above, a colleague of mine at Grinnell College suggested we move to a DNS-01 challenge, and formulated a propsal to do so.\nDNS-01 Proposal My colleague\u0026rsquo;s proposal reads like this:\nThe proposed design uses CNAME following so that TXT records can be created for the grinnell.edu domain in a custom (non-grinnell.edu) domain. On your side, the initial steps will be similar to what we do now. When you need a new host record, a ticket should be created requesting example.grinnell.edu, and with a note that you will need Let’s Encrypt verification. In order for CNAME following to work, a CNAME in the college’s external DNS must first be created. This record will follow the format of _acme-challenge.example.grinnell.edu, and will point to a custom domain (le-verify.info or something similar). We will register this custom domain name with Azure DNS and utilize a service principal account in Azure that will have permission to create TXT records in that custom domain. We will then give you a key to that service principal account so that you can configure Traefik to create the TXT records automatically as a part of the Let’s Encrypt verification process. When Let’s Encrypt goes to find the _acme-challenge.example.grinnell.edu record, it will be forwarded to the custom domain, see the TXT record, and then approve and sign the certificate for example.grinnell.edu. I have tested this using an NGINX ingress controller, but the documentation for Traefik shows that it supports the same kind of configuration. ... Here is some documentation that may explain things better than I have: CNAME Following https://letsencrypt.org/2019/10/09/onboarding-your-customers-with-lets-encrypt-and-acme.html An Example Ingress Controller’s Implementation of DNS verification: https://docs.traefik.io/https/acme/#dnschallenge April: DNS-01 Troubles When attempting to implement the proposal outlined above we got back some odd errors. My record of the result can be found in this Gist.\nMay: Moving to Traefik v2 All of the above material was generated using My dockerized-server Config running Traefik version 1.x. Since the Traefik community has moved on it seemed prudent to try upgrading the server to Traefik v2.x before posting a lot of debug info involving the previous version. So, that\u0026rsquo;s what I did, upgrade to Traefik 2.2.1.\ndocker-traefik2-host Our move to Traefik v2.2.1 is captured in a new Dockerized-server configuration I call docker-traefik2-host. The key to obtaining certs in docker-traefik2-host lies in the ./traefik/data/traefik.yml file and corresponding .env file which is NOT stored in GitHub. traefik.yml inlcudes a section of configuration like this:\n# ## for HTTP-01 challeng # certificatesResolvers: # http: # acme: # # - Uncomment caServer line below to run on the staging let\u0026#39;s encrypt server. Leave comment to go to prod. # caServer: https://acme-staging-v02.api.letsencrypt.org/directory # email: digital@grinnell.edu # storage: acme.json # httpChallenge: # entryPoint: http ## for DNS-01 challenge certificatesResolvers: http: acme: # - Uncomment caServer line below to run on the staging Let\u0026#39;s Encrypt server. Leave comment to go to prod. #caServer: https://acme-staging-v02.api.letsencrypt.org/directory email: digital@grinnell.edu storage: acme.json dnsChallenge: provider: azure The above configuration is intended to implement either an HTTP-01 or DNS-01 challenge, but never both. In the above example the host is being configured to use a DNS-01 challenge, and it uses the Let\u0026rsquo;s Encrypt production server since the \u0026ldquo;caServer\u0026rdquo; declaration of \u0026ldquo;staging\u0026rdquo; is commented out.\nFailure on Static.Grinnell.edu Unfortunately, the configuration shown above, when applied to the static.grinnell.edu host, failed even after being tweaked and tested several times. Along the way I eventually ran into Let\u0026rsquo;s Encrypt\u0026rsquo;s rate limit and got shut out of further testing for one week. During that week I attempted to implement this configuration on a different host, namely dgdocker3.grinnell.edu, where I encountered different failures.\nTesting on DGDocker3.Grinnell.edu DGDocker3.Grinnell.edu is a CentOS 7.8 host running Docker with my docker-traefik2-host resident in /opt/containers. It sits behind the college firewall so VPN access is required, and it\u0026rsquo;s configured to provide the following services:\nStack and Service Details Address landing-landing Dockerized Hugo static site https://dgdocker3.grinnell.edu/ traefik Traefik v2.2.1 with dashboard https://dgdocker3.grinnell.edu/dashboard/ portainer Portainer v1.23.2 dashboard https://dgdocker3.grinnell.edu/portainer/ Each service has its own subdirectory and docker-compose.yml file located there. All can be found in docker-traefik2-host. The all-important ./traefik/data/traefik.yml is also there.\nScripts The docker-traefik2-host project also features a pair of scripts to help facilitate testing. They are:\nScript Purpose destroy.sh Stops and removes all running containers, images and networks. Destroys the ./traefik/data/acme.json file and restores it to pristine condition. restart.sh Restarts all the services with verbose (--debug) logs echoed from Traefik\u0026rsquo;s /var/log/traefik.log file. Test 1 - HTTP-01 Challenge Using LE\u0026rsquo;s Staging Server My first test will attempt a clean restart of all services using LE\u0026rsquo;s staging CA-server and HTTP-01 challenge. The \u0026lsquo;./traefik/data/traefik.yml\u0026rsquo; file for this test is reflected in this gist.\nI initiated this test as root using:\ncd /opt/containers ./destroy.sh ./restart.sh The result of this test shows all three services are working and are reachable via VPN at the addresses listed above, but none have valid certs so they all require an exception. This is to be expected when using LE\u0026rsquo;s \u0026ldquo;staging\u0026rdquo; CA-server, but it seems there is more to this outcome since some errors are present.\nThe log and resulting acme.json from this test can be seen in this gist, and the first errors encountered state that:\ntime=\u0026#34;2020-05-17T13:09:14-04:00\u0026#34; level=debug msg=\u0026#34;http: TLS handshake error from 132.161.249.251:51447: remote error: tls: bad certificate\u0026#34; time=\u0026#34;2020-05-17T13:09:14-04:00\u0026#34; level=debug msg=\u0026#34;http: TLS handshake error from 132.161.249.251:51448: remote error: tls: bad certificate\u0026#34; Since this test appears to have failed \u0026ldquo;unexpectedly\u0026rdquo;, I\u0026rsquo;m going forego the next test that would attempt the same but using LE\u0026rsquo;s \u0026ldquo;production\u0026rdquo; CA-server, and proceed straight to DNS-01 testing. Test 2 - DNS-01 Challenge Using LE\u0026rsquo;s Staging Server My next test will attempt a clean restart of all services using LE\u0026rsquo;s staging CA-server and DNS-01 challenge. The \u0026lsquo;./traefik/data/traefik.yml\u0026rsquo; file for this test is reflected in this gist.\nI initiated this test as root using:\ncd /opt/containers ./destroy.sh ./restart.sh The result of this test shows all three services are working and are reachable via VPN at the addresses listed above, but none have valid certs so they all require an exception. This is to be expected when using LE\u0026rsquo;s \u0026ldquo;staging\u0026rdquo; CA-server, but it seems there is more to this outcome since some errors are present.\nThe log from this test can be seen in [this gist](The result of this test shows all three services are working and are reachable via VPN at the addresses listed above, but none have valid certs so they all require an exception. This is to be expected when using LE\u0026rsquo;s \u0026ldquo;staging\u0026rdquo; CA-server, but it seems there is more to this outcome since some errors are present.\nThe log from this test can be seen in this gist, and the first error encountered states that:\ntime=\u0026#34;2020-05-17T13:43:01-04:00\u0026#34; level=debug msg=\u0026#34;No ACME certificate generation required for domains [\\\u0026#34;dgdocker3.grinnell.edu\\\u0026#34;].\u0026#34; providerName=http.acme routerName=traefik-secure@docker rule=\u0026#34;Host(`dgdocker3.grinnell.edu`) \u0026amp;\u0026amp; (PathPrefix(`/api`) || PathPrefix(`/dashboard`))\u0026#34; time=\u0026#34;2020-05-17T13:43:01-04:00\u0026#34; level=debug msg=\u0026#34;http: TLS handshake error from 132.161.249.251:52136: remote error: tls: bad certificate\u0026#34; Since this test appears to have failed in the same \u0026ldquo;unexpected\u0026rdquo; manner as Test 1, I\u0026rsquo;m going forego subsequent tests until this can be resolved. Returning to Static.Grinnell.edu Since more than a week has passed since I hit LE\u0026rsquo;s rate limit, I thought that this evening I\u0026rsquo;d try my luck with static.grinnell.edu again, this time with an HTTP-01 challenge and LE\u0026rsquo;s production server. It worked, except that some of my stack_service names were incorrect. What I really wanted to learn from this is what an acme.json file should look like when valid certs have been created. The answer can be found in this gist.\nNote that I was ultimately able to get all the services on static.grinnell.edu working properly, with valid certs, by stopping (see below) those containers that had incorrect names, fixing the router\u0026rsquo;s service name in each corresponding docker-compose.yml, then doing a new docker-compose up -d to restart things. No additional cert validation or modification of Traefik was needed.\nStopping containers\u0026hellip; docker stop [id]; docker rm -v [id] Correct service-stack names\u0026hellip; the correct name convention is service-stack where service is the name of the service, not the container name, and stack is the name of the sub-directory Test 3 - Static to DNS-01 Production This morning, May 18, 2020, I switched the configuration on static.grinnell.edu back to DNS-01 using LE\u0026rsquo;s production CA-server, and tried again. The log from this test as well as an obfuscated acme.json can be seen in this gist.\nThe sites all appear to work, except for the landing page at https://static.grinnell.edu/ which returns a 404, but none have valid certs and therefore require browser security exceptions again. I found the landing page problem in restart.sh, fixed it, and did a docker-compose up -d in ./landing to get that page running.\nThe first error encountered in the log is at line 243 and it reads:\ntime=\u0026ldquo;2020-05-18T09:21:35-05:00\u0026rdquo; level=error msg=\u0026ldquo;Unable to obtain ACME certificate for domains \u0026quot;static.grinnell.edu\u0026quot;: cannot get ACME client azure: Get \u0026quot;http://169.254.169.254/metadata/instance/compute/subscriptionId?api-version=2017-12-01\u0026amp;format=text\u0026quot;: dial tcp 169.254.169.254:80: i/o timeout\u0026rdquo; providerName=http.acme routerName=traefik-secure@docker rule=\u0026ldquo;Host(static.grinnell.edu) \u0026amp;\u0026amp; (PathPrefix(/api) || PathPrefix(/dashboard))\u0026rdquo;\nTest 4 - Replace Invalid Certs with Valid This test is really a remediation step as in it I\u0026rsquo;ll replace the now \u0026ldquo;broken\u0026rdquo; ./traefik/data/acme.json file with the working copy (./traefik/data/http-01-acme.json) that was created in \u0026ldquo;Returning to Static.Grinnell.edu\u0026rdquo; above. I do this on the static.grinnell.edu host as root like so:\ncd /opt/containers/traefik/data rm -f acme.json cp -f http-01-acme.json acme.json Unfortunately, the old certs don\u0026rsquo;t match the new services so they are invalid and the sites all require browser security exceptions again.\nTest 5 - Static Returned to Staging with DNS-01 In this test I\u0026rsquo;ve moved the values of our Azure .env variables directly into traefik.yml and I have prudently switched the process back to using LE\u0026rsquo;s staging server. The complete log and obfuscated acme.json are in this gist.\nThe log contains a series of errors like the one below, and all seven sites are up and running but with browser security exceptions still required.\ntime=\u0026ldquo;2020-05-18T11:58:02-05:00\u0026rdquo; level=debug msg=\u0026ldquo;Serving default certificate for request: \u0026quot;static.grinnell.edu\u0026quot;\u0026rdquo; time=\u0026ldquo;2020-05-18T11:58:02-05:00\u0026rdquo; level=debug msg=\u0026ldquo;http: TLS handshake error from 132.161.249.72:57914: remote error: tls: bad certificate\u0026rdquo;\nTest 6 - Static Returned to Production with HTTP-01 I need to put static.grinnell.edu back to work and return to testing on dgdocker3.grinnell.edu, so this test will return static to using the LE production server and HTTP-01 challenge. The complete log and obfuscated acme.json are in this gist.\nAll seven sites are working, and have valid certs. That\u0026rsquo;s my cue to move back to dgdocker3.grinnell.edu.\nNote that even with working sites and new, valid certs I still see a series of errors like this one:\ntime=\u0026ldquo;2020-05-18T12:52:18-05:00\u0026rdquo; level=debug msg=\u0026ldquo;Serving default certificate for request: \u0026quot;static.grinnell.edu\u0026quot;\u0026rdquo; time=\u0026ldquo;2020-05-18T12:52:18-05:00\u0026rdquo; level=debug msg=\u0026ldquo;http: TLS handshake error from 132.161.249.72:60953: remote error: tls: bad certificate\u0026rdquo;\nSo, apparently those errors have nothing to do with the challenge? The plot thickens.\nBack to DGDocker3 The debug messages (not errors) like TLS handshake error from 132.161.249.72:60953: remote error: tls: bad certificate seem to be present in every test I\u0026rsquo;ve run, even when valid certs are issued. So it seems safe to assume they are not critical. To try and work around them I\u0026rsquo;m going to return my testing to dgdocker3.grinnell.edu and start anew there with Test 7.\nTest 7 - DGDocker3 Test with Staging and DNS-01 This test will reset dgdocker3.grinnell.edu using LE\u0026rsquo;s staging server and our DNS-01 challenge just as it was configured in Test 6 above. The complete log and obfuscated acme.json are in this gist.\nThere were no errors or warnings in the log, and all three sites are working without valid certs, therefore all require browser security exceptions, but that is to be expected since the LE staging server was used. The mysterious \u0026ldquo;TLS handshake error\u0026rdquo; debug messages do still appear. In light of this, my next test will use the same configuration, but switched back to DNS-01.\nTest 8 - DGDocker3 Test with Production and DNS-01 This test will reset dgdocker3.grinnell.edu using LE\u0026rsquo;s production server and our DNS-01 challenge just as it was configured in Test 7 above. The complete log and obfuscated acme.json are in this gist.\nAgain, there were no errors or warnings in the log, and all three sites are working, but they still have no valid certs, therefore all require browser security exceptions. Again, the \u0026ldquo;TLS handshake error\u0026rdquo; messages are still present. What are those meant to tell us?\nAnd that\u0026rsquo;s a good place to break\u0026hellip; because I\u0026rsquo;m exhausted and can\u0026rsquo;t imagine what to try next. Too many questions here, not enough answsers. \u0026#x1f626;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/071-dockerized-traefik-using-acme-dns-01/","tags":["Docker","Traefik","Portainer","Watchtower","dockerized-server","traefik.frontend.rule","ACME","DNS-01","HTTP-01","docker-traefik2-host","docker-compose","service-stack"],"title":"Dockerized Traefik Host Using ACME DNS-01 Challenge"},{"categories":null,"contents":" Attention: On 21-May-2020 an optional, but recommended, sixth step was added to this workflow in the form of a new Drush command: islandora_mods_post_processing, an addition to my previous work in islandora_mods_via_twig. See my new post, Islandora MODS Post Processing for complete details.\nA 5-Step Workflow This document is follow-up, with technical details, to Exporting, Editing, \u0026amp; Replacing MODS Datastreams, post 069, in my blog. In case you missed it, the aforementioned post was written specifically for metadata editors working on the 2020 Grinnell College Libraries review of Digital Grinnell MODS metadata.\nAttention: This document uses a shorthand ./ in place of the frequently referenced //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/ directory. For example, ./social-justice is equivalent to the Social Justice collection sub-directory at //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/social-justice.\nBriefly, the five steps in this workflow are:\nExport of all grinnell:* MODS datastreams using drush islandora_datastream_export. This step, last performed on April 14, 2020, was responsible for creating all of the grinnell_\u0026lt;PID\u0026gt;_MODS.xml exports found in ./\u0026lt;collection-PID\u0026gt;.\nExecute my Map-MODS-to-MASTER Python 3 script on iMac MA8660 to create a mods.tsv file for each collection, along with associated grinnell_\u0026lt;PID\u0026gt;_MODS.log and grinnell_\u0026lt;PID\u0026gt;_MODS.remainder files for each object. The resultant ./\u0026lt;collection-PID\u0026gt;/mods.tsv files are tab-seperated-value (.tsv) files, and they are key to this process.\nEdit the MODS .tsv files. Refer Exporting, Editing, \u0026amp; Replacing MODS Datastreams for details and guidance.\nUse drush islandora_mods_via_twig in each ready-for-update collection to generate new .xml MODS datastream files. For a specified collection, this command will find and read the ./\u0026lt;collection-PID\u0026gt;/mods-imvt.tsv and create one ./\u0026lt;collection-PID\u0026gt;/ready-for-datastream-replace/grinnell_\u0026lt;PID\u0026gt;_MODS.xml file for each object.\nExecute the drush islandora_datastream_replace command once for each collection. This command will process each ./\u0026lt;collection-PID\u0026gt;/ready-for-datastream-replace/grinnell_\u0026lt;PID\u0026gt;_MODS.xml file and replace the corresponding object\u0026rsquo;s MODS datastream with the contents of the .xml file. The digital_grinnell branch version of the islandora_datastream_replace command also performs an implicit update of the object\u0026rsquo;s \u0026ldquo;Title\u0026rdquo;, a transform of the new MODS to DC (Dublin Core), and a re-indexing of the new metadata in Solr.\nThe remainder of this document provides technical details, frequently in the form of command lines used to build and use the aforementioned tools.\nStep 1a - Installation of Drush islandora_datastream_export and islandora_datastream_replace Commands To help implement this process efficiently and effectively I first turned to Exporting, Editing, \u0026amp; Replacing MODS Datastreams, a workflow developed by the good folks at The California Historical Society. I initiated the workflow by installing two Drush tools on my local/development instance of ISLE on my Mac workstation.\nThe command line process in my local host/workstation terminal looked like this:\nApache=isle-apache-ld docker exec -w /var/www/html/sites/all/modules/islandora/ ${Apache} git clone https://github.com/Islandora-Labs/islandora_datastream_exporter.git --recursive docker exec -w /var/www/html/sites/all/modules/islandora/ ${Apache} git clone https://github.com/pc37utn/islandora_datastream_replace.git --recursive docker exec -w /var/www/html/sites/all/modules/islandora/ ${Apache} chown -R islandora:www-data * docker exec -w /var/www/html/sites/default ${Apache} drush en islandora_datastream_exporter islandora_datastream_replace -y docker exec -w /var/www/html/sites/default ${Apache} drush cc drush -y Local tests of these commands were successful so I proceeded to install them in the production instance of Digital Grinnell at dgdocker1.grinnell.edu. Before doing that I needed to change the definition of Apache to reflect the production instance of our Apache container, like so Apache=isle-apache-dg.\nCreated a Fork of Islandora Datastream Replace I also chose to \u0026ldquo;fork\u0026rdquo; the islandora_datastream_replace project so that I could do a little Digital.Grinnell customization of it. The fork I\u0026rsquo;m working with is here and my work is limited to the digital_grinnell branch of that fork.\nIn the digital_grinnell branch I modified the behavior of the islandora_datastream_replace command so that it implicitly performs an UpdateFromMODS operation that lives in our idu, or Islandora Drush Utilities module. The UpdateFromMODS, performed immediately after each datastream replace operation does the following:\nUpdates the object \u0026ldquo;Title\u0026rdquo;, one of its properties, to match the new value of /mods:mods/mods:titleInfo[not(@type)]/mods:title. Invokes the iduF DCTransform operation which runs the default XSLT transform of the new MODS to DC (Dublin Core) and creates a new \u0026ldquo;DC\u0026rdquo; datastream for the object. The iduF DCTransform operation also concludes with an implicit iduF IndexSolr operation to ensure that the new object metadata is properly indexed in Solr. Step 1b - Installation of Drush islandora_datastream_export and islandora_datastream_replace Commands in Production To install the commands in production I opened a terminal to dgdocker1.grinnell.edu as user islandora and executed the following commands there:\nApache=isle-apache-dg docker exec -w /var/www/html/sites/all/modules/islandora/ ${Apache} git clone https://github.com/Islandora-Labs/islandora_datastream_exporter.git --recursive docker exec -w /var/www/html/sites/all/modules/islandora/ ${Apache} git clone https://github.com/DigitalGrinnell/islandora_datastream_replace.git --recursive docker exec -w /var/www/html/sites/all/modules/islandora/ ${Apache} chown -R islandora:www-data * docker exec -w /var/www/html/sites/all/modules/islandora/islandora_datastream_replace ${Apache} git checkout -b digital_grinnell docker exec -w /var/www/html/sites/default ${Apache} drush en islandora_datastream_exporter islandora_datastream_replace -y docker exec -w /var/www/html/sites/default ${Apache} drush cc drush -y Step 1c - Mounting //STORAGE to DGDocker1 Attention! This step, and some that come later, will require that the network storage path //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1 be accessible to our production instance of Digital.Grinnell. To make that possible I had to run this sequence on DGDocker1:\ndocker exec -it isle-apache-dg bash mount -t cifs -o username=mcfatem /storage.grinnell.edu/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1 /mnt/metadata-review /mnt/metadata-review\nStep 1d - Using Drush islandora_datastream_export Unfortunately, the islandora_datastream_export results in my local test were woefully incomplete\u0026hellip; NONE of the child objects with a compound parent were exported. I\u0026rsquo;m still not entirely sure why child obejcts were omitted since the query I used should have captured all objects. In testing I did find that this seems to be a flaw in the islandora_datastream_export command, and specifically in its implementation of any Solr query.\nFortunately, the aforementioned command also has a SPARQL query option, and after some trial-and-error I got it to work properly. To do so I created an export.sh bash script, shown below, and used it on dgdocker1.grinnell.edu like so:\ndocker exec -it isle-apache-dg bash source export.sh The export.sh script is:\nApache=isle-apache-dg Target=/utility-scripts # wget https://gist.github.com/McFateM/5bd7e5b0fa5d2928b2799d039a4c0fab/raw/collections.list while read collection do cp -f ri-query.txt query.sparql sed -i \u0026#39;s|COLLECTION|\u0026#39;${collection}\u0026#39;|g\u0026#39; query.sparql docker cp query.sparql ${Apache}:${Target}/${collection}.sparql rm -f query.sparql q=${Target}/${collection}.sparql echo Processing collection \u0026#39;${collection}\u0026#39;; Query is \u0026#39;${q}\u0026#39;... docker exec -w ${Target} ${Apache} mkdir -p /mnt/metadata-review/${collection} docker exec -w /var/www/html/sites/default/ ${Apache} drush -u 1 islandora_datastream_export --export_target=/mnt/metadata-review/${collection} --query=${q} --query_type=islandora_datastream_exporter_ri_query --dsid=MODS done \u0026lt; collections.list In the case of the Digital Grinnell social-justice collection, for example, this script produced 32 .xml files, the correct number. Each collection\u0026rsquo;s set of exported .xml files can be found in the collection-specific subdirectory of //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/ and all have filenames of the form: grinnell_\u0026lt;PID\u0026gt;_MODS.xml. Note that objects which have no MODS datastream were not exported.\nStep 2 - Map-MODS-to-MASTER Python 3 Script The Map-MODS-to-MASTER script was developed, in Python 3, on iMac MA8660 at ~/GitHub/Map-MODS-to-MASTER to facilitate generation of mods.tsv and accompanying .log files for each Digital Grinnell collection from the .xml files found in subdirectories of //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/.\nThe Map-MODS-to-MASTER project can be found in the master branch of https://github.com/DigitalGrinnell/Map-MODS-to-MASTER. I choose to execute it using PyCharm from iMac MA8660 since the directory holding all of the .xml files and folders is already mapped to /Volumes/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1 on that iMac. Note that this //STORAGE location was choosen because the ./ALLSTAFF directory, and its subordinates, are accessible to all staff in the Grinnell College Libraries.\nIt should not be necessary to run this script ever again\u0026hellip;NEVER. However, if it becomes necessary to look back at this code and process, details can be found in Map-MODS-to-MASTER. Note: If it should ever become necessary to repeat the Map-MODS-to-MASTER process it might be wise to look at replacing the Python 3 script with a new Drush command, maybe islandora_map_mods_to_master, written in PHP and installed directly into the production instance of Digital.Grinnell.\nStep 3 - Editing the MODS .tsv Files Please refer to Refer to Exporting, Editing, \u0026amp; Replacing MODS Datastreams, post 069 in my blog, for details and guidance.\nStep 4 - Run drush islandora_mods_via_twig As each individual collection mods-imvt.tsv file is made ready-for-update, it will be necessary to run a drush islandora_mods_via_twig command to process the .tsv data. Running --help with that command produces:\n[islandora@dgdocker1 ~]$ docker exec -it isle-apache-dg bash root@122092fe8182:/# cd /var/www/html/sites/default/ root@122092fe8182:/var/www/html/sites/default# drush -u 1 islandora_mods_via_twig --help Generate MODS .xml files from the mods-imvt.tsv file for a specified collection. Examples: drush -u 1 islandora_mods_via_twig social-justice Process ../social-justice/mods-imvt.tsv, for example. Arguments: collection The name of the collection to be processed. Defaults to \u0026#34;social-justice\u0026#34;. Aliases: imvt So, my command sequence to run islandora_mods_via_twig for the \u0026ldquo;Social Justice\u0026rdquo; collection, as an example, was:\n[islandora@dgdocker1 ~]$ docker exec -it isle-apache-dg bash root@122092fe8182:/# cd /var/www/html/sites/default/ root@122092fe8182:/var/www/html/sites/default# drush -u 1 islandora_mods_via_twig social-justice When the islandora_mods_via_twig command is run, it processes the corresponding //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/\u0026lt;collection-PID\u0026gt;/mods-imvt.tsv file and creates one //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/\u0026lt;collection-PID\u0026gt;/ready-for-datastream-replace/grinnell_\u0026lt;PID\u0026gt;_MODS.xml file for each object.\nStep 5 - Run drush islandora_datastream_replace The whole point of this entire process is to get us back to this point with a set of reviewed and modified .xml files in a //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/\u0026lt;collection-PID\u0026gt;/ready-for-datastream-replace/ collection-specific subdirectory so that we can replace existing object MODS datastreams with new data, and we use the drush islandora_datastream_replace command to do this.\nRunning --help for the aformentioned command produced this:\nroot@122092fe8182:/var/www/html/sites/default# drush -u 1 islandora_datastream_replace --help Replaces a datastream in all objects given a file list in a directory. Examples: drush -u 1 islandora_datastream_replace --source=/mnt/metadata-review/social-justice/ready-for-datastream-replace --dsid=MODS --namespace=grinnell Replacing MODS datastream for objects in --source using the digital_grinnell branch of code. Options: --dsid The datastream id of the datastream. Required. --namespace The namespace of the pids. Required. --source The directory to get the datastreams and pid# from. Required. Aliases: idre It\u0026rsquo;s worth noting that this command looks for any files named MODS in whatever ABSOLUTE directory is named with the --source parameter. The command shown below was executed inside the Apache container, isle-apache-dg, on node DGDocker1, in order to process Digital Grinnell\u0026rsquo;s social-justice collection.\nroot@122092fe8182:drush -u 1 islandora_datastream_replace --source=/mnt/metadata-review/social-justice/ready-for-datastream-replace --dsid=MODS --namespace=grinnell The same command could have been executed directly from node DGDocker1 like so:\ndocker exec isle-apache-dg drush -u 1 -w /var/www/html/sites/default drush -u 1 islandora_datastream_replace --source=mnt/metadata-review/social-justice/ready-for-datastream-replace --dsid=MODS --namespace=grinnell And that\u0026rsquo;s a wrap. Until next time, stay safe and wash your hands! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/070-exporting-editing-replacing-mods-datastreams-technical-details/","tags":["MODS","Export","Replace","Twig","Islandora Multi-Importer","islandora_datastream_export","islandora_datastream_replace","islandora_mods_via_twig","Map-MODS-to-MASTER","islandora_mods_post_processing"],"title":"Exporting, Editing, \u0026 Replacing MODS Datastreams: Technical Details"},{"categories":null,"contents":" Attention: On 21-May-2020 an optional, but recommended, sixth step was added to this workflow in the form of a new Drush command: islandora_mods_post_processing, an addition to my previous work in islandora_mods_via_twig. See my new post, Islandora MODS Post Processing for complete details.\nThe transition to distance learning, social distancing, and more remote work at Grinnell College in the wake of the COVID-19 pandemic may afford GC Libraries an opportunity to do some overdue and necessary metadata cleaning in Digital.Grinnell.\nA 5-Step Workflow This turned out to be a much more difficult undertaking than I imagined, but as of mid-April, 2020, I have a 5-step workflow that actually works. This post will introduce all five steps, but only provides details for Step 3, Editing a MODS TSV File, the portion that metadata editors need to be most aware of. All technical details, as well as steps 1, 2, 4 and 5, will be addressed in Exporting, Editing, \u0026amp; Replacing MODS Datastreams: Technical Details.\nAttention: This document uses a shorthand ./ in place of the frequently referenced //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/ directory. For example, ./social-justice is equivalent to the Social Justice collection sub-directory at //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/social-justice.\nThe five steps are:\nExport of all grinnell:* MODS datastreams using drush islandora_datastream_export. This step, last performed on April 14, 2020, was responsible for creating all of the grinnell_\u0026lt;PID\u0026gt;_MODS.xml exports found in ./\u0026lt;collection-PID\u0026gt;.\nExecute my Map-MODS-to-MASTER Python 3 script on iMac MA8660 to create a mods.tsv file for each collection, along with associated grinnell_\u0026lt;PID\u0026gt;_MODS.log and grinnell_\u0026lt;PID\u0026gt;_MODS.remainder files for each object. The resultant ./\u0026lt;collection-PID\u0026gt;/mods.tsv files are tab-seperated-value (.tsv) files, and they are key to this process.\nEdit the MODS .tsv files. Refer to the dedicated section below for details and guidance.\nUse drush islandora_mods_via_twig in each ready-for-update collection to generate new .xml MODS datastream files. For a specified collection, this command will find and read the ./\u0026lt;collection-PID\u0026gt;/mods-imvt.tsv and create one ./\u0026lt;collection-PID\u0026gt;/ready-for-datastream-replace/grinnell_\u0026lt;PID\u0026gt;_MODS.xml file for each object.\nExecute the drush islandora_datastream_replace command once for each collection. This command will process each ./\u0026lt;collection-PID\u0026gt;/ready-for-datastream-replace/grinnell_\u0026lt;PID\u0026gt;_MODS.xml file and replace the corresponding object\u0026rsquo;s MODS datastream with the contents of the .xml file. The digital_grinnell branch version of the islandora_datastream_replace command also performs an implicit update of the object\u0026rsquo;s \u0026ldquo;Title\u0026rdquo;, a transform of the new MODS to DC (Dublin Core), and a re-indexing of the new metadata in Solr.\nEditing a mods.tsv File Creating or editing metadata can be a monumental task, and doing it effectively can demand a wealth of knowledge and experience working with metadata standards and practices. This step in our workflow is easily the most labor-intensive. The goal of this project is largely to present metadata editors with a form, in this case the mods.tsv or tab-seperated-value file, to make consistent editing of metadata possible. In addition to the mods.tsv file the workflow will rely on guidance and conventions that are documented in the Metadata Clean-up tab of the Digital_Grinnell_MODS_Master worksheet.\nA metadata editor should focus on only one collection at a time. The suggested practice for working through one collection is as follows:\nFind the collection\u0026rsquo;s mods.tsv file using the collection\u0026rsquo;s persistent identifier, or PID. For example, the target .tsv file for Digital.Grinnell\u0026rsquo;s \u0026ldquo;Social Justice\u0026rdquo; collection, with a PID equal to social-justice, will be found in //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/social-justice/mods.tsv.\nCopy the mods.tsv file, preferably to your local workstation, and optionally give it a new name, like social-justice-mods.tsv.\nOpen the Metadata Clean-up tab of the Digital_Grinnell_MODS_Master worksheet in a browser so that you have guidance available at all times.\n- Note that if you find yourself repeating very cumbersome changes while you edit, please consider taking notes in the _Metadata Clean-up_ tab and email [digital@grinnell.edu](mailto://digital.grinnell.edu) with any questions or concerns you may have about the process or the guidance. Open your copy of the .tsv file in Excel, Numbers, or any .csv or .tsv capable worksheet editor.\nEdit the data. We suggest doing so on a column-by-column, one column at at time where possible. You are likely to find that the values in a single column may be similar from row-to-row, as it should be. You may also find it possible to do large-scale find/replace in a column. For example, many of our records may list a corporate (organization) name as a \u0026ldquo;~ Supporting host\u0026rdquo;, but the proper form of that term is \u0026ldquo;~ supporting host\u0026rdquo;, in all lowercase, and you might save time by doing a find/replace operation to make all such changes.\nWhen you are done editing, be sure to save your work AND export the data back into a new .tsv file specifically named mods-imvt.tsv. Note that \u0026ldquo;-imvt\u0026rdquo; stands for islandora_mods_via_twig, the command that will subsequently used in the next step of our workflow.\nSave a copy of your mods-imvt.tsv file in //STORAGE/LIBRARY/ALLSTAFF/DG-Metadata-Review-2020-r1/\u0026lt;collection-PID/mods-imvt.tsv.\nEmail digital@grinnell.edu to let us know that you have a collection ready for processing, and be sure to provide the collection-PID in your email.\nExpect a follow-up email from digital@grinnell.edu in one or two days. After the metadata has been processed we may ask you to review some of the changes to make sure they appear correctly in Digital.Grinnell.\nAnd that\u0026rsquo;s a wrap. Until next time, thank you for your attention to our metadata! \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/069-exporting-editing-replacing-mods-datastreams/","tags":["MODS","Export","Replace","Twig","Islandora Multi-Importer","islandora_datastream_export","islandora_datastream_replace","islandora_mods_via_twig","islandora_mods_post_processing"],"title":"Exporting, Editing, \u0026 Replacing MODS Datastreams"},{"categories":null,"contents":"Digital.Grinnell employs a custom-built Drupal \u0026ldquo;view\u0026rdquo; we call the dg7 Collection View; it\u0026rsquo;s part of the code in our custom dg7 module where all of Digital.Grinnell\u0026rsquo;s hook implementations are also defined. Experience leads me to beleive that keeping a complex Drupal view in code is prudent, but overriding that code with a database copy of the view helps tremendously in terms of system performance. So, I recommend keeping the view code in the module, but that means that when significant changes are made, like the addition of a new colleciton, the code should be updated in the database to speed things up. Digital.Grinnell employs the following workflow to keep the view in code, but allow it to be selectively updated in the database as needed:\nPut the site into Maintenance Mode. Drupal\u0026rsquo;s default behavior with views in code is to reload and cache views every time they are needed. If/when the cache is cleared or expires this default process can be quite time-consuming, so it\u0026rsquo;s prudent to take control. Putting the site into Maintenance Mode helps in this regard because the dg7 module is coded to ONLY update the dg7 Collection View while the site is in that mode.\nVisit the admin/structure/views page and DELETE the existing dg7 Collection View from the database. Deleting the view triggers the system to re-populate it, something that only happens if the site is in Maintenance Mode.\nClear all caches to complete the reset of dg7 Collection View. An easy way to do this is to execute the following command on the DGDocker1 host:\ndocker exec -w /var/www/html/sites/default isle-apache-dg drush cc all When you return to the site you should now see messages like this:\ndg7_views_default_views has been called in MAINTENANCE MODE and dg7_collection does not exist so it will be created. Note that you MUST alter the new dg7_Collection view in order to secure the new definitions in the database! It\u0026#39;s easy, just edit the view and update all display titles to be empty. Do NOT delete the existing dg7_Collection view again unless you want to force an update from this code in MAINTENACE MODE! Force the view to refresh back into the database. Visit the admin/structure/views page, click Edit for the dg7 Collection View, click Title: none, then click Apply (all displays), and finally click Save. This action changes NOTHING, but it does force the view back into the database where the dg7 module code is overridden.\nTake the site out of Maintenance Mode. Visit the site\u0026rsquo;s admin/config/development/maintenance page, remove the Maintenance Mode checkmark, and click Save configuration.\nThis workflow was written for the production instance of Digital.Grinnell, but the same can be applied to local development or staging instances of Digital.Grinnell simply by changing URLs as needed.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip; \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/068-updating-dgs-collection-views/","tags":["Digital.Grinnell","dg7 Collection View","Maintenance Mode"],"title":"Updating DG's Collection Views"},{"categories":null,"contents":"Welcome to my new blog theme/style with new features. All of the content, aka \u0026lsquo;posts\u0026rsquo;, from my old blog have been moved here, but there are still lots of remnants of the new theme, Zzo, lurking here. Please be patient with this new format while I work to fully implement all the features that this setup provides.\nAlso, thank you, Mackenzie. My daughter played an important role in helping me with this transition.\nAnd it\u0026rsquo;s time to do some real work\u0026hellip; I\u0026rsquo;ll be back to share more here soon. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/067-new-blog-style-and-features/","tags":["Zzo","Blog","Hugo"],"title":"New Blog Style and Features"},{"categories":null,"contents":"I love git and GitHub, and I can certainly appreciate the usefullness of .gitignore, but there are times when I\u0026rsquo;d really like to move an ENTIRE project to a new home. I have in my head a process that might play out like this\u0026hellip;\nFetch a list of all the files and directories that .gitignore is ignoring. Pass that list to a tar or gzip command (maybe two of them) with encryption to create a secure, compressed archive. Commit the archive to the project repo in GitHub, or keep it in a safe place for restoration in the future. Navigate in your terminal to a target and restore the archive using the Step 1 Ok, the first step looks pretty easy. According to this StackOverflow answer we can use one or two git commands to do the trick, specifically:\ngit ls-files --others --ignored --exclude-standard git ls-files --others --ignored --exclude-standard --directory The first command above lists all the ignored files, and the second one lists all the ignored directories.\nStep 2 I\u0026rsquo;m going to try installing and using GnuPG and an example command sequence I found in Solution 2 at https://stackoverflow.com/questions/35584461/gpg-encryption-and-decryption-of-a-folder-using-command-line\nFirst, we capture the list of \u0026ldquo;ignored\u0026rdquo; files, then we tar it, then apply a GnuPG encryption, then remove the intermediate, unsecure artifact, like so:\ngit ls-files --others --ignored --exclude-standard \u0026gt; $(date --iso-8601).ignored.list tar czvf $(date --iso-8601).ignored.list.tar.gz --files-from $(date --iso-8601).ignored.list gpg --encrypt --recipient summitt.dweller@gmail.com $(date --iso-8601).ignored.list.tar.gz rm -fr $(date --iso-8601).ignored.list.tar.gz This process leaves us with \u0026lt;today\u0026gt;.ignored.list.tar.gz.gpg, a secure tarball that we can safely store and restore.\nStep 3 Not much to elaborate on here\u0026hellip; just keep that archive safe. Unfortunately, in the case of my wieting-D8-DO the archive is something north of 270 MB, way too big for GitHub.\nStep 4 - Restoring a GPG Archive, As Needed Copy the \u0026lt;date\u0026gt;.ignored.list.tar.gz file to your target/parent directory, presumably the same directory that the files were captured from originally, and run this sequence, substituting the datestamp prefix of the .gpg filename in place of \u0026lt;date\u0026gt;.\ngpg --decrypt \u0026lt;date\u0026gt;.ignored.list.tar.gz.gpg \u0026gt; ignored.list.tar.gz tar xzvf ignored.list.tar.gz rm -f ignored.lists.tar.gz *.ignored.list.tar.gz.gpg Installing GnuPG Tools Should work nicely once I get GnuPG installed and configured (see https://blog.ghostinthemachines.com/2015/03/01/how-to-use-gpg-command-line/) on my Mac. The installation should be as easy as brew install gnupg.\nCompleted install of GnuPG as Mark McFate \u0026lt;mark.mcfate@icloud.com\u0026gt; on my Mac Mini. 27-Feb-2020\ngpg --gen-key output is captured in my KeePass vault.\nCompleted install of GnuPG for administrator as Summitt Dweller \u0026lt;summitt.dweller@gmail.com\u0026gt; on summitt-dweller-DO-docker. 27-Feb-2020\nAdded rng-tools per this very helpful post!\ngpg --gen-key output is captured in my KeePass vault.\nDoes This Really Work? Why yes, yes it does. The proof is in the pudding, or in this case, it\u0026rsquo;s in a post I just pushed to my personal blog, specifically: Updating the Wieting Site in Drupal 8. Check it out.\nAnd that\u0026rsquo;s a break\u0026hellip; I\u0026rsquo;ll be back. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/066-archiving-what-git-ignores/","tags":["GitHub","git","ls-files",".gitignore","archive","GnuPG","gpg"],"title":"Archiving What Git Ignores"},{"categories":null,"contents":"I just found a handy git/GitHub workflow in a Quora post titled \u0026ldquo;How do I create a new GitHub repository from a branch in an existing repository?\u0026rdquo;. And I used it, successfully, to create a new GitHub repo for my updated Drupal 8 rendition of the Wieting Theatre\u0026rsquo;s website.\nThe new GitHub repo is wieting-D8-DO and it was created from the wieting branch of docker-compose-drupal.\nThe commands I used looked something like this:\ncd ~/GitHub git clone https://github.com/SummittDweller/docker-compose-drupal.git cd docker-compose-drupal git checkout master git reset --hard origin/wieting git checkout wieting git remote rm origin git remote add origin https://github.com/SummittDweller/wieting-D8-DO.git git config user.name \u0026#34;SummittDweller\u0026#34; git config user.email summitt.dweller@gmail.com git push -u origin master And that\u0026rsquo;s a wrap\u0026hellip; until next time. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/065-create-new-github-project-from-a-branch/","tags":["GitHub","New","Repo","Branch","Wieting","mogtofu33","docker-compose-drupal"],"title":"How to Create a New GitHub Repo from an Existing Branch"},{"categories":null,"contents":"I have ITS tickets, for seemingly simple DNS changes, that are now more than a month old, and because of that I\u0026rsquo;ve taken steps to try and do some ISLE staging work on one of my DigitalOcean droplets, namely summitt-services-droplet-01. In order to accommodate that I\u0026rsquo;ve moved nearly all of the sites and services from that droplet to my other, summitt-dweller-DO-docker. The site migration was a smooth process except for https://Wieting.TamaToledo.com. That Drupal 8 site has been difficult to upgrade and migrate largely because it was deployed using my old Port-Ability scripts, and about a year ago I scrapped Port-Ability in favor of Docksal, but I never got around to moving that particular site to a Docksal environment. Well, now I\u0026rsquo;m finding it almost impossible to complete that migration to Docksal.\nThe Problem with Docksal Docksal is a wonderful development environment, but I can\u0026rsquo;t find an effective, and easily repeatable, path from development to production when using it. Docksal provides system services including an SSH Agent, DNS, and Reverse Proxy as documented here. Docksal services are provided by a cli container/service which is also responsible for providing a robust set of fin commands. In addition to cli, a typical Docksal stack also provides containers for web and db, and those look a lot like what I like to deploy for Drupal in production. However, the cli container looks nothing like what I deploy in production, and therein lies the rub.\nThe Promise of Lando Hindsight is 20/20, so this must be the year to look back and make course corrections, right? Had I not fallen so quickly for the speed and glitz of Docksal I would have given some of its alternatives, like Lando, a closer look. The immediate promise of Lando is that it builds, in development, a stack that looks much more like what I wish to deploy in production, and it does so by not integrating as tightly as Docksal does.\nMigrating the Wieting Theatre Web Site to Lando Another thing that I like about Lando is the fact that Jeff Geerling has taken it for a spin and documented some of his experience with it in his blog. So I have elected to begin my adventures in Lando with this post.\nAfter following Jeff\u0026rsquo;s lead I discovered that it left me a little short of my goal\u0026hellip; to spin up an existing Drupal 8 site using Lando. So, I backed off a bit and returned to studying the contents of https://github.com/lando/lando. Along the way I found a post by the folks at colorfield.\nFollowing colorfield.be See Drupal and Docker the easy way with Lando.\n╭─mark@Marks-Mac-Mini ~/GitHub ╰─$ php -d memory_limit=-1 composer.phar create-project drupal-composer/drupal-project:8.x-dev wieting-lando --stability dev --no-interaction ...wait for it... ╭─mark@Marks-Mac-Mini ~/GitHub ╰─$ cd wieting-lando ╭─mark@Marks-Mac-Mini ~/GitHub/wieting-lando ╰─$ lando init ? From where should we get your app\u0026#39;s codebase? current working directory ? What recipe do you want to use? drupal8 ? Where is your webroot relative to the init destination? web ? What do you want to call this app? wieting NOW WE\u0026#39;RE COOKING WITH FIRE!!! Your app has been initialized! Go to the directory where your app was initialized and run `lando start` to get rolling. Check the LOCATION printed below if you are unsure where to go. Oh... and here are some vitals: NAME wieting LOCATION /Users/mark/GitHub/wieting-lando RECIPE drupal8 DOCS https://docs.devwithlando.io/tutorials/drupal8.html Then\u0026hellip;\n╭─mark@Marks-Mac-Mini ~/GitHub/wieting-lando ╰─$ atom . # to add config: keys for via:nginx and database:mariadb ╭─mark@Marks-Mac-Mini ~/GitHub/wieting-lando ╰─$ lando start Let\u0026#39;s get this party started! Starting app.. Creating network \u0026#34;landoproxyhyperion5000gandalfedition_edge\u0026#34; with driver \u0026#34;bridge\u0026#34; Creating landoproxyhyperion5000gandalfedition_proxy_1 ... done Creating network \u0026#34;wieting_default\u0026#34; with the default driver Creating wieting_database_1 ... done Creating wieting_appserver_1 ... done Creating wieting_appserver_nginx_1 ... done Waiting until appserver_nginx service is ready... Waiting until appserver service is ready... Waiting until database service is ready... BOOMSHAKALAKA!!! Your app has started up correctly. Here are some vitals: NAME wieting LOCATION /Users/mark/GitHub/wieting-lando SERVICES appserver_nginx, appserver, database APPSERVER_NGINX URLS https://localhost:32808 http://localhost:32809 http://wieting.lndo.site https://wieting.lndo.site A visit to https://wieting.lndo.site takes me to the /core/install.php script where I set the following\u0026hellip;\nLanguage: English Installation Profile: Standard Database type: MariaDB Database name: drupal8 Database username: drupal8 Database password: drupal8 Host: database BOOMSHAKALAKA! The site info was already there. The site is up!\nComposer, Drush and Drupal Console Did some checking just to be sure these work\u0026hellip;\n╭─mark@Marks-Mac-Mini ~/GitHub/wieting-lando ╰─$ lando composer --version Composer 1.9.3 2020-02-04 12:58:49 ╭─mark@Marks-Mac-Mini ~/GitHub/wieting-lando ╰─$ lando drush status Drupal version : 8.8.2 Site URI : http://default DB driver : mysql DB hostname : database DB port : 3306 DB username : drupal8 DB name : drupal8 Database : Connected Drupal bootstrap : Successful Default theme : bartik Admin theme : seven PHP binary : /usr/local/bin/php PHP config : PHP OS : Linux Drush script : /app/vendor/drush/drush/drush Drush version : 10.2.1 Drush temp : /tmp Drush configs : /app/vendor/drush/drush/drush.yml /app/drush/drush.yml Install profile : standard Drupal root : /app/web Site path : sites/default Files, Public : sites/default/files Files, Temp : /tmp ╭─mark@Marks-Mac-Mini ~/GitHub/wieting-lando ╰─$ lando drupal about Drupal Console (1.9.4) ====================== Copy configuration files. drupal init Download, install and serve Drupal 8 drupal quick:start Create a new Drupal project drupal site:new Install a Drupal project drupal site:install Lists all available commands drupal list Update project to the latest version. drupal self-update ╭─mark@Marks-Mac-Mini ~/GitHub/wieting-lando ╰─$ Woot!\nCommitting to GitHub Since there are no secrets in this config, yet, I\u0026rsquo;m going to push it to GitHub. Specifically\u0026hellip; SummittDweller/wieting-lando.\nRestarting Local Development I\u0026rsquo;m back at work on the campus of Grinnell College today and am looking to pick up last evening\u0026rsquo;s development here, so I need to \u0026ldquo;move\u0026rdquo; my project to a different host, namely MA8660. Let\u0026rsquo;s see if this works\u0026hellip;\n╭─markmcfate@ma8660 ~/GitHub ‹ruby-2.3.0› ╰─$ git clone https://github.com/SummittDweller/wieting-lando Cloning into \u0026#39;wieting-lando\u0026#39;... remote: Enumerating objects: 30, done. remote: Counting objects: 100% (30/30), done. remote: Compressing objects: 100% (22/22), done. remote: Total 30 (delta 0), reused 30 (delta 0), pack-reused 0 Unpacking objects: 100% (30/30), done. ╭─markmcfate@ma8660 ~/GitHub ‹ruby-2.3.0› ╰─$ cd wieting-lando Whoa! The wieting-lando directory looks a little empty, probably because I see that my .gitignore file ignored alot of stuff. \u0026#x1f626; No matter, let\u0026rsquo;s see if we can work some magic\u0026hellip;\n╭─markmcfate@ma8660 ~/GitHub/wieting-lando ‹ruby-2.3.0› ‹master› ╰─$ lando composer update ...and the magic happens... ╭─markmcfate@ma8660 ~/GitHub/wieting-lando ‹ruby-2.3.0› ‹master› ╰─$ lando start Let\u0026#39;s get this party started! Starting app.. ...more magic... Waiting until database service is ready... BOOMSHAKALAKA!!! Your app has started up correctly. Here are some vitals: NAME wieting LOCATION /Users/markmcfate/GitHub/wieting-lando SERVICES appserver_nginx, appserver, database APPSERVER_NGINX URLS https://localhost:32773 http://localhost:32774 http://wieting.lndo.site:8000 https://wieting.lndo.site:444 And BOOM, we are ready to install a new site again!\nSwitching Gears\u0026hellip;Again I\u0026rsquo;m turning my attention now to summitt-dweller-DO-docker and https://github.com/mogtofu33/docker-compose-drupal(https://github.com/mogtofu33/docker-compose-drupal) which I recently forked to https://github.com/SummittDweller/docker-compose-drupal.\nMy attempt to get my wieting-lando Drupal code up and working at DigitalOcean per the instructions in Installation and configuration working as administrator on summitt-dweller-DO-docker looks like this:\n╭─administrator@summitt-dweller-DO-docker /opt ╰─$ git clone https://github.com/SummittDweller/docker-compose-drupal.git ╭─administrator@summitt-dweller-DO-docker /opt ╰─$ cd docker-compose-drupal ╭─administrator@summitt-dweller-DO-docker /opt/docker-compose-drupal ‹master› ╰─$ git checkout -b wieting ╭─administrator@summitt-dweller-DO-docker /opt/docker-compose-drupal ‹wieting› ╰─$ cp docker-compose.tpl.yml docker-compose.yml\\ncp default.env .env ╭─administrator@summitt-dweller-DO-docker /opt/docker-compose-drupal ‹wieting› ╰─$ nano .env # to make recommended edits ╭─administrator@summitt-dweller-DO-docker /opt/docker-compose-drupal ‹wieting› ╰─$ nano docker-compose.yml # to make recommended edits ╭─administrator@summitt-dweller-DO-docker /opt/docker-compose-drupal ‹wieting› ╰─$ docker-compose config # to check the config...good to go Then from MA8660\u0026hellip;\n╭─markmcfate@ma8660 ~/GitHub/wieting-lando ‹ruby-2.3.0› ‹master*› ╰─$ rsync -aruvi . administrator@104.248.237.235:/opt/docker-compose-drupal/drupal/. --progress Then back on summitt-dweller-DO-docker\u0026hellip;\n╭─administrator@summitt-dweller-DO-docker /opt/docker-compose-drupal ‹wieting› ╰─$ docker-compose up --build -d But this command failed because of two issues:\nCannot start service portainer: driver failed programming external connectivity on endpoint wieting-portainer - Portainer is already running on this node, we don\u0026rsquo;t need it again! Cannot start service nginx: driver failed programming external connectivity on endpoint wieting-nginx - Sure, port 80 is already occupied\u0026hellip;we need some Traefik magic here! First step\u0026hellip; remove (comment out) the portainer service in docker-compose.yml, and bring back the dashboard service just to see how it works.\nNext step\u0026hellip; add some Traefik labels and port assignments (see summitt-dweller-DO-docker /opt/omeka-s2/docker-compose.yml for examples) to our docker-compose.yml to aleviate the nginx port conflict issues.\n^^^ Do As He Says ^^^ And that\u0026rsquo;s\u0026hellip;promising. I\u0026rsquo;ll be back. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/064-migration-to-lando/","tags":["Docksal","Lando","Drupal 8","Wieting","mogtofu33","docker-compose-drupal"],"title":"Migration to Lando"},{"categories":null,"contents":"These days I like to do all my terminal/command-line work in zsh, more specifically, with Oh My ZSH! and the bira theme. This previous post described the process I used on each of my Linux servers, and this post is similar, but written specifically for my Catalina (Macintosh OS X v10.15.x), workstations.\nThis is how I installed and configured zsh, and some other goodies, on my student Mac Mini workstation, MA6879\u0026hellip;\nbrew update brew install nano zsh chsh -s /bin/zsh mark exit # log back in after this echo $SHELL brew install wget git hugo docker # Docker and Hugo added just for good measure wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | zsh cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc source ~/.zshrc cd ~/.oh-my-zsh/themes/ ls -a nano ~/.zshrc # In the editor add (or replace similar) the following lines but without the leading # # ZSH_THEME=\u0026#39;bira\u0026#39; # plugins=(git extract web-search yum git-extras docker) exit # log back in after this And that\u0026rsquo;s a wrap\u0026hellip; until next time. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/063-installing-zsh-in-catalina/","tags":["zsh","Oh My ZSH!","bira","OS X","Mac","Catalina"],"title":"Installing ZSH in Catalina"},{"categories":null,"contents":"Just making a note here that Hugo, as of version 0.60.0, is now using the Goldmark markdown rendering library, and that library is CommonMark compliant. The official word, from this document states that:\nGoldmark is from Hugo 0.60 the default library used for Markdown. It’s fast, it’s CommonMark compliant and it’s very flexible. Note that the feature set of Goldmark vs Blackfriday isn’t the same; you gain a lot but also lose some, but we will work to bridge any gap in the upcoming Hugo versions.\nThe immediate impact of this change, in this blog and similar sites, is that line-break tags like \u0026lt;br/\u0026gt; inside table cells are no longer rendered as line breaks. The quick fix, bypassing Goldmark, is to add the following to the blog\u0026rsquo;s config.toml file:\n## Override default markdown engine to preserve BlackFriday handling. ## See https://gohugo.io/getting-started/configuration-markup/#goldmark [markup] defaultMarkdownHandler = \u0026#34;blackfriday\u0026#34; ","permalink":"https://static.grinnell.edu/dlad-blog/posts/061-hugo-and-commmonmark-compliance/","tags":["Hugo","Goldmark","CommonMark"],"title":"Hugo, Goldmark and CommonMark Compliance"},{"categories":null,"contents":"ISLE v1.3.0 has been running on my staging server, DGDockerX, for months now and it seems to be performing as-expected with one exception\u0026hellip; when I try to import a batch of objects using IMI, the Islandora Multi-Importer, I get the following error:\nThe website encountered an unexpected error. Please try again later. Examinations of Recent log messages seem to point to DNS issues that I\u0026rsquo;m unable to overcome because I have no control over our DNS records, campus networking, or firewalls. So this post is intended to chronicle steps I\u0026rsquo;m taking to stand-up an instance of dg-isle and dg-islandora on a \u0026ldquo;clean\u0026rdquo; DigitalOcean droplet, namely summitt-services-droplet-01, that I have been leasing for the past couple of years.\nDirectories I\u0026rsquo;ll begin by opening a terminal on my workstation/host and subsequently a terminal into the aforementioned droplet where I\u0026rsquo;ll login as user centos with a UID of 1000. Once I\u0026rsquo;m in there I\u0026rsquo;ll also clone the two aformentioned GitHub repositories to the DO host, like so:\nsummitt-services-droplet-01 Host Commands, as user centos cd /home/centos/opt git clone \u0026ndash;recursive https://github.com/Digital-Grinnell/dg-isle.git git clone \u0026ndash;recursive https://github.com/Digital-Grinnell/dg-islandora.git Next, I\u0026rsquo;ll attempt to rsync the contents of my DG-FEDORA directory from DGDockerX (IP address: 132.161.132.101), my original staging server, to the aforementioned DigitalOcean droplet/host (public IP address: 165.227.83.186).\nsummitt-services-droplet-01 Host Commands, as user centos mkdir /home/centos/data sudo mkdir -p /mnt/data/DG-FEDORA sudo chown -R centos:centos /mnt/data rsync -aruvi islandora@132.161.132.101:/mnt/data/DG-FEDORA/. /mnt/data/DG-FEDORA/ \u0026ndash;progress Note that the above rsync command DID NOT WORK, so I tried reversing the process by opening a VPN connection to Grinnell College, opening a terminal on DGDockerX, and \u0026ldquo;pushing\u0026rdquo; the files to summitt-services-droplet-01, but this also FAILED, presumably due to network/communications issues. So, ultimately I used a series of rsync commands to \u0026ldquo;pull\u0026rdquo; the files to my local workstation, then \u0026ldquo;push\u0026rdquo; them out to summitt-services-droplet-01.\nSo, in the end, the contents of dgdockerx.grinnell.edu:/mnt/data/DG-FEDORA were copied to summitt-services-droplet-01:/mnt/data/DG-FEDORA where the directory looks like this:\ndrwxrwxr-x. 10 centos centos 275 Feb 11 13:37 . drwxrwxrwx. 3 root root 23 Feb 10 10:19 .. drwxrwxrwx. 236 centos centos 4.0K Dec 17 14:53 datastreamStore -rwxrwxrwx. 1 centos centos 211 Dec 23 11:15 DG-FEDORA-0.md -rwxrwxrwx. 1 centos centos 1.1K Oct 5 09:36 docker-compose.DG-FEDORA.yml drwxrwxrwx. 2 centos centos 172 Dec 12 18:21 Extras drwxr-xr-x. 3 centos centos 85 Nov 11 14:10 from-DGDocker1 drwx------. 2 centos centos 28 Dec 1 14:44 .fseventsd -rwxrwxrwx. 1 centos centos 11K Oct 4 12:28 local.env drwxrwxrwx. 184 centos centos 4.0K Dec 13 12:51 objectStore -rwxrwxrwx. 1 centos centos 5.1K Dec 23 11:09 README.md drwxrwx---. 2 centos centos 6 Dec 17 14:47 site-public drwxrwxrwx. 7 centos centos 131 Oct 9 23:17 Storage drwxrwxrwx. 2 centos centos 53 Mar 12 2019 System Volume Information Note that the above directory and all its contents are owned by centos:centos.\nModifying the Environment (.env) Next, I opened a terminal on summitt-services-droplet-01, did cd ~/opt/dg-isle; git checkout staging and used sudo nano to edit files as necessary. The edits have all been saved and pushed back to the staging branch of https://GitHub/Digital-Grinnell/dg-isle.\nThe new .env file includes a second \u0026ldquo;staging\u0026rdquo; configuration block, the first is commented out, and second block refers to docker-compose.staging2.yml, a new file duplicated from docker-compose.staging.yml, but with modifications made explicitly for the summitt-services-droplet-01 server.\nLaunch the Staging.SummittServices.com Stack Having edited all necessary files, I will launch the staging stack using:\nsummitt-services-droplet-01 Host Commands, as user centos cd ~/opt/dg-isle git checkout staging docker-compose up -d docker logs -f isle-apache-dg The startup will take a couple of minutes, and it does not \u0026ldquo;signal\u0026rdquo; when it\u0026rsquo;s done, so that\u0026rsquo;s the reason for the last command above. The -f option will keep the output spooling to your terminal so that you don\u0026rsquo;t have to keep repeating the command over and over again. You will know the startup is complete when you see something like the following at the bottom of the log output:\n... Done setting proper permissions on files and directories XDEBUG OFF AH00558: apache2: Could not reliably determine the server\u0026#39;s fully qualified domain name, using 192.168.16.7. Set the \u0026#39;ServerName\u0026#39; directive globally to suppress this message AH00558: apache2: Could not reliably determine the server\u0026#39;s fully qualified domain name, using 192.168.16.7. Set the \u0026#39;ServerName\u0026#39; directive globally to suppress this message [Tue Feb 11 18:32:35.664921 2020] [mpm_prefork:notice] [pid 12683] AH00163: Apache/2.4.41 (Ubuntu) configured -- resuming normal operations [Tue Feb 11 18:32:35.665016 2020] [core:notice] [pid 12683] AH00094: Command line: \u0026#39;/usr/sbin/apache2 -D FOREGROUND\u0026#39; Press ctrl-c to interrupt the docker logs... command and get your terminal back.\nA visit to https://staging.summittservices.com yields an SQL error so it looks like we haven\u0026rsquo;t quite created a viable staging instance of Digital.Grinnell, yet. The next logical step is to backup the database from https://isle-stage.grinnell.edu, and import it here.\nBackup the Site Database Let\u0026rsquo;s begin by visiting the original staging site\u0026rsquo;s home page (VPN connection may be required) and using the Quick Backup block at the bottom of the right-hand sidebar. Normally I would select a Backup Destination: Manual Backups Directory option to save the database backup on the server, but in this case it will be advantageous to have the backup in-hand, locally. So, I choose Backup Destination: Download instead, and the result is in my local ~/Downloads directory, specifically:\n/Users/mark/Downloads/DigitalGrinnell-2020-02-11T12-46-36.mysql.gz\nUpload and Import the Database I uploaded the database to summitt-services-droplet-01 like so:\nLocal Workstation Commands cd ~/Downloads rsync -aruvi DigitalGrinnell-2020-02-11T12-46-36.mysql.gz centos@165.227.83.186:/home/centos/ \u0026ndash;progress Then, in a terminal on summitt-services-droplet-01 as user centos\u0026hellip;\nsummitt-services-droplet-01 Host Commands, as user centos cd ~ gunzip DigitalGrinnell-2020-02-11T12-46-36.mysql.gz sudo mv -f DigitalGrinnell-2020-02-11T12-46-36.mysql /mnt/data/DG-FEDORA/DG.sql Import the Database Backup Locally Since the site is not working I cannot use drush nor Backup and Migrate to do this, so I opened a terminal in the MySQL container, namely isle-mysql-dg and did this:\nMySQL Container Commands mysql -u root -p digital_grinnell # Supply root MySQL password # CREATE DATABASE IF NOT EXISTS digital_grinnell; USE digital_grinnell; source /temp/DG.sql White Screen of Death Sorry, I had to jump to another project for the past couple of weeks, and now that I\u0026rsquo;m back (March 2, 2020) the situation is not good. \u0026#x1f626; When I visit https://staging.summittservices.com now I get a dreaded WSOD. So I peek at the Apache container logs using docker logs isle-apache-ld and find this:\n[Mon Mar 02 20:35:50.641509 2020] [php7:error] [pid 12676] [client 192.168.176.4:44300] PHP Fatal error: require_once(): Failed opening required \u0026#39;/var/www/html/sites/all/modules/islandora/islandora_multi_importer/vendor/autoload.php\u0026#39; (include_path=\u0026#39;.:/usr/share/php\u0026#39;) in /var/www/html/sites/all/modules/islandora/islandora_multi_importer/islandora_multi_importer.module on line 19 Bottom line, there are critical issues with IMI, the Islandora Multi-Importer, just as there were in staging. Hmmm, what to do now?\nFixing IMI I open a terminal into the Apache container and attempt to repair/re-install IMI like so:\nApache Container Commands cd /var/www/html/sites/all/modules/islandora/islandora_multi-importer git remote -v git status composer install The commands and output from all of this are reflected in this gist.\nNow, a visit to https://staging.summittservices.com yields a very welcome \u0026ldquo;Site under maintenance\u0026rdquo; page. That\u0026rsquo;s progress! Let\u0026rsquo;s take the site out of \u0026ldquo;maintenance mode\u0026rdquo; using that same Apache container terminal session, like so:\nApache Container Commands cd /var/www/html/sites/default drush vset maintenance_mode 0 That worked, but also returned a handful of warnings summarized by these two:\nWarning: file_put_contents(private:///.htaccess): failed to open stream: \u0026#34;DrupalPrivateStreamWrapper::stream_open\u0026#34; call failed in file_create_htaccess() (line 496 of /var/www/html/includes/file.inc). User warning: The following module is missing from the file system: islandora_binary_object. For information about how to fix this, see the documentation page. in _drupal_trigger_error_with_delayed_logging() (line 1156 of /var/www/html/includes/bootstrap.inc). Looks like we have a missing module and at least one file/directory permissions issue.\nAddressing \u0026ldquo;Private\u0026rdquo; Directory Permissions To address the permissions issue I logged in to the site as \u0026ldquo;System Admin\u0026rdquo; and visited https://staging.summittservices.com/admin/config/media/file-system where I see that our \u0026ldquo;private\u0026rdquo; file system path is: /var/www/private.\nBack to my Apache container terminal to have a look at that\u0026hellip; Aha, /var/www/private does indeed exist, but it has ownership and protections inside the container of: drwxr-xr-x. 2 root root 6 Feb 10 16:18 private/. Let\u0026rsquo;s set the owner and group here to match other directories, specifically: islandora:www-data.\nApache Container Commands cd /var/www chown -R islandora:www-data private cd /var/www/html/sites/default drush vset maintenance_mode 0 Woot! The permissions warnings are gone.\nAddressing the Missing islandora_binary_object Warning So, remember back in the Installing the Missing Islandora and Custom Modules section of post 058, we commented out the installation of islandora_binary_object like so:\ncd /var/www/html/sites/all/modules/islandora git submodule add https://github.com/DigitalGrinnell/dg7.git git submodule add https://github.com/DigitalGrinnell/idu.git # git submodule add git://github.com/discoverygarden/islandora_binary_object.git git submodule add https://github.com/discoverygarden/islandora_collection_search git submodule add https://github.com/DigitalGrinnell/islandora_mods_display.git git submodule add https://github.com/Islandora-Labs/islandora_solution_pack_oralhistories.git # git submodule add git://github.com/nhart/islandora_pdfjs_reader.git git submodule add https://github.com/Islandora-Labs/islandora_solr_collection_view.git chown -R islandora:www-data * cd /var/www/html/sites/default drush cc all Well, it\u0026rsquo;s evidently time to put that back! So, I\u0026rsquo;m taking a new snapshot of the server then I\u0026rsquo;ll try to solve this mystery. A little research tells me that the islandora_binary_object command above is pointing to the wrong project for Islandora v7 (it points to an Islandora v8 module), so we require some additional configuration on the host, like so:\nHost / summitt-services-droplet-01 Commands sudo su cd /opt/dg-islandora git submodule add https://github.com/Islandora-Labs/islandora_binary_object sites/all/modules/islandora/islandora_binary_object chown -R islandora:www-data * That\u0026rsquo;s not a wrap. More about this soon\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/062-testing-dg-staging-on-digitalocean/","tags":["ISLE","Digital.Grinnell","DigitalOcean","development","staging","https://staging.summittservices.com"],"title":"Staging Digital.Grinnell (DG) on DigitalOcean (DO)"},{"categories":null,"contents":"ISLE v1.3.0 has been running on my staging server, DGDockerX, for about 6 weeks now and it seems to be performing as-expected with one exception\u0026hellip; when I try to import a batch of objects using IMI, the Islandora Multi-Importer, I get the following error:\nThe website encountered an unexpected error. Please try again later. An examination of Recent log messages shows\u0026hellip;\nGuzzleHttp\\Exception\\ConnectException: cURL error 6: Could not resolve host: sheets.googleapis.com (see https://curl.haxx.se/libcurl/c/libcurl-errors.html) in GuzzleHttp\\Handler\\CurlFactory::createRejection() (line 200 of /var/www/html/sites/all/modules/islandora/islandora_multi_importer/vendor/guzzlehttp/guzzle/src/Handler/CurlFactory.php). Engaging the Local Workflow Since I\u0026rsquo;m not at all sure what\u0026rsquo;s wrong, I feel like I need to rewind my process a bit and try to reproduce the same configuration, and error, on a local instance of this ISLE stack. This post will chronicle the steps I take to do so.\nDirectories I\u0026rsquo;ll begin by opening a terminal on my workstation/host, MA8660 as user mcfatem. Then I very carefully (note the use of the --recursive flags!) clone the aforementioned projects to the host\u0026rsquo;s ~/GitHub directory like so:\nHost Commands cd ~/GitHub\ngit clone \u0026ndash;recursive https://github.com/Digital-Grinnell/dg-isle.git\ngit clone \u0026ndash;recursive https://github.com/Digital-Grinnell/dg-islandora.git\ncd dg-isle Launch the dg.localdomain Stack I\u0026rsquo;m modifying the .env file in the dg-isle directory so that \u0026ldquo;local\u0026rdquo; is my target environment. Having done that, I will launch the local stack using:\nHost Commands cd ~/GitHub/dg-isle git checkout master docker-compose up -d docker logs -f isle-apache-dg The startup will take a couple of minutes, and it does not \u0026ldquo;signal\u0026rdquo; when it\u0026rsquo;s done, so that\u0026rsquo;s the reason for the last command above. The -f option will keep the output spooling to your terminal so that you don\u0026rsquo;t have to keep repeating the command over and over again. You will know the startup is complete when you see something like the following at the bottom of the log output:\n... Done setting proper permissions on files and directories XDEBUG OFF AH00558: apache2: Could not reliably determine the server\u0026#39;s fully qualified domain name, using 172.20.0.7. Set the \u0026#39;ServerName\u0026#39; directive globally to suppress this message AH00558: apache2: Could not reliably determine the server\u0026#39;s fully qualified domain name, using 172.20.0.7. Set the \u0026#39;ServerName\u0026#39; directive globally to suppress this message [Mon Feb 03 22:16:46.898249 2020] [mpm_prefork:notice] [pid 12698] AH00163: Apache/2.4.41 (Ubuntu) configured -- resuming normal operations [Mon Feb 03 22:16:46.898652 2020] [core:notice] [pid 12698] AH00094: Command line: \u0026#39;/usr/sbin/apache2 -D FOREGROUND Press ctrl-c to interrupt the docker logs... command and get your terminal back.\nA visit to https://dg.localdomain yields a very incomplete Bartik-themed site, and it looks like we are far from creating a local Digital.Grinnell. So, the next logical step is to backup the database from https://isle-stage.grinnell.edu, and import it here.\nBackup the Site Database Let\u0026rsquo;s begin by visiting the site\u0026rsquo;s home page and using the Quick Backup block at the bottom of the right-hand sidebar. Normally I would select a Backup Destination: Manual Backups Directory option to save the database backup on the server, but in this case it will be advantageous to have the backup in-hand, locally. So, I choose Backup Destination: Download instead, and the result is in my local ~/Downloads directory, specifically:\n/Users/markmcfate/Downloads/DigitalGrinnell-2020-02-03T14-33-13.mysql.gz\nImport the Database Backup Locally This is as simple as:\nOpening https://dg.localdomain in my browser, Logging in as a system admin, Open https://dg.localdomain/node#overlay=admin/module and enable the Backup and Migrate module, Save the configuration change, Open https://dg.localdomain/node#overlay=admin/config/system/backup_migrate/restore, Browse to the aforementioned database backup .gz file, and Click Restore now White Screen of Death Not good, when I visit https://dg.localdomain now I get a dreaded WSOD. So I peek at the Apache container logs using docker logs isle-apache-ld and find this:\n[Mon Feb 03 20:41:55.684244 2020] [php7:error] [pid 13002] [client 172.20.0.4:43034] PHP Fatal error: require_once(): Failed opening required \u0026#39;/var/www/html/sites/all/modules/islandora/islandora_multi_importer/vendor/autoload.php\u0026#39; (include_path=\u0026#39;.:/usr/share/php\u0026#39;) in /var/www/html/sites/all/modules/islandora/islandora_multi_importer/islandora_multi_importer.module on line 19, referer: https://dg.localdomain/admin/config/system/backup_migrate/restore?render=overlay 172.20.0.4 - - [03/Feb/2020:20:41:52 +0000] \u0026#34;GET /admin/config/system/backup_migrate/restore?render=overlay HTTP/1.1\u0026#34; 500 333 \u0026#34;https://dg.localdomain/admin/config/system/backup_migrate/restore?render=overlay\u0026#34; \u0026#34;Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:72.0) Gecko/20100101 Firefox/72.0 Bottom line, there are critical issues with IMI, the Islandora Multi-Importer, just as there were in staging. Hmm, what to do now?\nFixing IMI I open a terminal into the Apache container and attempt to repair/re-install IMI like so:\nApache Container Commands cd /var/www/html/sites/all/modules/islandora/islandora_multi-importer git remote -v git status composer install The commands and output from all of this are reflected in this gist.\nIs It Fixed? Yes, Sort Of A browser visit to https://dg.localdomain demonstrates that the site is back! However, the site is currently showing Operating in maintenance mode. Go online, and there are multiple warnings indicating:\nUser warning: The following module is missing from the file system: islandora_binary_object. When I click the Go online link and complete the operation, the site does indeed respond by showing me an almost-proper home page, complete with numerous collection objects. I says it\u0026rsquo;s \u0026ldquo;almost-proper\u0026rdquo; because in addition to the top-level collections, the site is also showing a host of individual objects. This should NOT be the case, but I can think of a handful of remedies, including:\nUsing the Development menu Clear cache link to do just what the name says. Ok, so that actually worked; no need to go farther.\nWill the Same Fixes Work on Staging? Only one way to find out. First, I\u0026rsquo;m going to snapshot the DGDockerX staging server. Done. Then\u0026hellip;\nApache Container Commands cd /var/www/html/sites/all/modules/islandora/islandora_multi-importer composer install And the output this time says:\nroot@60ccb7d0ae42:/var/www/html/sites/all/modules/islandora/islandora_multi_importer# composer install Loading composer repositories with package information Installing dependencies (including require-dev) from lock file Nothing to install or update Package phpoffice/phpexcel is abandoned, you should avoid using it. Use phpoffice/phpspreadsheet instead. Package silex/silex is abandoned, you should avoid using it. Use symfony/flex instead. Generating autoload file So, to fix this I tried, and got back this:\nroot@60ccb7d0ae42:/var/www/html/sites/all/modules/islandora/islandora_multi_importer# composer install Loading composer repositories with package information Installing dependencies (including require-dev) from lock file Nothing to install or update Package phpoffice/phpexcel is abandoned, you should avoid using it. Use phpoffice/phpspreadsheet instead. Package silex/silex is abandoned, you should avoid using it. Use symfony/flex instead. Generating autoload files root@60ccb7d0ae42:/var/www/html/sites/all/modules/islandora/islandora_multi_importer# root@60ccb7d0ae42:/var/www/html/sites/all/modules/islandora/islandora_multi_importer# root@60ccb7d0ae42:/var/www/html/sites/all/modules/islandora/islandora_multi_importer# root@60ccb7d0ae42:/var/www/html/sites/all/modules/islandora/islandora_multi_importer# composer remove phpoffice/phpexcel silex/silex Loading composer repositories with package information The \u0026#34;https://repo.packagist.org/packages.json\u0026#34; file could not be downloaded: php_network_getaddresses: getaddrinfo failed: Temporary failure in name resolution failed to open stream: php_network_getaddresses: getaddrinfo failed: Temporary failure in name resolution https://repo.packagist.org could not be fully loaded, package information was loaded from the local cache and may be out of date Updating dependencies (including require-dev) [Composer\\Downloader\\TransportException] The \u0026#34;http://repo.packagist.org/p/cache/taggable-cache%247eb77da84984bd6522fb9e3e91f4f82107555cba862c9f161cd3ff697dcc6f7c.json\u0026#34; file could not be downloaded: php_network_getaddresses: getaddrinfo failed: Temporary failure in name resolution failed to open stream: php_network_getaddresses: getaddrinfo failed: Temporary failure in name resolution remove [--dev] [--no-progress] [--no-update] [--no-scripts] [--update-no-dev] [--update-with-dependencies] [--no-update-with-dependencies] [--ignore-platform-reqs] [-o|--optimize-autoloader] [-a|--classmap-authoritative] [--apcu-autoloader] [--] \u0026lt;packages\u0026gt; (\u0026lt;packages\u0026gt;)... Next stop\u0026hellip; a staging copy of Digital.Grinnell on DigitalOcean? Yup, headed there now. \u0026#x1f626; But the address will be https://staging.summittservices.com, when it\u0026rsquo;s ready.\nThat\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/060-isle-workflow-test/","tags":["ISLE","dg.localdomain","local","development"],"title":"ISLE Workflow Test"},{"categories":null,"contents":"This post chronicles the steps I took to push my local dg.localdomain project, an ISLE v1.3.0 build, to staging on node DGDockerX as https://isle-stage.grinnell.edu using my dg-isle and dg-islandora repositories.\nDirectories I\u0026rsquo;ll begin by opening a terminal on the staging host, DGDockerX as user islandora. Then I very carefully (note the use of the --recursive flags!) clone the aforementioned projects to DGDockerX like so:\nHost / DGDockerX Commands cd /opt git clone \u0026ndash;recursive https://github.com/Digital-Grinnell/dg-isle.git git clone \u0026ndash;recursive https://github.com/Digital-Grinnell/dg-islandora.git cd dg-isle git checkout staging One Useful Git Config Change One thing I learned during this process is that all of the dg-isle config files that I’ve modified and/or mapped into the containers show up as “modified” when I do a git status on the host. The only apparent “modification” is that these are all owned on the host by mcfatem:mcfatem, but prior to spin-up these were owned by islandora:islandora. The files/directories are:\nconfig/apache/settings_php/settings.staging.php, config/fedora/gsearch/foxmlToSolr.xslt, config/fedora/gsearch/islandora_transforms/, and config/solr/schema.xml This is apparently a known condition that does no harm, but it can be easily ignored by specifying:\nHost / DGDockerX Commands cd /opt/dg-isle git config core.fileMode false Thank you, Noah Smith for sharing that bit of wisdom!\nStarting the Stack Having cloned the projects to the host as indicated above, we visit our host terminal and\u0026hellip;\nHost / DGDockerX Commands cd /opt/dg-isle docker-compose up -d docker logs -f isle-apache-dg The startup will take a couple of minutes, and it does not \u0026ldquo;signal\u0026rdquo; when it\u0026rsquo;s done, so that\u0026rsquo;s the reason for the last command above. The -f option will keep the output spooling to your terminal so that you don\u0026rsquo;t have to keep repeating the command over and over again. You will know the startup is complete when you see the following at the bottom of the log output:\n... Done setting proper permissions on files and directories XDEBUG OFF AH00558: apache2: Could not reliably determine the server\u0026#39;s fully qualified domain name, using 192.168.0.7. Set the \u0026#39;ServerName\u0026#39; directive globally to suppress this message AH00558: apache2: Could not reliably determine the server\u0026#39;s fully qualified domain name, using 192.168.0.7. Set the \u0026#39;ServerName\u0026#39; directive globally to suppress this message [Mon Dec 16 18:28:44.428224 2019] [mpm_prefork:notice] [pid 67455] AH00163: Apache/2.4.41 (Ubuntu) configured -- resuming normal operations [Mon Dec 16 18:28:44.428317 2019] [core:notice] [pid 67455] AH00094: Command line: \u0026#39;/usr/sbin/apache2 -D FOREGROUND\u0026#39; Press ctrl-c to interrupt the docker logs... command and get your terminal back.\nSome Settings Are Missing I found some settings were missing the first time I started the stack like this. A little research and debugging led me to believe that not all of the required configuration commands had been executed properly. In particular, I found that my large image (TIFF image) viewer wasn\u0026rsquo;t displaying anything at all, presumably because the database I imported from production was setup to use Adore-Djatoka, not IIIF Cantaloupe. The remedy was to take a snapshot of the server, then run the required migration_site_vsets.sh script inside the Apache container. Unfortunately that didn\u0026rsquo;t work for me at first and I got lots of messages indicating that Command variable-set needs a higher bootstrap level to run.... That error basically means that the drush commands inside the script need to be run from the appropriate directory so that drush can find the site and its database. So, to run it properly inside the Apache container\u0026hellip;\nApache Container Commands cd /var/www/html/sites/default /utility-scripts/isle_drupal_build_tools/migration_site_vsets.sh Note the cd command and path before the script is run, and there\u0026rsquo;s no \u0026ldquo;dot\u0026rdquo; in front of /utility-scripts.... This time everything worked except for a phpmailerException at the end of the run, but that\u0026rsquo;s of no consequence, and to be expected since this is a staging server and has no mail facilities of its own.\nA quick visit to the staging site shows that large images (for example see https://isle-stage.grinnell.edu/islandora/object/grinnell:25511) are working properly. It sure would be nice to have an automated test to verify that for me, automatically\u0026hellip; but we\u0026rsquo;ll address that issue a little later in this post. \u0026#x1f604;\nBackup the Site Database For now, let\u0026rsquo;s just make a backup of the site database for safe-keeping. We can do this by visiting the site\u0026rsquo;s home page and using the Quick Backup block at the bottom of the right-hand sidebar and selecting Backup Destination: Manual Backups Directory. This operation makes a backup of the site database and stores it in a pre-determined place\u0026hellip; in our case the docker-compose.staging.yml configuration file tells us that it\u0026rsquo;s stored somewhere in /mnt/data/DG-FEDORA/site-public. More specifically, this backup is /mnt/data/DG-FEDORA/site-public/backup_migrate/manual/DigitalGrinnell-2019-12-17T15-03-10.mysql.gz on the host, DGDockerX.\nMissing CModels and No PDF Viewer While visiting the site moments ago I noticed two more issues:\nAt https://isle-stage.grinnell.edu/admin/islandora/solution_pack_config/solution_packs the Binary cModel is missing, and A visit to https://isle-stage.grinnell.edu/islandora/object/grinnell:25500 shows a JPEG image of a single page, but since this object is a multi-page PDF, we are obviously missing our PDF viewer. So, remember back in the Installing the Missing Islandora and Custom Modules section of post 058, we commented out the installation of islandora_binary_object and islandora_pdfjs_reader like so:\ncd /var/www/html/sites/all/modules/islandora git submodule add https://github.com/DigitalGrinnell/dg7.git git submodule add https://github.com/DigitalGrinnell/idu.git # git submodule add git://github.com/discoverygarden/islandora_binary_object.git git submodule add https://github.com/discoverygarden/islandora_collection_search git submodule add https://github.com/DigitalGrinnell/islandora_mods_display.git git submodule add https://github.com/Islandora-Labs/islandora_solution_pack_oralhistories.git # git submodule add git://github.com/nhart/islandora_pdfjs_reader.git git submodule add https://github.com/Islandora-Labs/islandora_solr_collection_view.git chown -R islandora:www-data * cd /var/www/html/sites/default drush cc all Well, it\u0026rsquo;s evidently time to put them back! So, I\u0026rsquo;m taking a new snapshot of the server then I\u0026rsquo;ll try to solve this mystery. A little research tells me that the islandora_binary_object command above is pointing to the wrong project for Islandora v7 (it points to an Islandora v8 module). Also, the islandora_pdfjs_reader module has apparently been replaced by islandora_pdfjs, and it just requires some additional configuration. So, the entire command stream on the host should be:\nHost / DGDockerX Commands sudo su cd /opt/dg-islandora git submodule add https://github.com/Islandora-Labs/islandora_binary_object sites/all/modules/islandora/islandora_binary_object chown -R islandora:www-data * A look at the Installation and Configuration guidance provided in the islandora_pdfjs module\u0026rsquo;s README.md file suggests that I need to do a little more work in the Apache container so I did as it said, then selected the new PDF viewer at\nThen taking a look at our example PDF object, https://isle-stage.grinnell.edu/islandora/object/grinnell:25500, shows that it works!\nBinary Objects The https://github.com/discoverygarden/islandora_binary_object repository that Digital.Grinnell formerly used is now home to an Islandora version 8 module. :sad: That won\u0026rsquo;t do. Fortuntely, there is a version 7 copy of the old binary content model residing at https://github.com/Islandora-Labs/islandora_binary_object.\nThere\u0026rsquo;s no mention of islandora_binary_object in the host\u0026rsquo;s /opt/dg-islandora/.gitmodules file, so lets add this submodule with the following commands executed from the host:\nHost / DGDockerX Commands sudo su cd /opt/dg-islandora git submodule add https://github.com/Islandora-Labs/islandora_binary_object sites/all/modules/islandora/islandora_binary_object chown -R mcfatem:33 sites/all/modules/islandora/. Next step then is to enable the new module, and I did this by visiting the site\u0026rsquo;s module administration page. But when I toggled the Islandora Binary Object Storage on and submitted the form I got this:\nDatabaseSchemaObjectExistsException: Table islandora_binary_object_thumbnail_associations already exists. in DatabaseSchema-\u0026gt;createTable() (line 663 of /var/www/html/includes/database/schema.inc). DatabaseSchemaObjectExistsException: Table islandora_binary_object_thumbnail_mappings already exists. in DatabaseSchema-\u0026gt;createTable() (line 663 of /var/www/html/includes/database/schema.inc). Apparently the offending tables, presumably database leftovers from the production site, need to be removed before the new module can be installed. So, in the Apache container I\u0026rsquo;ll run this:\nApache Container Commands cd /var/www/html/sites/default drush sql-query \u0026ldquo;drop table islandora_binary_object_thumbnail_associations\u0026rdquo; drush sql-query \u0026ldquo;drop table islandora_binary_object_thumbnail_mappings\u0026rdquo; drush dis islandora_binary_object -y drush en islandora_binary_object -y drush cc all That seems to have done the trick. I\u0026rsquo;ll test it in the morning to confirm, right after another database backup.\nBackup the Site Database, Again Once again, we will visit the site\u0026rsquo;s home page and use the Quick Backup block at the bottom of the right-hand sidebar and selecting Backup Destination: Manual Backups Directory. This time our backup is /mnt/data/DG-FEDORA/site-public/backup_migrate/manual/DigitalGrinnell-2019-12-18T21-14-02.mysql.gz on the host, DGDockerX.\nNot quite a wrap. Be back soon\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/059-pushing-isle-to-staging/","tags":["ISLE","isle-stage.grinnell.edu","DGDockerX","staging","development","git config core.fileMode"],"title":"Pushing ISLE to Staging"},{"categories":null,"contents":"This post replaces my earlier post 021 with the same title. It is intended to chronicle my efforts to build a new ISLE v1.3.0 local development instance of Digital.Grinnell on my work-issued MacBook Air, MA7053.\nGoal The goal of this project is to spin up a pristine, local Islandora stack using the ISLE project at https://github.com/Digital-Grinnell/dg-isle/, then introduce elements like the Digital Grinnell theme and custom modules like DG7. Once these pieces are in-place and working, I\u0026rsquo;ll begin adding other critical components as well as a robust set of data gleaned from https://digital.grinnell.edu/.\nUsing This Document There are just a couple of notes regarding this document that I\u0026rsquo;d like to pass along to make it more useful.\nGists - You will find a few places in this post where I generated a gist to take the place of lengthy command output. Instead of a long stream of text you\u0026rsquo;ll find a simple link to a gist like this.\nWorkstation Commands - There are lots of places in this document where I\u0026rsquo;ve captured a series of command lines along with output from those commands in block text. Generally speaking, after each such block you will find a Workstation Commands table that can be used to conveniently copy and paste the necessary commands directly into your workstation. The tables look something like this:\nWorkstation Commands cd ~/Projects git clone https://github.com/DigitalGrinnell/ISLE cd ISLE git checkout -b ld Apache Container Commands - Similar to Workstation Commands, a tabulated list of commands may appear with a heading of Apache Container Commands. *Commands in such tables can be copied and pasted into your command line terminal, but ONLY after you have opened a shell into the Apache container. The asterisk (*) at the end of the table heading is there to remind you of this! See the next section of this document for additional details. These tables looks something like this: Apache Container Commands* cd /var/www/html/sites/all/modules/contrib drush dl backup_migrate drush -y en backup_migrate Opening a Shell in the Apache Container This is something I find myself doing quite often during ISLE configuration, so here\u0026rsquo;s a reminder of how I generally do this\u0026hellip;\nWorkstation Commands cd ~/Projects/ISLE docker exec -it isle-apache-ld bash Cloning to Local The first step is to clone my fork of ISLE, namely dg-isle, to my workstation at ~/Projects/GitHub/dg-isle, checkout the local-dg-fedora branch there, and begin, like so\u0026hellip;\nWorkstation Commands cd ~/Projects/GitHub git clone https://github.com/Digital-Grinnell/dg-isle.git cd dg-isle git checkout local-dg-fedora Cleaning Up I typically use the following command stream to clean up any Docker cruft before I begin anew. Note: Uncomment the third line ONLY if you want to delete images and download new ones. If you do, be patient, it could take several minutes depending on connection speed.\nWorkstation Commands docker stop $(docker ps -q) docker rm -v $(docker ps -qa) # docker image rm $(docker image ls -q) \u0026ndash;force docker system prune \u0026ndash;force Connecting to FEDORA The docker-compose.override.yml file in the local-dg-fedora branch of my dg-isle project includes 3 lines that direct FEDORA and FGSearch to use the mounted and pre-configured /Volumes/DG-FEDORA USB stick for object storage. The commands and process required to use the USB stick are presented in post 046, \u0026ldquo;DG-FEDORA: A Portable Object Repository\u0026rdquo;.\nRestarting the Stack Moving to Step 7 in the install documentation produced this gist.\nWorkstation Commands cd ~/Projects/GitHub/dg-isle time docker-compose up -d Running the Drupal Installer Script Moving on to Step 8 according to the documentation\u0026hellip;\nWorkstation Commands cd ~/Projects/GitHub/dg-isle time docker exec -it isle-apache-ld bash /utility-scripts/isle_drupal_build_tools/isle_islandora_installer.sh It was at this point I discovered a new gem in iTerm2: If you hit Command + shift + A the terminal will select/highlight all of the output from the last command. Exactly what I was hoping for. I\u0026rsquo;ve copied all that output and stuck it in this gist rather than pasting it all here.\nTesting the Site Moving on to Step 9 in the documentation\u0026hellip;\nA web browser visit to https://dg.localdomain/ shows that the standard ISLE stack is working~, and I was able to successfully login as admin with my super-secret password.\nInstalling the DG Theme Step 10 in the documentation calls for ingest of some sample objects, but this is where I depart from the script since I\u0026rsquo;ve done this a number of times before.\nSo my focus here turned to installing the digital_grinnell_bootstrap theme instead. Initially I did this with a pair of git clone... commands, but later in this process I\u0026rsquo;m tasked with saving my Islandora / Drupal code as-a-whole into a larger Git repository that will include these themes. Cloning a Git repository inside another can lead to significant workflow problems, so lets use Git submodules instead.\nApache Container Commands* cd /var/www/html/sites/all/themes git submodule add -b 7.x-3.x https://github.com/drupalprojects/bootstrap.git chown -R islandora:www-data * mkdir -p /var/www/html/sites/default/themes cd /var/www/html/sites/default/themes git submodule add https://github.com/DigitalGrinnell/digital_grinnell_bootstrap.git chown -R islandora:www-data * cd /var/www/html/sites/default drush -y pm-enable bootstrap digital_grinnell_bootstrap drush vset theme_default digital_grinnell_bootstrap Success! The theme is in place and active on my dg.localdomain site. Just one more tweak here\u0026hellip;\nI visited #overlay=admin/appearance/settings/digital_grinnell_bootstrap and made sure ONLY the following boxes are checked:\nLogo Shortcut Icon Use the default logo Use the default shortcut icon All other theme settings should be default values and need not be changed.\nA visit to the site with a refresh showed that this worked!\nInstall the Islandora Multi-Importer (IMI) It\u0026rsquo;s important that we take this step BEFORE any that follow, otherwise the module will not install properly for reasons unknown. Note that I\u0026rsquo;m installing a Digital.Grinnell-specific fork of the module here, and the process is this:\nApache Container Commands* cd /var/www/html/sites/all/modules/islandora git submodule add https://github.com/DigitalGrinnell/islandora_multi_importer.git chown -R islandora:www-data * cd islandora_multi_importer composer install drush -y en islandora_multi_importer Install the Missing Backup and Migrate Module The Backup and Migrate module will be needed to quickly get our new ISLE configured as we\u0026rsquo;d like. Install it like so:\nApache Container Commands* cd /var/www/html/sites/all/modules/contrib drush dl backup_migrate drush -y en backup_migrate Backup and Restore the Database Using Backup and Migrate From the https://digital.grinnell.edu production site\u0026hellip;\nLogin as System Admin From the Development menu (on the right) select Clear Cache On the home page (https://digital.grinnell.edu), scroll to the bottom of the right-hand column. Use the Quick Backup dialog, with all the defaults, to create and download a fresh backup. Alternatively, you could\u0026hellip;\nNavigate to https://digital.grinnell.edu/admin/config/system/backup_migrate/export/advanced In the Load Settings box select Default Settings w/ Users Click Backup now to backup the site Click Save to save the file to your workstation Downloads folder In the https://dg.localdomain site\u0026hellip;\nVisit #overlay=admin/config/system/backup_migrate/restore Click the Restore tab Select the Restore from an uploaded file option Click Browse in the Upload a Backup File Navigate to your workstation Downloads folder and choose the backup file created moments ago Click Restore now Navigate your browser back to https://dg.localdomain/ Take note of any warnings or errors that may be present. Restore Results\u0026hellip;Lots of Warnings OK, so when I did all of the above backup/restore process what I got back in the \u0026ldquo;Navigate your browser\u0026hellip;\u0026rdquo; step was an unreadable host of warnings. Without panic I very calmly returned to my terminal and the shell open in the Apache container and:\nApache Container Commands* cd /var/www/html/sites/default drush cc all This returned a number of warnings about missing modules and such. No matter, that was to be expected and the full list of warnings is captured in this gist.\nThe remedy for most of these missing bits was to do the following while still in my open Apache terminal/shell:\nApache Container Commands* cd /var/www/html/sites/default drush dl masquerade announcements email git_deploy maillog r4032login smtp views_bootstrap admin_theme google_analytics_counter drush cc all Visiting the site again shows that most of the Drupal missing modules are happy now, but there are still a number of Islandora bits missing, so I was left with the output as shown in this gist.\nNext steps and sections, still working \u0026ldquo;off-script\u0026rdquo;, will install all of these missing parts.\nInstalling the Missing Islandora and Custom Modules If I recall correctly, all of the missing Islandora and custom modules listed above can be found in the Apache container on DGDocker1, our production instance of ISLE, at /var/www/html/sites/all/modules/islandora. So I started this process by opening a new shell in the aforementioned container on DGDocker1 like so:\n╭─mcfatem@dgdocker1 ~ ╰─$ docker exec -it isle-apache-dg bash root@90ae0691e764:/# cd /var/www/html/sites/all/modules/islandora root@90ae0691e764:/var/www/html/sites/all/modules/islandora# l dg7/ islandora_multi_importer/ islandora_solution_pack_compound/ idu/ islandora_oai/ islandora_solution_pack_disk_image/ islandora/ islandora_object_lock/ islandora_solution_pack_entities/ islandora_bagit/ islandora_ocr/ islandora_solution_pack_image/ islandora_batch/ islandora_openseadragon/ islandora_solution_pack_large_image/ islandora_binary_object/ islandora_paged_content/ islandora_solution_pack_newspaper/ islandora_book_batch/ islandora_pathauto/ islandora_solution_pack_oralhistories/ islandora_bookmark/ islandora_pdfjs/ islandora_solution_pack_pdf/ islandora_checksum/ islandora_pdfjs_reader/ islandora_solution_pack_video/ islandora_checksum_checker/ islandora_premis/ islandora_solution_pack_web_archive/ islandora_collection_search/ islandora_scholar/ islandora_sync/ islandora_context/ islandora_simple_workflow/ islandora_videojs/ islandora_feeds/ islandora_solr_collection_view/ islandora_webform/ islandora_fits/ islandora_solr_facet_pages/ islandora_xacml_editor/ islandora_image_annotation/ islandora_solr_metadata/ islandora_xml_forms/ islandora_importer/ islandora_solr_search/ islandora_xmlsitemap/ islandora_internet_archive_bookreader/ islandora_solr_views/ islandora_xquery/ islandora_jwplayer/ islandora_solution_pack_audio/ objective_forms/ islandora_marcxml/ islandora_solution_pack_book/ php_lib/ Apache Container Commands* cd /var/www/html/sites/all/modules/islandora ls My recollection has been confirmed! So the next step was to visit each missing module\u0026rsquo;s folder to see what git remote each is tied to, like so:\nroot@90ae0691e764:/var/www/html/sites/all/modules/islandora# cd dg7; git remote -v origin\thttps://github.com/DigitalGrinnell/dg7.git (fetch) origin\thttps://github.com/DigitalGrinnell/dg7.git (push) root@90ae0691e764:/var/www/html/sites/all/modules/islandora/dg7# cd ../idu; git remote -v origin\thttps://github.com/DigitalGrinnell/idu.git (fetch) origin\thttps://github.com/DigitalGrinnell/idu.git (push) root@90ae0691e764:/var/www/html/sites/all/modules/islandora/idu# cd ../islandora_binary_object/; git remote -v origin\tgit://github.com/discoverygarden/islandora_binary_object.git (fetch) origin\tgit://github.com/discoverygarden/islandora_binary_object.git (push) root@90ae0691e764:/var/www/html/sites/all/modules/islandora/islandora_binary_object# cd ../islandora_collection_search/; git remote -v origin\thttps://github.com/discoverygarden/islandora_collection_search (fetch) origin\thttps://github.com/discoverygarden/islandora_collection_search (push) root@90ae0691e764:/var/www/html/sites/all/modules/islandora/islandora_collection_search# cd ../islandora_mods_display/; git remote -v origin\thttps://github.com/DigitalGrinnell/islandora_mods_display.git (fetch) origin\thttps://github.com/DigitalGrinnell/islandora_mods_display.git (push) root@90ae0691e764:/var/www/html/sites/all/modules/islandora/islandora_mods_display# cd ../islandora_solution_pack_oralhistories/; git remote -v origin\thttps://github.com/Islandora-Labs/islandora_solution_pack_oralhistories.git (fetch) origin\thttps://github.com/Islandora-Labs/islandora_solution_pack_oralhistories.git (push) root@90ae0691e764:/var/www/html/sites/all/modules/islandora/islandora_solution_pack_oralhistories# cd ../islandora_pdfjs_reader/; git remote -v origin\tgit://github.com/nhart/islandora_pdfjs_reader.git (fetch) origin\tgit://github.com/nhart/islandora_pdfjs_reader.git (push) root@90ae0691e764:/var/www/html/sites/all/modules/islandora/islandora_pdfjs_reader# cd ../islandora_solr_collection_view/; git remote -v origin\thttps://github.com/Islandora-Labs/islandora_solr_collection_view.git (fetch) origin\thttps://github.com/Islandora-Labs/islandora_solr_collection_view.git (push) Apache Container Commands (on PRODUCTION ISLE only!)* cd /var/www/html/sites/all/modules/islandora cd dg7; git remote -v cd ../idu; git remote -v cd ../islandora_binary_object/; git remote -v cd ../islandora_collection_search/; git remote -v cd ../islandora_mods_display/; git remote -v cd ../islandora_solution_pack_oralhistories/; git remote -v cd ../islandora_pdfjs_reader/; git remote -v cd ../islandora_solr_collection_view/; git remote -v Note that I did NOT bother with the islandora_multi_importer (IMI) directory since I know for a fact that IMI requires installation via Composer. I also didn\u0026rsquo;t bother looking for transcript_ui because it is a known sub-module of islandora_solution_pack_oralhistories.\nIt looks like all of the others can just be added as Git submodules like so:\nApache Container Commands* cd /var/www/html/sites/all/modules/islandora git submodule add https://github.com/DigitalGrinnell/dg7.git git submodule add https://github.com/DigitalGrinnell/idu.git # git submodule add git://github.com/discoverygarden/islandora_binary_object.git git submodule add https://github.com/discoverygarden/islandora_collection_search git submodule add https://github.com/DigitalGrinnell/islandora_mods_display.git git submodule add https://github.com/Islandora-Labs/islandora_solution_pack_oralhistories.git # git submodule add git://github.com/nhart/islandora_pdfjs_reader.git git submodule add https://github.com/Islandora-Labs/islandora_solr_collection_view.git chown -R islandora:www-data * cd /var/www/html/sites/default drush cc all The chown command line above was required to bring ALL of the new modules\u0026rsquo; ownership into line with everything else in dg.localdomain. Also note that two of the lines, for islandora_binary_object and islandora_pdfjs_reader, are commented out because of known issues with installation of those modules.\nTemporarily Eliminate Warnings So my site, https://dg.localdomain/, is still issuing a few annoying warnings about missing pieces. It\u0026rsquo;s a safe bet that we don\u0026rsquo;t need these modules, at least not right now, so just do this:\nApache Container Commands* cd /var/www/html/sites/default drush -y dis islandora_binary_object islandora_pdfjs_reader drush sqlq \u0026ldquo;DELETE FROM system WHERE name = \u0026lsquo;islandora_binary_object\u0026rsquo; AND type = \u0026lsquo;module\u0026rsquo;;\u0026rdquo; drush sqlq \u0026ldquo;DELETE FROM system WHERE name = \u0026lsquo;islandora_pdfjs_reader\u0026rsquo; AND type = \u0026lsquo;module\u0026rsquo;;\u0026rdquo; drush sqlq \u0026ldquo;DELETE FROM system WHERE name = \u0026lsquo;islandora_google_scholar\u0026rsquo; AND type = \u0026lsquo;module\u0026rsquo;;\u0026rdquo; drush sqlq \u0026ldquo;DELETE FROM system WHERE name = \u0026lsquo;phpexcel\u0026rsquo; AND type = \u0026lsquo;module\u0026rsquo;;\u0026rdquo; drush sqlq \u0026ldquo;DELETE FROM system WHERE name = \u0026rsquo;ldap_servers\u0026rsquo; AND type = \u0026lsquo;module\u0026rsquo;;\u0026rdquo; drush sqlq \u0026ldquo;DELETE FROM system WHERE name = \u0026lsquo;ihc\u0026rsquo; AND type = \u0026lsquo;module\u0026rsquo;;\u0026rdquo; drush cc all drush cc all You saw that correctly, I did drush cc all twice, just for good measure. Now, just a couple more issues to deal with\u0026hellip;\nNeed a private File System At this point the system is still issuing some warnings, and the most annoying is:\nWarning: file_put_contents(private:///.htaccess): failed to open stream: \u0026amp;quot;DrupalPrivateStreamWrapper::stream_open\u0026amp;quot; call failed in file_put_contents() (line 496 of /var/www/html/includes/file.inc). A visit in my browser to https://dg.localdomain/#overlay=admin/reports/status helps to pinpoint the problem\u0026hellip; we don\u0026rsquo;t yet have a private file system. Let\u0026rsquo;s create one like so:\nApache Container Commands* cd /var/www mkdir private chown islandora:www-data private chmod 774 private cd /var/www/html/sites/default drush cc all Now that same status report, https://dg.localdomain/#overlay=admin/reports/status, shows that we are still operating in maintenance mode, and some of our newest modules may require database updates. To remedy those two conditions:\nApache Container Commands* cd /var/www/html/sites/default drush updatedb -y drush vset maintenance_mode 0 drush cc all Solr Schema is NOT Right Ok, at this point I believe that I have a good Drupal database, a working Fedora repository, and a solid Islandora/Drupal codebase; but the Solr schema associated with this configuration is NOT up-to-speed with Digital.Grinnell\u0026rsquo;s so the dg7 code and the dg7_collection view are expecting Solr fields that do not yet exist here. What to do?\nSave the current database using Backup and Migrate. Save the current codebase to the host using mkdir -p ../dg-islandora; docker cp isle-apache-ld:/var/www/html/. ../dg-islandora and putting all of it into a new Digital-Grinnell/dg-islandora repository on GitHub for safe-keeping. Update the FEDORA and Solr schema and configuration using the guidance found in https://github.com/Digital-Grinnell/ISLE-DG-Essentials/blob/master/README.md Try pulling up the site again. Huzzah! It works! Final Step\u0026hellip;Capture the Working Code in dg-islandora To wrap this up I followed Step 11 in the install-local-new.md document to capture the state of my Islandora/Drupal code. In doing so I created my PRIVATE code repository, dg-islandora.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/058-rebuilding-isle-ld/","tags":["ISLE","local","development"],"title":"Rebuilding ISLE-ld (for Local Development)"},{"categories":null,"contents":" Granular ISLE Customization This post is part of a series describing Digital.Grinnell customizations to ISLE, in a \u0026ldquo;granular\u0026rdquo; format\u0026hellip; one small customization at a time. An index of all documents in this series is included at the end of Granular ISLE Customization: Series Guidelines. Goal Statement In this \u0026ldquo;granular\u0026rdquo; post we will install Backup and Migrate, a tremendous Drupal module that I use extensively for backup, restoration, migration and maintenance of Digital.Grinnell.\nInstall and Enable the Backup and Migrate Module using Drush DG7 is installed and enabled in the same manner as most Drupal or Islandora contrib modules, like so:\nApache Container Commands* cd /var/www/html/sites/all/modules/contrib drush dl backup_migrate drush -y en backup_migrate And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/057-granular-isle-customization-instal-backup-and-migrate/","tags":["granular","customization","ISLE","Backup","Migrate"],"title":"Granular ISLE Customization: Install 'Backup and Migrate'"},{"categories":null,"contents":" Credits: This document is an abstraction of some fine documentation authored and posted by my ICG colleague and friend, David Keiser-Clark. ICG Git Workflow: How to work with Git The examples in this document use my work with the Islandora Collaboration Group\u0026rsquo;s (ICG) ISLE-Drupal-Build-Tools repository, as well as my fork and local clone of that repository. I recommend having a look at the GitHub Glossary for a list of terms used frequently in this post, and many of the referenced documents. The terms original, canonical, and upstream are also used in this post to describe the GitHub repository at the root of the project being managed. Configuration: In most cases you will do Steps 1-3 only once! If you move to a new machine execute Steps 2-3 only. Only \u0026ldquo;fork\u0026rdquo; once! Do not repeat Step 1 if you\u0026rsquo;ve already forked the original/canonical repo. Only \u0026ldquo;clone\u0026rdquo; once! Do not repeat Step 2 if you already have a local clone of your fork. Only add this remote once! Do not perform this step if your local repo already has an upstream remote. Always fork the repo (repository) you are working on. This is accomplished by logging into your GitHub account and selecting Fork near the top right of the repo\u0026rsquo;s page. Navigate your browser to the GitHub project you wish to work on. Example: https://github.com/Islandora-Collaboration-Group/ISLE-Drupal-Build-Tools Click the Fork button near the top right of the repo\u0026rsquo;s GitHub page. This will either create a new fork in your own GitHub account, or prompt you to choose an account if you have more than one. In either case, make a note of where the fork is created! In this document we\u0026rsquo;ll reference your fork\u0026rsquo;s URI as upstream. Example: https://github.com/Digital-Grinnell/ISLE-Drupal-Build-Tools Clone your fork down to your local machine. Navigate your browser to the fork. Example: https://github.com/Digital-Grinnell/ISLE-Drupal-Build-Tools Click on the Clone button to copy the fork\u0026rsquo;s URI to your clipboard. Open terminal/shell/powershell/cmd, navigate to your preferred project \u0026ldquo;parent\u0026rdquo; directory, and git clone \u0026lt;paste from clipboard\u0026gt;. Change into the directory (cd) with the files you just cloned. Before you start working, add an upstream pointer to the original/canonical repo that you forked. Navigate your browser back to the original/canonical GitHub project. Example: https://github.com/Islandora-Collaboration-Group/ISLE-Drupal-Build-Tools From this repo, NOT your fork or local clone, click on the Clone button and copy the https URI to your clipboard. In terminal/shell/powershell/cmd enter git remote add upstream \u0026lt;paste from clipboard\u0026gt;. Make certain your master branches are even with the original/canonical master STOP! Get up-to-date before you do anything, fetch your remotes so your local clone has the most recent commits. Change into the directory (cd) with the files you cloned. In terminal/shell/powershell/cmd enter git fetch --all. Checkout and pull the upsteam master to your local master branch. Checkout your master: git checkout master Pull the upstream master into yours so your local is up-to-date: git pull upstream master Push your local master branch BACK to your fork in GitHub. If all is well and your git pull... resulted in a fast-forward or \u0026ldquo;Already up to date.\u0026rdquo;, then: git push origin master If your git pull... did not fast-forward and a merge message appeared, then there were differences in your branches. Never work on master. Create an issue and a topic/fix/enhancement/document branch for your work, and have at! Create an issue for your work. Navigate your browser to the original/canonical GitHub project you wish to work on. Example: https://github.com/Islandora-Collaboration-Group/ISLE-Drupal-Build-Tools Find and open the Issues tab (its icon is an exclamation point in a circle) near the top of the page. Look through the list of all issues, both Open and Closed, for any mention of the problem you wish to solve. If you find an existing issue, study it and determine if you can add your work to the existing issue. If an appropriate existing issue is not found, click New issue to create one and describe the problem you will be attacking. Take note of the new, or existing, sequential number assigned to your issue. In subsequent steps you should refer to your issue using its number (Example: #20) in references like these examples: #20, issue-20. Create your branch and check it out. Create a branch with: git branch \u0026lt;helpful and identifying name\u0026gt;. Example: git branch issue-20 Checkout your new branch with git checkout \u0026lt;helpful and identifying name\u0026gt;. Example: git checkout issue-20 Start your work and commit locally, aka \u0026ldquo;save your work\u0026rdquo;, at times (probably more than once) that feel logical. Create logical checkpoints (i.e., commits) when you feel you\u0026rsquo;ve finished on a particular \u0026ldquo;part\u0026rdquo; of your work. Example: You\u0026rsquo;ve just created a new file and added some stubbed content: Commit it! Commits are references in your work and can be helpful if you need to go back to an earlier version of your work, sort of like an \u0026ldquo;undo\u0026rdquo; command. By committing regularly, you give yourself utmost flexibility and it\u0026rsquo;s a good practice/habit. Creating commits.\nIn terminal/shell/powershell/cmd enter git status to see a list of files changed, added, and removed. Use git add \u0026lt;file\u0026gt; or git rm \u0026lt;file\u0026gt; to stage (add or remove) files from your commit. If you want to add all files to the commit you may shorthand it with git add -A; the -A flag is short for \u0026ldquo;All\u0026rdquo;. Create your commit after files are staged: git commit. Enter a commit message that is helpful for you and us! Helpful hint: Always write in the present tense: \u0026ldquo;Update \u0026lt;somefile.ext\u0026gt; to include all of the appropriate modules.\u0026rdquo; Continue your work, going through this step as many times as needed. Finalizing and preparing for a pull request (PR) Pushing back to origin will update your fork in GitHub. After your final commit and feel you\u0026rsquo;re ready to PR back to the project: git push origin \u0026lt;name-of-your-branch\u0026gt;. Visit your forked GitHub repo and switch branches to your new branch. Select New pull request (top-left) and tell GitHub, if it isn\u0026rsquo;t already, to compare against remote branches. Select the original/canonical master first, then your repo and branch. Create the pull request (PR) and send it. Enter a description of what your commits do as a whole. How should this be tested? Who should be notified? @mention them if you know. Is there anything else we should know before we review and test your work? With your description complete click the Create Pull Request button and you\u0026rsquo;re done! Thank you! And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/056-how-to-work-with-git/","tags":["Git","GitHub","workflow","fork","clone","commit","origin","upstream"],"title":"How to Work with Git"},{"categories":null,"contents":"The first step is to run git config -l to see what the current configuration is. If the user.name and/or user.email properties are incorrect, change them using something like this:\ngit config --global user.name \u0026#34;Mark McFate\u0026#34; git config --global user.email \u0026#34;yourEMail@address.here\u0026#34; That\u0026rsquo;s only half the battle. I love OSX and the Keychain Access app is wonderful, except when I\u0026rsquo;m working with git and GitHub in a terminal, which I do quite often. The real problem is that I have 4 different identities in GitHub\u0026hellip; crazy, I know. Changing from one identity to another has been a real pain-in-the-a$$, up until I found this gem of a post.\nBasically, what it tells me to do from the Keychain Access app is this:\nIn Finder, search for the Keychain Access app. In Keychain Access, search for github.com. Find the \u0026ldquo;internet password\u0026rdquo; entry for github.com. Edit or delete the entry accordingly. Works like a charm! The next git... command I specify, if necessary, will prompt me for my GitHub username and password \u0026ndash; or more appropriately, my Personal Access Token, since I now have 2-factor authentication enabled \u0026ndash; and those are automatically saved until I repeat the above process. Woot!\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/055-easily-change-github-user-in-osx/","tags":["OSX","GitHub","user","login","keychain","credentials","terminal"],"title":"Easily Change GitHub User in OSX"},{"categories":null,"contents":"Grinnell\u0026rsquo;s dockerized version of Omeka-S has been running for several weeks now, and we recently ran out of disk space for object data on the host, a CentOS node we named DGDocker2. My posts 041, Configuring DGDocker2 and 042, My dockerized-server Config address the original configuration of DGDocker2 in detail.\nOmeka-S is configured on DGDocker2 to reside in /opt/omeka-s-docker, and inside that directory is a subdirectory named volume. The portions of the application stack that deliver Omeka-S are configured largely in /opt/omeka-s-docker/docker-compose.yml, and portions of that file related to this discussion include:\nservices: ... omeka: ... volumes: - omeka:/var/www/html/volume volumes: ... omeka: So, there\u0026rsquo;s a Docker-managed volume named omeka that maps into the omeka container as /var/www/html/volume. This ../volume directory contains two subdirectories, ./config and ./files. We are really only interested in ./files, but because of the way things are mapped I found it prudent to deal with /opt/omeka-s-docker/volume itself, rather than trying to separate ./config from ./files.\nThe quick and easy solution to change this configuration did NOT work! Consequently, I\u0026rsquo;m hiding that work from public view\u0026hellip;until a long-term solution is found.\nSince the quick-fix didn\u0026rsquo;t work, we are going to extend the original system, or \u0026ldquo;root\u0026rdquo;, disk and supplement the stack with some automated \u0026ldquo;backup\u0026rdquo; operations to capture any changes made so that we can eventually roll those into a properly configured and dockerized Omeka-S. The backup operations involve the following:\nIn the mariadb container, a new script at /daily-script.sh with the following contents:\n#!/bin/bash ## This is daily-script.sh for the Omeka-S \u0026#34;mariadb\u0026#34; container. /usr/bin/mysqldump -u root --password=${MYSQL_ROOT_PASSWORD} omeka \u0026gt; omeka-database-backup.sql On the DGDocker2 host, there\u0026rsquo;s a new script at /opt/omeka-s-docker/daily-backup.sh with contents of:\n#!/bin/bash mkdir -p /omeka-digital/temporary docker exec mariadb ./daily-script.sh docker cp mariadb:/omeka-database-backup.sql /omeka-digital/temporary/omeka-database-backup.sql docker cp omeka:/var/www/html/modules /omeka-digital/temporary docker cp omeka:/var/www/html/themes /omeka-digital/temporary tar -zcvPf \u0026#34;/omeka-digital/backups/$(date \u0026#39;+%Y-%m-%d\u0026#39;).tar.gz\u0026#34; /omeka-digital/temporary --remove-files And finally, there\u0026rsquo;s a new \u0026ldquo;root\u0026rdquo; crontab entry on DGDocker2 of:\n0 18 * * * /opt/omeka-s-docker/daily-backup.sh The result of all this should be a daily backup of about 17MB saved on the NFS share at /omeka-digital/backups/\u0026lt;today\u0026gt;.tar.gz, where \u0026lt;today\u0026gt; is the current date in a format like 2019-11-15. For example, a test backup run earlier today produced:\n╭─root@dgdocker2 /omeka-digital/backups ╰─# ll total 17M -rw-r--r--. 1 root root 17M Nov 15 14:44 2019-11-15.tar.gz And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/054-moving-omeka-s-data-to-nfs/","tags":["Omeka-S","NFS","backup"],"title":"Moving Omeka-S Data to NFS"},{"categories":null,"contents":"For some time now Digital.Grinnell has been using ISLE in production, with an instance that was built from a \u0026ldquo;non-standard\u0026rdquo; workflow, one that was a little different than the Git workflow established in the ISLE install docs. Consequently, DG\u0026rsquo;s situation is a little different than what\u0026rsquo;s documented for ISLE, so my mission here is to describe and execute the steps I\u0026rsquo;m taking to get back into the documented workflow.\nThe steps will involve and achieve the following:\nEstablish a working, local instance of my production DG site with an ISLE-1.3.0 code base. Create (or find) and run a script to remove all bits of past Git workflows from the working local instance. Follow the Git workflow steps of install-local-new.md and install-local-migrate.md to create NEW dg-isle and dg-islandora projects from the working, local DG. These are the yourprojectnamehere-isle and yourprojectnamehere-islandora pieces mentioned in the documents. The following ISLE Workflows presentation from one of the ICG\u0026rsquo;s recent events should prove useful in this adventure.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/053-dgs-isle-git-workflow/","tags":["ISLE","Git","worflow","migration"],"title":"DG's ISLE Git Workflow"},{"categories":null,"contents":" Granular ISLE Customization This post is part of a series describing Digital.Grinnell customizations to ISLE, in a \u0026ldquo;granular\u0026rdquo; format\u0026hellip; one small customization at a time. An index of all documents in this series is included at the end of Granular ISLE Customization: Series Guidelines. Goal Statement In this \u0026ldquo;granular\u0026rdquo; post we will install Digital.Grinnell\u0026rsquo;s custom-built theme, namely digital_grinnell_bootstrap. The experience documented here involves an existing ISLE instance created using Building ISLE 1.3.0 (ld) for Local Development.\nCommands The install and config process was simply this stream of commands entered directly into the running Apache container:\nApache Container Commands* cd /var/www/html/sites/all/themes git clone https://github.com/drupalprojects/bootstrap.git chown -R islandora:www-data * cd bootstrap git checkout 7.x-3.x drush -y en bootstrap mkdir -p /var/www/html/sites/default/themes cd /var/www/html/sites/default/themes git clone https://github.com/DigitalGrinnell/digital_grinnell_bootstrap.git chown -R islandora:www-data * cd digital_grinnell_bootstrap drush -y pm-enable digital_grinnell_bootstrap drush vset theme_default digital_grinnell_bootstrap Using git submodule Rather Than git clone If the git clone... commands here report errors consider changing them to git submodule add... commands. Using git clone inside of a Git repo can lead to problems down-the-line.\nTweaking the Local Site Success! The theme is in place and active on my dg.localdomain local site. Just a couple more tweaks here\u0026hellip;\nI visited https://dg.localdomain/#overlay=admin/appearance/settings/digital_grinnell_bootstrap and made sure ONLY the following boxes are checked:\nLogo Shortcut Icon Use the default logo Use the default shortcut icon All other theme settings should be default values and need not be changed.\nThe other tweak\u0026hellip; visit https://dg.localdomain/#overlay=admin/structure/block and turn the Search form OFF by setting its block region to -None-. Make sure you save these changes.\nIt Works! A visit to the site with a refresh showed that this worked!\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/052-granular-isle-customization-installing-the-dg-theme/","tags":["granular","customization","ISLE","DG","theme"],"title":"Granular ISLE Customization: Installing the DG Theme"},{"categories":null,"contents":"This is a follow-up to previous posts 034, Building ISLE 1.2.0 (ld) and 037, Migrating Digital.Grinnell (DG) to ISLE 1.2.0 (ld) for Local Development where I successfully completed a \u0026ldquo;local\u0026rdquo; build of ISLE v1.2.0 and subsequently started \u0026ldquo;customization\u0026rdquo; of that local instance. So, this post\u0026rsquo;s intent is to complete the goal stated in Migrating Digital.Grinnell (DG) to ISLE 1.2.0 (ld) for Local Development, but for ISLE 1.3.0, specifically to:\nThe goal of this project is to spin up a pristine, local Islandora stack using an updated fork of the ISLE project at https://github.com/Digital-Grinnell/dg-isle, then introduce elements like the Digital Grinnell theme and custom modules like DG7. Once these pieces are in-place and working, I\u0026rsquo;ll begin adding other critical components as well as a robust set of data gleaned from https://digital.grinnell.edu.\nAs before, this effort will involve an ld, or local development, instance of Digital.Grinnell on one of my Mac workstations. Unlike my previous work, this instance will follow the guidance of a different document, specifically install-local-migrate.md.\nHost: MA6879 For portability, and longevity, this work is being conducted on MA6879, the \u0026ldquo;student\u0026rdquo; Mac Mini that normally resides in my office on campus.\nFork Synchronization Before beginning this process I need to get my Github environment updated by synchronizing my ISLE fork with the canonical copy. I followed this workflow to do so, like this:\nWorkstation Commands cd ~/Projects git clone https://github.com/McFateM/ISLE.git cd ISLE git remote add upstream https://github.com/Islandora-Collaboration-Group/ISLE.git git fetch upstream git pull upstream master git push origin atom . Nice! In case you haven\u0026rsquo;t seen it before, the last command in that sequence, atom ., simply opens my Atom editor to the new local instance of the ISLE project.\nInstalling per ISLE\u0026rsquo;s install-local-migrate.md This is first-and-foremost a local development copy of ISLE, but with considerable Digital.Grinnell customization, so I\u0026rsquo;m following the process outlined in the project\u0026rsquo;s ./docs/install/install-local-migrate.md. References to Step X that follow refer to corresponding sections of https://github.com/Islandora-Collaboration-Group/ISLE/blob/master/docs/install/install-local-migrate.md. Each section or \u0026ldquo;Step\u0026rdquo; listed below is also a link back to the corresponding section of the canonical document.\nStep 0: Copy Production Data to Your Personal Computer Drupal Site Files and Code Per the aforementioned guidance, I did this\u0026hellip;\nOpened a zsh shell (terminal) on Digital.Grinnell production node DGDocker1 as user islandora using iTerm2 at ssh://islandora@dgdocker1.grinnell.edu. Then, in that shell on the DGDocker1 host I did:\nDGDocker1 Commands cd ~ mkdir -p migration-copy/var/www/html/sites/default/files cd migration-copy docker cp isle-apache-dg:/var/www/html/sites/default/files/. var/www/html/sites/default/files/ Note that the -p option on mkdir is critical, it will create all, or part of, the entire directory tree as-specified, if it does not already exist. The final command, docker cp..., subsequently takes advantage of this new directory tree and copies all of the Drupal .../default/files from the production container to a similar directory on the host, DGDocker1 in this case.\nFor the 2nd bullet item in this step, I\u0026rsquo;ve identified that my Drupal/Islandora customizations should eventually reside in the private repo that is https://github.com/Digital-Grinnell/dg-islandora.\nDrupal Site Database For this process I already have a workflow in place, and it\u0026rsquo;s a little different than what\u0026rsquo;s documented. So here\u0026rsquo;s what I did:\nVisit my production site at https://digital.grinnell.edu. Login as the System Admin, that\u0026rsquo;s User 1 or the super-user in Drupal terms. The home page at https://digital.grinnell.edu provides, in the right-hand menu bar, a Management menu with a first option to Clear cache. Click it. The home page also provides a Quick Backup block where the default options do a great job of backing up only what\u0026rsquo;s needed. Accept all defaults and click the Backup Now button. This feature takes the site offline, makes and downloads a backup of the database (in my case it created digital.grinnell.edu-2019-10-31T14-07-37.mysql), and brings the site back online\u0026hellip;\u0026ldquo;automagically\u0026rdquo;. Fedora Hash Size (Conditional) I found this section a little confusing, which I assume means that it does not apply to Digital.Grinnell. Moving on.\nSolr Schema \u0026amp; Islandora Transforms This section definitely applies to Digital.Grinnell! \u0026#x1f60a; Since I recently completed spin-up of a \u0026ldquo;Demo\u0026rdquo; ISLE using install-local-new.md, and since this is \u0026ldquo;not my first rodeo\u0026rdquo;, I choose to take the \u0026ldquo;advanced\u0026rdquo; path here and will \u0026ldquo;diff \u0026amp; merge current production customization edits into ISLE configs\u0026rdquo;. Wish me luck. \u0026#x1f340;\nTo begin this portion of the process I created a new ~/diff-and-merge-customizations directory structure and with https://isle.localdomain running the Demo, I copied files from it like so:\nWorkstation Commands mkdir -p ~/diff-and-merge-customizations/ld mkdir -p ~/diff-and-merge-customizations/prod cd ~/diff-and-merge-customizations/ld docker cp isle-solr-ld:/usr/local/solr/collection1/conf/schema.xml schema.xml docker cp isle-fedora-ld:/usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/foxmlToSolr.xslt foxmlToSolr.xslt docker cp isle-fedora-ld:/usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/islandora_transforms islandora_transforms I subsequently pulled the three diff targets from my production instance of ISLE using these commands on DGDocker1, followed by another set on my workstation:\nDGDocker1 Commands mkdir -p ~/migration-copy/solr mkdir -p migration-copy/fedora cd migration-copy/solr docker cp isle-solr-dg:/usr/local/solr/collection1/conf/schema.xml . cd ../fedora docker cp isle-fedora-dg:/usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/foxmlToSolr.xslt . docker cp isle-fedora-dg:/usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/islandora_transforms islandora_transforms Workstation Commands cd ~/diff-and-merge-customizations/prod rsync -aruvi islandora@dgdocker1.grinnell.edu:migration-copy/solr/. . rsync -aruvi islandora@dgdocker1.grinnell.edu:migration-copy/fedora/. . This left me with a local ~/diff-and-merge-customizations directory with sub-dirs ld and prod. The diff-and-merge operation subsequently compared the contents of these two sub-dirs, merging the differences from prod into the corresponding files found in ld. I did both the diff and merge operations using Atom.\nThe command and output from the first diff of the islandora_transforms was:\n╭─digital@MA6879 ~ ╰─$ cd ~/diff-and-merge-customizations/ld ╭─digital@MA6879 ~/diff-and-merge-customizations/ld ╰─$ diff -r islandora_transforms ../prod/islandora_transforms 1 ↵ Only in islandora_transforms: .git diff -r islandora_transforms/WORKFLOW_to_solr.xslt ../prod/islandora_transforms/WORKFLOW_to_solr.xslt 95c95 \u0026lt;/xsl:stylesheet\u0026gt; \\ No newline at end of file --- \u0026lt;/xsl:stylesheet\u0026gt; So, no significant differences here. Yay!\nNext, I compared the two files, one-at-a-time, using Atom and its Split Diff package. There were very few differences in foxmToSolr.xslt so it was easy to merge. I took the significant differences in the prod copy and merged them into the ld copy. I did the same with schema.xml, but it was an entirely different beast. There were 34 sections of differences, and in many cases it was not clear if our existing customizations should be merged. As a result, the changes I made in schema.xml, and some I did NOT make, will need to be carefully re-evaluated as this local migrate \u0026ldquo;test\u0026rdquo; proceeds.\nAt this point I\u0026rsquo;ve saved my \u0026ldquo;merged\u0026rdquo; customizations in ~/diff-and-merge-customizations/ld until Step 2a, below.\nPhew, that was a lot of Step 0!\nStep 1: Edit /etc/hosts File Easy peasy, relatively speaking. \u0026#x1f604;\nStep 2: Setup Git for the ISLE Project OK, I already have two private Github repositories\u0026hellip;\nhttps://gitubh.com/Digital-Grinnell/dg-isle for my customized copy of the ISLE project, and https://github.com/Digital-Grinnell/dg-islandora for my customized copy of the Drupal and Islandora code. However, in my case neither of these repositories is \u0026ldquo;complete\u0026rdquo; because they are products of the install-local-new.md process, so my focus in this step will be to get these two repos in line with the Git workflow that\u0026rsquo;s being established.\nRegarding the dg-isle repo\u0026hellip;\nI cloned it to my local workstation and set things up as directed with: Workstation Commands cd ~/Projects git clone https://github.com/Digital-Grinnell/dg-isle.git cd dg-isle git remote add icg-upstream https://github.com/Islandora-Collaboration-Group/ISLE.git git fetch icg-upstream git pull icg-upstream master git push -u origin master Step 2a: Add Customizations from Step 0 Before executing the documented commands I elected to create a new branch in my local dg-isle repo, so my workflow was this:\nWorkstation Commands cd ~/Projects/dg-isle git checkout -b solr-and-gsearch-customizations mkdir -p ./config/solr mkdir -p ./config/fedora/gsearch cp -f ~/diff-and-merge-customizations/ld/schema.xml ./config/solr/ cp -f ~/diff-and-merge-customizations/ld/foxmlToSolr.xslt ./config/fedora/gsearch/ cp -fr ~/diff-and-merge-customizations/ld/islandora_transforms ./config/fedora/gsearch/ Next, I edited docker-compose.local.yml as prescribed, and then saved it all like so:\nWorkstation Commands cd ~/Projects/dg-isle git checkout solr-and-gsearch-customizations git add -A git commit -m \u0026ldquo;Customizations from Step 2a\u0026rdquo; git push \u0026ndash;set-upstream origin solr-and-gsearch-customizations Step 3: Git Clone the Production Islandora Drupal Site Code OK, this section will deal with my Islandora/Drupal code repository with all my customization in it, in my case that\u0026rsquo;s https://github.com/Digital-Grinnell/dg-islandora, but my copy isn\u0026rsquo;t complete yet, so\u0026hellip;\nPost 038 - Building My dg-islandora Code Repository Earlier I created blog post 038-Building-My-dg-islandora-Code-Repository. It combined elements of post 021 with post 034 to create a \u0026ldquo;customized\u0026rdquo; local ISLE v1.2.0 instance with features of Digital.Grinnell.\nThe product of all that work is the Digital-Grinnell/dg-islandora private GitHub repo.\nI followed the procedure outline in Step 3: Git Clone the Production Islandora Drupal Site Code with my Digital-Grinnell/dg-islandora private repo like so:\nWorkstation Commands cd ~/Projects git clone https://github.com/Digital-Grinnell/dg-islandora.git cd dg-islandora atom . Again, the final atom . command opened my newly cloned project in Atom to simplify editing. I followed the guidance of Step 3 to update my docker-compose.local.yml as directed. This concludes Step 3; moving on.\nStep 4: Edit the \u0026ldquo;.env\u0026rdquo; File to Change to the Local Environment I simply confirmed the suggested edits to my .env file using the Atom editing session opened above.\nStep 5: Create New Users and Passwords by Editing \u0026ldquo;local.env\u0026rdquo; File Since this is \u0026ldquo;local\u0026rdquo; I\u0026rsquo;ll keep no secrets here. I set all the variable passwords, and hashes to \u0026ldquo;password\u0026rdquo;, and \u0026ldquo;thisisalengthyhashstring\u0026rdquo;, respectively, as permitted. All other variables, like usernames and more, I set to \u0026ldquo;local\u0026rdquo;. Note that I left the original \u0026ldquo;# Replace\u0026hellip;\u0026rdquo; strings intact, but made sure to push each one to a new line.\nThere\u0026rsquo;s a copy of my complete customized local.env file in this gist.\nStep 6: Create New Self-Signed Certs for Your Project Simply followed the instructions to-the-letter.\nStep 7: Download the ISLE Images Simply followed the instructions to-the-letter.\nStep 8: Launch Process Simply followed the instructions to-the-letter.\nStep 9: Import the Production MySQL Drupal Database In this step, employing Method B: Use the Command Line, I returned to the DGDocker1 production database, digital.grinnell.edu-2019-10-31T14-07-37.mysql.gz, saved in the Drupal Site Database section of this document.\nFirst, I unzipped the aforementioned database archive using my MacBook\u0026rsquo;s Archive Utility to produce digital.grinnell.edu-2019-10-31T14-07-37.mysql on my workstation desktop. The full path to this file was /Users/digital/Desktop/digital.grinnell.edu-2019-10-31T14-07-37.mysql. Then, on my workstation and in the MySQL container:\nWorkstation Commands docker cp /Users/digital/Desktop/digital.grinnell.edu-2019-10-31T14-07-37.mysql isle-mysql-ld:/database.mysql docker exec -it isle-mysql-ld bash MySQL Container Commands mysql -u local -p local \u0026lt; database.mysql exit Step 10: Run Islandora Drupal Site Scripts Following the provided guidance yielded a set of workstation commands like this:\nWorkstation Commands docker cp scripts/apache/migration_site_vsets.sh isle-apache-ld:/var/www/html/migration_site_vsets.sh docker exec -it isle-apache-ld bash -c \u0026ldquo;chmod +x /var/www/html/migration_site_vsets.sh\u0026rdquo; docker exec -it isle-apache-ld bash -c \u0026ldquo;cd /var/www/html \u0026amp;\u0026amp; ./migration_site_vsets.sh\u0026rdquo; docker cp scripts/apache/install_solution_packs.sh isle-apache-ld:/var/www/html/install_solution_packs.sh docker exec -it isle-apache-ld bash -c \u0026ldquo;chmod +x /var/www/html/install_solution_packs.sh\u0026rdquo; docker exec -it isle-apache-ld bash -c \u0026ldquo;cd /var/www/html \u0026amp;\u0026amp; ./install_solution_packs.sh\u0026rdquo; Output from the migration_site_vsets.sh command:\nThe following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;masquerade\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;announcements\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;backup_migrate\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following theme is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;bootstrap\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;dg7\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following theme is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;digital_grinnell_bootstrap\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;email\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;git_deploy\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;idu\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;ihc\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;islandora_binary_object\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;islandora_collection_search\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;islandora_google_scholar\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;islandora_mods_display\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;islandora_multi_importer\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;islandora_oralhistories\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;islandora_pdfjs_reader\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;islandora_solr_collection_view\u0026lt;/em\u0026gt;. For information about how to fix this, see [warning] \u0026lt;a href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;ldap_servers\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;maillog\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;phpexcel\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;r4032login\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;smtp\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;transcripts_ui\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;views_bootstrap\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 The following module is missing from the file system: \u0026lt;em class=\u0026#34;placeholder\u0026#34;\u0026gt;field_group\u0026lt;/em\u0026gt;. For information about how to fix this, see \u0026lt;a [warning] href=\u0026#34;https://www.drupal.org/node/2487215\u0026#34;\u0026gt;the documentation page\u0026lt;/a\u0026gt;. bootstrap.inc:1156 WD php: PDOException: SQLSTATE[42S02]: Base table or view not found: 1146 Table \u0026#39;local.users\u0026#39; doesn\u0026#39;t exist: SELECT base.uid AS uid, base.name AS name, [error] base.pass AS pass, base.mail AS mail, base.theme AS theme, base.signature AS signature, base.signature_format AS signature_format, base.created AS created, base.access AS access, base.login AS login, base.status AS status, base.timezone AS timezone, base.language AS language, base.picture AS picture, base.init AS init, base.data AS data FROM {users} base WHERE (base.uid IN (:db_condition_placeholder_0)) ; Array ( [:db_condition_placeholder_0] =\u0026gt; 1 ) in DrupalDefaultEntityController-\u0026gt;load() (line 198 of /var/www/html/includes/entity.inc). ","permalink":"https://static.grinnell.edu/dlad-blog/posts/051-migrating-dg-to-isle-1.3.0-ld/","tags":["ISLE","v1.3.0","local","migration"],"title":"Migrating Digital.Grinnell (DG) to ISLE 1.3.0 (ld) for Local Development"},{"categories":null,"contents":" Warning! The DG7 module contains code with numerous dependencies, and the most sinister of these is a Grinnell-specific version of the Solr schema. Do NOT attempt to use this module early in a stack-building process, nor outside the Digital.Grinnell environment. Granular ISLE Customization This post is part of a series describing Digital.Grinnell customizations to ISLE, in a \u0026ldquo;granular\u0026rdquo; format\u0026hellip; one small customization at a time. An index of all documents in this series is included at the end of Granular ISLE Customization: Series Guidelines. Goal Statement In this \u0026ldquo;granular\u0026rdquo; post we will install DG7, the Digital Grinnell v7 module, into an existing ISLE instance created using the DigitalGrinnell/public-isle project.\nDG7 is home to numerous custom PHP functions and Drupal hook implementations designed specifically for Digital.Grinnell.\nInstall and Enable the DG7 Module DG7 is installed and enabled in the same manner as most Drupal or Islandora contrib modules, like so:\nApache Container Commands* cd /var/www/html/sites/all/modules/islandora git clone https://github.com/DigitalGrinnell/dg7.git chown -R islandora:www-data * drush -y en dg7 And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/050-granular-isle-customization-installing-dg7/","tags":["granular","customization","ISLE","DG7","hooks"],"title":"Granular ISLE Customization: Installing DG7"},{"categories":null,"contents":" Granular ISLE Customization This post is part of a series describing Digital.Grinnell customizations to ISLE, in a \u0026ldquo;granular\u0026rdquo; format\u0026hellip; one small customization at a time. An index of all documents in this series is included at the end of Granular ISLE Customization: Series Guidelines. Goal Statement In this \u0026ldquo;granular\u0026rdquo; post I\u0026rsquo;ll introduce two customizations to IMI that implement and engage Drupal \u0026ldquo;hook\u0026rdquo; functions, namely:\nhook_islandora_multi_importer_remote_file_get(), and hook_form_islandora_multi_importer_form_alter(). Drupal 7 Hooks See Understanding the hook system for Drupal modules to better understand what Drupal v7 hook functions are, and how they work. If you would like additional assistance with \u0026ldquo;hooks\u0026rdquo; do not hesitate to contact the author of this post using [this email link](mailto:digital@grinnell.edu?subject=Implementing IMI Hooks).\nMy Implementation Details Both of these hook implementations reside in my dg7 custom module, a module designed exclusively to hold custom code, mostly hook implementations, for Digital.Grinnell in Islandora v7.\nhook_islandora_multi_importer_remote_file_get The code I\u0026rsquo;ve built for this hook implementation can be seen in this gist.\nhook_form_islandora_multi_importer_form_alter The code I\u0026rsquo;ve built for this hook implementation can be seen in this gist.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/049-granular-isle-customization-implementing-imi-hooks/","tags":["granular","customization","ISLE","Islandora Multi-Importer","IMI","hook_form_alter","islandora_multi_importer_form","hook_islandora_multi_importer_remote_file_get"],"title":"Granular ISLE Customization: Implementing IMI Hooks"},{"categories":null,"contents":" Granular ISLE Customization This post is part of a series describing Digital.Grinnell customizations to ISLE, in a \u0026ldquo;granular\u0026rdquo; format\u0026hellip; one small customization at a time. An index of all documents in this series is included at the end of Granular ISLE Customization: Series Guidelines. Goal Statement In this \u0026ldquo;granular\u0026rdquo; post we will install IMI, the Islandora Multi-Importer module, into an existing ISLE instance, for example: https://dg.localdomain/.\nInstall the Islandora Multi-Importer (IMI) It\u0026rsquo;s important that we take this step BEFORE other customizations, otherwise the module may not install properly. The result is captured in this gist.\nApache Container Commands* cd /var/www/html/sites/all/modules/islandora git clone https://github.com/mnylc/islandora_multi_importer.git chown -R islandora:www-data * cd islandora_multi_importer composer install drush -y en islandora_multi_importer An Alternative IMI Installation The commands documented above will pull the \u0026ldquo;master\u0026rdquo; branch of the canonical IMI from mnylc, but if you\u0026rsquo;d like to take another of my features for a spin, try installing my fork (sync\u0026rsquo;d with the mnylc master on 04-Oct-2019), like so:\nApache Container Commands* cd /var/www/html/sites/all/modules/islandora git clone https://github.com/DigitalGrinnell/islandora_multi_importer.git chown -R islandora:www-data * cd islandora_multi_importer git checkout Issue-15 composer install drush -y en islandora_multi_importer The feature that\u0026rsquo;s unique to this fork and branch is mentioned briefly in project \u0026ldquo;Issue\u0026rdquo; number 15.\nBonus: A Robust Twig Template for MODS In my opinion, IMI\u0026rsquo;s greatest strength is the way it leverages Twig templates to translate CSV (comma-separated value) data into viable MODS metadata. And the real beauty in Twig is what you can do with it once you understand how it works.\nWhile an exhaustive explanation of Twig is beyond the scope of this post, I can provide what I believe is a very robust Twig example, created specifically for MODS and IMI at Grinnell College. This is the template that currently drives ingest into Digital.Grinnell.edu. The template includes extensive comments up-front, and the current, revision 14, version is included in this Gist.\nInstalling the Template This part is easy. To install the aforementioned Twig template, or any IMI template follow these simple steps:\nCopy the entire contents of this Gist, or any suitable Twig template, to your paste buffer. In your web browser, visit the /multi_importer#overlay=admin/islandora address of your Islandora instance, for example: https://dg.localdomain/multi_importer#overlay=admin/islandora/. Choose Multi Importer Twig templates or /multi_importer#overlay=admin/islandora/twigtemplates in your Islandora instance. Choose New Twig template or /multi_importer#overlay=admin/islandora/twigtemplates/create. Paste the text of your Twig template into the available Twig Template Input window pane. At the bottom of the form, enter a descriptive name for your template. In my case the name is Digital_Grinnell_MODS_Master.twig Revision 14. Click Save Template. Your Twig template is now ready for use. Enjoy!\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/048-granular-isle-customization-installing-imi/","tags":["granular","customization","ISLE","Islandora Multi-Importer","IMI","Twig"],"title":"Granular ISLE Customization: Installing IMI"},{"categories":null,"contents":" Granular ISLE Customization This post provides guidelines for a series of posts describing Digital.Grinnell customizations to ISLE, in a \u0026ldquo;granular\u0026rdquo; format\u0026hellip; one small customization at a time. Using the Granular ISLE Customization Posts There are just a couple of notes regarding the subject posts that I\u0026rsquo;d like to pass along to make them more useful.\nGists - You will find a few places in this series where I generated a gist to take the place of lengthy command output. Instead of a long stream of text you\u0026rsquo;ll find a simple link to a gist like this.\nWorkstation Commands - There are lots of places in this series where I\u0026rsquo;ve captured a sequence of commands along with output from those commands in block text. Generally speaking, after each such block you will find a Workstation Commands table that can be used to conveniently copy and paste the necessary commands directly into your workstation. The tables look something like this:\nWorkstation Commands cd ~/Projects git clone https://github.com/DigitalGrinnell/ISLE cd ISLE git checkout -b ld Apache Container Commands - Similar to Workstation Commands, a tabulated list of commands may appear with a heading of Apache Container Commands. *Commands in such tables can be copied and pasted into your command line terminal, but ONLY after you have opened a shell into the Apache container. The asterisk (*) at the end of the table heading is there to remind you of this! See the next section of this document for additional details. These tables looks something like this: Apache Container Commands* cd /var/www/html/sites/all/modules/contrib drush dl backup_migrate drush -y en backup_migrate Opening a Shell in the Apache Container This is something I find myself doing quite often during ISLE configuration, so here\u0026rsquo;s a reminder of how I generally do this\u0026hellip;\n╭─markmcfate@ma8660 ~/Projects/public-isle ‹ruby-2.3.0› ‹ld*› ╰─$ docker exec -it isle-apache-ld bash root@9bec4edd3964:/# cd /var/www/html root@9bec4edd3964:/var/www/html# Workstation Commands cd ~/Projects/public-isle docker exec -it isle-apache-ld bash The Granular ISLE Customization Series That\u0026rsquo;s all we need in terms of guidelines. I hope you find the series useful!\nPosts in the Granular ISLE Customization series include:\nTitle Synopsis Granular ISLE Customization: Series Guidelines This document. Granular ISLE Customization: Installing IMI Instructions for installing IMI, the Islandora Multi-Importer module, and a robust sample of TWIG templating. Granular ISLE Customization: Implementing IMI Hooks Guidance to build your own IMI hook implementations to improve the Islandora Multi-Importer. Granular ISLE Customization: Installing the DG Theme Guidance to install and configure the digital_grinnell_bootstrap theme. Granular ISLE Customization: Installing DG7 Some brief notes about installation of Digital.Grinnell\u0026rsquo;s DG7 module\u0026hellip;where all the hooks live. ","permalink":"https://static.grinnell.edu/dlad-blog/posts/047-granular-isle-customizations-the-series/","tags":["granular","customization","ISLE","guidelines","gists"],"title":"Granular ISLE Customization: Series Guidelines"},{"categories":null,"contents":"This morning, Tuesday, September 17, 2019, I awoke to find our https://static.grinnell.edu server, and all of the services on it, unreachable via the web. I managed to open a shell on the host and found that the server was up-and-running as expected, but a quick docker ps command indicated that one of the key services on the server, namely Traefik, had stopped and then failed to restart, repeatedly. Traefik is the service that\u0026rsquo;s responsible for routing web traffic on the static host. No wonder the web sites were not responding!\nThe confusing part of this mystery was the stream of messages that appeared in the Traefik logs. When I did a docker logs traefik command I got nothing back, presumably because Traefik was not running, it couldn\u0026rsquo;t \u0026ldquo;start\u0026rdquo; at all. So I logged in as root (using sudo su) then cleared away all Docker config and restarted the stack using:\ndocker stop $(docker ps -q); docker rm -v $(docker ps -qa); docker image rm $(docker image ls -q); docker system prune --force cd /opt docker-compose up Note the omission of the usual \u0026ldquo;-d\u0026rdquo; flag on the docker-compose up command, that bypasses the usual \u0026ldquo;detached\u0026rdquo; mode and makes the docker-compose command run interactively, with log data returning directly to the console. Well, that didn\u0026rsquo;t show me much, except for the same, repeated error message (with different timestamps, of course):\n2019/09/17 13:36:34 command traefik error: invalid node traefik: no child In response to this I assumed that maybe something on the server was outdated, so I did:\napt-get update apt-get upgrade reboot This did indeed update most of the packages running on the server, not a bad thing, but it did not solve the problem. So, I started to tear into the /opt/docker-compose.yml file. I replaced a couple of variables there with safe but hard-coded strings and started it again. No change. I did a search for any wisdom/guidance on the web and found none, so I focused on the Traefik command itself, and saw nothing obvious there.\nIt was then that I remembered reading a couple of weeks ago that Traefik was about to release a new major version, moving from v1.x to v2.0. Hmmmm, I wondered then in maybe I was pulling a new image version, one in which the command syntax had changed? Yep, that seems to have been the case! So, my old /opt/docker-compose.yml file included this specification:\nservices: proxy: container_name: traefik_proxy image: traefik command: \u0026gt;- --docker --logLevel=INFO \\ ... I took a look at all of the current Traefik image tags in Docker Hub and sure enough, it looked like a \u0026ldquo;default\u0026rdquo; image specification like image: traefik would now pull a v2.x copy, but my process was written for v1.x. So I changed this bit of /opt/docker-compose.yml to read as follows:\nservices: proxy: container_name: traefik_proxy image: traefik:alpine command: \u0026gt;- --docker --logLevel=INFO \\ ... I cleaned up the existing stack and restarted it with:\ndocker stop $(docker ps -q); docker rm -v $(docker ps -qa); docker image rm $(docker image ls -q); docker system prune --force; docker network prune --force cd /opt docker-compose up -d It worked! The addition of the :alpine version specification for the image did the trick, forcing the command to pull a v1.x copy of Traefik.\nThis change did alter the network config a bit though. After the restart the bridge network created here was named opt_webgateway, not traefik_webgateway, as it was before. Consequently, to restart the supported services on this host I ran these command sequences.\nTo restart the static.grinnell.edu landing page site (https://static.grinnell.edu):\nNAME=static-landing-page HOST=static.grinnell.edu IMAGE=\u0026#34;mcfatem/static-landing\u0026#34; docker container run -d --name ${NAME} \\ --label traefik.backend=${NAME} \\ --label traefik.docker.network=opt_webgateway \\ --label \u0026#34;traefik.frontend.rule=Host:${HOST}\u0026#34; \\ --label traefik.port=80 \\ --label com.centurylinklabs.watchtower.enable=true \\ --network opt_webgateway \\ --restart always \\ ${IMAGE} To restart the VAF site (https://vaf.grinnell.edu):\nNAME=vaf HOST=vaf.grinnell.edu IMAGE=\u0026#34;mcfatem/vaf\u0026#34; docker container run -d --name ${NAME} \\ --label traefik.backend=${NAME} \\ --label traefik.docker.network=opt_webgateway \\ --label \u0026#34;traefik.frontend.rule=Host:${HOST}\u0026#34; \\ --label traefik.port=80 \\ --label com.centurylinklabs.watchtower.enable=true \\ --network opt_webgateway \\ --restart always \\ ${IMAGE} To restart the VAF-Kiosk site (https://vaf-kiosk.grinnell.edu):\n╰─$ NAME=vaf-kiosk HOST=vaf-kiosk.grinnell.edu IMAGE=\u0026#34;mcfatem/vaf-kiosk\u0026#34; docker container run -d --name ${NAME} \\ --label traefik.backend=${NAME} \\ --label traefik.docker.network=opt_webgateway \\ --label \u0026#34;traefik.frontend.rule=Host:${HOST}\u0026#34; \\ --label traefik.port=80 \\ --label com.centurylinklabs.watchtower.enable=true \\ --network opt_webgateway \\ --restart always \\ ${IMAGE} To restart this blog, site (https://static.grinnell.edu/blogs/McFateM):\nNAME=blogs-mcfatem HOST=static.grinnell.edu IMAGE=\u0026#34;mcfatem/blogs-mcfatem\u0026#34; docker container run -d --name ${NAME} \\ --label traefik.backend=${NAME} \\ --label traefik.docker.network=opt_webgateway \\ --label \u0026#34;traefik.frontend.rule=Host:${HOST};PathPrefixStrip:/blogs/McFateM\u0026#34; \\ --label traefik.port=80 \\ --label com.centurylinklabs.watchtower.enable=true \\ --network opt_webgateway \\ --restart always \\ ${IMAGE} And all is as it should be now at static.grinnell.edu, so that\u0026rsquo;s a wrap\u0026hellip; until next time. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/045-repairing-static.grinnell.edu/","tags":["static","Traefik","docker-compose"],"title":"Repairing Static.Grinnell.edu"},{"categories":null,"contents":"Teaser Late last night (don\u0026rsquo;t ask how late it was) I discovered a really slick trick, aka \u0026ldquo;feature\u0026rdquo;, of docker-compose. Full disclosure: I love docker-compose \u0026ldquo;overrides\u0026rdquo;, a feature I found a couple of months ago. However, implementing overrides in a granular fashion, as I\u0026rsquo;d like, and within a docker-compose hierarchy of environments like the ISLE stack, can be difficult and counter-productive. I may have found a workable compromise last night. If you\u0026rsquo;re interested, please, read on\u0026hellip;\nHistory About a year ago I created a \u0026ldquo;portable\u0026rdquo; FEDORA object repository \u0026ldquo;on a stick\u0026rdquo;, a USB stick. I call that \u0026ldquo;stick\u0026rdquo; DG-FEDORA, because it contains a number of sample objects gleaned from Digital.Grinnell.edu. Shortly after I created the \u0026ldquo;stick\u0026rdquo; I started using a docker-compose.override.yml file (see Adding and overriding configuration for details) to implement it in some of my ISLE work. But there were lots of other \u0026ldquo;customizations\u0026rdquo; that I also wanted to implement, and over time my docker-compose.override.yml grew too large to be easily maintained\u0026hellip;there was just too much stuff to squeeze into a file that was supposed to make life easier.\nISLE Details The ISLE stack now uses a very simple and elegant .env file \u0026ndash; four lines of environment variables, nothing more \u0026ndash; to ultimately control what the docker-compose command does. The last environment variable in the file is COMPOSE_FILE, and I found that this variable can be used to specify \u0026ldquo;multiple\u0026rdquo; .yml files to build a stack; and docker-compose does so using a well-documented and sensible set of rules. Nirvana!\nThe Solution So what I\u0026rsquo;ve done is simply provide a modified .env file and some instruction for using it properly. My .env file (there is a copy of it on the DG-FEDORA USB stick) reads like this:\n#### Activated ISLE environment # To use an environment other than the default Demo, please change values below # from the default Demo to one of the following: Local, Test, Staging or Production # For more information, consult https://islandora-collaboration-group.github.io/ISLE/install/install-environments/ COMPOSE_PROJECT_NAME=isle_demo BASE_DOMAIN=isle.localdomain CONTAINER_SHORT_ID=ld # COMPOSE_FILE=docker-compose.demo.yml COMPOSE_FILE=docker-compose.demo.yml:docker-compose.DG-FEDORA.yml In the file above, note that I\u0026rsquo;ve commented out the original definition of COMPOSE_FILE and added my own definition which appends a second .yml file reference to the original. That 2nd file, docker-compose.DG-FEDORA.yml is also provided on the DG-FEDORA USB stick and it reads like this:\nversion: \u0026#39;3.7\u0026#39; #### docker-compose up -d; ## Local Environment - Used for Drupal site development, more extensive metadata, Solr and Fedora development work on a local laptop or workstation ## Updated 2019-09 - Release 1.3.0-dev (@ 1.3.0-dev) services: # Bind-mount the datastreamStore and objectStore directories in /Volumes/DG-FEDORA (the USB stick) # into the FEDORA container to become our FEDORA datastream and object store directories. FEDORA digital # live in these directories, so this effectively puts your FEDORA repository on the DG-FEDORA stick. fedora: volumes: - /Volumes/DG-FEDORA/datastreamStore:/usr/local/fedora/data/datastreamStore - /Volumes/DG-FEDORA/objectStore:/usr/local/fedora/data/objectStore # Bind-mount the /Volumes/DG-FEDORA/Storage directory tree in the Apache container. This bind-mount to /mnt/storage # is usefule with IMI, the Islandora Multi-Importer when a local* file fetch hook is defined, as it is in the DG7 # custom module. apache: volumes: - /Volumes/DG-FEDORA/Storage:/mnt/storage Simple. It\u0026rsquo;s just a repeat of the original docker-compose.demo.yml file with only the necessary \u0026ldquo;overrides\u0026rdquo; included.\nThe DG-FEDORA USB stick also includes a README.md file and since you don\u0026rsquo;t have a copy of the \u0026ldquo;stick\u0026rdquo;, I\u0026rsquo;ll share that with you here.\nA Master Copy of DG-FEDORA My Digital.Grinnell staging server, DGDockerX is a networked CentOS 7 virtual machine with lots of available storage, but no accessible USB ports, of course\u0026hellip;it\u0026rsquo;s a VM after all. Consequently, it\u0026rsquo;s not practical to maintain all of my \u0026ldquo;portable\u0026rdquo; FEDORA data on a USB stick alone. So, on DGDockerX I\u0026rsquo;ve created a copy of the DG-FEDORA USB stick at /mnt/data/DG-FEDORA, and I\u0026rsquo;ve declared it to be copy \u0026ldquo;zero\u0026rdquo;, so on it there\u0026rsquo;s a DG-FEDORA-0.md file proclaiming it to be the \u0026ldquo;master\u0026rdquo; copy of DG-FEDORA.\nIt\u0026rsquo;s worth noting, perhaps, that at the time of this writing, DG-FEDORA-0 contains 126 FEDORA objects. Anything less than that number should be considered an incomplete set.\nThe README.md file in the DG-FEDORA-0 MASTER volume suggests using a command like this to copy the MASTER repository to a mounted DG-FEDORA USB stick:\nsudo rsync -aruvi --exclude DG-FEDORA-0.md --exclude from-DGDocker1 --exclude site-public islandora@132.161.132.101:/mnt/data/DG-FEDORA/. /Volumes/DG-FEDORA/. --progress README.md from DG-FEDORA DG-FEDORA is the name of a USB stick/volume (I actually maintain 2 USB memory sticks named DG-FEDORA, numbers 1 and 2) which holds a small sample of FEDORA digital objects gleaned from Digital.Grinnell. Using DG-FEDORA you can easily add a pre-populated repository of objects to your \u0026ldquo;demo\u0026rdquo; or \u0026ldquo;local\u0026rdquo; environment ISLE project in as little as 5 minutes.\nPrerequisites To successfully use DG-FEDORA your system will need to meet the following prerequisite requirements.\nYour workstation must meet all the hardware requirements of ISLE. See ISLE\u0026rsquo;s ./docs/install/host-hardware-requirements.md for details. Your workstation environment must meet all the minimum software requirements of ISLE. See ISLE\u0026rsquo;s ./docs/install/host-software-dependencies.md for details. This workflow assumes your workstation is running OSX. Other workstation types supported by ISLE may be acceptable, but the DG-FEDORA USB volume will have to be mounted differently. You must have a working \u0026ldquo;demo\u0026rdquo; or \u0026ldquo;local\u0026rdquo; ISLE environment already running. Mounting DG-FEDORA In MacOSX you simply have to plug the DG-FEDORA USB stick into an available USB port on your Mac workstation. After a few seconds the USB stick will be automatically mounted as /Volumes/DG-FEDORA, and the /Volumes directory is automatically \u0026ldquo;shared\u0026rdquo; with Docker.\nMake DG-FEDORA Writable In MacOSX, open a terminal and use the following command to make DG-FEDORA writable, so that you can save newly ingested objects and updates to existing objects:\nsudo mount -u -w /Volumes/DG-FEDORA Not Using MacOSX? If your workstation is not a Mac, you will need to insert the USB stick and take appropriate steps to:\nMount the stick with read/write permissions as /Volumes/DG-FEDORA. Share the /Volumes or /Volumes/DG-FEDORA directory in your Docker settings/preferences. Modifying the ISLE Environment Navigate to your ISLE project directory and execute the following operations:\nShut down ISLE if it is running. Example: cd ~/pathto/ISLE; docker-compose down. These commands will do no harm if ISLE is not currently running. Copy two files from DG-FEDORA to your local project. Example: cd /Volumes/DG-FEDORA; cp -f .env docker-compose.DG-FEDORA.yml ~/pathto/ISLE/. Edit the new ISLE project .env file according to directions within the file. Example: nano ~/pathto/ISLE/.env. The objective is to select the appropriate \u0026ldquo;demo\u0026rdquo; or \u0026ldquo;local\u0026rdquo; environment as needed. Navigate to your project directory and restart the stack. Example: cd ~/pathto/ISLE; docker-compose up -d. Wait until the stack has started, open your browser and visit your site at https://isle.localdomain (demo) or https://yourprojectnamehere.localdomain (local). Rebuild FEDORA\u0026rsquo;s resourceIndex Rebuild your FEDORA resourceIndex using the steps documented in Step 17: On Remote Production - Re-Index Fedora \u0026amp; Solr.\nOpen a terminal in the isle-fedora-ld container, docker exec -it isle-fedora-ld bash, and then run cd utility_scripts/; ./rebuildFedora.sh. Rebuild the Solr Index Once the previous rebuild process is complete, you should rebuild your Solr search index using the remaining steps documented in Step 17: On Remote Production - Re-Index Fedora \u0026amp; Solr.\nOpen a terminal in the isle-fedora-ld container, docker exec -it isle-fedora-ld bash (or using the terminal opened in the previous step), and then run cd utility_scripts/; ./updateSolrIndex.sh. This rebuilding process may take a few minutes. Proceed to the check your work after some minutes have passed.\nCheck Your Work Visit the repository home page at https://isle.localdomain/islandora/object/islandora:root (demo) or https://yourprojectnamehere.localdomain/islandora/object/islandora:root (local). You should see new collections on the first page of your display. Follow the install documentation for enabling the Islandora Simple Search block and test Solr by searching for a term like \u0026ldquo;Ley\u0026rdquo;. Feedback is Welcome This is very much a work-in-progress. If you have an opinion, or would like to suggest improvements, please share!\nThat\u0026rsquo;s a wrap\u0026hellip; until next time. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/046-dg-fedora-a-portable-object-repository/","tags":["DG-FEDORA","FEDORA","portable","docker-compose","override","COMPOSE_FILE","re-index"],"title":"DG-FEDORA: A Portable FEDORA Repository"},{"categories":null,"contents":"These days I like to do all my terminal/command-line work in zsh, more specifically, with Oh My ZSH! and the bira theme. So, on my new node dgdocker3, I added nano, zsh, and some other goodies by largely following How to Setup ZSH and Oh-my-zsh on Linux.\nThis is how I did it\u0026hellip;\nsudo yum install nano sudo yum install zsh chsh -s /bin/zsh mcfatem exit # log back in after this echo $SHELL sudo yum install wget git wget https://github.com/robbyrussell/oh-my-zsh/raw/master/tools/install.sh -O - | zsh cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc source ~/.zshrc cd ~/.oh-my-zsh/themes/ ls -a nano ~/.zshrc # In the editor add (or replace similar) the following lines but without the leading # # ZSH_THEME=\u0026#39;bira\u0026#39; # plugins=(git extract web-search yum git-extras docker) exit # log back in after this And that\u0026rsquo;s a wrap\u0026hellip; until next time. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/044-installing-zsh-in-centos/","tags":["zsh","CentOS","Oh My ZSH!","bira"],"title":"Installing ZSH in CentOS"},{"categories":null,"contents":"Today\u0026rsquo;s quest\u0026hellip; to build a new Handle.net server for Digital.Grinnell, preferably one that is \u0026ldquo;Dockerized\u0026rdquo;. I\u0026rsquo;m going to start by forking datacite/docker-handle, a project that looks promising, and following it along with the documentation in chapter 3 of the HANDLE.NET (version 9) Technical Manual. The aforementioned fork can now be found in DigitalGrinnell/docker-handle.\nThe digital7 Saga My old friend and server (or should that be servant?), digital7, used to be the home of Digital.Grinnell in Islandora v7, before Docker and ISLE came along. It\u0026rsquo;s an Ubuntu 14.04.5 LTS server, and in addition to hosting Digital.Grinnell, it also used to host our HANDLE.NET server. Hmmm\u0026hellip;\nI tried valiantly to upgrade old digital7 from the OS on up, but failed. In the end, digital7 was retired, and its IP address assigned to a new CentOS 7 server named DGDocker3. So, I\u0026rsquo;ll be making a new home for HANDLE.NET services on DGDocker1, instead.\nPort Status The next step in my quest was to check some ports on DGDocker1, so I found you get signal\u0026rsquo;s Port Forwarding Tester, entered the IP address assigned to the host and then tested the two ports that HANDLE.NET demands. Rats! Neither was open, so I dispatched a help desk ticket to open them up. About a week later\u0026hellip; they\u0026rsquo;re open.\ndocker-handle I took my fork of the aforementioned datacite/docker-handle for a spin on DGDocker1, but honestly, it looks like a train wreck. I could not fathom how to make it work since most of the .env variables documented in the README.md file don\u0026rsquo;t seem to do anything in the configuration. \u0026#x1f615; I also found https://github.com/horizon-institute/handle.net-server-docker but it\u0026rsquo;s a Handle v8 instance, and I\u0026rsquo;m aiming for v9.\nA Quick Solution So, on 18-Sep-2019 I set out to install a non-Dockerized Handle server on DGDocker1. Installation followed the standard guidance found in Technical Manual, Handle.Net Version 9 and my instance now lives in /hs/handle-9.2.0 and /hs/svr_1 on the DGDocker1 host. In order to make my iduH tools work, I had to modify the /opt/ISLE/docker-compose.yml file so that it contains the following additional volume bind-mount:\n# Next line added 19-Sep-2019 so that batch jobs for the HANDLE server at the host\u0026#39;s /hs directory can run properly. - /hs:/hs That line maps the host\u0026#39;s /hs folder into the isle-apache-dg container as the same path, enabling Handle batch and other commands to run within the container itself. After these additions I tweaked the iduH command parameters found in idu_constants.inc, then I opened a shell into the Apache container on DGDocker1 via docker exec -it isle-apache-dg bash, and did:\ncd /var/www/html/sites/default drush -u 1 iduH grinnell:2-5000 MODIFY; drush -u 1 iduF grinnell:2-5000 SelfTransform --reorder # Repeat drush... for other PID ranges It worked! Note that in some instances a command of the form drush -u 1 iduH grinnell:2-5000 CREATE was necessary instead of \u0026ldquo;MODIFY\u0026rdquo;.\nRestarting the Handle Server Unfortunately, DGDocker1 sometimes has to be rebooted, and when that is necessary I also have to restart the Handle server using a command sequence (with output) like this, as root:\n[root@dgdocker1 handle-9.2.0]# pwd /hs/handle-9.2.0 [root@dgdocker1 handle-9.2.0]# ./bin/hdl-server /hs/svr_1 Handle.Net Server Software version 9.2.0 Enter the passphrase for this server\u0026#39;s authentication private key: Note: Your passphrase will be displayed as it is entered \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; my super secret handle server password here \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; HTTP handle Request Listener: address: 132.161.132.103 port: 8000 Starting HTTP server... TCP handle Request Listener: address: 132.161.132.103 port: 2641 UDP handle Request Listener: address: 132.161.132.103 port: 2641 Starting UDP request handlers... Starting TCP request handlers... ^Z [3]+ Stopped ./bin/hdl-server /hs/svr_1 [root@dgdocker1 handle-9.2.0]# bg [3]+ ./bin/hdl-server /hs/svr_1 \u0026amp; [root@dgdocker1 handle-9.2.0]# exit exit There are stopped jobs. And that\u0026rsquo;s a wrap good place for a break\u0026hellip; until I return to formally \u0026ldquo;Dockerize\u0026rdquo; this effort. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/043-a-dockerzied-handle-server/","tags":["Docker","HANDLE.NET","hdl","restart"],"title":"A Dockerized 'Handle' Server"},{"categories":null,"contents":"This post picks up from where Configuring DGDocker2 left off. In it I will establish a workflow to setup a \u0026ldquo;Dockerized\u0026rdquo; server complete with Traefik, Portainer, and Who Am I. It should be relatively easy to add additional non-static services to any server that is initially configured using this package. For \u0026ldquo;static\u0026rdquo; servers have a look at post 008 docker-bootstrap Workflow .\nCapture As a Project Picking up from the end of Configuring DGDocker2, my first step on the dgdocker2 server was to move everything into a single subdirectory of /opt; I called the new directory dockerized-server, like so:\nmkdir -p /opt/dockerized-server mv -f /opt/traefik /opt/dockerized-server/traefik mv -f /opt/portainer /opt/dockerized-server/portainer mv -f /opt/whoami /opt/dockerized-server/whoami Then, I built a new /opt/dockerized-server/docker-compose.yml file to launch Traefik, Portainer, and WhoAmI.\nversion: \u0026#34;3\u0026#34; #### docker-compose up -d services: traefik: image: traefik:1.7.14-alpine command: --configFile=/traefik.toml container_name: traefik restart: always ports: - \u0026#34;80:80\u0026#34; - \u0026#34;443:443\u0026#34; volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - /opt/dockerized-server/data/traefik.toml:/traefik.toml - /opt/dockerized-server/data/acme.json:/acme.json networks: - web labels: - traefik.enable=true - \u0026#34;traefik.frontend.rule=Host:traefik2.grinnell.edu\u0026#34; # - \u0026#34;traefik.frontend.rule=PathPrefixStrip:/traefik\u0026#34; # - \u0026#34;traefik.frontend.redirect.regex=^(.*)/traefik$$\u0026#34; # - \u0026#34;traefik.frontend.redirect.replacement=$$1/traefik/\u0026#34; # - \u0026#34;traefik.frontend.rule=PathPrefix:/traefik;ReplacePathRegex: ^/traefik/(.*) /$$1\u0026#34; - traefik.port=8080 portainer: image: portainer/portainer container_name: portainer command: --admin-password \u0026#34;$$2y$$05$$pJEzHJBzfoYYS7/hGAedcOP8XdsqNXE7j.LHFBVjueASOqOvvjGOy\u0026#34; -H unix:///var/run/docker.sock # command: -H unix:///var/run/docker.sock --no-auth networks: - web - internal ports: - \u0026#34;9010:9000\u0026#34; ## Remapped to avoid conflicts on systems/servers with portainer already running. volumes: - /var/run/docker.sock:/var/run/docker.sock - portainer-data:/data labels: - traefik.port=9000 - traefik.docker.network=web - traefik.enable=true - \u0026#34;traefik.frontend.rule=PathPrefixStrip:/portainer\u0026#34; - \u0026#34;traefik.frontend.redirect.regex=^(.*)/portainer$$\u0026#34; - \u0026#34;traefik.frontend.redirect.replacement=$$1/portainer/\u0026#34; - \u0026#34;traefik.frontend.rule=PathPrefix:/portainer;ReplacePathRegex: ^/portainer/(.*) /$$1\u0026#34; whoami: image: emilevauge/whoami labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.frontend.rule=Host:omeka-s.grinnell.edu\u0026#34; - \u0026#34;traefik.frontend.passHostHeader=true\u0026#34; - \u0026#34;traefik.frontend.headers.SSLRedirect=true\u0026#34; networks: - web - internal networks: web: external: true internal: external: false volumes: portainer-data: Use the Let\u0026rsquo;s Encrypt Staging Server To avoid additional rate-limit issues with Let\u0026rsquo;s Encrypt, I\u0026rsquo;m going to switch to using their \u0026ldquo;staging\u0026rdquo; server. That requires the addition of this snippet to our /opt/dockerized-server/traefik/traefik.toml file:\n# CA server to use # Uncomment the line to run on the staging Let\u0026#39;s Encrypt server # Leave comment to go to prod # # Optional # caServer = \u0026#34;https://acme-staging.api.letsencrypt.org/directory\u0026#34; A Fresh Start Now, all that\u0026rsquo;s required to spin up the new server with the aforementioned parts, in this case on dgdocker2, is a command sequence like this:\n# Clean up first! docker stop $(docker ps -q); docker rm -v $(docker ps -qa); docker image rm -f $(docker image ls -q); docker system prune --force; # Navigate into the project cd /opt/dockerized-server # Launch docker network create web docker-compose --log-level DEBUG up -d A Quick Test Since the above command sequence produced no errors, it\u0026rsquo;s time to test what we have. The expectation is that our three services should now be running on dgdocker2, and they should respond in any web browser at the addresses shown here:\nTraefik dashboard - https://traefik2.grinnell.edu Portainer dashboard - https://omeka-s.grinnell.edu/portainer Who Am I info dump - https://omeka-s.grinnell.edu Confirmed! All of the above are working properly, albeit with invalid/temporary certs (due to Let\u0026rsquo;s Encrypt rate limiting).\nPushing to GitHub No project is complete these days without a GitHub component (or something very similar). So, my next step was to create a new GitHub repository at https://github.com/DigitalGrinnell/dockerized-server, and push the contents of my dgdocker2:/opt/dockerized-server directory to it, like so:\ngit init git add -A git commit -m \u0026#34;Initial commit\u0026#34; git remote add origin https://github.com/McFateM/dockerized-server.git git push -u origin master Back to Configuring DGDocker2 At this point you might return to Configuring DGDocker2 where I\u0026rsquo;ll finally add Omeka-S to dgdocker2.\nAnd that\u0026rsquo;s a wrap\u0026hellip; until next time. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/042-my-dockerized-server-config/","tags":["Docker","Traefik","Portainer","Who Am I","traefik.frontend.rule"],"title":"My dockerized-server Config"},{"categories":null,"contents":"My mission today is to successfully migrate the images/containers/services chronicled in post 030, \u0026ldquo;Dockerized Omeka-S: Starting Over\u0026rdquo; to Docker-ready node dgdocker2 without compromising any of the services that already run there.\nPushing WMI Omeka-S to Production on dgdocker2 Grinnell\u0026rsquo;s dgdocker2 server, specifically dgdocker2.grinnell.edu with an IP address of 132.161.132.143, is a Docker-ready CentOS 7 node that\u0026rsquo;s currently supporting the following containers and configuration:\n╭─root@dgdocker2 ~ ╰─# docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES ef20d71ffea8 mcfatem/ohscribe \u0026#34;./boot.sh\u0026#34; 6 days ago Up 6 days 5000/tcp ohscribe b525f4670cd2 mariadb:latest \u0026#34;docker-entrypoint.s…\u0026#34; 2 weeks ago Up 2 weeks 3306/tcp omekasdocker_mariadb_1 7f107a24c204 traefik:latest \u0026#34;/traefik --docker -…\u0026#34; 2 weeks ago Up 2 weeks 0.0.0.0:80-\u0026gt;80/tcp, 0.0.0.0:443-\u0026gt;443/tcp, 0.0.0.0:8080-\u0026gt;8080/tcp traefik_proxy 9282ab53ecc4 portainer/portainer:latest \u0026#34;/portainer --admin-…\u0026#34; 5 weeks ago Up 5 weeks 0.0.0.0:9000-\u0026gt;9000/tcp portainer 60ce06301101 dodeeric/omeka-s:latest \u0026#34;docker-php-entrypoi…\u0026#34; 7 weeks ago Up 7 weeks 80/tcp omekasdocker_omeka_1 54bd82694f3c phpmyadmin/phpmyadmin:latest \u0026#34;/docker-entrypoint.…\u0026#34; 2 months ago Up 2 months 80/tcp omekasdocker_pma_1 0cd019c5456e emilevauge/whoami \u0026#34;/whoamI\u0026#34; 2 months ago Up 2 months 80/tcp omekasdocker_whoami_1 7b3d4961ec21 v2tec/watchtower \u0026#34;/watchtower\u0026#34; 2 months ago Up 2 months watchtower Grinnell\u0026rsquo;s DNS is configured with the following addresses pointed to dgdocker2:\nhttps://textline.grinnell.edu - 404 page not found https://portainer2.grinnell.edu - 404 page not found https://omeka-s.grinnell.edu - Multi-Cultural Reunion 2019 home page https://traefik2.grinnell.edu - 404 page not found https://rootstalkx.grinnell.edu - 404 page not found https://ohscribe.grinnell.edu - OHScribe home page https://dgdocker2.grinnell.edu - 404 page not found The information following each address is the status or page returned when I tried opening each on 3-September-2019.\nThe https://omeka-s.grinnell.edu on dgdocker2 is experimental (at least it was in August 2019) and soon-to-be-replaced with our new Omeka-S. Consequently, the only properly configured service on this node is OHScribe, and the Traefik container is properly configured to serve it as well as the experimental Omeka-S instance. All of the other containers/services should be removed, and the new Omeka-S with WMI configured to work with the existing Traefik.\nSince nearly all of the containers/services running on dgdocker2 are broken or obsolete, I\u0026rsquo;m going to remove them all and clean up the node using this sequence as a copy/paste one-liner\u0026hellip;\ndocker stop $(docker ps -q); docker rm -v $(docker ps -qa); docker image rm -f $(docker image ls -q); docker system prune --force; Deploying a Stand-Alone Traefik Reverse-Proxy There are at least a dozen ways to do this, and I really don\u0026rsquo;t want to reinvent the wheel here, so I searched the web for some of the latest info and settled on this post from DigitalOcean. It\u0026rsquo;s current, I like DigitalOcean\u0026rsquo;s approach in general, and it appears to be well-documented.\nPerhaps the best of Traefik\u0026rsquo;s qualities is its ability to support additional services/containers using labels. Let\u0026rsquo;s roll with that. The plan here is to turn dgdocker2 into the home for many Omeka-S instances with the server answering to https://omeka-s.grinnell.edu.\nOne-Time/Preliminary Stuff I\u0026rsquo;m starting now with a \u0026ldquo;clean\u0026rdquo;, Docker-ready node in dgdocker2. From a terminal/shell opened as root on dgdocker2 we need some preliminary stuff:\n# Create a home on dgdocker2 for the project mkdir -p /opt/traefik cd /opt/traefik nano traefik.toml The traefik.toml file should look like this:\ndefaultEntryPoints = [\u0026#34;http\u0026#34;, \u0026#34;https\u0026#34;] # CA server to use # Uncomment the line to run on the staging Let\u0026#39;s Encrypt server # Leave comment to go to prod # # Optional # caServer = \u0026#34;https://acme-staging.api.letsencrypt.org/directory\u0026#34; [entryPoints] [entryPoints.dashboard] address = \u0026#34;:8080\u0026#34; [entryPoints.dashboard.auth] [entryPoints.dashboard.auth.basic] users = [\u0026#34;admin:$2y$05$pJEzHJBzfoYYS7/hGAedcOP8XdsqNXE7j.LHFBVjueASOqOvvjGOy\u0026#34;] [entryPoints.http] address = \u0026#34;:80\u0026#34; [entryPoints.http.redirect] entryPoint = \u0026#34;https\u0026#34; [entryPoints.https] address = \u0026#34;:443\u0026#34; [entryPoints.https.tls] minVersion = \u0026#34;VersionTLS12\u0026#34; cipherSuites = [ \u0026#34;TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\u0026#34;, \u0026#34;TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\u0026#34;, \u0026#34;TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\u0026#34;, \u0026#34;TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\u0026#34;, \u0026#34;TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\u0026#34;, \u0026#34;TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\u0026#34;, \u0026#34;TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256\u0026#34;, \u0026#34;TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\u0026#34; ] [api] entrypoint=\u0026#34;dashboard\u0026#34; [acme] ### email = \u0026#34;your_email@your_domain\u0026#34; email = \u0026#34;digital@grinnell.edu\u0026#34; storage = \u0026#34;acme.json\u0026#34; entryPoint = \u0026#34;https\u0026#34; onHostRule = true [acme.httpChallenge] entryPoint = \u0026#34;http\u0026#34; [docker] ### domain = \u0026#34;your_domain\u0026#34; domain = \u0026#34;omeka-s.grinnell.edu\u0026#34; watch = true network = \u0026#34;web\u0026#34; Note: The 11 lines, including \u0026ldquo;minVerson\u0026rdquo; and \u0026ldquo;cipherSuites\u0026rdquo; definitions, which appear in the \u0026ldquo;[entryPoints.https.tls]\u0026rdquo; section above were lifted from \u0026ldquo;Removing Traefik\u0026rsquo;s Weak Cipher Suites\u0026rdquo;.\nThe \u0026ldquo;preliminary\u0026rdquo; steps above, and the creation of the traefik.toml file should NOT be repeated, they are good-to-go!\nLaunching Traefik # Clean up first! docker stop $(docker ps -q); docker rm -v $(docker ps -qa); # docker image rm -f $(docker image ls -q); docker system prune --force; # rm -f /opt/traefik/acme.json # probably not necessary? # Create the \u0026#34;web\u0026#34; network docker network create web # Create a home on dgdocker2 for the project... if one does not already exist mkdir -p /opt/traefik cd /opt/traefik # Setup for Let\u0026#39;s Encrypt certs touch acme.json chmod 600 acme.json # Launch Traefik docker run -d \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v $PWD/traefik.toml:/traefik.toml \\ -v $PWD/acme.json:/acme.json \\ -p 80:80 \\ -p 443:443 \\ -l traefik.frontend.rule=Host:traefik2.grinnell.edu \\ -l traefik.port=8080 \\ --network web \\ --name traefik \\ traefik:1.7.14-alpine Ok, let\u0026rsquo;s see what we\u0026rsquo;ve got\u0026hellip;\nEureka! https://traefik2.grinnell.edu returns an admin login prompt for my new Traefik instance at https://traefik2.grinnell.edu/dashboard/, as promised, and it\u0026rsquo;s complete with a green lock icon indicating that we have a valid TLS cert for it. Presumably this Traefik will have NO weak ciphers or vulnerabilities. Note to self: Test this assumption!\nLet\u0026rsquo;s Add Portainer In addition to the Treafik dashboard, I like having Portainer available to help with stack management too. So, let\u0026rsquo;s add that using docker-compose and an appropriately modified version of the guidance provided in Step 3 - Registering Containers with Traefik.\nThe aforementioned guidance wants us to create a new project directory (optional: we could use the /opt/traefik directory we already have) and populate it with a docker-compose.yml file with contents like this:\nversion: \u0026#34;3\u0026#34; networks: # This \u0026#34;networks\u0026#34; section is key. \u0026#34;web\u0026#34; refers to our already-running Docker network web: external: true internal: external: false services: blog: image: wordpress:4.9.8-apache environment: WORDPRESS_DB_PASSWORD: labels: - traefik.backend=blog - traefik.frontend.rule=Host:blog.your_domain - traefik.docker.network=web - traefik.port=80 networks: - internal - web depends_on: - mysql mysql: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: networks: - internal labels: - traefik.enable=false adminer: image: adminer:4.6.3-standalone labels: - traefik.backend=adminer - traefik.frontend.rule=Host:db-admin.your_domain - traefik.docker.network=web - traefik.port=8080 networks: - internal - web depends_on: - mysql The Portainer configuration that I like to use was derived from the docker-compose.demo.yml file in the ISLE project, and it typically looks something like this:\nversion: \u0026#34;3\u0026#34; #### docker-compose up -d networks: web: external: true ## Connect to the existing \u0026#34;web\u0026#34; network! internal: external: false services: portainer2: ## Renamed to avoid conflicts on systems/servers with portainer already running. image: portainer/portainer container_name: portainer2 command: -H unix:///var/run/docker.sock --no-auth ## Swap this out with an auth challenge for security! networks: - web - internal ports: - \u0026#34;9010:9000\u0026#34; ## Remapped to avoid conflicts on systems/servers with portainer already running. volumes: - /var/run/docker.sock:/var/run/docker.sock - portainer-data:/data labels: - traefik.port=9000 - traefik.docker.network=web ## Another critical reference to the \u0026#34;web\u0026#34; network - traefik.enable=true - \u0026#34;traefik.frontend.rule=Host:portainer2.grinnell.edu;\u0026#34; volumes: portainer-data: I built this content into a new /opt/portainer/docker-compose.yml file and subsequently launched Portainer like so:\ncd /opt/portainer docker-compose up -d Visiting https://portainer2.grinnell.edu in my browser shows that it works and has a valid TLS cert too!\nSecuring Portainer Auth The previous outcome is great, but there are at least 3 issues that need to be dealt with. The first issue is Portainer authentication. My initial spin of Portainer, above, is an \u0026ldquo;unprotected\u0026rdquo; instance. Anyone can currently visit https://portainer2.grinnell.edu and see what the stack looks like there. Not good. The culprit is the last line shown in this snippet from our docker-compose.yml file:\nservices: portainer2: ... command: -H unix:///var/run/docker.sock --no-auth The remedy is to preserve the indentation, that\u0026rsquo;s critical in a .yml file, but change that line to read:\ncommand: --admin-password \u0026#34;$$2y$$05$$pJEzHJBzfoYYS7/hGAedcOP8XdsqNXE7j.LHFBVjueASOqOvvjGOy\u0026#34; -H unix:///var/run/docker.sock The hash following \u0026ldquo;\u0026ndash;admin-password\u0026rdquo; is one I generated for my own use with a htpasswd -nb admin... command as documented in Step 1 — Configuring and Running Traefik. Important: Note that in this instance every single dollar sign ($) is DOUBLED and the hash appears in double quotes!\nVisiting https://portainer2.grinnell.edu again and this time the Portainer interface is behind an authentication login pop-up. Nice!\nSwitching to Subdirectory Addressing Now we have https://traefik2.grinnell.edu and https://portainer2.grinnell.edu both working properly on dgdocker2 in what I call a \u0026ldquo;sub-second-top\u0026rdquo; domain name structure. I so named this structure because it follows the convention documented in The Parts of a URL: A Short \u0026amp; Sweet Guide. That blog post identifies the parts of a URL as:\nscheme://subdomain.second-level.top-level/subdirectory\nIn case you didn\u0026rsquo;t pick up on it, the \u0026ldquo;2\u0026rdquo; at the end of each subdomain reflects the fact that the server, or host, is named dgdocker2.\nWhile these addresses are fine, they require considerable coordination with the folks who manage our DNS names; I enlisted their help months ago to \u0026ldquo;create\u0026rdquo; the two addresses we now have. To avoid having to coordinate every change I\u0026rsquo;d like to change things up and identify this server, and the services that run on it, to the world in a form like:\nhttps://omeka-s.grinnell.edu/something This implies that Traefik should respond at https://omeka-s.grinnell.edu/traefik, and Portainer at https://omeka-s.grinnell.edu/portainer. Likewise, our first Omeka-S site, World Music Instruments, or WMI, should respond at https://omeka-s.grinnell.edu/wmi.\nI\u0026rsquo;ve already asked our DNS managers to make https://omeka-s.grinnell.edu resolve to our dgdocker2 host, so all that\u0026rsquo;s necessary now is a change in some of our Traefik labels to specify a different URL structure. Specifically, we need to change the expression in our docker-compose.yml \u0026ldquo;traefik.frontend.rule\u0026rdquo; label from this:\n- \u0026quot;traefik.frontend.rule=Host:portainer2.grinnell.edu;\u0026quot; \u0026hellip;to this set of configuration labels:\n- \u0026quot;traefik.frontend.rule=PathPrefixStrip:/portainer\u0026quot; - \u0026quot;traefik.frontend.redirect.regex=^(.*)/portainer$$\u0026quot; - \u0026quot;traefik.frontend.redirect.replacement=$$1/portainer/\u0026quot; - \u0026quot;traefik.frontend.rule=PathPrefix:/portainer;ReplacePathRegex: ^/portainer/(.*) /$$1\u0026quot; This nice example was taken verbatim from Using labels in docker-compose.yml. After editing these changes into /opt/portainer/docker-compose.yml I did a new cd /opt/portainer; docker-compose up -d and\u0026hellip;\nNow, visiting https://omeka-s.grinnell.edu/portainer brings my authentication-protected Portainer interface up as planned. However, my TLS cert for this domain is not valid yet. Hmmm, wonder why that is? In any case\u0026hellip; this is progress!\nMoving Traefik to a Subdirectory I\u0026rsquo;m going to make the same kind of changes for Traefik now, but this time the modifications are to the docker run... command that I use to launch it. Some trial, and lots of errors, lead me to this new docker run... command syntax:\ndocker run -d \\ -v /var/run/docker.sock:/var/run/docker.sock \\ -v $PWD/traefik.toml:/traefik.toml \\ -v $PWD/acme.json:/acme.json \\ -p 80:80 \\ -p 443:443 \\ -l traefik.frontend.rule=PathPrefixStrip:/traefik \\ -l traefik.frontend.redirect.regex=\u0026#39;^(.*)/traefik$\u0026#39; \\ -l traefik.frontend.redirect.replacement=$1/traefik/ \\ -l traefik.port=8080 \\ --network web \\ --name traefik \\ traefik:1.7.14-alpine Unfortunately, just like Portainer, my new Traefik address failed to get a valid TLS cert. \u0026#x1f626;\nWho Am I Everything on the dgdocker2 server responds to a \u0026ldquo;subdirectory\u0026rdquo; address now and there\u0026rsquo;s nothing registered at https://omeka-s.grinnell.edu. To help eliminate the possibility that this is a problem I\u0026rsquo;m going to try adding a WhoAmI service, at the aforementioned address, using the configuration documented in this simple and straightforward repo.\n# Create a home on dgdocker2 for the project... if one does not already exist cd /opt git clone https://github.com/lukasnellen/dc-whoami.git whoami cd /opt/whoami # Edit the docker-compose.yml file as needed nano docker-compose.yml # see completed edits below Continuing after edits to /opt/whoami/docker-compose.yml\u0026hellip;\ndocker-compose --log-level DEBUG up -d Now, if I visit https://omeka-s.grinnell.edu I can see that the WhoAmI is working, but again, it does not have a valid cert. \u0026#x1f626;\nInvestigating Invalid Certs In an attempt to determine why my certs are not valid, I found Debugging Let\u0026rsquo;s Encrypt Errors, Sometimes It\u0026rsquo;s Not Your Fault. From my workstation I tried some of the suggestions in the post and got these results:\n╭─mark@Marks-Mac-Mini ~/Projects/blogs-McFateM ‹master*› ╰─$ host omeka-s.grinnell.edu omeka-s.grinnell.edu has address 132.161.132.143 ╭─mark@Marks-Mac-Mini ~/Projects/blogs-McFateM ‹master*› ╰─$ host omeka-s.grinnell.edu 8.8.8.8 Using domain server: Name: 8.8.8.8 Address: 8.8.8.8#53 Aliases: omeka-s.grinnell.edu has address 132.161.132.143 ╭─mark@Marks-Mac-Mini ~/Projects/blogs-McFateM ‹master*› ╰─$ curl -k https://omeka-s.grinnell.edu Hostname: 67c6f570dc5b IP: 127.0.0.1 IP: 192.168.80.3 IP: 192.168.96.2 GET / HTTP/1.1 Host: omeka-s.grinnell.edu User-Agent: curl/7.54.0 Accept: */* Accept-Encoding: gzip X-Forwarded-For: 173.18.136.80 X-Forwarded-Host: omeka-s.grinnell.edu X-Forwarded-Port: 443 X-Forwarded-Proto: https X-Forwarded-Server: 34e5bc377410 X-Real-Ip: 173.18.136.80 These results make me believe that our DNS entries are NOT the problem. That leaves me believing that I\u0026rsquo;ve probably hit a Let\u0026rsquo;s Encrypt rate limit. \u0026#x1f626; So, moving on, I\u0026rsquo;m going to accept the invalid certs and just try to get Omeka-S up and running.\nWho Am I That didn\u0026rsquo;t help with the invalid certs issue. So, now I\u0026rsquo;m thinking the problem here is that nothing is registered at https://omeka-s.grinnell.edu; everything lives in subdirectory paths \u0026ldquo;below\u0026rdquo; that subdomain. Based on that hunch, I\u0026rsquo;m going to try adding a WhoAmI service, at the aforementioned address, using the configuration documented in this simple and straightforward repo.\n# Create a home on dgdocker2 for the project... if one does not already exist cd /opt git clone https://github.com/lukasnellen/dc-whoami.git whoami cd /opt/whoami # Edit the docker-compose.yml file as needed nano docker-compose.yml # see completed edits below Continuing after edits to /opt/whoami/docker-compose.yml\u0026hellip;\ndocker-compose --log-level DEBUG up -d Now, if I visit https://omeka-s.grinnell.edu I can see that the WhoAmI is working, but again, it does not have a valid cert.\nCapture As a Project I like the direction this server setup has taken, apart from the invalid certs issue \u0026#x1f626;, so I\u0026rsquo;m taking steps to formally \u0026ldquo;capture\u0026rdquo; this setup. I will chronicle that process in My dockerized-server Config.\nBack to Omeka-S Configuration Having wrapped up My dockerized-server Config, I\u0026rsquo;m back to finally get Omeka-S configured on dgdocker2. Unfortunately, while configuring this final spin of Omeka-S I ran short on time and failed to document every step. However, the outcome is working nicely at dgdocker2:/opt and is captured in a new GitHub repo at McFateM/omeka-s-dgdocker2.\nThis repo includes:\nA dockerized-server component where Traefik, Portainer and Who Am I are configured; A solr component Solr is configured; An omeka-s-docker component where Omeka-S, MariaDB, and PHPMyAdmin (PMA) are configured; A docker-reset.sh script that can be used to reset the host\u0026rsquo;s Docker; and A launch-stack.sh script that can be used to reset Docker and then re-start the entire stack. Persistence As currently configured, the stack maintains persistent Omeka site data in a Docker volume (NOT a \u0026ldquo;bind mount\u0026rdquo;, but a named volume managed by Docker). There is a comment line in the docker-reset.sh that can be enabled to wipe the aforementioned volume clean; use it with extreme caution! There\u0026rsquo;s also a comment line in omeka-s-docker/docker-compose.yml that can be enabled to re-initialize the omeka database with a backup of the original World Music Instruments site on server omeka1.\nLaunch and Addressing I did a git clone https://github.com/DigitalGrinnell/omeka-s-dgdocker2 /opt and then source /opt/launch-stack.sh from a root terminal/shell on dgdocker2. The result is this working set of services and addresses:\nService Address Note Traefik dashboard https://traefik2.grinnell.edu Requires authentication Portainer dashboard https://omeka-s.grinnell.edu/portainer/ Requires authentication Who Am I https://omeka-s.grinnell.edu/who-am-i Solr administration https://portainer2.grinnell.edu Temporary. Requires authentication MariaDB administration None See ./pma/ below PHPMyAdmin https://omeka-s.grinnell.edu/pma/ Trailing slash is REQUIRED Omeka-S https://omeka-s.grinnell.edu Addressing Update Today, 11-Sep-2019, I got word that my DNS requests for subdomain names solr2.grinnell.edu and pma2.grinnell.edu were completed. So this morning I made necessary changes to dgdocker2:/opt/omeka-s-docker/docker-compose.yml and did a new docker-compose up -d in that directory. It worked, and I didn\u0026rsquo;t even have to take the stack down and restart it!\nSo, we now have this updated, and final, addressing scheme:\nService Address Note Traefik dashboard https://traefik2.grinnell.edu Requires authentication Portainer dashboard https://omeka-s.grinnell.edu/portainer/ Requires authentication Who Am I https://omeka-s.grinnell.edu/who-am-i Solr administration https://solr2.grinnell.edu No longer temporary MariaDB administration None See pma2.grinnell.edu below PHPMyAdmin https://pma2.grinnell.edu No longer temporary. Behaving properly Omeka-S https://omeka-s.grinnell.edu Now with Valid Certs! Earlier in this process we learned that Solr won\u0026rsquo;t work properly without a valid TLS certificate, not a temporary one. So I was forced to move our certificate authority server spec from Let\u0026rsquo;s Encrypt \u0026ldquo;stagging\u0026rdquo; back to \u0026ldquo;production\u0026rdquo; (see the ./dockerized-server/data/traefik.toml file for details). Fortunately, when I ran our addressing update (see section above) the production certs obtained from Let\u0026rsquo;s Encrypt were valid this time. Woot! I guess that means that our recent rate-limit induced ban had expired? It also means I can now close the books on this project. Double woot!\nAnd that\u0026rsquo;s a wrap\u0026hellip; until next time.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/041-configuring-dgdocker2/","tags":["Docker","Traefik","Portainer","Omeka-S","traefik.frontend.rule"],"title":"Configuring DGDocker2"},{"categories":null,"contents":"Note: The abbreviation IMI is used frequently in this post to represent the Islandora Multi-Importer, a CSV-file-driven batch ingest tool used by numerous institutions in the Islandora community.\nAlso, while updating this post I found this gem\u0026hellip; Diagrams in Documentation (Markdown Guide).\nThis post is an addition to the discussion in post 028, Mounting //Storage for IMI Ingest in Digital.Grinnell which was written largely to remind me how the //Storage mount works. \u0026#x1f615; Unlike post 028, this post is meant for myself AND for others at Grinnell College who prepare digital objects for ingest via IMI. For those individuals\u0026hellip; it\u0026rsquo;s not necessary that you understand all of this, but it you choose to read-on, I hope you find this information helpful.\nIMI Process Overview At its core, IMI is a CSV ingest batch tool, but at Grinnell we use it almost exclusively with Google Sheets rather than actual .csv files. We do this largely so that each batch of objects we ingest, or modify, can have one (or more if necessary) shared data files\u0026hellip; with emphasis on shared. Also, since IMI supports direct use of Google Sheets we are able to ingest objects without having to create and juggle multiple copies or revisions of .csv files which can be difficult to track and effectively manage.\nHere at Grinnell we also have a single metadata template and metadata \u0026ldquo;form\u0026rdquo; so we don\u0026rsquo;t have to maintain a different template or form for each content type or cModel. Instead, we maintain a \u0026ldquo;master\u0026rdquo; Google Sheet or workbook that gets \u0026ldquo;cloned\u0026rdquo; for each set of objects that we ingest. The \u0026ldquo;master\u0026rdquo;, Digital_Grinnell_MODS_Master workbook is shared with a small group of Grinnell College staff who are responsible for it\u0026rsquo;s use and maintenance.\nThe workflow is perhaps best explained with a (rather complex and busy) diagram:\nTabs or Worksheets The \u0026ldquo;master\u0026rdquo; workbook features four \u0026ldquo;tabs\u0026rdquo; or worksheets:\nTab Name Description Instructions A set of instructions for cloning the master and using this workflow. History Presents a brief revision history of the workbook. Twig The latest revision of the Twig template used to transform cell contents into MODS field data. MASTER A small set of sample data. When the \u0026ldquo;master\u0026rdquo; workbook is cloned, this tab/worksheet is where content editors put their data. It\u0026rsquo;s worth noting here that none of these tab names contain spaces. That\u0026rsquo;s intentional, and a good practice to follow in general because, while spaces are allowed in names, they can cause problems when they appear in things like web addresses (URLs). Since our tab names eventually appear in URLs elsewhere in this workflow, I kindly ask that you omit spaces if/when you duplicate or create new tabs or worksheets.\nMASTER The MASTER tab, briefly introduced in the table above, deserves its own section\u0026hellip;so here it is. If you\u0026rsquo;re a Grinnell College content editor or digital collection manager who submits objects for ingest into Digital.Grinnell, then this tab is for you. It\u0026rsquo;s where you put your object metadata, content controls, and content references. Note that the tab name, MASTER, is meaningful too, and it follows a new tab-naming convention documented in the Instructions tab and here in this post in August 2019 (revised in September 2019).\nIt\u0026rsquo;s also important to note that the MASTER worksheet name should NEVER be changed! It is the \u0026ldquo;master\u0026rdquo; copy of the data and, as such, it should be copied to a new tab/worksheet when it is \u0026ldquo;ready\u0026rdquo; for ingest. See the examples section below for more on this.\nTab Names Convention Tab or worksheet names, like MASTER, are meaningful and intended to convey the status of the tab\u0026rsquo;s contents. Content editors should always work in the MASTER worksheet, adding and modifying data as-necessary. When content in MASTER is ready for review and ingest the editor should first make a copy of the MASTER worksheet, naming it READY-[date], where [date] reflects the date the worksheet copy was made.\nNote that in any tab, the contents of the PID column also helps to indicate the \u0026ldquo;state\u0026rdquo; of the worksheet and the data within. Once an object has been ingested, that object\u0026rsquo;s PID should appear in the corresponding row of the PID column, and the PID column should always be the left-most column in the worksheet. So, the presence of a PID in this column indicates that the corresponding object already exists in the repository.\nExamples The following example tab names may help understand this convention.\n- `MASTER` This is the default/initial data tab/worksheet name and it should never be changed. Differently named tabs should always be made from **copies of this tab!** This name simply indicates that data entry is not complete and is not yet ready for ingest. - `Ready-26Aug2019` This example name indicates that the content editor has completed preparation of the information in `MASTER` and made a copy which is ready for review and ingest on the date indicated in the third term. A tab name like this should be included in an appropriate Trello card update so that the DG administrator knows the content is ready for ingest. - `Ingested-28Aug2019` This example name indicates that on 28-Aug-2019 a DG administrator made a copy of a worksheet that was ready-for-ingest, reviewed, and successfully ingested the data creating a new set of objects. - `Updated-28Aug2019` This example name is similar to the name above, but indicates that on 28-Aug-2019 a DG administrator ingested the content to update one or more existing DG objects. And that\u0026rsquo;s a wrap good place for a break! More to come, after the break, of course. \u0026#x1f604;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/040-digital.grinnells-imi-workflow/","tags":["Digital.Grinnell","IMI","Islandora Multi-Importer","workflow","Google Sheets"],"title":"Digital.Grinnell's IMI Workflow"},{"categories":null,"contents":" Attention! The Docksal portion of this discussion DID NOT WORK PROPERLY so I\u0026rsquo;ve hidden it from public view. Don\u0026rsquo;t use this project with Docksal (fin commands) until further notice! ","permalink":"https://static.grinnell.edu/dlad-blog/posts/039-omeka-s-in-docksal/","tags":["Omeka-S","Docksal","fin"],"title":"Omeka-S in Docksal"},{"categories":null,"contents":"As promised in post 037, this post combines elements of post 021 with updated post 034 to create a \u0026ldquo;customized\u0026rdquo; local ISLE instance with features of Digital.Grinnell. In this November 2019 update I\u0026rsquo;m building the repository on my MacBook ma7053 using ISLE-1.3.0. The target of this endeavor will be a properly populated Digital.Grinnell custom Islandora code repository featuring ISLE-1.3.0 code.\nProcess Overview I believe the process I need to engage here can be outlined like this:\nRepeat or follow-up on post 034. Assuming a successful test of the site produced in that post, the next steps will include\u0026hellip;\nMove to post 21 and complete the following steps, in order:\n- [Installing the DG Theme](/posts/021-rebuilding-isle-ld/#installing-the-dg-theme) - [Install the Islandora Multi-Importer (IMI)](/posts/021-rebuilding-isle-ld/#install-the-islandora-multi-importer-IMI) - [Install the Missing Backup and Migrate Module](/posts/021-rebuilding-isle-ld/#install-the-missing-backup-and-migrate-module) - [Backup and Restore the Database Using Backup and Migrate](/posts/021-rebuilding-isle-ld/#backup-and-restore-the-database-using-backup-and-migrate) - [Restore Results...Lots of Warnings](/posts/021-rebuilding-isle-ld/#restore-results-lots-of-warnings) - [Installing the Missing Islandora/Custom Bits](/posts/021-rebuilding-isle-ld/#installing-the-missing-islandora-custom-bits) - [Temporarily Eliminate Warnings](/posts/021-rebuilding-isle-ld/#temporarily-eliminate-warnings) Note: Many of these steps should be moved to new, and existing, Granular ISLE Customization: Series Guidelines posts.\nRepeat a process similar to Step 11: Check-In the Newly Created Islandora Drupal Site Code Into Your Git Repository, with an aim of updating the dg-islandora repository. \u0026ndash; Breaking here for DrupalCorn Camp on 5-Nov-2019 The remainder of this document is hidden from public view pending a substantial update.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/038-building-my-dg-islandora-code-repository/","tags":["ISLE","v1.3.0","git","dg-islandora","code repository"],"title":"Building My `dg-islandora` Code Repository"},{"categories":null,"contents":"This post is a follow-up to previous post 034 where I successfully completed a \u0026ldquo;local\u0026rdquo; build of ISLE v1.2.0, but did no \u0026ldquo;customization\u0026rdquo; of that local instance. So, this post\u0026rsquo;s intent is to complete the goal stated in post 034, specifically to:\nThe goal of this project is to spin up a pristine, local Islandora stack using an updated fork of the ISLE project at https://github.com/DigitalGrinnell/dg-isle, then introduce elements like the Digital Grinnell theme and custom modules like DG7. Once these pieces are in-place and working, I\u0026rsquo;ll begin adding other critical components as well as a robust set of data gleaned from https://digital.grinnell.edu.\nAs before, this effort will involve an ld, or local development, instance of Digital.Grinnell on one of my Mac workstations. Unlike my previous work, this instance will follow the guidance of a different document, specifically install-local-migrate.md.\nFork Synchronization Before beginning this process I need to get my Github environment updated by synchronizing my ISLE fork with the canonical copy. I followed this workflow to do so, like this:\nWorkstation Commands cd ~/Projects git clone https://github.com/McFateM/ISLE.git cd ISLE git remote add upstream https://github.com/Islandora-Collaboration-Group/ISLE.git git fetch upstream git pull upstream master git push atom . Nice! In case you haven\u0026rsquo;t seen it before, the last command in that sequence, atom ., simply opens my Atom editor to the new local instance of the ISLE project.\nInstalling per ISLE\u0026rsquo;s install-local-migrate.md This is first-and-foremost a local development copy of ISLE, but with considerable Digital Grinnell customization, so I\u0026rsquo;m following the process outlined in the project\u0026rsquo;s ./docs/install/install-local-migrate.md. References to Step X that follow refer to corresponding sections of https://github.com/Islandora-Collaboration-Group/ISLE/blob/master/docs/install/install-local-migrate.md. Each section or \u0026ldquo;Step\u0026rdquo; listed below is also a link back to the corresponding section of the canonical document.\nStep 0: Copy Production Data to Your Local Drupal Site Files \u0026amp; Code Per the aforementioned guidance, I did this\u0026hellip;\nOpened a zsh shell (terminal) on Digital.Grinnell production node DGDocker1 as user islandora using iTerm2 at ssh://islandora@dgdocker1.grinnell.edu. Then, in that shell on the DGDocker1 host I did:\nDGDocker1 Commands cd ~ mkdir -p migration-copy/var/www/html/sites/default/files cd migration-copy docker cp isle-apache-dg:/var/www/html/sites/default/files/. var/www/html/sites/default/files/ Note that the -p option on mkdir is critical, it will create all, or port of, the entire directory tree as-specified, if it does not already exist. The final command, docker cp..., subsequently takes advantage of this new directory tree and copies all of the Drupal .../default/files from the production container to a similar directory on the host, DGDocker1 in this case.\nFor the 2nd bullet item in this step, I\u0026rsquo;ve identified that my Drupal/Islandora customizations should eventually reside in the private repo that is https://github.com/McFateM/dg-islandora.\nDrupal Site Database For this process I already have a workflow in place, and it\u0026rsquo;s a little different than what\u0026rsquo;s documented. So here\u0026rsquo;s what I did:\nVisit my production site at https://digital.grinnell.edu. Login as the System Admin, that\u0026rsquo;s User 1 or the super-user in Drupal terms. The home page at https://digital.grinnell.edu provides, in the right-hand menu bar, a Management menu with a first option to Clear cache. Click it. The home page also provides a Quick Backup block where the default options do a great job of backing up only what\u0026rsquo;s needed. Accept all defaults and click the Backup Now button. This feature takes the site offline, makes and downloads a backup of the database (in my case it created digital.grinnell.edu-2019-08-13T15-51-20.mysql), and brings the site back online\u0026hellip;\u0026ldquo;automagically\u0026rdquo;. Fedora Hash Size (Conditional) I found this section a little confusing, which I assume means that it does not apply to Digital.Grinnell. Moving on.\nSolr Schema \u0026amp; Islandora Transforms This section definitely applies to Digital.Grinnell! \u0026#x1f60a; Since I recently completed spin-up of a \u0026ldquo;Demo\u0026rdquo; ISLE using install-local-new.md, and since this is \u0026ldquo;not my first rodeo\u0026rdquo;, I choose to take the \u0026ldquo;advanced\u0026rdquo; path here and will \u0026ldquo;diff \u0026amp; merge current production customization edits into ISLE configs\u0026rdquo;. Wish me luck. \u0026#x1f340;\nTo begin this portion of the process I created a new ~/diff-and-merge-customizations directory structure and with https://isle.localdomain running the Demo, I copied files from it like so:\nWorkstation Commands mkdir -p ~/diff-and-merge-customizations/ld mkdir -p ~/diff-and-merge-customizations/prod cd ~/diff-and-merge-customizations/ld docker cp isle-solr-ld:/usr/local/solr/collection1/conf/schema.xml schema.xml docker cp isle-fedora-ld:/usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/foxmlToSolr.xslt foxmlToSolr.xslt docker cp isle-fedora-ld:/usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/islandora_transforms islandora_transforms I subsequently pulled the three diff targets from my production instance of ISLE using these commands on DGDocker1, followed by another set on my workstation:\nDGDocker1 Commands mkdir -p ~/migration-copy/solr mkdir -p migration-copy/fedora cd migration-copy/solr docker cp isle-solr-dg:/usr/local/solr/collection1/conf/schema.xml . cd ../fedora docker cp isle-fedora-dg:/usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/foxmlToSolr.xslt . docker cp isle-fedora-dg:/usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/islandora_transforms islandora_transforms Workstation Commands cd ~/diff-and-merge-customizations/prod rsync -aruvi islandora@dgdocker1.grinnell.edu:migration-copy/solr/. . rsync -aruvi islandora@dgdocker1.grinnell.edu:migration-copy/fedora/. . This left me with a local ~/diff-and-merge-customizations directory with sub-dirs ld and prod. The diff-and-merge operation subsequently compared the contents of these two sub-dirs, merging the differences from prod into the corresponding files found in ld. I did both the diff and merge operations using Atom.\nThe command and output from the first diff of the islandora_transforms was:\n╭─markmcfate@ma8660 ~ ╰─$ cd ~/diff-and-merge-customization/ld ╭─markmcfate@ma8660 ~/diff-and-merge-customization/ld ╰─$ diff -r islandora_transforms ../prod/islandora_transforms 1 ↵ Only in islandora_transforms: .git diff -r islandora_transforms/WORKFLOW_to_solr.xslt ../prod/islandora_transforms/WORKFLOW_to_solr.xslt 95c95 \u0026lt;/xsl:stylesheet\u0026gt; \\ No newline at end of file --- \u0026lt;/xsl:stylesheet\u0026gt; So, no significant differences here. Yay!\nNext, I compared the two files, one-at-a-time, using Atom and its Split Diff package. There were very few differences in foxmToSolr.xslt so it was easy to merge. I took the significant differences in the prod copy and merged them into the ld copy. I did the same with schema.xml, but it was an entirely different beast. There were 34 sections of differences, and in many cases it was not clear if our existing customizations should be merged. As a result, the changes I made in schema.xml, and some I did NOT make, will need to be carefully re-evaluated as this local migrate \u0026ldquo;test\u0026rdquo; proceeds.\nAt this point I\u0026rsquo;ve saved my \u0026ldquo;merged\u0026rdquo; customizations in ~/diff-and-merge-customizations/ld until Step 2a, below.\nPhew, that was a lot of Step 0!\nStep 1: Edit /etc/hosts File Easy peasy, relatively speaking. \u0026#x1f604;\nStep 2: Setup Git for the ISLE Project OK, I already have two private Github repositories\u0026hellip;\nhttps://gitubh.com/McFateM/dg-isle for my customized copy of the ISLE project, and https://github.com/McFateM/dg-islandora for my customized copy of the Drupal and Islandora code. However, in my case neither of these repositories is \u0026ldquo;complete\u0026rdquo; because they are products of the install-local-new.md process, so my focus in this step will be to get these two repos in line with the Git workflow that\u0026rsquo;s being established.\nRegarding the dg-isle repo\u0026hellip;\nI cloned it to my local workstation and set things up as directed with: Workstation Commands cd ~/Projects git clone https://github.com/McFateM/dg-isle.git cd dg-isle git remote add icg-upstream https://github.com/Islandora-Collaboration-Group/ISLE.git git fetch icg-upstream git pull icg-upstream master git push -u origin master Step 2a: Add Customizations from Step 0 Before executing the documented commands I elected to create a new branch in my local dg-isle repo, so my workflow was this:\nWorkstation Commands cd ~/Projects/dg-isle git checkoput -b solr-and-gsearch-customizations mkdir -p ./config/solr mkdir -p ./config/fedora/gsearch cp -f ~/diff-and-merge-customizations/ld/schema.xml ./config/solr/ cp -f ~/diff-and-merge-customizations/ld/foxmlToSolr.xslt ./config/fedora/gsearch/ cp -fr ~/diff-and-merge-customizations/ld/islandora_transforms ./config/fedora/gsearch/ Next, I edited docker-compose.local.yml as prescribed, and then saved it all like so:\nWorkstation Commands cd ~/Projects/dg-isle git checkout solr-and-gsearch-customizations git add -A git commit -m \u0026ldquo;Customizations from Step 2a\u0026rdquo; git push \u0026ndash;set-upstream origin solr-and-gsearch-customizations Step 3. git clone the Production Drupal Site Code OK, this section will deal with my Islandora/Drupal code repository with all my customization in it, in my case that\u0026rsquo;s https://github.com/McFateM/dg-islandora, but my copy isn\u0026rsquo;t complete yet, so\u0026hellip;\nPost 038 - Building My dg-islandora Code Repository I\u0026rsquo;m creating a new blog post that will become 038-Building-My-dg-islandora-Code-Repository. It will combine elements of post 021 with post 034 to create a \u0026ldquo;customized\u0026rdquo; local ISLE v1.2.0 instance with features of Digital.Grinnell.\nOnce that process is complete, I\u0026rsquo;ll return here to continue Step 3. Until then\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/037-migrating-dg-to-isle-1.2.0-ld/","tags":["ISLE","v1.2.0","local","migration"],"title":"Migrating Digital.Grinnell (DG) to ISLE 1.2.0 (ld) for Local Development"},{"categories":null,"contents":"Grocy looks lika a great little PHP stack application for me. It\u0026rsquo;s aim is to help folks organize and inventory their \u0026ldquo;stuff\u0026rdquo;, with a slant toward food and groceries. I need this!\nSince I\u0026rsquo;m also a big fan of Docker and Docksal, naturally I wanted to spin Grocy up in one of these environments. Thankfully, the heavy lifting of getting this \u0026ldquo;Dockerized\u0026rdquo; has already been done, and that fine work is shared in GitHub at grocy/grocy-docker. So my quest last evening started with a fork of this GitHub project to SummittDweller/grocy-docker, where I\u0026rsquo;ve created a new docksal branch.\nDisclaimer Note that grocy is not really work-related for me, but this process of \u0026ldquo;Dockerizing\u0026rdquo; and \u0026ldquo;Docksalizing\u0026rdquo; it is. So that\u0026rsquo;s why this post is here and not in my personal blog. Dockerizing the Local Stack I first forked the project using GitHub\u0026rsquo;s interface, then switched to my personal MacBook and the terminal there, like this:\nWorkstation Commands cd ~/Projects git clone https://github.com/SummittDweller/grocy-docker # clone the fork to local cd grocy-docker git checkout -b docksal # create a new docksal local branch atom . # open the new local project directory in my Atom editor docker-compose pull # pull the images as instructed docker-compose up -d # bring up the stack before any modifications I should mention that before all of this I edited my MacBook\u0026rsquo;s /etc/hosts file to make sure I had only one active entry there for 127.0.0.1 localhost. \u0026#x2611;\u0026#xfe0f; Confirmed.\nSo, I opened my browser to https://localhost as directed in the grocy-docker documentation. \u0026#x1f4af; Nice!\nThe original docker-compose.yml file for this project creates a Docker-managed volume called database and maps that volume to the grocy container\u0026rsquo;s /www directory, one level above the /www/public webroot. The pertinent lines from docker-compose.yml look like this:\ngrocy: ... volumes: - database:/www ... volumes: database: That effectively \u0026ldquo;persists\u0026rdquo; any changes I make, until I remove them, of course. To test persistence, with the stack still running at https://localhost I entered the default admin user and admin password, navigated to the Master data options and proceeded to add a couple locations to my data.\nNext, with the stack still running I made a new host copy of it all, brought the stack down, changed the configuration, then brought it back up again with the new configuration, like so:\nWorkstation Commands cd ~/Projects/grocy-docker mkdir docroot # makes a new folder on the host docker cp grocy:/www/. ./docroot/ # copy contents of www in the grocy container to the host\u0026rsquo;s new directory docker-compose down # bring the stack down While this approach is OK, I wanted something a little more flexible with an accessible, persistent copy of the code and data on my MacBook host, so I used Atom to edit the docksal copy of docker-compose.yml to look like this:\ngrocy: ... volumes: # - database:/www - ./docroot:/www ... # volumes: # database: Then\u0026hellip;\nWorkstation Commands cd ~/Projects/grocy-docker docker-compose up -d # bring the stack back up again with the new directory mapping Ok, another visit to https://localhost confirms that the stack is working, and when I navigated back into the Master data my locations are still there. \u0026#x2611;\u0026#xfe0f; Woot!\nDocksalizing the Local Stack Ok, not sure I did this \u0026ldquo;correctly\u0026rdquo;, but this process works.\nWorkstation Commands cd ~/Projects/grocy-docker git checkout -b docksal # switch to the docksal local branch atom . # open the new local project directory in my Atom editor fin config generate # accept the default and make this a Docksal stack The last command in the above sequence created a new .docksal directory in my project. Yay! It also created a new index.php file in the project\u0026rsquo;s ./docroot directory. That\u0026rsquo;s not quite right (and I\u0026rsquo;m not sure how to make fin config generate behave better), but easy to \u0026ldquo;fix\u0026rdquo;.\nI opened ./.docksal/docksal.env in Atom and changed it to read:\nDOCKSAL_STACK=default # DOCROOT=docroot DOCROOT=docroot/public This simple change effectively moves the default DOCROOT target from ./docroot/index.php to the correct grocy target of ./docroot/public/index.php.\nTo test this change\u0026hellip;\nWorkstation Commands cd ~/Projects/grocy-docker git checkout -b docksal docker-compose down # bring the stack down docker stop $(docker ps -q) # stop the containers docker rm -v $(docker ps -qa) # remove the stopped containers fin up # bring up the new \u0026ldquo;Docksalized\u0026rdquo; stack Now a visit to http://grocy-docker.docksal as directed\u0026hellip; and the stack is still working! When I navigated back into the Master data my locations are still there too. \u0026#x2611;\u0026#xfe0f; Woot!\nTo bring the stack down\u0026hellip;\nWorkstation Commands cd ~/Projects/grocy-docker git checkout -b docksal fin stop # bring the \u0026ldquo;Docksalized\u0026rdquo; stack down Rinse and repeat as necessary.\nWeekend Improvements (the Best Kind!) A few days ago, when this post was originally written, I also posted a \u0026ldquo;feature request\u0026rdquo; into the Grocy issues queue; specifically, it is issue 341, which is now closed.\nWithin a few hours there were comments posted to this issue; apparently other folks thought this would be a nice addition too. :simple_smile: Not long after that, Grocy\u0026rsquo;s author, Bernd Bestel posted this comment:\nSure, any contributions are very welcome. :)\n\u0026hellip;but I\u0026rsquo;m already fiddling together a first draft about this \u0026hellip; will show you in a couple of minutes \u0026hellip; feel free to improve it then\u0026hellip; :)\nWow\u0026hellip; I definitely owe Bernd a beverage of his choosing next time I\u0026rsquo;m in Germany! But I digress\u0026hellip;\nSince I\u0026rsquo;m developing on a Mac, and not Windows, I can\u0026rsquo;t easily run the Grocy code \u0026ldquo;directly\u0026rdquo; out-of-the-box; I need Docker for that. Understandably, the grocy-docker project builds, or pulls, the latest tagged version of grocy/grocy, not the latest code, which resides in the master branch of the repo. So I set to work trying to figure out how to build a Docker, and then Docksal, environment from Bernd\u0026rsquo;s master.\nI made this key substitution in the project\u0026rsquo;s Dockerfile-grocy, along with a few associated and necessary changes:\n# wget -t 3 -T 30 -nv -O \u0026#34;grocy.zip\u0026#34; \u0026#34;https://github.com/grocy/grocy/archive/v${GROCY_VERSION}.zip\u0026#34; \u0026amp;\u0026amp; \\ wget -t 3 -T 30 -nv -O \u0026#34;grocy.zip\u0026#34; \u0026#34;https://github.com/grocy/grocy/zipball/master\u0026#34; \u0026amp;\u0026amp; \\ \u0026hellip;and it works using this local command-line process to implement it all.\nWorkstation Commands cd ~/Projects/grocy-docker git checkout -b docksal docker stop $(docker ps -q); docker rm -v $(docker ps -qa); docker image rm \u0026ndash;force $(docker image ls -q); docker system prune \u0026ndash;force rm -fr docroot/* docroot/.yarnrc docker-compose build; docker-compose up -d docker cp grocy:/www/. docroot/ docker-compose down touch docroot/data/demo.txt fin up The first three lines ensure I\u0026rsquo;m working in the right location, then bring ALL Docker stacks down and clean-up. Line 4 removes any remnants of my previous ./docroot capture, without removing ./docroot itself. The 5th line rebuilds grocy in Docker, not Docksal, and starts up the stack. Line 6 makes a copy of the running grocy container for safe-keeping. Line 7 brings the Docker stack down. Line 8 creates an empty ./docroot/data/demo.txt file in order to populate my project with a robust set of default locations and items (see this documentation for details). Line 9 brings the new stack back up in Docksal.\nWoot! It works. I\u0026rsquo;m pushing the latest changes to the docksal branch of https://github.com/SummittDweller/grocy-docker.git at this very moment.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/036-building-grocy-in-docksal/","tags":["Grocy","Docker","Docksal","fin","docker-compose"],"title":"Building Grocy in Docksal"},{"categories":null,"contents":"I\u0026rsquo;ve been seeing a lot of .URL will be deprecated... warnings in my Hugo compilations lately, and just now figured out a slick replacement for it: canonifyURLs = true. The documentation for this parameter says\u0026hellip;\nBy default, all relative URLs encountered in the input are left unmodified, e.g. /css/foo.css would stay as /css/foo.css. The canonifyURLs field in your site config has a default value of false.\nBy setting canonifyURLs to true, all relative URLs would instead be canonicalized using baseURL. For example, assuming you have baseURL = https://example.com/, the relative URL /css/foo.css would be turned into the absolute URL https://example.com/css/foo.css.\nTurning this parameter on in my personal blog allowed me to make the following code change work:\nOld code: \u0026lt;a href=\u0026quot;{{ .URL }}index.xml\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;{{ .URL }}/img/rss.png\u0026quot; class=\u0026quot;rss-icon\u0026quot; /\u0026gt;\u0026lt;/a\u0026gt; New code: \u0026lt;a href=\u0026quot;index.xml\u0026quot;\u0026gt;\u0026lt;img src=\u0026quot;/img/rss.png\u0026quot; class=\u0026quot;rss-icon\u0026quot; /\u0026gt;\u0026lt;/a\u0026gt; Note that the to-be-deprecated {{ .URL }} references are gone!\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/035-canonifyurls-in-hugo/","tags":["Hugo","canonifyURLs"],"title":"CanonifyURLs in Hugo"},{"categories":null,"contents":"This post, an updated (the original was written in August 2019 for ISLE-1.2.0) follow-up to a previous post is intended to chronicle my efforts to build a new ISLE v1.2.0 ISLE-1.3.0, ld, or local development, instance of Digital.Grinnell on my work-issued MacBook, ma7053.\nGoal Statement The goal of this project is to spin up a pristine, local Islandora stack using an updated fork of the ISLE project at https://github.com/Digital-Grinnell/dg-isle, then introduce elements like the Digital Grinnell theme and custom modules like DG7. Once these pieces are in-place and working, I\u0026rsquo;ll begin adding other critical components as well as a robust set of data gleaned from https://digital.grinnell.edu.\nUsing This Document There are just a couple of notes regarding this document that I\u0026rsquo;d like to pass along to make it more useful.\nGists - You will find a few places in this post where I generated a gist to take the place of lengthy command output. Instead of a long stream of text you\u0026rsquo;ll find a simple link to a gist like this.\nWorkstation Commands - There are lots of places in this document where I\u0026rsquo;ve captured a series of command lines along with output from those commands in block text. Generally speaking, after each such block you will find a Workstation Commands table that can be used to conveniently copy and paste the necessary commands directly into your workstation. The tables look something like this:\nWorkstation Commands cd ~/Projects git clone https://github.com/DigitalGrinnell/ISLE cd ISLE git checkout -b ld Apache Container Commands - Similar to Workstation Commands, a tabulated list of commands may appear with a heading of Apache Container Commands. *Commands in such tables can be copied and pasted into your command line terminal, but ONLY after you have opened a shell into the Apache container. The asterisk (*) at the end of the table heading is there to remind you of this! See the next section of this document for additional details. These tables looks something like this: Apache Container Commands* cd /var/www/html/sites/all/modules/contrib drush dl backup_migrate drush -y en backup_migrate Opening a Shell in the Apache Container This is something I find myself doing quite often during ISLE configuration, so here\u0026rsquo;s a reminder of how I generally do this\u0026hellip;\n╭─markmcfate@ma7053 ~/Projects/dg-isle ‹ruby-2.3.0› ‹ld*› ╰─$ docker exec -it isle-apache-ld bash root@9bec4edd3964:/# cd /var/www/html root@9bec4edd3964:/var/www/html# Workstation Commands cd ~/Projects/ISLE docker exec -it isle-apache-ld bash Installing per install-local-new.md This is first-and-foremost a local development copy of ISLE, but with considerable Digital.Grinnell customization, so I\u0026rsquo;m following the process outlined in the project\u0026rsquo;s ./docs/install/install-local-new.md. References to Step X that follow refer to corresponding sections of install-local-new.md. Each section or \u0026ldquo;Step\u0026rdquo; listed below is also a link back to the corresponding section of the canonical document.\nStep 1: Choose a Project Name I choose a very short and simple name, dg, as my value for yourprojectnamehere.\nStep 1.5: Edit \u0026ldquo;/etc/hosts\u0026rdquo; File In this step I found it necessary to be very diligent about editing my /etc/hosts file, even though I\u0026rsquo;ve done this a thousand times (and that\u0026rsquo;s not exaggerated). Why? Because every time I update Docker Desktop for Mac, the hideous thing adds a new 127.0.0.1 entry to the bottom of my /etc/hosts file, effectively overriding my own additions.\nThe critical line in /etc/hosts now reads:\n127.0.0.1 localhost dg.localdomain admin.dg.localdomain images.dg.localdomain portainer.dg.localdomain And there\u0026rsquo;s no comment, and no active 127.0.0.1... lines later in the file!\nStep 2: Setup Git Project Repositories Since I choose dg as my value for yourprojectnamehere, and since this step calls for creation of two private repos, I logged in to Github as \u0026ldquo;Digital-Grinnell\u0026rdquo; and created:\nhttps://github.com/Digital-Grinnell/dg-isle https://github.com/Digital-Grinnell/dg-islandora Then I cloned the empty https://github.com/Digital-Grinnell/dg-isle to ma7053 and made the prescribed additions like so:\nWorkstation Commands cd ~/Projects git clone https://github.com/Digital-Grinnell/dg-isle cd dg-isle git remote add icg-upstream https://github.com/Islandora-Collaboration-Group/ISLE.git git remote -v git fetch icg-upstream git pull icg-upstream master git push -u origin master atom . The final command in that sequence simply opened a copy of the local dg-isle project files in my Atom editor.\nStep 3: Edit the \u0026ldquo;.env\u0026rdquo; File to Point to the Local Environment Since this is a short-lived, local, development project, I have no reservations about sharing this with you. So, as instructed I modified .env to read as follows:\n#### Activated ISLE environment # To use an environment other than the default Demo, please change values below # from the default Demo to one of the following: Local, Test, Staging or Production # For more information, consult https://islandora-collaboration-group.github.io/ISLE/install/install-environments/ COMPOSE_PROJECT_NAME=dg_local BASE_DOMAIN=dg.localdomain CONTAINER_SHORT_ID=ld COMPOSE_FILE=docker-compose.local.yml Step 4: Create New Users and Passwords by Editing \u0026ldquo;local.env\u0026rdquo; File No secrets here either\u0026hellip; Since this is a local instance of ISLE, I set the following:\nItem(s) My Value All site, user, and database names local All passwords password All hashes thisisalengthyhash Also note that I kept all of the original # Replace this\u0026hellip; comments by pushing them to the line below. A copy of my customized local.env file can be found in this gist.\nStep 5: Create New Self-Signed Certs for Your Project As before, no secrets here, so there\u0026rsquo;s a copy of my customized local.sh file in this gist and my traefik.local.toml file in this gist.\nCerts were generated as prescribed. All is well.\nStep 6: Download the ISLE Images Only comment here is that you must navigate to your project folder, like cd ~/Projects/dg-isle, before you do docker-compose pull. The command will not work properly in any other directory!\nI used the command time docker-compose pull so:\nWorkstation Commands cd ~/Projects/dg-isle time docker-compose pull And my download time was: docker-compose pull 3.95s user 0.89s system 2% cpu 3:21.16 total.\nStep 7: Launch Process Same comment here as above, you must navigate to your project folder, like cd ~/Projects/dg-isle, before you do docker-compose up -d. The command will not work properly in any other directory!\nWorkstation Commands cd ~/Projects/dg-isle; time docker-compose up -d; sleep 300; docker ps; docker ps -a And my launch time was: docker-compose up -d 1.45s user 0.34s system 14% cpu 12.546 total. Elapsed time was, of course, on the order of 6 minutes.\nStep 8: Run Islandora Drupal Site Install Script Ok, fingers crossed for good luck. \u0026#x270c;\u0026#xfe0f; I left docker-compose.local.yml just as it was, right out of the box, and did almost as instructed:\nWorkstation Commands cd ~/Projects/dg-isle; time docker exec -it isle-apache-ld bash -c \u0026ldquo;cd /utility-scripts/isle_drupal_build_tools \u0026amp;\u0026amp; ./isle_islandora_installer.sh\u0026rdquo; Note the addition of the time command out front; that was put there to record how long this process takes. My time... results: docker exec -it isle-apache-ld bash -c 0.21s user 0.15s system 0% cpu 31:07.34 total.\nStep 9: Test the Site Woot! It works! \u0026#x2728;\u0026#x1f44d;\u0026#x2728; No funky errors this time.\nFor the record (remember, no secrets here) my admin username and password for the site are: local and password. Pretty sneaky, eh? And the address is https://dg.localdomain.\nStep 10: Ingest Sample Objects OK, so this is where I generally depart from the \u0026ldquo;script\u0026rdquo;, because I\u0026rsquo;m lazy, and I already have a portable Fedora repository to test from. So, my process in this step follows what I wrote more than a month ago, 046 DG-FEDORA: A Portable Object Repository.\nNext, I added the Islandora Simple Search block as instructed in Step 10: Ingest Sample Objects, and ran some search tests. \u0026#x2611;\u0026#xfe0f; It works!\nStep 11: Check-In the Newly Created Islandora Drupal Site Code Into Your Git Repository OK, I have to admit this \u0026ldquo;step\u0026rdquo; was confusing at first, but it works and seems to make sense now that I\u0026rsquo;ve completed it. I\u0026rsquo;ll spare you the output details, because there was a LOT of it, but here is the exact command sequence I used\u0026hellip;\nWorkstation Commands cd ~/Projects/dg-isle git checkout master cd data/apache/html git init git add . git commit -m \u0026ldquo;Created new dg.localdomain site\u0026rdquo; git remote add origin https://github.com/Digital-Grinnell/dg-islandora.git git push -u origin master Something Missing Here? Step 11 is essentially the end of the install-local-new.md process, but it left me with a local ISLE project directory, namely ~/Projects/dg-isle, with the following Git status and remotes\u0026hellip;\n╭─markmcfate@ma7053 ~/Projects/dg-isle ‹ruby-2.3.0› ‹master*› ╰─$ git status; git remote -v On branch master Your branch is up to date with \u0026#39;origin/master\u0026#39;. Changes not staged for commit: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to update what will be committed) (use \u0026#34;git checkout -- \u0026lt;file\u0026gt;...\u0026#34; to discard changes in working directory) modified: .env modified: config/apache/settings_php/settings.local.php modified: config/proxy/traefik.local.toml modified: local.env modified: scripts/proxy/ssl-certs/local.sh Untracked files: (use \u0026#34;git add \u0026lt;file\u0026gt;...\u0026#34; to include in what will be committed) config/proxy/ssl-certs/dg.localdomain.key config/proxy/ssl-certs/dg.localdomain.pem docker-compose.DG-FEDORA.yml no changes added to commit (use \u0026#34;git add\u0026#34; and/or \u0026#34;git commit -a\u0026#34;) icg-upstream\thttps://github.com/Islandora-Collaboration-Group/ISLE.git (fetch) icg-upstream\thttps://github.com/Islandora-Collaboration-Group/ISLE.git (push) origin\thttps://github.com/Digital-Grinnell/dg-isle (fetch) origin\thttps://github.com/Digital-Grinnell/dg-isle (push) Since this is a local ISLE with no secrets, it seems prudent to add, commit, and push at least my \u0026ldquo;off-script\u0026rdquo; additions, .env and docker-compose.DG-FEDORA.yml. To safeguard against making a bad move I\u0026rsquo;m going to approach this like so:\nWorkstation Commands cd ~/Projects/dg-isle git checkout -b local-with-dg-fedora git add .env docker-compose.DG-FEDORA.yml git commit -m \u0026ldquo;Added DG-FEDORA to dg.localdomain site\u0026rdquo; git push -u origin local-with-dg-fedora I hope that was the right thing to do, because I just did it. \u0026#x2757;\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/034-building-isle-1.3.0-ld/","tags":["ISLE","ISLE-1.3.0","local"],"title":"Building ISLE 1.3.0 (ld) for Local Development"},{"categories":null,"contents":"So, Hugo supports the use of front matter \u0026ldquo;date\u0026rdquo; variables including: date, publishDate and lastmod. I won\u0026rsquo;t explain the details of each variable because the aforementioned resource has a nice, concise explanation of them all.\nUntil recently this blog only dealt with the \u0026ldquo;date\u0026rdquo; field since I used to have Atom configured to automatically update that field for me when I save changes to a file. However, Hugo treats \u0026ldquo;date\u0026rdquo; more like the date of publication (publishDate) than the last modification (lastmod) date, so things got a little screwy if/when I edited an old post.\nMy latest change aims to fix that\u0026hellip; I\u0026rsquo;ve changed \u0026ldquo;date\u0026rdquo; to \u0026ldquo;publishDate\u0026rdquo; and also added a \u0026ldquo;lastmod\u0026rdquo; field to the front matter of new posts. Along with this I\u0026rsquo;ve updated the way the blog posts are sorted, now by \u0026ldquo;lastmod\u0026rdquo;, not \u0026ldquo;publishDate\u0026rdquo;, and I\u0026rsquo;ve updated Atom so that it automatically updates my \u0026ldquo;lastmod\u0026rdquo; date when I edit and save any file.\nFor the record, all of this involves the atom-timestamp package and a \u0026ldquo;Timestamp prefix\u0026rdquo; setting of:\n[lastmod|Updated]:[ \\t]+[\u0026#34;]? And the Hugo code change to make it work properly for this blog appears in ./layouts/_default/list.html where the new line of code reads:\n{{ range $index, $element := $paginator.Pages.ByLastmod.Reverse }} And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/033-adding-lastmod-date/","tags":["Hugo","lastmod","publishDate","atom-timestamp","paginator","ByLastmod","Reverse"],"title":"Adding a LastMod Date"},{"categories":null,"contents":"None of this is my creation, but it\u0026rsquo;s too valuable to forget! So kudos to Goran Nikolovski and his blog post.\nThe problem I ran into involved the Drupal update.php script, and an orphaned bit of configuration data. When I tried running update.php the first of 13 pending database updates kept throwing an exception telling me of a missing plugin, and that effectively killed the other 12 updates. \u0026#x1f626; Well, I really didn\u0026rsquo;t care that it was missing (and Drupal should not either) because the update was there to delete it, but since the update kept failing, we had ourselves an impossible loop.\nThe first technique in Goran\u0026rsquo;s post worked for me. Specifically, it suggested I visit the /admin/config/development/configuration/single/export page in my site, select Configuration type of Action in my case, then the errant Configuration name, and at the bottom of the screen I could now see the full path to that configuration item. Goran\u0026rsquo;s first technique then uses drush to wash that config item away. In my case the command was:\ndrush config-delete system.action.auto_nodetitle_update_action Bingo! This did the trick and enabled update.php to run without exception.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/032-deleting-config-items-in-drupal-8/","tags":["Drupal","config","orphan"],"title":"Deleting Config Items in Drupal 8"},{"categories":null,"contents":"This command snippet needs a blog post of its own! I typically use the following command stream to clean up any Docker cruft before I begin anew. Note: Uncomment the third line ONLY if you want to delete images and download new ones. If you do, be patient, it could take several minutes depending on connection speed.\nWorkstation Commands docker stop $(docker ps -q) docker rm -v $(docker ps -qa) # docker image rm $(docker image ls -q) docker system prune \u0026ndash;force And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/031-resetting-docker/","tags":["Docker","reset","prune"],"title":"Resetting Docker"},{"categories":null,"contents":" Attention! The Docksal portion of this discussion DID NOT WORK PROPERLY so I\u0026rsquo;ve hidden it from public view. Don\u0026rsquo;t use this project with Docksal (fin commands) until further notice! I\u0026rsquo;ve created a new fork of dodeeric/omeka-s-docker at DigitalGrinnell/omeka-s-docker, and it introduces a new docker-compose.yml file for spinning Omeka-S up locally, but WITHOUT Docksal (due to problems with the integration originally documented here).\nSystem requirements for local development of this project currently include:\nDocker (Community Edition) Git Local Development and Testing If your workstation is able to run the aforementioned required components then the following steps can be used to launch and develop a local instance. Assuming your workstation is Linux or a Mac, you\u0026rsquo;ll need to edit your /etc/hosts with an editor of your choice, and sudo privileges might be required. For me this was\u0026hellip;\nsudo nano /etc/hosts In the /etc/hosts file comment out any line beginning with 127.0.0.1 and add the following two lines just above it\u0026hellip;\n### For omeka-s-docker 127.0.0.1 localhost omeka.localdomain pma.localdomain gramps.localdomain The new 127.0.0.1... line will enable you to use http://omeka.localdomain to open and work with your new Omeka-S instance in any browser on your workstation.\nNow to launch Omeka-S, return to your workstation terminal and\u0026hellip;\ncd ~/Projects # or any path of your choice git clone https://github.com/DigitalGrinnell/omeka-s-docker.git cd omeka-s-docker docker-compose up -d The docker-compose up -d command in this sequence should launch the project locally. Once it is complete you should be able to open any browser and visit http://omeka.localdomain to work with Omeka-S, or http://pma.localdomain if you want PHPMyAdmin.\nAdding Solr We don\u0026rsquo;t need gramps in our configuration, but we do need Solr, so first step is to modify your /etc/hosts file entry to look like this:\n### For omeka-s-docker 127.0.0.1 localhost omeka.localdomain pma.localdomain solr.localdomain Solr can easily be added to the current stack with some simple changes/additions in the docker-compose.yml file. I gleaned my changes largely from the Using Docker Compose example at https://docs.docker.com/samples/library/solr/.\nA New Branch Next, we should create a new branch of our repo to work in, and since Docksal won\u0026rsquo;t be a part of the new work I\u0026rsquo;m going to take a bold step and remove it from the branch, like so:\ncd ~/Projects/omeka-s-docker git checkout master # This is just a precaution git checkout -b master-with-solr rm -fr .docksal atom . The new docker-compose.yml section for Solr looks like this:\n## Adding solr per `Using Docker Compose` example at https://docs.docker.com/samples/library/solr/ solr: image: solr restart: always networks: - network1 # ports: # MAM: `ports` is not required since we have traefik.port mapped to Solr\u0026#39;s 8983 below. # - \u0026#34;8983:8983\u0026#34; volumes: - solr-data:/opt/solr/server/solr/mycores entrypoint: - docker-entrypoint.sh - solr-precreate - mycore labels: - \u0026#34;traefik.backend=solr\u0026#34; - \u0026#34;traefik.port=8983\u0026#34; - \u0026#34;traefik.frontend.rule=Host:solr.localdomain\u0026#34; Removing Gramps I did a docker-compose up -d and see that this stack, complete with Solr, appears to be working nicely now. So I\u0026rsquo;m taking steps to remove all references to Gramps, which is no longer needed here. That leaves us with these comments and explanatory text gleaned from the top of the new docker-compose.yml file:\n## This is a modified copy of dodeeric\u0026#39;s original docker-compose-traefik.yml with ## localhost addresses of: ## ## - omeka.localdomain ## - pma.localdomain ## - solr.localdomain ## ## Note that you can also see the Traefik dashboard at http://omeka.localdomain:8080 ## ## These addresses need to be defined/enabled locally with an entry in /etc/hosts of: ## ## ### For omeka-s-docker ## 127.0.0.1 localhost omeka.localdomain pma.localdomain solr.localdomain ## Customizing the Image and Rebuilding Time to add the centerrow-master theme changes held in https://github.com/DigitalGrinnell/centerrow. So, I created a new .zip file from the aforementioned repo, and saved that to my local project as centerrow-master.zip, then I modified the project\u0026rsquo;s Dockerfile to pull this .zip in place of the centerrow-v1.4.0.zip copy. To take advantage of that change I needed to build a new Docker image and employ it going forward. After making changes to the Dockerfile and docker-compose.yml files, the command history was:\ncd ~/Projects/omeka-s-docker git checkout master-with-solr sudo docker image build -t mcfatem/omeka-s:aug18 . sudo docker image tag mcfatem/omeka-s:aug18 mcfatem/omeka-s:latest sudo docker login --username=mcfatem sudo docker image push mcfatem/omeka-s:aug18 sudo docker image push mcfatem/omeka-s:latest The docker-compose.yml change was from this:\nomeka: ... image: dodeeric/omeka-s:latest \u0026hellip;to this:\nomeka: ... image: mcfatem/omeka-s:latest After these changes/additions my docker-compose up -d appears to work properly making all of the following local sites available:\nhttp://omeka.localdomain/ \u0026lt;\u0026ndash; Omeka-S installer http://omeka.localdomain:8080 \u0026lt;\u0026ndash; Traefik dashboard http://pma.localdomain/ \u0026lt;\u0026ndash; PHPMyAdmin http://solr.localdomain \u0026lt;\u0026ndash; Solr admin Adding WMI Data A wmi.sql file dumped from Grinnell\u0026rsquo;s Omeka-Classic World Music Instruments collection is now available in the project root. The file was dumped by opening an SSH session on Grinnell\u0026rsquo;s omeka1 server (ssh vagrant@omeka1.grinnell.edu) and running the following commands there:\ncd /var/www/html/WorldMusicInstruments mysqldump -h localhost -u wMIAdmin -p wmi \u0026gt; wmi.sql rsync -aruvi wmi.sql markmcfate@132.161.249.239:/Users/markmcfate/Projects/omeka-s-docker/wmi.sql --progress Unfortunately, this dump was HUGE, like 1.2 gigabytes! So, I elected to try again but without the voluminous \u0026ldquo;sessions\u0026rdquo; table data, like so:\ncd /var/www/html/WorldMusicInstruments mysqldump -u wMIAdmin -p -h localhost wmi --ignore-table=database.sessions \u0026gt; ./wmi-dump.sql \\ \u0026amp;\u0026amp; mysqldump -u wMIAdmin -p -h localhost -d wmi sessions \u0026gt;\u0026gt; ./wmi-dump.sql rsync -aruvi wmi-dump.sql markmcfate@132.161.249.239:/Users/markmcfate/Projects/omeka-s-docker/wmi-dump.sql --progress That did the trick. Now the wmi-dump.sql file is only 7.8 megabytes in size.\nThere are a few ways to get our Omeka-S instance populated with data like this, perhaps the most popular is to mount the .sql file into the database container\u0026rsquo;s /docker-entrypoint-initdb.d/ directory. I could not get this to work, probably because we\u0026rsquo;ve already mounted a Docker volume named mariadb in that container. No matter, I found a slick one-liner in this gist that does the trick. My version of the command was:\n╭─mark@Marks-Mac-Mini ~/Projects/omeka-s-docker ‹master-with-solr*› ╰─$ docker exec -i omeka-s-docker_mariadb_1 /bin/bash -c \u0026#34;export TERM=xterm \u0026amp;\u0026amp; mysql -uomeka -pomeka omeka\u0026#34; \u0026lt; wmi.sql Image Updates Since wrapping up the previous posting I\u0026rsquo;ve made some additional changes to this branch. Specifically, my Omeka-S image has been upgraded to version 2.0.1, and I corrected the path that the centerrow-master theme gets unpacked into (it was centerrow-master but needs to be just centerrow). I also added some necessary packages to the omeka service Dockerfile in order to support Solr.\nHaving made these changes I repeated the Docker image build process from above like so:\ncd ~/Projects/omeka-s-docker git checkout master-with-solr sudo docker image build -t mcfatem/omeka-s:august . sudo docker image tag mcfatem/omeka-s:august mcfatem/omeka-s:latest sudo docker login --username=mcfatem sudo docker image push mcfatem/omeka-s:august sudo docker image push mcfatem/omeka-s:latest The omeka service portion of docker-compose.yml is still pulling and using mcfatem/omeka-s:latest from Docker Hub.\nRetrieving WMI Content or \u0026ldquo;Files\u0026rdquo; The following is a reminder to myself\u0026hellip; to retrieve the /var/www/html/files data from the World Musical Instruments (WMI) collection, try this on your host:\n╭─markmcfate@ma8660 ~/Projects/omeka-s-docker ‹ruby-2.3.0› ‹master-with-solr*› ╰─$ rsync -aruvi vagrant@omeka1.grinnell.edu:/var/www/html/MusicalInstruments/files/. files/ --verbose ╭─markmcfate@ma8660 ~/Projects/omeka-s-docker ‹ruby-2.3.0› ‹master-with-solr*› ╰─$ docker cp ./files/. omeka-s-docker_omeka_1:/var/www/html/files/ And that\u0026rsquo;s a wrap\u0026hellip;for now.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/030-dockerized-omeka-s-starting-over/","tags":["Omeka-S","Docker","docker-compose"],"title":"Dockerized Omeka-S: Starting Over"},{"categories":null,"contents":"A set of 21 PDF objects were ingested into Digital.Grinnell\u0026rsquo;s Faculty Scholarship collection using IMI on 22-July-2019; unfortunately none of these PDFs contained OCR (optical character recognition) or \u0026ldquo;text recognition\u0026rdquo; data, so none of them generated a valid FULL_TEXT datastream. FULL_TEXT datastreams are required to make PDF, and similar text content, searchable and discoverable in Digital.Grinnell.\nIn order to confirm that the lack of OCR was in fact the problem, I ran a little test on https://digital.grinnell.edu/islandora/object/grinnell:26702, one of the 21 objects.\nIn my test I\u0026hellip;\nsigned in to Digital.Grinnell as an admin, opened the object (see address above) in my browser, clicked Manage to see all the object details, clicked Datastreams to see the list of all the object\u0026rsquo;s datastreams, clicked the download link corresponding to the OBJ datastream - this allowed me to download a copy of the PDF file to my workstation. Once the PDF was downloaded I opened it on my workstation in Adobe Acrobat Pro, clicked Tools and Text Recognition, then I chose* In This File. After a few minutes I had a new PDF with OCR\u0026rsquo;d and searchable text. I saved that new PDF on my workstation, went back into the Manage tab in my browser, clicked replace in the OBJ datastream line, then uploaded the new PDF file to Digital.Grinnell. Once the upload was complete the system automatically generated new derivatives for the object which now has a valid FULL_TEXT datastream, so this should make the content searchable and discoverable.\n*Note that if I had multiple PDFs to process I believe I could have selected the In Multiple Files option to save some time and OCR several PDFs in one operation.\nThe lesson-to-be-learned here is to\u0026hellip; always run \u0026quot;Text Recognition\u0026quot; on a PDF BEFORE it is ingested into Digital.Grinnell. But, if you forget, this procedure in the hands of any Digital.Grinnell admin, can save the day! \u0026#x1f604;\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/029-pdf-ingest-in-digital-grinnell/","tags":["PDF","OCR","Digital.Grinnell","FULL_TEXT"],"title":"PDF Ingest in Digital.Grinnell"},{"categories":null,"contents":"Claiming another small victory today! Why? Well, the Digital.Grinnell instance of IMI (Islandora Multi-Importer) module is customized so that choosing \u0026ldquo;*local\u0026rdquo; as an object ingest source invokes a hook function I created in our DG7 module. That hook enables IMI to \u0026ldquo;find\u0026rdquo; named files/content (things like PDFs, images, etc.) in the Grinnell College //STORAGE server. //STORAGE can be mounted as a CIFS (Common Internet File System) and used to drive ingest if the right package/drivers are made available to Islandora. That can be a little tricky in ISLE, but it\u0026rsquo;s manageable.\nMy process for providing CIFS access to //STORAGE and for mounting the objects went like this:\nSome time ago my colleagues and I created a directory on the server named //STORAGE/MEDIADB/DGIngest. It is password protected, as is all of //STORAGE, and URL-accessible. The aforementioned IMI \u0026lsquo;hook\u0026rsquo; in DG7 is programmed to find content in a \u0026ldquo;local\u0026rdquo; directory named /mnt/storage. The code is such that it can find a named file within ANY path subordinate to this directory. So\u0026hellip; Today I opened a terminal to my ISLE production host, DGDocker1.Grinnell.edu, and subsequently opened a terminal into its Apache container like so: docker exec -it isle-apache-dg bash. Inside the Aapache container terminal I executed the following, as \u0026ldquo;root\u0026rdquo;, to install a CIFS package: apt-get update; apt install -y cifs-utils. This was a success and need not be repeated unless I ever have to recreate the Apache container. Still in the Apache container terminal I mounted the target share using: mount -t cifs -o username=mcfatem //storage.grinnell.edu/MEDIADB/DGIngest /mnt/storage. I was subsequently prompted for my password and provided it. Eureka! It worked. My subsequent IMI ingests were able to successfully find content stored in the aforementioned directory and its subordinates. And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/028-mounting-storage-for-imi-ingest-in-digital-grinnell/","tags":["Islandora Multi-Importer","IMI","Digital.Grinnell","CIFS Mount","ISLE","DG7"],"title":"Mounting //STORAGE for IMI Ingest in Digital.Grinnell"},{"categories":null,"contents":"My memory isn\u0026rsquo;t what it used to be, but I have this blog. \u0026#x1f604; So when I realized that my primary work machine, an iMac, had not been configured with iTerm2 as its default terminal for ssh, I went looking for the solution\u0026hellip;again. Found it here!\nThe trick is to open iTerm2 and follow these two simple steps\u0026hellip;\nQ: How do I set iTerm2 as the handler for ssh:// links?\nA: Two steps:\nCreate a new profile called \u0026ldquo;ssh\u0026rdquo;. In the General tab, select the Command: button and enter $$ as the command. In Preferences-\u0026gt;Profiles-\u0026gt;General, select \u0026ldquo;ssh\u0026rdquo; for \u0026ldquo;Select URL Schemes\u0026hellip;.\u0026rdquo; And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/027-replace-osx-terminal-with-iterm2-for-ssh/","tags":["iTerm2","ssh"],"title":"Replace OSX Terminal with iTerm2 for SSH"},{"categories":null,"contents":"I recently constructed a new, local/development instance of ISLE (see my previous post) largely in order to debug a mystery in Digital.Grinnell\u0026rsquo;s display of oral histories. My Trello card for the issue reads:\nOur newest AOH entries, and some older objects, will not display a transcript after upgrade to the latest version of the OH module. OHScribe is needed to aid in re-processing transcripts for these objects, and some XDebug work will also be required.\nEngaging XDebug and PHPStorm allowed me to peek inside the relatively complex oral histories (OH) module where I found that some of our OH objects were missing key Solr field elements, like or_transcripts and or_speaker. Ultimately I tracked the problem to our Solr config which is based on work found in https://github.com/discoverygarden/islandora_transforms and https://github.com/discoverygarden/basic-solr-config; the former repository here lists the later as a dependency.\nIn case you\u0026rsquo;re not already familiar with these projects, they both represent \u0026ldquo;starting points\u0026rdquo;, largely config and .xslt code that\u0026rsquo;s intended to be customized. In my experience this adds up to trouble since maintenance and upgrade of any multi-faceted, customized code like this can be tedious and therefore, dangerous.\nThe crux of my problem is that these repos use very long Java/Tomcat paths, and the most significant of these is /usr/local/fedora/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/. However, in a standard ISLE instace the equivalent path must be /usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex/. See the difference? The ISLE path does NOT include a ../fedora/.. subdirectory like the Discovery Garden paths do.\nUnfortunately, when I updated my FGSearch and Solr configs earlier this year I missed some, but not all, of these ../fedora/.. directory references during a \u0026ldquo;manual\u0026rdquo; compare and merge process, and that made things like oral histories go afoul, and without much warning.\nMy solution to this problem going forward is the creation of this new public repository. The repo\u0026rsquo;s description in GitHub describes is:\nThe contents of isle-fedora-ld\u0026rsquo;s /usr/local/tomcat/webapps/fedoragsearch/WEB-INF/classes/fgsconfigFinal/index/FgsIndex directory\nThe repo has no README.md file, yet, because the description is so succinct and accurate; but I\u0026rsquo;ll make it a point to add more detail soon.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/026-oral-history-solr-fix/","tags":["Solr config","FedoraGSearch config","XSLT","Oral Histories","ISLE"],"title":"Missing Oral History Transcripts in DG - Fixed"},{"categories":null,"contents":"While working on another post I finally made the decision to improve the appearance of this blog a bit, and unfortunately I\u0026rsquo;d forgotten exactly how to do that. Even more unfortunate, I never blogged about the process so I had to \u0026ldquo;discover\u0026rdquo; the details all over again. \u0026#x1f622; This post is intended to remedy that.\nSASS The theme used here, m10c, employs .scss, or SASS (Syntactically Awesome Style Sheets) files. Subsequently, a SASS compiler is required to process them and produce suitable .css to control the display you see now.\nSince I use Atom for all of my maintenance and content here, it only made sense to create a workflow that would make compilation of .scss changes automatic. Doing so on each of my Mac\u0026rsquo;s involved these simple steps:\nInstall SASS - I used homebrew from the command line based on guidance found here, like so: brew install sass/sass/sass Install the atom-sass package in Atom per the guidance provided with the package. That\u0026rsquo;s it. Now, when I edit my ~/Projects/blogs-McFateM/assets/css/_extra.scss file using Atom and save the changes, the corresponding .css is automatically created for me.\nNow, I need to research the techniques found in this post about Hugo with SASS and this subsequent post about including SASS variables from config.toml and figure out how to make them work for me. Unfortunately, I haven\u0026rsquo;t figured out how to make my atom-sass recognize config variables from my config.toml file yet. \u0026#x1f60f; I probably need to study this Hugo pipes document too.\nEmoji Also\u0026hellip; I\u0026rsquo;ve forgotten more than once to document the super-simple Emoji capabilities of Hugo and the handy cheatsheet available at https://www.webfx.com/tools/emoji-cheat-sheet/. Not forgotten anymore. Now, just don\u0026rsquo;t forget to add enableEmoji = true to your config.toml, and emoji: true to the frontmatter of your posts!\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/024-compiling-scss-for-this-blog/","tags":["SASS",".scss","emoji"],"title":"Compiling SCSS (SASS) for This Blog"},{"categories":null,"contents":"In an earlier post I chronicle the exhaustive steps taken to create a \u0026ldquo;debuggable\u0026rdquo; local/development instance of Digital.Grinnell that behaves exactly like the real thing, except with a much smaller, portable FEDORA repository under it. I\u0026rsquo;m claiming success on that front, but there is one glaring kludge in the process that I have yet to work out.\nThe Kludge So, my debugging of an ISLE stack involves the coordinated configuration and engagement of XDebug inside the Apache container, and PHPStorm, along with persistence of the stack\u0026rsquo;s PHP codebase\u0026hellip; and therein lies the rub.\nThe purpose of my local instance was to verify that all of the latest stack improvements work properly in the Digital.Grinnell environment, so naturally I built a stack using all of the latest code; not at all difficult in ISLE. However, since the codebase is assembled, it can\u0026rsquo;t easily be mapped or mounted for persistence, at least not initially since mounting it from the host suggests that it must exist BEFORE the stack is assembled. Just to be clear, a persistent mount of the code is critical for PHPStorm, since the code on the host is used to edit, set breakpoints, and a slew of other typical \u0026ldquo;debug\u0026rdquo; activities.\nFor reference, the code I\u0026rsquo;m interested in debugging lies in the Apache container on the /var/www/html/sites/all/modules path. The \u0026ldquo;override\u0026rdquo; technique I\u0026rsquo;m using here is essentially what\u0026rsquo;s suggested/documented in https://docs.docker.com/compose/extends/.\nMy approach to this so far is to:\nBuild and launch the stack without mapping anything to /var/www/html/.... This allows ISLE to assemble the latest copy of each component. After verifying that the stack works, use docker cp on the host to copy the new code from the Apache container back to the host, like so: - `cd ~/Projects/ISLE; docker cp isle-apache-ld:/var/www/html ./persistent/html` Now, alter the docker-compose.override.yml file to include all of the necessary debug configuration bits (see this post for additional details). Build and launch ISLE again using docker-compose up -d, just like before. The difference is that with ./persistent/html now mapped into the container, PHPStorm debugging will work as it should. In my opinion, Step 2 + Step 3 = kludge. The question is\u0026hellip;\nHow can I get around this and still build a pristine ISLE for both evaluation AND debugging?\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/025-kludge-debugging-isle/","tags":["ISLE","XDebug","PHPStorm"],"title":"Debugging PHP in ISLE: a Kludge"},{"categories":null,"contents":"Debugging ISLE on a Mac This guidance applies to debugging PHP code in a local ISLE-ld, that\u0026rsquo;s http://isle.localdomain, instance using PHPStorm.\nModify ISLE\u0026rsquo;s docker-compose.override.yml Before engaging PHPStorm we need to make one change to our ISLE-ld configuration by running a docker cp command, making a change to our docker-compose.override.yml file, and restarting the stack. Here are the commands and procedure.\nWorkstation Commands cd ~/Projects/ISLE mkdir -p persistent/html docker cp isle-apache-ld:/var/www/html/. ./persistent/html The above commands will make a new ./persistent/html directory on the host, if one does not already exist, and the docker cp command will copy the current contents of the Apache container\u0026rsquo;s /var/www/html directory to the host. Next we need to modify docker-compose.override.yml to map the ./persistent/html directory into the container.\nOn the host, open docker-compose.override.yml in your favorite editor and remove comments from the three lines that read:\napache: volumes: ./persistent/html:/var/www/html # necessary for PHPStorm debugging! Important! Proper indentation in docker-compose.override.yml, like all .yml files, is CRITICAL! There should be 2 spaces (one tab) at the start of the apache: line, 4 spaces before volumes:, and 6 spaces before .persistent.... Having saved the modified docker-compose.override.yml file, restart things by doing the following.\nWorkstation Commands cd ~/Projects/ISLE docker-compose down docker-compose up -d In a minute or two your http://isle.localdomain should be back up and running, and ready for debugging in PHPStorm.\nPHPStorm Configuration In the PHPStorm menu go to: Preferences \u0026gt; Languages \u0026amp; Frameworks \u0026gt; PHP \u0026gt; Debug \u0026gt; DBGp Proxy and set the following settings:\nIDE key: `PHPSTORM` Host: `docker.for.mac.localhost` Port: `9009` Next, we need to configure a server. This is how PHPStorm will map the file paths in your local system to the ones in your container.\nThe following is a ONE TIME procedure. If you\u0026rsquo;ve already done this then all you need to do is select the PHPStorm project, one you created and named earlier, from the PHPStorm splash screen.\nFrom the PHPStorm splash screen choose Create New Project from Existing Files. Click Next. CRITICAL\u0026hellip;pay attention to this! Select Source files are in a local directory, no Web server is yet configured.. Click Next. A directory map of your host should appear. Navigate in this map to your ISLE project, the folder where your docker-compose.yml file exists. There should also be a ./persistent/html folder there from steps we took in the previous section. Choose the html folder, click Project Root, then click Finish. You now have a new \u0026ldquo;local\u0026rdquo; project named html. Relax a little while your PHPStorm project is indexed for the first time. Launching a Web Debug Session Once you\u0026rsquo;re all setup and have your PHPStorm project open\u0026hellip;\nSet breakpoints in your code, toggle Start listening for PHP connections on (the green/red telephone icon). When turned on the red parts of the icon turn green. Open the Run menu in PHPStorm and look near the bottom of the window for the Break at first line in PHP scripts option. Ensure that it is toggled ON so that your debug session will encouter at least one breakpoint. Now open your browser and navigate to your site (mine is http://isle.localdomain). In PHPStorm you may see a pop-up window titled Incoming Connection from XDebug. Click Accept. If everything is working properly your PHPStorm Debug window pane will open and if you click the Debugger tab you\u0026rsquo;ll see your index.php code and a cursor with the first line of code highlighted, usually: define('DRUPAL_ROOT', getcwd());. Debugging CLI (aka Drush) Commands Visit the PHPStorm menu Preferences \u0026gt; Languages \u0026amp; Frameworks \u0026gt; PHP \u0026gt; Servers and add a server named Docker with the following parameters:\nHost: docker.for.mac.localhost Port: 80 Debugger: Xdebug Use path mappings: Checked File/Directory: /Users/mark/ISLE-ld/html Absolute path on server: /var/www/html Open a terminal into the isle-apache-ld container and run export PHP_IDE_CONFIG=serverName=Docker to complete the configuration.\nSet breakpoints in your Drush code and run any drush command in the same isle-apache-ld terminal with debug listening toggled on in PHPStorm.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/023-debugging-isle-ld-in-phpstorm/","tags":["docker-compose","override","ISLE","XDebug","PHPStorm"],"title":"Debugging ISLE-ld (Local Development) in PHPStorm"},{"categories":null,"contents":"I\u0026rsquo;m working remotely from a desk on the 3rd floor of the MSOE (Milwaukee School of Engineering) this morning and just ran into a problem with this blog\u0026hellip; some of my single and double quotes are rendered as \u0026ldquo;curly quotes\u0026rdquo; so I can\u0026rsquo;t effectively copy and paste them into a command line. While searching for a fix I found an awesome Hugo resource. It lists, among other things, ALL of Hugo\u0026rsquo;s standard configuration variables!\nThe settings I\u0026rsquo;m most interested in right now are part of BlackFriday, Hugo\u0026rsquo;s markdown rendering engine. I\u0026rsquo;m setting them in config.toml with a section like this:\n[blackfriday] smartypants = false hrefTargetBlank = true And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/022-awesome-hugo-resource-config-variables/","tags":["Hugo","config","BlackFriday"],"title":"Awesome Hugo Resource - Config Variables Summary"},{"categories":null,"contents":"I just returned from 12-days of vacation and travel that took me unexpectedly to Paris and northern Finland, and as-expected to Norway (nearly all of it), and Iceland (expected, but not as promised). Since this is a professional blog I won\u0026rsquo;t elaborate here, but look for my \u0026ldquo;personal\u0026rdquo; blog at https://blog.summittdweller.com/posts/, a perpetual work-in-progress. I will briefly comment on two things:\nIt IS possible to work remotely from Europe, and even from a cruise at sea, but I found it difficult to be really productive. We cruised aboard the MS Finnmarken, a Hutigruten working-boat cruise (they also ferry mail, small cargo, and vehicles between coastal cities, so we made port perhaps 20 times) for 5 days from Kirkenes to Bergen along the coast of Norway. The Wi-Fi on the ship was great for most things, but not suitable for sustained SSH connections like the one I require to effectively work in remote locations. SSH was supported, but only for periods of about 5 minutes at a time\u0026hellip;not long enough to connect and get a lot done.\nI will NEVER use the services of Iceland Air again. Consult my personal blog to read my litany of issues.\nApart from all of the issues with Iceland Air, I had a great time and would return again to Norway in a heartbeat. In fact, if I had the means to relocate myself and my family, I think I\u0026rsquo;d jump at the chance to do so.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/020-lesson-learned-vacationing-in-europe/","tags":["vacation","Europe","travel"],"title":"Lessons Learned: Vacationing In Europe"},{"categories":null,"contents":" Update: 24-July-2019 The Docksal process outlined here is NOT working reliably. See this new post for updated info. My fork of the dodeeric/omeka-s-docker project can be found at McFateM/omeka-s-docker, and it introduces a new docker-compose.yml file for spinning Omeka-S up on any Dockerized server, and a Docksal .docksal directory to enable local development using fin up.\nNote What follows is reflected in the README.md file at https://github.com/McFateM/omeka-s-docker. System requirements for local development of this project currently include:\nDocker (Community Edition) Git Docksal The workflows mentioned here were created on a Mac workstation and successfully pushed to a staging environment running Ubuntu 16.04.\nLocal Development and Testing If your workstation is able to run the aforementioned required components then the following steps can be used to launch and develop a local instance. In a terminal on your workstation with Docker running\u0026hellip;\ncd ~/Projects # or any path of your choice git clone https://github.com/McFateM/omeka-s-docker.git cd omeka-s-docker fin up The fin up command in this sequence should launch the project locally. It should report\u0026hellip;\nProject URL: http://omeka-s-docker.docksal:8080 However, that statement is not entirely accurate in the case of Omeka-S. The correct address will NOT require the :8080 suffix. So, the target address should be:\nhttp://omeka-s-docker.docksal You should be able to start (fin up) and stop (fin down) this local project as often as needed. Any data you add to Omeka in this mode should persist as long as you don\u0026rsquo;t remove the Omeka container or reset Docker entirely.\nDeploy to Staging The requirements for an Omeka-S staging environment are essentially the same as the local workstation, except that Docksal is NOT required.\nIn addition, the staging server needs to also have been configured using ./init and the workflow outlined in my docker-bootstrap Workflow. The aforementioned ./init script was successfully run against dgdocker2.grinnell.edu on June 7, 2019.\nIf all of those requirements have been met the site can be deployed to staging (currently this involves the dgdocker2.grinnell.edu server). In a terminal open to DGDocker2 with Docker running\u0026hellip;\ncd ~/Projects # or any path of your choice git clone https://github.com/McFateM/omeka-s-docker.git cd omeka-s-docker docker-compose up -d You should be able to start (docker-compose up -d) and stop (docker-compose stop) this project as often as needed. Any data you add to Omeka in this mode should persist as long as you don\u0026rsquo;t remove the Omeka container or reset Docker entirely.\nOmeka-S was successfully launched as https://omeka-s.grinnell.edu on June 10, 2019. Other associated apps also running on DGDocker2 are tabulated below.\nDescription URL Notes Portainer https://dgdocker2.grinnell.edu Deployed as part of docker-boostrap and the ./init script. This instance is password protected. Simple WhoAmI test application https://dgdocker2.grinnell.edu/whoami For testing proxy settings and connectivity. PHPMyAdmin https://dgdocker2.grinnell.edu/pma/ Note that the trailing slash is REQUIRED! And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/019-dockerized-omeka-s/","tags":null,"title":"A Dockerized Omeka-S for Development and Staging"},{"categories":null,"contents":"Ok, this is info I should have documented here a long, long time ago. For many months now this tidbit of wisdom has lived on a post-it in my office. Not the best strategy for someone who works from home and travels a lot.\nApparently OSX ships with built-in Apache and/or NGINX servers, presumably to facilitate creation of web content that\u0026rsquo;s local to the machine. Well, in my Dockerized workflows those port assignments typically get in the way. When they do, like when I do a fin up to launch a local development project using Docksal, I see error messages like the following in my terminal:\ndocker: Error response from daemon: driver failed programming external connectivity on endpoint docksal-vhost-proxy (4b78f8d326c8461a2c6896f0def4491c9d115feeee3680da1f3a5285a707aa08): Error starting userland proxy: Bind for 192.168.64.100:8080 failed: port is already allocated. Errors like this simply indicate that something, a process or service (container), is already listening on the indicated port. Frequently that \u0026lsquo;something\u0026rsquo; is the aforementioned built-in \u0026lsquo;Apache\u0026rsquo; or \u0026lsquo;NGINX\u0026rsquo; web server on my Mac.\nDisabling OSX Built-In Web Servers There is a simple fix\u0026hellip; in my Mac terminal I simply need to enter:\nsudo apachectl stop sudo nginx -s stop Changing Docksal Proxy Port Assignments Docksal itself can also get in the way because by default its proxy service listens locally on ports 80 and 443. In cases like my omeka-s-docker project, I needed my Omeka service listening on port 80.\nFortunately, there is another simple fix, and it\u0026rsquo;s nicely documented in this post. In my Omeka case I simply added the following lines to the .docksal/docksal.env file in the project directory:\nDOCKSAL_VHOST_PROXY_PORT_HTTP=8080 DOCKSAL_VHOST_PROXY_PORT_HTTPS=8443 These variable settings instruct the Docksal proxy to listen on ports 8080 and 8443, making ports 80 and 443 available for my Omeka service.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/018-free-ports-80-and-8080-in-osx/","tags":null,"title":"Free Ports 80 and 8080 in OSX"},{"categories":null,"contents":"The last couple days I attended Drupal Flyover Camp 2019 at UMKC in Kansas City, MO. I picked up on a few tricks and tools that I thought I\u0026rsquo;d mention here, just so I don\u0026rsquo;t forget some of the details.\nFriday - Day 1 My favorite presentation of the day was Visual Regression Testing with BackstopJS by David Needham. There\u0026rsquo;s some cool tech here that I think could be very useful with things like Digital Grinnell, Rootstalk, and even this blog.\nDavid Needham\u0026rsquo;s presentation also included some live command line work and I was also impressed with his terminal and environment. David used iTerm2 as a replacement for the standard Mac \u0026ldquo;Terminal\u0026rdquo; app, and combined that with Z shell or \u0026lsquo;zsh\u0026rsquo; instead of \u0026lsquo;bash\u0026rsquo;. I have since started using both and am finding lots of nice features that I believe will help me in my work. I\u0026rsquo;ve adopted the bira theme for \u0026lsquo;zsh\u0026rsquo; and am loving it. David also shared a gist with some nice \u0026lsquo;bash\u0026rsquo; profile improvements.\nFrom David\u0026rsquo;s presentation I was also encouraged to learn how to launch Atom from the command line. That simple change is documented here.\nI also attended the session titled Bringing Sanity to Your Git Workflow and picked up on a couple of gems from presenter Josh Fabean.\nThe first of Josh\u0026rsquo;s gems was git add -p or --patch, an option that lets you interactively pick and choose only those modifications that you want added, or indexed, for a commit. The option gives the user ability to review differences in individual files before adding modified contents to the index.\nThe second gem is this visualizing-git online tool. Check it out.\nSaturday - Day 2 Jeff Geerling\u0026rsquo;s presentation of Everything I Know About Kubernetes I Learned from a Cluster of Raspberry Pis was both informative and entertaining. Great stuff. I was especially pleased to see that Jeff still relies on Ansible as his \u0026ldquo;golden hammer\u0026rdquo;. I think I will revisit his book and get back in the habit of using Ansible. I also recommend keeping an eye on Jeff\u0026rsquo;s blog and projects list.\nTess Flynn, aka \u0026lsquo;socketwench\u0026rsquo; conducted another high-energy presentation, this time it was Dr. Upal is In: Healthcheck Your Site!. From this presentation I\u0026rsquo;ll follow-up with a look at the details of the Site Audit Module, the Healthcheck Module, and Hacked!.\nLast, but not least, was the Regression Resolved: Compare Months of Commits in Seconds with Git Bisect presentation by David Needham. Simply awesome.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/017-drupal-flyover-camp-2019/","tags":null,"title":"Drupal Flyover Camp 2019"},{"categories":null,"contents":"True to form, just after posting my lengthy description of VAF cert problems I figured out what was wrong and how to fix it. Naturally, just after.\nSo, the root of my VAF woes stemmed from the fact that Let\u0026rsquo;s Encrypt, upon my request, had previously issued an untrusted cert for https://vaf.grinnell.edu (because I used the staging environment during development of this blog), and I was unable to find it or override it with a trusted cert. I was under the impression that in my workflow the cert was being stored inside one of my Docker containers\u0026hellip; and it was. But I couldn\u0026rsquo;t fathom why the untrusted cert seemed to \u0026ldquo;persist\u0026rdquo;, even though I had deleted and regenerated those containers many times. Hmmm\u0026hellip;\nAll of this was compounded by the fact that in my configuration the Traefik container is built from \u0026ldquo;scratch\u0026rdquo; so that it is very \u0026ldquo;lean\u0026rdquo;. That essentially means there\u0026rsquo;s no shell in the container, so I can\u0026rsquo;t just do docker exec -it traefik_proxy sh to open a terminal and look inside.\nFinally, this morning, I took a hard look at the files/docker-compose.yml in my docker-bootstrap project and it dawned on me\u0026hellip; that workflow explicitly saves certs into a protected /root/acme.json file as part of the traefik_proxy service configuration, like so:\nproxy: container_name: traefik_proxy image: traefik command: \u0026gt;- --docker --logLevel=INFO \\ ... --acme.storage=\u0026#34;/root/acme.json\u0026#34; \\ ... That\u0026rsquo;s common enough. But why would that data persist even when I rebuild traefik_proxy?\nWhat I didn\u0026rsquo;t realize, until this morning, was that elsewhere in files/docker-compose.yml there\u0026rsquo;s this:\nproxy: ... volumes: - /var/run/docker.sock:/var/run/docker.sock - /dev/null:/traefik.toml - /root/acme.json:/root/acme.json That last line is the key! It\u0026rsquo;s telling the configuration that /root/acme.json should persist on the SERVER and map to /root/acme.json inside the traefik_proxy container. Duh, that\u0026rsquo;s where the certs live\u0026hellip;and persist.\nThe steps I took to fix this, from a terminal on static.grinnell.edu, were:\nroot@static:~# cp -f acme.json acme.json.bak root@static:~# rm -f acme.json root@static:~# touch acme.json root@static:~# chmod 500 acme.json root@static:~# docker stop $(docker ps -q); docker rm -v $(docker ps -qa); docker system prune The first four lines above made a backup of acme.json, just in case, then removed and replaced it with a pristine, empty file with proper permissions. The last line stopped all of the Docker containers running on the server, removed them and their associated volumes, then pruned away all remnants (images, networks, etc.) of those containers.\nAfter this I started rebuilding the server and services using the process documented in docker-bootstrap Workflow. Problem solved! Woot! Along the way I watched the process create fresh, new, trusted certs in the server\u0026rsquo;s new /root/acme.json file.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/016-fixing-vaf-cert-problems/","tags":null,"title":"Fixing the VAF Cert Problem"},{"categories":null,"contents":"The workflow I use to create, publish, and update this blog is discussed in three of my earlier posts, namely docker-bootstrap Workflow, Building This Blog, and Developing This Blog. This workflow works nicely in the case of this blog, but my daughter and I created another site, Visualizing Abolition and Freedom, frequently referred to as simply VAF, where the same workflow doesn\u0026rsquo;t quite work. The problem, I believe is with the manner in which I attempted to obtain TLS certs from Let\u0026rsquo;s Encrypt.\nIn a nutshell, information security policy here at Grinnell College dictates that a site/server be scanned for vulnerabilities and deemed \u0026ldquo;safe\u0026rdquo; before we open the firewall to allow traffic from outside the campus network. Prudent practice indeed. However, Let\u0026rsquo;s Encrypt is one such agent and their HTTP-01 Challenge that I use needs access to a site in order to validate any certificate requests I make. So we sort of have a chicken/egg, Catch-22 scenario\u0026hellip; I can\u0026rsquo;t get a trusted cert until the site/server is secure and open, and I can\u0026rsquo;t secure or open the site until I have a trusted cert.\nThe simple solution is to initially use Let\u0026rsquo;s Encrypt\u0026rsquo;s \u0026ldquo;ACME staging environment\u0026rdquo; to obtain a temporary, un-trusted cert. Once that cert is in place we run vulnerability scans and make necessary changes until the only remaining vulnerability is the un-trusted cert itself. When we arrive at that point the firewall is configured to accept traffic from off-campus, and we apply for a new, valid, and trusted cert from Let\u0026rsquo;s Encrypt\u0026rsquo;s production environment.\nSimple. But in the case of VAF it doesn\u0026rsquo;t work, yet, with my workflow. If you examine the diagram I created for the docker-bootstrap Workflow post on May 15, 2019, you\u0026rsquo;ll see that deploying a site into production is really 2-step process:\nFirst the server is prepared by \u0026ldquo;initializing\u0026rdquo; it via docker-bootstrap\u0026rsquo;s ./init command. This command creates services for Traefik, Watchtower, and Portainer. This step is performed only once, no matter the number of sites added to the server in each Step 2. I believe all of the ACME cert specifications are specified in this step as part of Traefik\u0026rsquo;s configuration; with the URL of each individual site added later in Step 2, as the sites themselves are added. Step 1 knows nothing of the individual sites, like VAF and this blog, that may follow.\nTypical ACME specifications included in Step 1 are:\n--acme \\ --acme.caserver=\u0026#34;https://acme-v02.api.letsencrypt.org/directory\u0026#34; \\ --acme.acmelogging \\ --acme.dnschallenge=false \\ --acme.entrypoint=\u0026#34;https\u0026#34; \\ --acme.httpchallenge \\ --acme.httpChallenge.entryPoint=\u0026#34;http\u0026#34; \\ --acme.onhostrule=true \\ --acme.storage=\u0026#34;/root/acme.json\u0026#34; \\ --acme.email=\u0026#34;digital@grinnell.edu\u0026#34; \\ The second step is repeated once for each site to be launched on the server, and it specifies a docker container run... command like this VAF example:\ndocker container run -d --name vaf \\ --label traefik.backend=vaf \\ --label traefik.docker.network=traefik_webgateway \\ --label \u0026#34;traefik.frontend.rule=Host:vaf.grinnell.edu\u0026#34; \\ --label traefik.port=80 \\ --label com.centurylinklabs.watchtower.enable=true \\ --network traefik_webgateway \\ --restart always \\ --label acme.caserver=\u0026#34;https://acme-02.api.letsencrypt.org/directory\u0026#34; \\ mcfatem/vaf Part of the problem here, I believe, is that I\u0026rsquo;m using a Traefik container built from \u0026ldquo;scratch\u0026rdquo;, so there\u0026rsquo;s no shell in the image, which means I can\u0026rsquo;t open a terminal inside the container to see what\u0026rsquo;s happening.\nExactly when is each cert generated? That is the question I\u0026rsquo;m struggling with. It would seem impossible for certs to be generated in Step 1 when we haven\u0026rsquo;t even identified what an individual site\u0026rsquo;s URL will be yet. Still, all of the ACME parameters that control cert creation, including the specification of acme.caserver are specified in Step 1 as part of the Traefik configuration.\nAre the ACME parameters in Step 1 buffered for later use in Step 2? If that is what\u0026rsquo;s happening, how do I switch a site from using the \u0026ldquo;staging\u0026rdquo; CA environment to using the \u0026ldquo;production\u0026rdquo; CA?\nCan I provide an acme.caserver label as part of Step 2 instead of Step 1? Maybe the answer is specifying the \u0026ldquo;production\u0026rdquo; environment in an acme.caserver label during Step 2? Something like this:\ndocker container run -d --name ${NAME} \\ --label acme.caserver=\u0026#34;https://acme-02.api.letsencrypt.org/directory\u0026#34; ... I\u0026rsquo;m going to put this last notion to the test now. Wish me luck, and I\u0026rsquo;ll report back here in a few minutes.\nNope. Still don\u0026rsquo;t have a trusted cert for VAF. Next step I believe is to wipe the server clean, pull an :alpine image of Traefik (so I can shell in and see more detail), then try to rebuild it all piece-by-piece.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/015-working-with-lets-encrypt/","tags":null,"title":"Working with Let's Encrypt to Generate Certs"},{"categories":null,"contents":"Recently I made several significant updates to this blog, including changes to my Atom configuration, and the introduction of a new theme, plus a \u0026ldquo;theme component\u0026rdquo; to help manage search. Both of these updates are perhaps best described in this portion of the README.md file in my blog\u0026rsquo;s Github repo so I won\u0026rsquo;t bother with the details here.\nAnd that’s a wrap. Until next time…\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/014-updates-to-atom-and-themes/","tags":null,"title":"Updates to Atom and Themes"},{"categories":null,"contents":"I recently made changes to this blog that made the old theme \u0026ldquo;un-responsive\u0026rdquo;, it no longer reformatted nicely for small, mobile devices. So I\u0026rsquo;ve switched the theme to use hugo-theme-m10c, and I think it\u0026rsquo;s a welcome improvement.\nIn addition, I found this repo which provides an awesome wrapper in the form of a very elegant Hugo theme component around some work that I mentioned in an earlier post, namely this awesome gist. Consequently, the mark.js keyword highlighting that was \u0026ldquo;broken\u0026rdquo; in my earlier search implementation is now working!\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/013-new-theme-and-search/","tags":null,"title":"New Theme and Search"},{"categories":null,"contents":"\nLooking to add a simple, single, new page to this site? Have a look at this content in the site's `./content/add-a-simple-page.md` file. Pay particular attention to the front matter where the type and layout are declared, as well as a list of aliases::\naliases: - help - add-this-page - add-a-page-to-the-site type: page layout: single But note that all of these aliases re-direct to the canonical URL which takes its name, ./add-a-simple-page, from the name of this markdown document, ./content/add-a-simple-page.md.\n","permalink":"https://static.grinnell.edu/dlad-blog/add-a-simple-page/","tags":null,"title":"Help!"},{"categories":null,"contents":"I am DevOps and I love Hugo\u0026hellip; in spite of the fact that the logo colors clash with my blog theme. \u0026#x1f604; More About Me What more do you need to know? Ok, so my official title at Grinnell College is actually Digital Library Applications Developer.\n","permalink":"https://static.grinnell.edu/dlad-blog/about/","tags":null,"title":"About Me"},{"categories":null,"contents":"I realized today that I previously documented how to \u0026ldquo;begin\u0026rdquo; a blog like this using Docksal to assist, but I failed to remind myself how to make structural/programming changes to it now that it is well-established. Since I\u0026rsquo;d like to add BleveSearch to this blog, and similar sites, I need to make some \u0026ldquo;structural\u0026rdquo; changes, and I want to do so locally before pushing them to production.\nThe process of making updates like this is basically:\nOpen a local terminal and navigate to this project. In my case that means cd ~/Projects/blogs-McFateM. If there\u0026rsquo;s no ./themes folder here I need to add one and populate it like so: mkdir themes \u0026amp;\u0026amp; cd $_; git clone https://github.com/digitalcraftsman/hugo-minimalist-theme.git. Now, do a fin up to get Docksal restarted. This will provide a local address where you can access the project in a browser, but it WILL NOT WORK! So do fin develop to get it re-started properly in our Hugo environment. Just remember that your terminal/shell is running the local development copy, so you have to execute a CTRL-c in the terminal to terminate it. And that’s a wrap. Until next time…\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/012-developing-this-blog/","tags":null,"title":"Developing This Blog"},{"categories":null,"contents":"Yes, yes you can!\nIt\u0026rsquo;s not \u0026ldquo;done\u0026rdquo; yet (is anything ever really done?) but you can now \u0026lsquo;search\u0026rsquo; the content, titles, and tags of this blog! For now just visit the new /search page and enter the term(s) you would like to look for. I\u0026rsquo;ll explain this better once it is \u0026ldquo;done\u0026rdquo;, I hope.\nThis wonderful addition to the site comes to you compliments of this awesome gist, with a little hacking by your\u0026rsquo;s truly.\nThere\u0026rsquo;s just one problem, perhaps summarized in my comment on the gist posted moments ago. The original comment says:\nI think search is a great addition to Hugo! But I had to butcher things a bit to get this working with my theme and config. But it does work, with one exception, I get only one match returned in my results, apparently because of this JS error thrown in the first match:\nTypeError: $(...).mark is not a function The offending code looks like this:\nfunction populateResults(result){ $.each(result,function(key,value){ ... $.each(snippetHighlights,function(snipkey,snipvalue){ $(\u0026#34;#summary-\u0026#34;+key).mark(snipvalue); // $(\u0026#34;#summary-\u0026#34;+key); }); }); } This seems to be telling me that my returned Objects, namely matches, did NOT inherit the mark function. Sound right? Note that if I comment out the .mark reference (see code snippet above) then I get ALL my results, but of course with nothing highlighted.\nSo, if I\u0026rsquo;m right about this, can anyone tell me how to define or attach (not sure what the right Javascript concept is) mark as a function of each returned object?\nBeing a relative Hugo noob, and a total Javascript idiot (there, I said it) I am wondering if there\u0026rsquo;s an easy way to fix this, because I understand what .mark is intended to do, and I\u0026rsquo;d really love to have that feature. Thanks in advance!\nAnd that\u0026rsquo;s a wrap, well, at least until I find a solution. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/011-search-me/","tags":null,"title":"Search Me?"},{"categories":null,"contents":"So I have my Atom config stored in a Github repo so that I can easily keep Atom in-sync between the various platforms I use here and at home. Today I added the atom-shell-commands package to my Atom config and have configured it with a “command” that takes care of updating my blog when I add a new post (like this one).\nThe command configuration in my .atom/config.cson looks like this:\n\u0026#34;atom-shell-commands\u0026#34;: commands: [ { name: \u0026#34;Update My Blog\u0026#34; command: \u0026#34;./update-blog.sh\u0026#34; options: cwd: \u0026#34;{ProjectDir}\u0026#34; keymap: \u0026#39;ctrl-2\u0026#39; save: \u0026#34;True\u0026#34; } ] The ./update-blog.sh script referenced in the configuration contains:\n#!/bin/bash cd ~/Projects/blogs-McFateM docker image build -t blog-update . docker login docker tag blog-update mcfatem/blogs-mcfatem:latest docker push mcfatem/blogs-mcfatem:latest If you’re reading this from my blog site then it must be working because I used the new command to push this content to the site!\nAnd that’s a wrap. Until next time…\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/010-updating-this-blog-with-atom/","tags":null,"title":"Updating This Blog with Atom"},{"categories":null,"contents":"This post is as much about adding an image to a \u0026ldquo;live\u0026rdquo; Markdown document (this blog post), as it is about my docker-bootstrap workflow.\nThe workflow is perhaps best described and summarized in a diagram I recently created, and exported to a PDF. It should be self-explanatory, and with any luck you can see it below\u0026hellip;\nPosting an \u0026ldquo;inline\u0026rdquo; image into a Markdown document is pretty easy using syntax like this:\n![alt text](https://github.com/adam-p/markdown-here/raw/master/src/common/images/icon48.png \u0026#34;Logo Title Text 1\u0026#34;) But, does that work if the \u0026ldquo;image\u0026rdquo; is a PDF? Let\u0026rsquo;s see\u0026hellip; nope, not directly.\nSo the trick appears to be coding the \u0026ldquo;image\u0026rdquo; in raw HTML, with a download option for browsers that don\u0026rsquo;t support PDF display, like so:\n\u0026lt;object data=\u0026#34;https://github.com/McFateM/docker-bootstrap/raw/master/docker-bootstrap-Diagram.pdf\u0026#34; type=\u0026#34;application/pdf\u0026#34; width=\u0026#34;1000px\u0026#34; height=\u0026#34;700px\u0026#34;\u0026gt; \u0026lt;embed src=\u0026#34;https://github.com/McFateM/docker-bootstrap/raw/master/docker-bootstrap-Diagram.pdf\u0026#34;\u0026gt; \u0026lt;p\u0026gt;This browser does not support PDFs. Please download the PDF to view it: \u0026lt;a href=\u0026#34;https://github.com/McFateM/docker-bootstrap/raw/master/docker-bootstrap-Diagram.pdf\u0026#34;\u0026gt;Download PDF\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt; \u0026lt;/embed\u0026gt; \u0026lt;/object\u0026gt; This browser does not support PDFs. Please download the PDF to view it: Download PDF.\nBut in the case of a Github hosted PDF, that also does not work as I get a Blocked by Content Security Policy error for the PDF portion of the page, or at best, my browser is unable to display the PDF so I see the line/paragraph above this one.\nSo what next? I reverted back to Markdown syntax referencing a .png copy of the diagram instead of the PDF:\nThat appears to work nicely! Two things to note\u0026hellip;\nBe sure you reference the \u0026ldquo;raw\u0026rdquo; copy of the diagram from Github, not the \u0026ldquo;blob\u0026rdquo; that\u0026rsquo;s displayed on the repo\u0026rsquo;s Github page. I used draw.io to create and export this diagram. When I export it as a .png I have to be sure to include a 20 pixel \u0026ldquo;border\u0026rdquo;, otherwise my image has no margins and looks really bad in a browser with a dark page background. And that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/008-docker-bootstrap-workflow/","tags":null,"title":"docker-bootstrap Workflow"},{"categories":null,"contents":"This is a subject that\u0026rsquo;s grabbed my attention recently here at Grinnell College. Specifically, I wanted a way to run my own SSL vulnerability scans of servers inside the campus firewall, something that outside agents could not do effectively. About a month ago I came upon a tool for this task, testssl.sh, and I\u0026rsquo;ve installed it on my Docker staging server, DGDockerX.\nThe tool resides in the islandora user\u0026rsquo;s home directory on DGDockerX and I\u0026rsquo;m able to run it from a terminal open to that node like so:\ncd ~/testssl.sh ./testssl.sh --help As you might imagine, running the application with a --help flag produces a listing of all available commands.\nTypically I\u0026rsquo;ll do something like: ./testssl.sh static.grinnell.edu\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/009-testssl-on-dgdockerx/","tags":null,"title":"Testing SSL Vulnerabilities"},{"categories":null,"contents":"Had a conversation about lots of topics this afternoon and bumped into bleve, along the way, including a discussion about adding search capability to a Hugo site. bleeeve is based on the Go programming language, as is Hugo, so I hope implementation is super simple.\nI think in the near future I\u0026rsquo;ll have a look at adding such a feature to this blog, and perhaps to the Visualizing Abolition and Freedom site too!\nIn conversation we also bumped into Lunr JS and I might consider attempting to implement it instead. But Lunr is all Javascript and I\u0026rsquo;d rather steer clear of that if at all possible.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/006-implementing-bleve-search/","tags":null,"title":"Bleve Search - Note to Self"},{"categories":null,"contents":"Most of the servers I deploy to and manage here at Grinnell College are now \u0026ldquo;Dockerized\u0026rdquo;, and all of those use Traefik to manage traffic, of course. Before a web app or server can be opened for access to the world here, it has to pass a vulnerability scan, and I\u0026rsquo;m not privy to the specifics of that scan. However, I do know that \u0026ldquo;weak cipher suites\u0026rdquo; are a common source of failure among my newest servers. It took a couple of weeks of searching, and trial/error solution attempts to identify the nature and specific source of these weaknesses, and to eradicate them. In my case Traefik was the \u0026ldquo;source\u0026rdquo; and the solution was/is to add the following configuration in the applicable docker-compose.yml files, or docker run\u0026hellip; command:\n--entrypoints=\u0026#34;Name:http Address::80 Redirect.EntryPoint:https\u0026#34; \\ --entryPoints=\u0026#34;Name:https Address::443 TLS TLS.MinVersion:VersionTLS12 TLS.CipherSuites:TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\u0026#34; \\ --defaultentrypoints=\u0026#34;http,https\u0026#34; In a traefik.toml file the syntax should look something like this in the \u0026ldquo;[entryPoints.https.tls]\u0026rdquo; section:\n[entryPoints.https.tls] minVersion = \u0026#34;VersionTLS12\u0026#34; cipherSuites = [ \u0026#34;TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384\u0026#34;, \u0026#34;TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384\u0026#34;, \u0026#34;TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\u0026#34;, \u0026#34;TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305\u0026#34;, \u0026#34;TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256\u0026#34;, \u0026#34;TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256\u0026#34;, \u0026#34;TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256\u0026#34;, \u0026#34;TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\u0026#34; ] In this code snippet, pulled from https://github.com/McFateM/docker-bootstrap/blob/master/files/docker-compose.yml, the second --entryPoints line holds the key. That line specifies a TLS.MinVersion that excludes most of the older, weak default ciphers. It also overrides the default suites with a short list of stronger suites.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/005-removing-traefik-weak-ciphers/","tags":["Docker","Traefik","configuration","weak ciphers"],"title":"Removing Traefik's Weak Cipher Suites"},{"categories":null,"contents":"One of the really cool things I like about the workflow documented in Juan Treminio\u0026rsquo;s blog post is the ability to setup auto-build in Docker Hub. Unfortunately, that comes at a cost. Docker Hub\u0026rsquo;s \u0026lsquo;free\u0026rsquo; account option will support only one parallel auto-build, so if you have more than one project you\u0026rsquo;d like to auto-build at Docker Hub you\u0026rsquo;ll need to pay for an account. The current cost is, I think, $7/month for an account that will handle up to 5 parallel auto-build projects. That\u0026rsquo;s not horrible, but for now I just have this one project with a few more that are on-the-horizon. So, in the case of this blog I\u0026rsquo;m going to bypass auto-build and just document the process I\u0026rsquo;ll use to update and push this blog to production. I touched on it in my previous post and pertinent portions of that post are reproduced here\u0026hellip;\nI created a new Docker Hub repository and pushed the image created in Juan\u0026rsquo;s Manual deployment process steps to that repository. I started by creating a new Docker Hub repository with the same name as my project, blogs-McFateM. An important note: Docker Hub will not accept uppercase letters in repository names, so as you can see above, I converted all uppercase to lowercase. The result of this step, for me, was an empty https://hub.docker.com/r/mcfatem/blogs-mcfatem/.\nThe remaining steps in my \u0026lsquo;emergency-push\u0026rsquo; process were all run from my local machine, with a terminal open to my project directory. The necessary commands were:\ndocker image build -t hugo-test . # \u0026lt;-- executed from my project directory, builds a new up-to-date image docker login docker tag hugo-test mcfatem/blogs-mcfatem:latest docker push mcfatem/blogs-mcfatem:latest A little explanation of the commands\u0026hellip;\ndocker login - Requests your Docker Hub account credentials so that your terminal session can communicate with Docker Hub.\ndocker tag... - Essentially renames your local image, the one created previously as hugo-test, to match your account and project name. In my case the account is mcfatem, my Docker Hub username, and the project is blogs-McFateM.\ndocker push... - This command pushes the newly renamed image to your Docker Hub repository, and tags the image as :latest. This last part, the \u0026lsquo;latest\u0026rsquo; tag, is important because early on we set Traefik up on our production server to look for images tagged as \u0026lsquo;latest\u0026rsquo;. If you give your image a different tag, it won\u0026rsquo;t work properly.\nAssuming all of the above worked properly, I should now have a fresh Docker \u0026lsquo;image\u0026rsquo; of this blog tagged as :latest in Docker Hub and waiting to be deployed. What\u0026rsquo;s still hella cool about this whole process is that Watchtower is still watching and is NOT dependent upon auto-build. So, within a few minutes of my docker push... command my live blog is automagically updated!\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/004-bypassing-auto-build/","tags":null,"title":"Bypassing Docker Hub Auto-Build"},{"categories":null,"contents":"Juan Treminio\u0026rsquo;s blog post does a nice job of covering the steps necessary to engage Watchtower, GitHub, and an automated build configuration in Docker Hub. The entire process can be used to push your initial Hugo project to production, watch for changes in your GitHub project repo, compile the changes, build a new Docker image, and automatically push it to production. Like I said, it\u0026rsquo;s an awesome workflow, and there\u0026rsquo;s no need to repeat much of it here. However, I will provide some insight into issues we encountered, and the solutions we employed.\nPlease open Juan\u0026rsquo;s blog post and focus on the section that begins like so:\nDocker Hub Automated Builds After you have played around a bit with Hugo, commit any changes you have made and push to a public repo on Github.\nWork your way through the Docker Hub Automated Builds section, and if you run into problems return here and have a look at the last section of this post.\nTroubleshooting - My Experience I honestly had lots of problems creating an automated build of this blog\u0026hellip;\nI was also under a time-crunch to get this blog into production so I found a relatively simple way to do so without engaging automatic builds; I created a new Docker Hub repository and pushed the image created in Juan\u0026rsquo;s Manual deployment process steps to that repository. I started by creating a new Docker Hub repository with the same name as my project, blogs-McFateM. An important note: Docker Hub will not accept uppercase letters in repository names, so as you can see above, I converted all uppercase to lowercase. The result of this step, for me, was an empty https://hub.docker.com/r/mcfatem/blogs-mcfatem/.\nThe remaining steps in my \u0026lsquo;emergency-push\u0026rsquo; process were all run from my local machine, with a terminal open to my project directory. The necessary commands were:\ndocker login docker tag hugo-test mcfatem/blogs-mcfatem:latest docker push mcfatem/blogs-mcfatem:latest A little explanation of the commands\u0026hellip;\ndocker login - Requests your Docker Hub account credentials so that your terminal session can communicate with Docker Hub.\ndocker tag... - Essentially renames your local image, the one created previously as hugo-test, to match your account and project name. In my case the account is mcfatem, my Docker Hub username, and the project is blogs-McFateM.\ndocker push... - This command pushes the newly renamed image to your Docker Hub repository, and tags the image as :latest. This last part, the \u0026lsquo;latest\u0026rsquo; tag, is important because early on we set Traefik up on our production server to look for images tagged as \u0026lsquo;latest\u0026rsquo;. If you give your image a different tag, it won\u0026rsquo;t work properly.\nIf you\u0026rsquo;ve successfully completed the automated build setup, or my \u0026lsquo;emergency-push\u0026rsquo; process, then the next step is to launch your project in production.\nLaunching Your Project You now have a working Docker image resting comfortably in Docker Hub. This final step will launch your project as a container on your production host. Juan\u0026rsquo;s section covers this nicely. It begins like so:\nStarting your blog Once the first build is finished on the Docker Hub we can create the initial container for our blog on our server.\nSSH into your server and run the following:\nNAME=jtreminio_com HOST=jtreminio.com IMAGE=\u0026#34;jtreminio/jtreminio.com\u0026#34; docker container run -d --name ${NAME} \\ --label traefik.backend=${NAME} \\ --label traefik.docker.network=traefik_webgateway \\ --label traefik.frontend.rule=Host:${HOST} \\ --label traefik.port=80 \\ --label com.centurylinklabs.watchtower.enable=true \\ --network traefik_webgateway \\ --restart always \\ ${IMAGE} Make sure to change NAME, HOST and IMAGE to your own information!\nFor this blog, I saved the applicable commands in this gist where there is one notable change, in the traefik.frontend.rule label. Unlike Juan\u0026rsquo;s command set, my label specifies both a Host and a PathPrefixStrip argument. The PathPrefixStrip:/blogs/McFateM tells Traefik to listen for a \u0026lsquo;complete\u0026rsquo; address with a path prefix, so https://static.grinnell.edu/blogs/McFateM. The rule sends all such requests to the blogs-mcfatem \u0026lsquo;backend\u0026rsquo; container, and strips the path prefix, /blogs/McFateM, off before doing so.\nAnd that\u0026rsquo;s a wrap. Until next time\u0026hellip;\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/003-pushing-to-production/","tags":null,"title":"Pushing This Blog to Production"},{"categories":null,"contents":"In this post I will attempt to chronicle the steps my associates and I took to complete the configuration of static.grinnell.edu, and to eventually create this blog following Juan Treminio\u0026rsquo;s lead. Small portions of Juan\u0026rsquo;s blog post are reproduced here, with permission, so that you can follow along in his work. Those passages appear with a colored background like so:\n\u0026hellip;I will walk you through the complete process of setting up a static website that you can deploy new versions with a simple git push.\nPushing to your repo will trigger an automated build process that will generate minified HTML/CSS/JS files, package them in an Nginx image, add a new image to Docker Hub, deploy a new container on your host and automatically generate and maintain free SSL certificates using Let\u0026rsquo;s Encrypt\u0026hellip;\nThe only prerequisite is you have Docker installed on your local machine. All other dependencies will be in Docker containers.\nTechnologies and Services We\u0026rsquo;ve added one tool, Docksal, to Juan\u0026rsquo;s original workflow, so the complete list of technologies and services now includes:\nDocksal for creating and managing a quick-and-easy local environment for development and testing, Hugo for static site generator, Ansible for configuring the server, Docker for containers, Traefik for routing traffic to correct container, and automatic SSL certificates, Watchtower for keeping latest Docker image running on your site. Let\u0026rsquo;s Encrypt for free, automated SSL certificate. Production Server Configuration - A Simple One-Time Process If you have root access to a networked Linux server it can be configured as a production host with a script that Juan has provided.\nI have created a simple Ansible-based configuration that\ninstalls Docker, opens ports 80, 443 and 8080 on firewall (optional), adds Traefik and configures Let\u0026rsquo;s Encrypt support, creates a Watchtower container. It\u0026rsquo;s important to note that Juan\u0026rsquo;s ./init script is installed \u0026ldquo;locally\u0026rdquo; on your development host, presumably a desktop or laptop workstation, and it makes changes \u0026lsquo;remotely\u0026rsquo; to your production server. To install ./init just follow the instruction provided in his docker-bootstrap project.\nI had a sudo-enabled account, but not root access, on my production server so I copied the public SSH key from my laptop to the server\u0026rsquo;s root account so that ./init could do it\u0026rsquo;s thing. It worked wonderfully.\nJuan\u0026rsquo;s blog post explains the individual parts of the process including a sample of the output you should see if things go well. The process is also idempotent, so you can run it more than once with no ill-effects, and it will only do what\u0026rsquo;s required to get your server configured. A little trial-and-error is all it took for me to get it done, and the errors were never catastrophic nor the trials too stressful.\nNext Up\u0026hellip; Docksal This is the point where I suggest we diverge from Juan\u0026rsquo;s workflow, just for a little while. Install Docksal on your local machine using the instructions provided at https://docksal.io/. While you are there, visit the \u0026lsquo;docs\u0026rsquo; pages and take a quick tour of the documentation so you get a sense of what Docksal is, and how to use it. Pay particular attention to the section dedicated to \u0026lsquo;fin\u0026rsquo;, the command you\u0026rsquo;ll use to do nearly everything. If you\u0026rsquo;re like me, you won\u0026rsquo;t regret this little side-trip.\nOnce Docksal is installed you will find a new, hidden .docksal directory in your \u0026lsquo;home\u0026rsquo; directory; this is where Docksal lives on your machine.\nNow open a local terminal and navigate within to any directory of your choice. I did this expecting to eventually create a group of static web sites destined for Grinnell\u0026rsquo;s static server, so in my case:\ncd ~ mkdir static-sites cd static-sites Next, invoke the Docksal fin command in your terminal. fin with no trailing arguments should produce a \u0026lsquo;help\u0026rsquo; screen that begins something like this:\nDocksal control cli utility v1.79.4 Usage: fin \u0026lt;command\u0026gt; Management Commands: db \u0026lt;command\u0026gt; Manage databases (fin help db) project \u0026lt;command\u0026gt; Manage project(s) (fin help project) ssh-key \u0026lt;command\u0026gt; Manage SSH keys (fin help ssh-key) system \u0026lt;command\u0026gt; Manage Docksal (fin help system) Commands: bash [service] Open shell into service\u0026#39;s container. Defaults to cli logs [service] Show service logs (e.g., Apache logs, MySQL logs) and Unison logs (fin help logs) To create your Hugo project using Docksal enter the command: fin project create. The terminal will prompt you to enter a name for your project. I choose blogs-McFateM because this blog is intended to be one among many destined for the static server. Please be sure to remember your project name as we will refer to it often.\nNext, Docksal will present you with a list of zero-configuration project types and it prompts you to select one. Choose 11. Hugo by entering the number eleven and hitting return. Nothing has actually happened because Docksal gives you an indication of what\u0026rsquo;s about to happen should you choose to proceed. Enter y to proceed or n to abort.\nIf you cleared Docksal to proceed it will spin up a set of containers including:\na command line interface (CLI) service to respond to fin commands, an Apache web service, a virtual host (v-host) proxy service, a DNS service to resolve web addresses, and an SSH-agent to help manage permissions and access to your project. Note that if the creation of the DNS and/or SSH-agent services needs elevated privileges you may be prompted by the process to enter your local user\u0026rsquo;s password to authorize them.\nThe creation of your new Hugo project also created a new directory with the name you specified, and with its own .docksal subdirectory. In my case the ~/static-sites/blogs-McFateM directory was created along with ~/static-sites/blogs-McFateM/.docksal. This new .docksal subdirectory includes definitions for some new fin commands that are specific to Hugo.\nIf all is well, the output of your fin project create command should end with something like this:\nCompiling a static site... | EN +------------------+----+ Pages | 10 Paginator pages | 0 Non-page files | 0 Static files | 3 Processed images | 0 Aliases | 1 Sitemaps | 1 Cleaned | 0 Total in 301 ms Done! Open http://static.blogs-McFateM.docksal To develop a Hugo site with real-time updates use fin develop (see README.md for details) Read more about Hugo on https://gohugo.io/getting-started/quick-start/ There\u0026rsquo;s other useful information included in the output of fin project create so I encourage you to look back at the details.\nfin commands always act relative to your current working directory. So if you navigate within your terminal to your new project name folder, the equivalent in my case of cd ~/static-sites/blogs-McFateM, you can launch Hugo-specific \u0026lsquo;custom\u0026rsquo; commands. Navigate to your new project directory and enter fin with no arguments. Once again you\u0026rsquo;re presented with a \u0026lsquo;help\u0026rsquo; screen, but this time the output ends with a few \u0026lsquo;custom\u0026rsquo; commands. They include:\nCustom commands: compile Compile a static site from Hugo sources develop Start Hugo server for real-time development hugo Run Hugo commands \u0026lt;accepts params\u0026gt; init Initialize demo Hugo site fin develop is the command you will use most. From your project\u0026rsquo;s directory entering fin develop is all it takes to compile your site (Hugo generates static content into a public subdirectory during this process), serve it via a new local DNS entry, and monitor content or configuration changes you make to your site. fin develop will automatically compile any change you make on-the-fly.\nWhen you run fin develop you should see something like this:\nMarks-MacBook-Air:blogs-McFateM mark$ fin develop Server will be available at: http://blogs-McFateM.docksal | EN +------------------+----+ Pages | 22 Paginator pages | 0 Non-page files | 0 Static files | 20 Processed images | 0 Aliases | 6 Sitemaps | 1 Cleaned | 0 Total in 176 ms Watching for changes in /var/www/{content,data,layouts,static,themes} Watching for config changes in /var/www/config.toml Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://blogs-McFateM.docksal/ (bind address 0.0.0.0) Press Ctrl+C to stop Change detected, rebuilding site 2018-11-18 20:43:48.312 +0000 Source changed \u0026#34;/var/www/content/post/a-blog-is-born.md\u0026#34;: WRITE Total in 29 ms Note that the site address created by fin project create and fin develop are different since the former prepends a subdomain of static to the project name. As long as the container is running you can use either address, but I recommend using the later address created by fin develop since changes are automatically compiled in that environment, as mentioned below.\nWhile fin develop is running (note that you must enter Ctrl-C in your terminal to stop it) you can open a tab to the site\u0026rsquo;s address (it appears in your output) in any web browser on your local machine and your site should appear. If you make a change to the site\u0026rsquo;s content or configuration, and save the change, fin develop should automatically compile the change for you. Having made a change just return to your site\u0026rsquo;s tab in your browser and click \u0026lsquo;refresh\u0026rsquo;; your change should appear.\nUsing these simple steps you can quickly and easily configure, change, test and repeat! When you want to suspend your work just return to your terminal, enter Ctrl-C if fin develop is still running, then enter fin stop. The fin stop command will terminate the containers associated with your project, but it won\u0026rsquo;t remove them.\nWhen you want to resume work on your project just open a terminal, navigate back to your project directory, and enter fin up. This will rebuild, if necessary, and restart your containers in short order. Note that fin up will report a local URL that is supposed to open your site; however, that address may not work properly. To get back into development just follow-up, pun intended, with a fin develop command and visit the address that it reports. Also, don\u0026rsquo;t forget to refresh your browser after each saved change to your content or configuration.\nBuild and Test Locally Docksal is great for development, but in this case we need something closer to \u0026lsquo;reality\u0026rsquo; to do some \u0026lsquo;real\u0026rsquo; testing before pushing to production. The process that Juan outlines will ultimately build a lightweight Docker \u0026lsquo;multi-stage\u0026rsquo; image that includes Hugo, Nginx, and your Hugo project. That Docker image is really what we need to test, and it\u0026rsquo;s not hard to do.\nI encourage you to look at Juan\u0026rsquo;s blog post beginning with this section:\nManual deployment process steps With a single command Hugo takes all your HTML/Markdown content and generates static files in /public\u0026hellip;\nMake sure you run these commands on your local machine, in a terminal opened to your project directory:\ndocker image build -t hugo-test .\ndocker image ls\ndocker container run \u0026ndash;rm -it -p 8080:80 hugo-test\nThese commands should produce an image named/tagged hugo-test, and run that image; one that should be identical to what you\u0026rsquo;ll eventually push to production. While the container is running you should be able to visit your project at http://localhost:8080.\nTroubleshooting - My Experience If the docker image build -t hugo-test . command fails to build an image, as my first attempt did, be sure to look at your config.toml file carefully. Initially, my config.toml file didn\u0026rsquo;t contain any theme key/value pair, presumably because it came from a \u0026lsquo;sample\u0026rsquo; project that intentionally left the theme definition up to the user. The docker image build command includes a step that will \u0026lsquo;minify\u0026rsquo; many of the project\u0026rsquo;s parts to reduce overall image size. If you don\u0026rsquo;t define a theme in your config.toml file minify won\u0026rsquo;t work.\nYou may find that setting the right baseurl or baseURL key/value pair in config.toml can be tricky. I find myself changing this value depending on the environment, local or production, that I\u0026rsquo;m targeting. As I understand it, the proper form for the baseurl value depends on your host, obviously, and your theme. This blog is designed to run from a \u0026lsquo;path\u0026rsquo;, that is to say its address consists of a host (static.grinnell.edu) with a trailing /path like so: https://static.grinnell.edu/blogs/McFateM. I found that specifying just the /path with NO leading slash in baseurl did the trick, like so: baseurl = blogs/McFateM. This form seemed to work well across all hosts\u0026hellip; my local Docksal development instance, the local image created earlier in this section, and in production too. Your mileage may vary. If your project loads as simple text, with no theme, or if your front page works but others do not, then your baseurl could be to blame.\nFinally, if you can\u0026rsquo;t spin up your local Docker image at http://localhost:8080, and you see Docker errors about port conflicts, then something (Nginx servers are notorious for this) on your local machine is probably already listening to port 8080. Rather than trying to find and fix the source of the problem, simply change the port mapping in your docker container run... command to something like this: docker container run --rm -it -p 8081:80 hugo-test. If that\u0026rsquo;s successful you should be able to visit your project site at http://localhost:8081.\nAnother Reason to Build/Test Locally In my next post we\u0026rsquo;ll return largely to Juan Treminio\u0026rsquo;s post to push this blog to production (Yay!) with the help of Docker Hub and automated builds. However, when I tried setting up an automated build I ran into problems, and I\u0026rsquo;ll discuss them in the upcoming post. Fortunately, I found that I could push to Docker Hub the hugo-test image created in this section, and bypass the automated build in order to help work out the issues. More on that in my next post.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/002-building-this-blog/","tags":null,"title":"Building this Blog with Hugo, Docker, Docksal, and More"},{"categories":null,"contents":"Have you ever wondered how a blog is born? The story behind this blog begins with my interest in stepping back from the CMS world, primarily Drupal, to discover the joys of static site generation. The journey begins in earnest at the 2016 DLF Forum: Milwaukee on the eve of the United States\u0026rsquo; 2016 national election, when all the buzz that wasn\u0026rsquo;t political, was about building static web sites, and Jekyll.\nA few weeks after the DLF Forum this server was born, thanks to the my colleagues in the Grinnell College (GC) Libraries, and Grinnell\u0026rsquo;s Information Technology Services (ITS) department. JekyllDev was its name, and Jelkyll development was its intended purpose. Life and work quickly got in the way of interests, as they are apt to do, and JekyllDev subsequently sat idle for nearly 2 years. During that span my work offered opportunities to learn about \u0026lsquo;DevOps\u0026rsquo; technologies like Ansible, Vagrant, Docker, and ultimately Docksal.\nI was introduced to Docksal at a Drupal conference in the Fall of 2018, and immediately looked for ways to inform my local development work with it. About the same time there was renewed interest in creation of a static site associated with a project destined to appear in the Humanities and Social Studies Complex, the HSSC, at Grinnell College. I wondered if Docksal could help me, and others at GC, with development of a Jekyll site? It could, but along the way I discovered that Docksal already had a pre-built \u0026rsquo;template\u0026rsquo; for spinning up development sites in Hugo. So I started to play with that\u0026hellip;Hugo sites created using Docksal. It\u0026rsquo;s an absolutely awesome development workflow!\nUsing Docksal I was able to quickly spin up local copies of my work, including the blog you are reading now. But my workflow for pushing content to production was still lacking something. I wanted an efficient, Docker-based deployment strategy that would accommodate collaborative development and provide quick, automated builds. Fortunately, I stumbled upon the tremendous work of Juan Treminio, specifically his blog post titled Setting Up a Static Site with Hugo and Push to Deploy. It is exactly what I was looking for. ITS was kind enough to rename JekyllDev to something a bit more generic, namely static, and this server, static.grinnell.edu is the result.\nMy next post will attempt to chronicle the steps my associates and I took to complete the configuration of static.grinnell.edu, and ultimately to create this blog following Juan Treminio\u0026rsquo;s lead.\n","permalink":"https://static.grinnell.edu/dlad-blog/posts/001-a-blog-is-born/","tags":["docker","docksal","lets-encrypt","ansible","hugo"],"title":"A Blog is Born"},{"categories":null,"contents":"","permalink":"https://static.grinnell.edu/dlad-blog/posts/_index/","tags":null,"title":"Posts"},{"categories":null,"contents":"","permalink":"https://static.grinnell.edu/dlad-blog/search/","tags":null,"title":"Site Search"}]